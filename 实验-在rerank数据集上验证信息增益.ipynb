{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import jsonlines\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "from core.models.entailment import EntailmentDeberta\n",
    "from core.data.data_utils import load_ds_from_json\n",
    "from rank_eval import eval_beir_rank_result\n",
    "\n",
    "def load_pickle_file(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def save_pickle_file(file_path, data):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_scores = load_pickle_file('output/rerank/entropy_scores_small.pkl')\n",
    "small_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEIR_DATASET_NAMES = [\"trec-covid\", \"climate-fever\", \"dbpedia-entity\", \"fever\", \"fiqa\", \"hotpotqa\", \"msmarco\",  \"nfcorpus\", \"nq\", \"scidocs\", \"scifact\"]\n",
    "\n",
    "# scores = []\n",
    "# for dataset_name in tqdm(BEIR_DATASET_NAMES):\n",
    "#     print(dataset_name)\n",
    "#     try:\n",
    "#         scores.append({\n",
    "#             'dataset': dataset_name,\n",
    "#             'rank': small_scores[dataset_name]['rank']['ndcg']['NDCG@5'],\n",
    "#             'entropy': small_scores[dataset_name]['entropy']['ndcg']['NDCG@5'],\n",
    "#         })\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         print(f\"Error in {dataset_name}\")\n",
    "# print(f\"small_scores: {scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_data = ['Reranker', 'Avg.', 'trec-covid', 'climate-fever', 'dbpedia-entity', 'fever', 'fiqa', 'hotpotqa', 'msmarco',  'nfcorpus', 'nq', 'scidocs', 'scifact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_scores.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics = set()\n",
    "for d in small_scores.values(): # d = {'entropy': {'map': {'MAP@1': 0.00172,\n",
    "    for  v in d.values(): # v = {'map': {'MAP@1': 0.00172,\n",
    "        for k1, m in v.items(): # k1=map, m={'MAP@1': 0.00172\n",
    "            for k2 in m.keys():\n",
    "                all_metrics.add((k1, k2)) # ('map', 'MAP@1')\n",
    "all_metrics = sorted(all_metrics)\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"fiqa\", \"msmarco\", \"scifact\"\n",
    "dataset_names = [\"trec-covid\", \"climate-fever\", \"dbpedia-entity\", \"fever\", \"hotpotqa\", \"nfcorpus\", \"nq\", \"scidocs\"]\n",
    "methods = [\"rank\", \"entropy\"]\n",
    "all_metrics = [('map', 'MAP@1'), ('map', 'MAP@10'), ('map', 'MAP@3'), ('map', 'MAP@5'), ('mrr', 'MRR@1'), ('mrr', 'MRR@10'), ('mrr', 'MRR@3'), ('mrr', 'MRR@5'), ('ndcg', 'NDCG@1'), ('ndcg', 'NDCG@10'), ('ndcg', 'NDCG@3'), ('ndcg', 'NDCG@5'), ('precision', 'P@1'), ('precision', 'P@10'), ('precision', 'P@3'), ('precision', 'P@5'), ('recall', 'Recall@1'), ('recall', 'Recall@10'), ('recall', 'Recall@3'), ('recall', 'Recall@5'), ('recall_cap', 'R_cap@1'), ('recall_cap', 'R_cap@10'), ('recall_cap', 'R_cap@3'), ('recall_cap', 'R_cap@5')]\n",
    "\n",
    "# 利用numpy，将small_scores建立高维数组，[指标][方法][数据集]\n",
    "import numpy as np\n",
    "score_array = np.zeros((len(all_metrics), len(methods), len(dataset_names)))\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, method in enumerate(methods):\n",
    "        for k, (metric1, metric2) in enumerate(all_metrics):\n",
    "            try:\n",
    "                score_array[k, j, i] = small_scores[dataset_name][method][metric1][metric2]\n",
    "            except Exception as e:\n",
    "                pass\n",
    "                # print(e)\n",
    "                # print(f\"Error in {dataset_name}\")\n",
    "print(score_array.shape)\n",
    "\n",
    "# 将score_array转换为DataFrame，将方法名称和指标名称（all_metrics的第二个元素）作为行列索引，数据集这列取平均值\n",
    "import pandas as pd\n",
    "# df = pd.DataFrame(score_array.mean(axis=-1), index=all_metrics, columns=methods)\n",
    "df = pd.DataFrame(score_array.mean(axis=-1), index=[m[1] for m in all_metrics], columns=methods)\n",
    "df\n",
    "\n",
    "# 过滤掉不需要的指标，只保留@5结尾的\n",
    "df[df.index.str.endswith('@5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 示例数据\n",
    "data = {\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n",
    "    \"Score1\": [85, 92, 78, 88],\n",
    "    \"Score2\": [91, 84, 88, 90],\n",
    "    \"Score3\": [87, 89, 93, 86],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 找出每列的最大值\n",
    "max_values = df[[\"Score1\", \"Score2\", \"Score3\"]].max()\n",
    "\n",
    "# 创建一个新的 DataFrame，用于设置加粗格式\n",
    "def format_bold(val, max_val):\n",
    "    return f\"**{val}**\" if val == max_val else f\"{val}\"\n",
    "\n",
    "formatted_df = df.copy()\n",
    "for col in [\"Score1\", \"Score2\", \"Score3\"]:\n",
    "    formatted_df[col] = df[col].apply(format_bold, max_val=max_values[col])\n",
    "\n",
    "# 绘制表格\n",
    "fig, ax = plt.subplots(figsize=(6, 2))\n",
    "ax.axis(\"tight\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "# 渲染表格\n",
    "table = ax.table(\n",
    "    cellText=df.values,\n",
    "    colLabels=df.columns,\n",
    "    cellLoc=\"center\",\n",
    "    loc=\"center\",\n",
    ")\n",
    "\n",
    "# 加粗最大值的单元格\n",
    "for i, col in enumerate([\"Score1\", \"Score2\", \"Score3\"], start=1):\n",
    "    max_row = df[col].idxmax()  # 找到最大值所在行\n",
    "    cell = table[(max_row + 1, i)]  # 获取对应单元格\n",
    "    cell.set_text_props(fontweight=\"bold\", color=\"red\")  # 设置加粗和颜色\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看rerank数据详情"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取tsv文件\n",
    "import pandas as pd\n",
    "df = pd.read_csv('output/rerank/hotpotqa/rerank-small.tsv', sep='\\t', header=None, names=['qid', 'doc_id', 'score'])\n",
    "df[df['score'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_rank_dataset import load_dataset\n",
    "queries, docs, scores = load_dataset('/home/song/dataset/beir/hotpotqa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['query'] = df['qid'].apply(lambda x: queries[str(x)])\n",
    "# df['doc'] = df['doc_id'].apply(lambda x: docs[str(x)])\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "data = {}\n",
    "# # 遍历scores\n",
    "# for qid in scores:\n",
    "#     qid = str(qid)\n",
    "#     for doc_id in scores[qid]:\n",
    "#         doc_id = str(doc_id)\n",
    "#         score = scores[qid][doc_id]\n",
    "#         if score > 0:\n",
    "#             if qid not in data:\n",
    "#                 data[qid] = {\n",
    "#                     'qid': str(qid),\n",
    "#                     'query': queries[qid],\n",
    "#                     'best': [],\n",
    "#                     'docs': [],\n",
    "#                 }\n",
    "#             data[qid]['best'].append({'doc_id': doc_id, 'doc': docs[doc_id], 'score': score})\n",
    "\n",
    "def new_data(qid):\n",
    "    d = {\n",
    "        'qid': str(qid),\n",
    "        'query': queries[str(qid)],\n",
    "        'best': [],\n",
    "        'docs': [],\n",
    "    }\n",
    "    for doc_id in scores[qid]:\n",
    "        doc_id = str(doc_id)\n",
    "        score = scores[qid][doc_id]\n",
    "        if score > 0:\n",
    "            d['best'].append({'doc_id': doc_id, 'doc': docs[doc_id], 'score': score})\n",
    "    return d\n",
    "\n",
    "# 遍历df\n",
    "for i, row in df.iterrows():\n",
    "    qid = str(row['qid'])\n",
    "    doc_id = str(row['doc_id'])\n",
    "    score = row['score']\n",
    "    if qid not in data:\n",
    "        data[qid] = new_data(qid)\n",
    "    data[qid]['docs'].append({'doc_id': doc_id, 'doc': docs[doc_id], 'score': score})\n",
    "\n",
    "# 保存json文件\n",
    "with open('output/tmp/rerank-small.json', 'w') as writer:\n",
    "    json.dump(data, writer, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "for qid, d in data.items():\n",
    "    best = set([doc['doc_id'] for doc in d['best']])\n",
    "    docs1 = [doc['doc_id'] for doc in d['docs']]\n",
    "    min_index = len(docs1)\n",
    "    for i, doc in enumerate(docs1):\n",
    "        if doc in best:\n",
    "            min_index = i\n",
    "            break\n",
    "    ranks.append(min_index)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化ranks\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(ranks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取tsv文件\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/home/song/dataset/first/beir_rank/hotpotqa/rank.tsv', sep='\\t', header=None, names=['qid', 'doc_id', 'score'])\n",
    "# df[df['score'] == 0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于每个qid，保留前10行\n",
    "df = df.groupby('qid').head(10)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "data = {}\n",
    "\n",
    "def new_data(qid):\n",
    "    d = {\n",
    "        'qid': str(qid),\n",
    "        'query': queries[str(qid)],\n",
    "        'best': [],\n",
    "        'docs': [],\n",
    "    }\n",
    "    for doc_id in scores[qid]:\n",
    "        doc_id = str(doc_id)\n",
    "        score = scores[qid][doc_id]\n",
    "        if score > 0:\n",
    "            d['best'].append({'doc_id': doc_id, 'doc': docs[doc_id], 'score': score})\n",
    "    return d\n",
    "\n",
    "# 遍历df\n",
    "for i, row in df.iterrows():\n",
    "    qid = str(row['qid'])\n",
    "    doc_id = str(row['doc_id'])\n",
    "    score = row['score']\n",
    "    if qid not in data:\n",
    "        data[qid] = new_data(qid)\n",
    "    data[qid]['docs'].append({'doc_id': doc_id, 'doc': docs[doc_id], 'score': score})\n",
    "\n",
    "# 保存json文件\n",
    "with open('output/tmp/rank-small.json', 'w') as writer:\n",
    "    json.dump(data, writer, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "for qid, d in data.items():\n",
    "    best = set([doc['doc_id'] for doc in d['best']])\n",
    "    docs1 = [doc['doc_id'] for doc in d['docs']]\n",
    "    min_index = len(docs1)\n",
    "    for i, doc in enumerate(docs1):\n",
    "        if doc in best:\n",
    "            min_index = i\n",
    "            break\n",
    "    ranks.append(min_index)\n",
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化ranks\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(ranks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 读取rank数据，只取前10个，保存为tsv文件\n",
    "# dataset_names = [\"trec-covid\", \"climate-fever\", \"dbpedia-entity\", \"fever\", \"fiqa\", \"hotpotqa\", \"msmarco\",  \"nfcorpus\", \"nq\", \"scidocs\", \"scifact\"]\n",
    "# for dataset_name in tqdm(dataset_names):\n",
    "#     rank_result_path = f'/home/song/dataset/first/beir_rank/{dataset_name}/rank.tsv'\n",
    "#     df = pd.read_csv(rank_result_path, sep='\\t', header=None, names=['qid', 'doc_id', 'score'])\n",
    "#     df = df.groupby('qid').head(10)\n",
    "#     df.to_csv(f'dataset/rank/{dataset_name}/{dataset_name}-rank10.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: q1\n",
      "  map: 1.0000\n",
      "  ndcg: 1.0000\n",
      "Query: q2\n",
      "  map: 1.0000\n",
      "  ndcg: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import pytrec_eval\n",
    "\n",
    "# 定义真实的相关性（qrels）\n",
    "qrels = {\n",
    "    'q1': {'d1': 1, 'd2': 0, 'd3': 1},\n",
    "    'q2': {'d2': 1, 'd4': 1, 'd5': 0}\n",
    "}\n",
    "\n",
    "# 定义检索系统的结果（results）\n",
    "results = {\n",
    "    'q1': {'d3': 1.0, 'd1': 1.5, 'd2': 0.5},\n",
    "    'q2': {'d2': 2.0, 'd4': 1.0, 'd5': 0.5}\n",
    "}\n",
    "\n",
    "# 初始化评估器\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map', 'ndcg'})\n",
    "\n",
    "# 计算检索指标\n",
    "metrics = evaluator.evaluate(results)\n",
    "\n",
    "# 输出评估结果\n",
    "for query_id, query_metrics in metrics.items():\n",
    "    print(f\"Query: {query_id}\")\n",
    "    for metric, value in query_metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aec026b567847f5b7d2538eae29f897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1/11 [00:02<00:22,  2.21s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m, in \u001b[0;36mrun_eval\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     rank_result_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/rank/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-rank10.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     35\u001b[0m     entropy_result_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput/rerank/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/entropy-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSIZE_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tsv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 36\u001b[0m     all_scores[dataset_name] \u001b[38;5;241m=\u001b[39m \u001b[43meval_beir_rerank_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank_result_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentropy_result_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/code/frag4/rank_eval.py:40\u001b[0m, in \u001b[0;36meval_beir_rerank_result\u001b[0;34m(rank_result_path, entropy_result_path, dataset_path, dataset_name, k_values)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_beir_rerank_result\u001b[39m(rank_result_path, entropy_result_path, dataset_path, dataset_name, k_values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m10\u001b[39m]):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# print(f\"> Start evaluating rank results\")\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# print(f\"rank_result_path: {rank_result_path}\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     corpus, queries, qrels \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_scores\u001b[39m(results):\n\u001b[1;32m     43\u001b[0m         scores \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/code/frag4/rank_eval.py:29\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(dataset_path, dataset_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(dataset_path, dataset_name):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# if dataset_name == \"msmarco\": # why?\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#     corpus, queries, qrels = GenericDataLoader(data_folder=dataset_path).load(split=\"dev\")\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#     corpus, queries, qrels = GenericDataLoader(data_folder=dataset_path).load(split=\"test\")\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     corpus, queries, qrels \u001b[38;5;241m=\u001b[39m \u001b[43mGenericDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m corpus, queries, qrels\n",
      "File \u001b[0;32m~/miniconda3/envs/frag/lib/python3.11/site-packages/beir/datasets/data_loader.py:68\u001b[0m, in \u001b[0;36mGenericDataLoader.load\u001b[0;34m(self, split)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus):\n\u001b[1;32m     67\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading Corpus...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m Documents.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus), split\u001b[38;5;241m.\u001b[39mupper())\n\u001b[1;32m     70\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc Example: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus\u001b[38;5;241m.\u001b[39mvalues())[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/frag/lib/python3.11/site-packages/beir/datasets/data_loader.py:98\u001b[0m, in \u001b[0;36mGenericDataLoader._load_corpus\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_corpus\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 98\u001b[0m     num_lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fIn:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m tqdm(fIn, total\u001b[38;5;241m=\u001b[39mnum_lines):\n",
      "File \u001b[0;32m~/miniconda3/envs/frag/lib/python3.11/site-packages/beir/datasets/data_loader.py:98\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_corpus\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 98\u001b[0m     num_lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fIn:\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m tqdm(fIn, total\u001b[38;5;241m=\u001b[39mnum_lines):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 3, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>entropy</th>\n",
       "      <th>rerank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAP@5</th>\n",
       "      <td>0.267825</td>\n",
       "      <td>0.065489</td>\n",
       "      <td>0.065489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRR@5</th>\n",
       "      <td>0.576873</td>\n",
       "      <td>0.109799</td>\n",
       "      <td>0.109799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@5</th>\n",
       "      <td>0.449309</td>\n",
       "      <td>0.217816</td>\n",
       "      <td>0.217816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@5</th>\n",
       "      <td>0.276875</td>\n",
       "      <td>0.200887</td>\n",
       "      <td>0.200887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall@5</th>\n",
       "      <td>0.336066</td>\n",
       "      <td>0.146546</td>\n",
       "      <td>0.146546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_cap@5</th>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.077000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rank   entropy    rerank\n",
       "MAP@5     0.267825  0.065489  0.065489\n",
       "MRR@5     0.576873  0.109799  0.109799\n",
       "NDCG@5    0.449309  0.217816  0.217816\n",
       "P@5       0.276875  0.200887  0.200887\n",
       "Recall@5  0.336066  0.146546  0.146546\n",
       "R_cap@5   0.087000  0.077000  0.077000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import jsonlines\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "from core.models.entailment import EntailmentDeberta\n",
    "from core.data.data_utils import load_ds_from_json\n",
    "from rank_eval import eval_beir_rerank_result\n",
    "\n",
    "def load_pickle_file(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def save_pickle_file(file_path, data):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def run_eval():\n",
    "\n",
    "    BEIR_DATASET_NAMES = [\"trec-covid\", \"climate-fever\", \"dbpedia-entity\", \"fever\", \"fiqa\", \"hotpotqa\", \"msmarco\",  \"nfcorpus\", \"nq\", \"scidocs\", \"scifact\"]\n",
    "    # SIZE_NAME = \"toy\"\n",
    "    # SIZE_NAME = \"small\"\n",
    "\n",
    "    all_scores = {}\n",
    "\n",
    "    for SIZE_NAME in [\"small\"]:\n",
    "        for dataset_name in tqdm(BEIR_DATASET_NAMES):\n",
    "            try:\n",
    "                dataset_path = f'/home/song/dataset/beir/{dataset_name}'\n",
    "                rank_result_path = f'dataset/rank/{dataset_name}/{dataset_name}-rank10.tsv'\n",
    "                entropy_result_path = f'output/rerank/{dataset_name}/entropy-{SIZE_NAME}.tsv'\n",
    "                all_scores[dataset_name] = eval_beir_rerank_result(rank_result_path, entropy_result_path, dataset_path, dataset_name, k_values=[1,3,5,10])\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "        # Save all_scores\n",
    "        save_pickle_file(f\"output/rerank/entropy_scores_{SIZE_NAME}.pkl\", all_scores)\n",
    "    return all_scores\n",
    "# all_scores = load_pickle_file('output/rerank/entropy_scores_small.pkl')\n",
    "\n",
    "%time all_scores = run_eval()\n",
    "\n",
    "def calc_avg_score(all_scores, dataset_names, methods, all_metrics):\n",
    "    # 利用numpy，将all_scores建立高维数组，[指标][方法][数据集]\n",
    "    import numpy as np\n",
    "    score_array = np.zeros((len(all_metrics), len(methods), len(dataset_names)))\n",
    "    for i, dataset_name in enumerate(dataset_names):\n",
    "        for j, method in enumerate(methods):\n",
    "            for k, (metric1, metric2) in enumerate(all_metrics):\n",
    "                try:\n",
    "                    score_array[k, j, i] = all_scores[dataset_name][method][metric1][metric2]\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                    # print(e)\n",
    "                    # print(f\"Error in {dataset_name}\")\n",
    "    print(score_array.shape)\n",
    "\n",
    "    # 将score_array转换为DataFrame，将方法名称和指标名称（all_metrics的第二个元素）作为行列索引，数据集这列取平均值\n",
    "    import pandas as pd\n",
    "    # df = pd.DataFrame(score_array.mean(axis=-1), index=all_metrics, columns=methods)\n",
    "    df = pd.DataFrame(score_array.mean(axis=-1), index=[m[1] for m in all_metrics], columns=methods)\n",
    "    return df\n",
    "\n",
    "df = calc_avg_score(all_scores, dataset_names, methods, all_metrics)\n",
    "# 过滤掉不需要的指标，只保留@5结尾的\n",
    "df[df.index.str.endswith('@5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 3, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>entropy</th>\n",
       "      <th>rerank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MAP@5</th>\n",
       "      <td>0.267825</td>\n",
       "      <td>0.065489</td>\n",
       "      <td>0.065489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRR@5</th>\n",
       "      <td>0.576873</td>\n",
       "      <td>0.109799</td>\n",
       "      <td>0.109799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NDCG@5</th>\n",
       "      <td>0.449309</td>\n",
       "      <td>0.217816</td>\n",
       "      <td>0.217816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P@5</th>\n",
       "      <td>0.276875</td>\n",
       "      <td>0.200887</td>\n",
       "      <td>0.200887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall@5</th>\n",
       "      <td>0.336066</td>\n",
       "      <td>0.146546</td>\n",
       "      <td>0.146546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R_cap@5</th>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.077000</td>\n",
       "      <td>0.077000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rank   entropy    rerank\n",
       "MAP@5     0.267825  0.065489  0.065489\n",
       "MRR@5     0.576873  0.109799  0.109799\n",
       "NDCG@5    0.449309  0.217816  0.217816\n",
       "P@5       0.276875  0.200887  0.200887\n",
       "Recall@5  0.336066  0.146546  0.146546\n",
       "R_cap@5   0.087000  0.077000  0.077000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_avg_score(all_scores, dataset_names, methods, all_metrics):\n",
    "    # 利用numpy，将all_scores建立高维数组，[指标][方法][数据集]\n",
    "    import numpy as np\n",
    "    score_array = np.zeros((len(all_metrics), len(methods), len(dataset_names)))\n",
    "    for i, dataset_name in enumerate(dataset_names):\n",
    "        for j, method in enumerate(methods):\n",
    "            for k, (metric1, metric2) in enumerate(all_metrics):\n",
    "                try:\n",
    "                    score_array[k, j, i] = all_scores[dataset_name][method][metric1][metric2]\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "                    # print(e)\n",
    "                    # print(f\"Error in {dataset_name}\")\n",
    "    print(score_array.shape)\n",
    "\n",
    "    # 将score_array转换为DataFrame，将方法名称和指标名称（all_metrics的第二个元素）作为行列索引，数据集这列取平均值\n",
    "    import pandas as pd\n",
    "    # df = pd.DataFrame(score_array.mean(axis=-1), index=all_metrics, columns=methods)\n",
    "    df = pd.DataFrame(score_array.mean(axis=-1), index=[m[1] for m in all_metrics], columns=methods)\n",
    "    return df\n",
    "\n",
    "# \"fiqa\", \"msmarco\", \"scifact\"\n",
    "dataset_names = [\"trec-covid\", \"climate-fever\", \"dbpedia-entity\", \"fever\", \"hotpotqa\", \"nfcorpus\", \"nq\", \"scidocs\"]\n",
    "methods = [\"rank\", \"entropy\", \"rerank\"]\n",
    "all_metrics = [('map', 'MAP@1'), ('map', 'MAP@10'), ('map', 'MAP@3'), ('map', 'MAP@5'), ('mrr', 'MRR@1'), ('mrr', 'MRR@10'), ('mrr', 'MRR@3'), ('mrr', 'MRR@5'), ('ndcg', 'NDCG@1'), ('ndcg', 'NDCG@10'), ('ndcg', 'NDCG@3'), ('ndcg', 'NDCG@5'), ('precision', 'P@1'), ('precision', 'P@10'), ('precision', 'P@3'), ('precision', 'P@5'), ('recall', 'Recall@1'), ('recall', 'Recall@10'), ('recall', 'Recall@3'), ('recall', 'Recall@5'), ('recall_cap', 'R_cap@1'), ('recall_cap', 'R_cap@10'), ('recall_cap', 'R_cap@3'), ('recall_cap', 'R_cap@5')]\n",
    "\n",
    "# 利用numpy，将all_scores建立高维数组，[指标][方法][数据集]\n",
    "import numpy as np\n",
    "score_array = np.zeros((len(all_metrics), len(methods), len(dataset_names)))\n",
    "for i, dataset_name in enumerate(dataset_names):\n",
    "    for j, method in enumerate(methods):\n",
    "        for k, (metric1, metric2) in enumerate(all_metrics):\n",
    "            try:\n",
    "                score_array[k, j, i] = all_scores[dataset_name][method][metric1][metric2]\n",
    "            except Exception as e:\n",
    "                pass\n",
    "                # print(e)\n",
    "                # print(f\"Error in {dataset_name}\")\n",
    "print(score_array.shape)\n",
    "\n",
    "# 将score_array转换为DataFrame，将方法名称和指标名称（all_metrics的第二个元素）作为行列索引，数据集这列取平均值\n",
    "import pandas as pd\n",
    "# df = pd.DataFrame(score_array.mean(axis=-1), index=all_metrics, columns=methods)\n",
    "df = pd.DataFrame(score_array.mean(axis=-1), index=[m[1] for m in all_metrics], columns=methods)\n",
    "df\n",
    "\n",
    "# 过滤掉不需要的指标，只保留@5结尾的\n",
    "df[df.index.str.endswith('@5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
