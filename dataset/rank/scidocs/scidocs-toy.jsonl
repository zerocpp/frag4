{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "no"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-no", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "802b80852996d87dc16082b86f6e77115eb6c9a6"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-802b80852996d87dc16082b86f6e77115eb6c9a6", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "In this article, a methodology to extract Flash EEPROM memory contents is presented. Samples are first backside prepared to expose the tunnel oxide of floating gate transistors. Then, a Scanning Electron Microscope (SEM) in the so called Passive Voltage Contrast (PVC) mode allows distinguishing ‘0’ and ‘1’ bit values stored in individual memory cell. Using SEM operator-free acquisition and standard image processing technique we demonstrate the possible automating of such technique over a full memory. The presented fast, efficient and low cost technique is successfully implemented on 0.35μm technology node microcontrollers and on a 0.21μm smart card type integrated circuit. The technique is at least two orders of magnitude faster than state-of-the-art Scanning Probe Microscopy (SPM) methods. Without adequate protection an adversary could obtain the full memory array content within minutes. The technique is a first step for reverse engineering secure embedded systems."}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "a0fd813b9218813e1b020d03a3099de7677dd145"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-a0fd813b9218813e1b020d03a3099de7677dd145", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "Manipulation in micro or nanoscale with robotic manipulators under observation of electron microscopes is a widely used strategy for fabrication of nanodevices and nanoscale material property characterization. These types of manipulation systems can handle the relatively larger scale of objects. However, the complexity of manipulation increases highly for 3D manipulation. Since the manipulation system consists of multiple components including manipulator, microscope, and also some end-effector tools, a proper offline visualization of the system is necessary for operation. Therefore, we propose a web-based virtual interface between the user and the actual manipulator operated under digital microscope initially. It gives the operator 3D positional feedback from the virtual model by mapping data read during remote operation. The same interface is used for remote operation of the manipulator within the SEM chamber and a manipulation task is performed."}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "5de3ba76eeead6a6ee3295220080ee881f84bd27"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-5de3ba76eeead6a6ee3295220080ee881f84bd27", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "The objective of this paper is to propose a new homography-based approach to image-based visual tracking and servoing. The visual tracking algorithm proposed in the paper is based on a new efficient second-order minimization method. Theoretical analysis and comparative experiments with other tracking approaches show that the proposed method has a higher convergence rate than standard first-order minimization techniques. Therefore, it is well adapted to real-time robotic applications. The output of the visual tracking is a homography linking the current and the reference image of a planar target. Using the homography, a task function isomorphic to the camera pose has been designed. A new image-based control law is proposed which does not need any measure of the 3D structure of the observed target (e.g. the normal to the plane). The theoretical proof of the existence of the isomorphism between the task function and the camera pose and the theoretical proof of the stability of the control law are provided. The experimental results, obtained with a 6 d.o.f. robot, show the advantages of the proposed method with respect to the existing approaches. KEY WORDS—visual tracking, visual servoing, efficient second-order minimization, homography-based control law"}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "4ee0ad8e256523256c8d21790189388ed4beca7e"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-4ee0ad8e256523256c8d21790189388ed4beca7e", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "PRNU-based techniques guarantee a good forgery detection performance irrespective of the specific type of forgery. The presence or absence of the camera PRNU pattern is detected by a correlation test. Given the very low power of the PRNU signal, however, the correlation must be averaged over a pretty large window, reducing the algorithm's ability to reveal small forgeries. To improve resolution, we estimate correlation with a spatially adaptive filtering technique, with weights computed over a suitable pilot image. Implementation efficiency is achieved by resorting to the recently proposed guided filters. Experiments prove that the proposed filtering strategy allows for a much better detection performance in the case of small forgeries."}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "c6dac8aca55dc7326e5cb996b386db5bce4da46e"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-c6dac8aca55dc7326e5cb996b386db5bce4da46e", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "Point-of-Interest (POI) recommendation has received increasing attention in Location-based Social Networks (LBSNs). It involves user behavior analysis, movement pattern model and trajectory sequence prediction, in order to recommend personalized services to target user. Existing POI recommendation methods are confronted with three problems: (1) they only consider the location information of users' check-ins, which causes data sparsity; (2) they fail to consider the order of users' visited locations, which is valuable to reflect the interest or preference of users; (3) users cannot be recommended the suitable services when they move into the new place. To address the above issues, we propose a semantical pattern and preference-aware service mining method called SEM-PPA to make full use of the semantic information of locations for personalized POI recommendation. In SEM-PPA, we firstly propose a novel algorithm to classify the locations into different types for location identification; then we construct the user model for each user from four aspects, which are location trajectory, semantic trajectory, location popularity and user familiarity; in addition, a potential friends discovery algorithm based on movement pattern is proposed. Finally, we conduct extensive experiments to evaluate the recommendation accuracy and recommendation effectiveness on two real-life datasets from GeoLife and Beijing POI. Experimental results show that SEM-PPA can achieve better recommendation performance in particular for sparse data and recommendation accuracy in comparison with other methods."}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "343cc181987202cf4b98e61738d0b310927c1fcf"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-343cc181987202cf4b98e61738d0b310927c1fcf", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "The substrate integrated waveguide (SIW) technique makes it possible that a complete circuit including planar circuitry, transitions, and rectangular waveguides are fabricated in planar form using a standard printed circuit board or other planar processing techniques. In this paper, guided wave and modes characteristics of such an SIW periodic structure are studied in detail for the first time. A numerical multimode calibration procedure is proposed and developed with a commercial software package on the basis of a full-wave finite-element method for the accurate extraction of complex propagation constants of the SIW structure. Two different lengths of the SIW are numerically simulated under multimode excitation. By means of our proposed technique, the complex propagation constant of each SIW mode can accurately be extracted and the electromagnetic bandstop phenomena of periodic structures are also investigated. Experiments are made to validate our proposed technique. Simple design rules are provided and discussed."}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "a65476a4fadd112b64f519a6f51a71f6077ed0ae"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-a65476a4fadd112b64f519a6f51a71f6077ed0ae", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "We report a monolithic wafer-level fabrication method for a hemispherical reflector coupled light-emitting-diode (LED) array using isotropic etching of silicon. These neural stimulators collect the backside as well as the front side emission of the μ-LEDs and thus provide higher intensity, which is imperative for opsin expressions in optogenetics experiments. Aluminum was used as the reflective layer and the planarization of polymer on the reflector cavity was done using polydimethylsiloxane (PDMS). The lateral and vertical profiles of silicon etching were measured and the light intensity increase due to the reflector was investigated. It was found that the intensity increases by a minimum of 49% and maximum of 65% when coupling a reflector with the μ-LEDs."}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "d8354a6d188a9bfca01586a5467670650f3e3a8a"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-d8354a6d188a9bfca01586a5467670650f3e3a8a", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "The next generation of implantable high-power neuroprosthetic devices such as visual prostheses and brain computer interfaces are going to be powered by transcutaneous inductive power links formed between a pair of printed spiral coils (PSC) that are batch-fabricated using micromachining technology. Optimizing the power efficiency of the wireless link is imperative to minimize the size of the external energy source, heating dissipation in the tissue, and interference with other devices. Previous design methodologies for coils made of 1-D filaments are not comprehensive and accurate enough to consider all geometrical aspects of PSCs with planar 3-D conductors as well as design constraints imposed by implantable device application and fabrication technology. We have outlined the theoretical foundation of optimal power transmission efficiency in an inductive link, and combined it with semi-empirical models to predict parasitic components in PSCs. We have used this foundation to devise an iterative PSC design methodology that starts with a set of realistic design constraints and ends with the optimal PSC pair geometries. We have executed this procedure on two design examples at 1 and 5 MHz achieving power transmission efficiencies of 41.2% and 85.8%, respectively, at 10-mm spacing. All results are verified with simulations using a commercial field solver (HFSS) as well as measurements using PSCs fabricated on printed circuit boards."}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "58a64cc6a1dd8269ab19b9de271e202ab3e6de92"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-58a64cc6a1dd8269ab19b9de271e202ab3e6de92", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "We present in this paper the development of a high-resolution projection micro-stereolithography (P SL) process by using the Digital Micromirror Device (DMDTM, Texas Instruments) as a dynamic mask. This unique technology provides a parallel fabrication of complex three-dimensional (3D) microstructures used for micro electro-mechanical systems (MEMS). Based on the understanding of underlying mechanisms, a process model has been developed with all critical parameters obtained from the experimental measurement. By coupling the experimental measurement and the process model, the photon-induced curing behavior of the resin has been quantitatively studied. The role o erty of the r n d ©"}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "122374a3baf1e0efde03301226344a2d728eafc3"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-122374a3baf1e0efde03301226344a2d728eafc3", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "PURPOSE\nThe purpose of this study is to investigate the feasibility of increasing the system spatial resolution and scanning speed of Hologic Selenia Dimensions digital breast tomosynthesis (DBT) scanner by replacing the rotating mammography x-ray tube with a specially designed carbon nanotube (CNT) x-ray source array, which generates all the projection images needed for tomosynthesis reconstruction by electronically activating individual x-ray sources without any mechanical motion. The stationary digital breast tomosynthesis (s-DBT) design aims to (i) increase the system spatial resolution by eliminating image blurring due to x-ray tube motion and (ii) reduce the scanning time. Low spatial resolution and long scanning time are the two main technical limitations of current DBT technology.\n\n\nMETHODS\nA CNT x-ray source array was designed and evaluated against a set of targeted system performance parameters. Simulations were performed to determine the maximum anode heat load at the desired focal spot size and to design the electron focusing optics. Field emission current from CNT cathode was measured for an extended period of time to determine the stable life time of CNT cathode for an expected clinical operation scenario. The source array was manufactured, tested, and integrated with a Selenia scanner. An electronic control unit was developed to interface the source array with the detection system and to scan and regulate x-ray beams. The performance of the s-DBT system was evaluated using physical phantoms.\n\n\nRESULTS\nThe spatially distributed CNT x-ray source array comprised 31 individually addressable x-ray sources covering a 30 angular span with 1 pitch and an isotropic focal spot size of 0.6 mm at full width at half-maximum. Stable operation at 28 kV(peak) anode voltage and 38 mA tube current was demonstrated with extended lifetime and good source-to-source consistency. For the standard imaging protocol of 15 views over 14, 100 mAs dose, and 2 × 2 detector binning, the projection resolution along the scanning direction increased from 4.0 cycles/mm [at 10% modulation-transfer-function (MTF)] in DBT to 5.1 cycles/mm in s-DBT at magnification factor of 1.08. The improvement is more pronounced for faster scanning speeds, wider angular coverage, and smaller detector pixel sizes. The scanning speed depends on the detector, the number of views, and the imaging dose. With 240 ms detector readout time, the s-DBT system scanning time is 6.3 s for a 15-view, 100 mAs scan regardless of the angular coverage. The scanning speed can be reduced to less than 4 s when detectors become faster. Initial phantom studies showed good quality reconstructed images.\n\n\nCONCLUSIONS\nA prototype s-DBT scanner has been developed and evaluated by retrofitting the Selenia rotating gantry DBT scanner with a spatially distributed CNT x-ray source array. Preliminary results show that it improves system spatial resolution substantially by eliminating image blur due to x-ray focal spot motion. The scanner speed of s-DBT system is independent of angular coverage and can be increased with faster detector without image degration. The accelerated lifetime measurement demonstrated the long term stability of CNT x-ray source array with typical clinical operation lifetime over 3 years."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "no"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-no", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "af77547dc79c67367675e76f28c3bbf3032a9a12"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-af77547dc79c67367675e76f28c3bbf3032a9a12", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "In mobile cloud computing, mobile devices can rely on cloud computing and information storage resource to perform computationally intensive operations such as searching, data mining, and multimedia processing. In addition to providing traditional computation services, mobile cloud also enhances the operation of traditional ad hoc network by treating mobile devices as service nodes, e.g., sensing services. The sensed information, such as location coordinates, health related information, should be processed and stored in a secure fashion to protect user's privacy in the cloud. To this end, we present a new mobile cloud data processing framework through trust management and private data isolation. Finally, an implementation pilot for improving teenagers' driving safety, which is called FocusDrive, is presented to demonstrate the solution."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "f9823bc7eec44a9a6cd7b629c8f6430fe82877fd"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-f9823bc7eec44a9a6cd7b629c8f6430fe82877fd", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "Cloud computing provides convenient on-demand network access to a shared pool of configurable computing resources. The resources can be rapidly deployed with great efficiency and minimal management overhead. Cloud is an insecure computing platform from the view point of the cloud users, the system must design mechanisms that not only protect sensitive information by enabling computations with encrypted data, but also protect users from malicious behaviours by enabling the validation of the computation result. In this paper, we propose a new data encoding scheme called layered interleaving, designed for time-sensitive packet recovery in the presence of bursty loss. It is high-speed data recovery scheme with minimal loss probability and using a forward error correction scheme to handle bursty loss. The proposed approach is highly efficient in recovering the singleton losses almost immediately and from bursty data losses."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "de33698f7b2264bf7313f43d1de8c2d19e2a2f7a"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-de33698f7b2264bf7313f43d1de8c2d19e2a2f7a", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "Abstract. Mobile cloud computing has emerged aiming at assisting mobile devices in processing computationally or data intensive tasks using cloud resources. This paper presents an optimization approach for utilizing cloud services for mobile client in mobile cloud, which considers the benefit of both mobile device users and cloud datacenters. The mobile cloud service provisioning optimization is conducted in parallel under the deadline, budget and energy expenditure constraint. Mobile cloud provider runs multiple VMs to execute the jobs for mobile device users, the cloud providers want to maximize the revenue and minimize the electrical cost. The mobile device user gives the suitable payment to the cloud datacenter provider for available cloud resources for optimize the benefit. The paper proposes a distributed optimization algorithm for utilizing cloud services for mobile devices. The experiment is to test convergence of the proposed algorithm and also compare it with other related work. The experiments study the impacts of job arrival rate, deadline and mobility speeds on energy consumption ratio, execution success ratio, resource allocation efficiency and cost. The experiment shows that the proposed algorithm outperforms other related work in terms of some performance metrics such as allocation efficiency."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "4c9774c5e57a4b7535eb19f6584f75c8b9c2cdcc"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-4c9774c5e57a4b7535eb19f6584f75c8b9c2cdcc", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "Cloud computing is an emerging computing model in which resources of the computing communications are provided as services over the Internet. Privacy and security of cloud storage services are very important and become a challenge in cloud computing due to loss of control over data and its dependence on the cloud computing provider. While there is a huge amount of transferring data in cloud system, the risk of accessing data by attackers raises. Considering the problem of building a secure cloud storage service, current scheme is proposed which is based on combination of RSA and AES encryption methods to share the data among users in a secure cloud system. The proposed method allows providing difficulty for attackers as well as reducing the time of information transmission between user and cloud data storage."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "541c3ab3ce75594c403126413b9c866fa7fba57a"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-541c3ab3ce75594c403126413b9c866fa7fba57a", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "Cloud computing has paved a way for resource-constrained mobile devices to speed up their computing tasks and to expand their storage capacity. However, cloud computing is not necessary a panacea for all mobile applications. The high network latency to cloud data centers may not be ideal for delay-sensitive applications while storing everything on public clouds risks users' security and privacy. In this paper, we discuss two preliminary ideas, one for mobile application offloading and the other for mobile storage expansion, by leveraging the edge intelligence offered by fog computing to help mobile applications. Preliminary experiments conducted based on implemented prototypes show that fog computing can provide an effective and sometimes better alternative to help mobile applications."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "ec1df457a2be681227f79de3ce932fccb65ee2bb"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-ec1df457a2be681227f79de3ce932fccb65ee2bb", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "The dynamic mobility and limitations in computational power, battery resources, and memory availability are main bottlenecks in fully harnessing mobile devices as data mining platforms. Therefore, the mobile devices are augmented with cloud resources in mobile edge cloud computing (MECC) environments to seamlessly execute data mining tasks. The MECC infrastructures provide compute, network, and storage services in one-hop wireless distance from mobile devices to minimize the latency in communication as well as provide localized computations to reduce the burden on federated cloud systems. However, when and how to offload the computation is a hard problem. In this paper, we present an opportunistic computation offloading scheme to efficiently execute data mining tasks in MECC environments. The scheme provides the suitable execution mode after analyzing the amount of unprocessed data, privacy configurations, contextual information, and available on-board local resources (memory, CPU, and battery power). We develop a mobile application for online activity recognition and evaluate the proposed scheme using the event data stream of 5 million activities collected from 12 users for 15 days. The experiments show significant improvement in execution time and battery power consumption resulting in 98% data reduction."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "43d40cf9251ba497f6ea3957bfc3c189fd11d421"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-43d40cf9251ba497f6ea3957bfc3c189fd11d421", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "In modern societies, the number of mobile users has dramatically risen in recent years. In this paper, an efficient authentication scheme for distributed mobile cloud computing services is proposed. The proposed scheme provides security and convenience for mobile users to access multiple mobile cloud computing services from multiple service providers using only a single private key. The security strength of the proposed scheme is based on bilinear pairing cryptosystem and dynamic nonce generation. In addition, the scheme supports mutual authentication, key exchange, user anonymity, and user untraceability. From system implementation point of view, verification tables are not required for the trusted smart card generator (SCG) service and cloud computing service providers when adopting the proposed scheme. In consequence, this scheme reduces the usage of memory spaces on these corresponding service providers. In one mobile user authentication session, only the targeted cloud service provider needs to interact with the service requestor (user). The trusted SCG serves as the secure key distributor for distributed cloud service providers and mobile clients. In the proposed scheme, the trusted SCG service is not involved in individual user authentication process. With this design, our scheme reduces authentication processing time required by communication and computation between cloud service providers and traditional trusted third party service. Formal security proof and performance analyses are conducted to show that the scheme is both secure and efficient."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "be98ba27ffdaa99834d12a1aa9c905c7bc6848c1"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-be98ba27ffdaa99834d12a1aa9c905c7bc6848c1", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "Cloud computing is an emerging computing paradigm that enables users to store their data in a cloud server to enjoy scalable and on-demand services. Nevertheless, it also brings many security issues, since cloud service providers (CSPs) are not in the same trusted domain as users. To protect data privacy against untrusted CSPs, existing solutions apply cryptographic methods (e.g., encryption mechanisms) and provide decryption keys only to authorized users. However, sharing cloud data among authorized users at a fine-grained level is still a challenging issue, especially when dealing with dynamic user groups. In this paper, we propose a secure and efficient fine-grained access control and data sharing scheme for dynamic user groups by: 1) defining and enforcing access policies based on the attributes of the data; 2) permitting the key generation center to efficiently update user credentials for dynamic user groups; and 3) allowing some expensive computation tasks to be performed by untrusted CSPs without requiring any delegation key. Specifically, we first design an efficient revocable attribute-based encryption (ABE) scheme with the property of ciphertext delegation by exploiting and uniquely combining techniques of identity-based encryption, ABE, subset-cover framework, and ciphertext encoding mechanism. We then present a fine-grained access control and data sharing system for on-demand services with dynamic user groups in the cloud. The experimental data show that our proposed scheme is more efficient and scalable than the state-of-the-art solution."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "9a6abed922749b4680465c5200bf5aefd26306e3"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-9a6abed922749b4680465c5200bf5aefd26306e3", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "Cloud computing enables cost-effective, self-service, and elastic hosting of applications in the Cloud. Applications may be partially or completely moved to the Cloud. When hosting or moving the database layer to the Cloud, challenges such as avoidance of disclosure of critical data have to be faced. The main challenges are handling different levels of confidentiality and satisfying security and privacy requirements. We provide reusable solutions in the form of patterns."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "67ab675999e4b11904743070e295bc22476080fe"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-67ab675999e4b11904743070e295bc22476080fe", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "The inherently limited processing power and battery lifetime of mobile phones hinder the possible execution of computationally intensive applications like content-based video analysis or 3D modeling. Offloading of computationally intensive application parts from the mobile platform into a remote cloud infrastructure or nearby idle computers addresses this problem. This paper presents our Mobile Augmentation Cloud Services (MACS) middleware which enables adaptive extension of Android application execution from a mobile client into the cloud. Applications are developed by using the standard Android development pattern. The middleware does the heavy lifting of adaptive application partitioning, resource monitoring and computation offloading. These elastic mobile applications can run as usual mobile application, but they can also reach transparently remote computing resources. Two prototype applications using the MACS middleware demonstrate the benefits of the approach. The evaluation shows that applications, which involve complicated algorithms and large computations, can benefit from offloading with around 95% energy savings and significant performance gains compared to local execution only."}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "no"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-no", "question": "Bank distress in the news: Describing events through deep learning", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "048e82ac9c88c458a50cc6289662e9cb2ecb4fc9"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-048e82ac9c88c458a50cc6289662e9cb2ecb4fc9", "question": "Bank distress in the news: Describing events through deep learning", "context": "State-of-the-art event encoding approaches rely on sentence or phrase level labeling, which are both time consuming and infeasible to extend to large scale text corpora and emerging domains. Using a multiple instance learning approach, we take advantage of the fact that while labels at the sentence level are difficult to obtain, they are relatively easy to gather at the document level. This enables us to view the problems of event detection and extraction in a unified manner. Using distributed representations of text, we develop a multiple instance formulation that simultaneously classifies news articles and extracts sentences indicative of events without any engineered features. We evaluate our model in its ability to detect news articles about civil unrest events (from Spanish text) across ten Latin American countries and identify the key sentences pertaining to these events. Our model, trained without annotated sentence labels, yields performance that is competitive with selected state-of-the-art models for event detection and sentence identification. Additionally, qualitative experimental results show that the extracted event-related sentences are informative and enhance various downstream applications such as article summarization, visualization, and event encoding."}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "09f83b83fd3b0114c2c902212101152c2d2d1259"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-09f83b83fd3b0114c2c902212101152c2d2d1259", "question": "Bank distress in the news: Describing events through deep learning", "context": ""}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "8f67ff7d7a4fc72d87f82ae340dba4365b7ea664"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-8f67ff7d7a4fc72d87f82ae340dba4365b7ea664", "question": "Bank distress in the news: Describing events through deep learning", "context": "Designing appropriate features for acoustic event recognition tasks is an active field of research. Expressive features should both improve the performance of the tasks and also be interpret-able. Currently, heuristically designed features based on the domain knowledge requires tremendous effort in hand-crafting, while features extracted through deep network are difficult for human to interpret. In this work, we explore the experience guided learning method for designing acoustic features. This is a novel hybrid approach combining both domain knowledge and purely data driven feature designing. Based on the procedure of log Mel-filter banks, we design a filter bank learning layer. We concatenate this layer with a convolutional neural network (CNN) model. After training the network, the weight of the filter bank learning layer is extracted to facilitate the design of acoustic features. We smooth the trained weight of the learning layer and re-initialize it in filter bank learning layer as audio feature extractor. For the environmental sound recognition task based on the Urbansound8K dataset [1], the experience guided learning leads to a 2% accuracy improvement compared with the fixed feature extractors (the log Mel-filter bank). The shape of the new filter banks are visualized and explained to prove the effectiveness of the feature design process."}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "4938e8c8c9ea3d351d283181819af5e5801efbed"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-4938e8c8c9ea3d351d283181819af5e5801efbed", "question": "Bank distress in the news: Describing events through deep learning", "context": "Neural Tensor Network for Learning Event Embeddings Event Representation E = (O1, P, O2, T), where P is the action, O1 is the actor, O2 is the object and T is the timestamp (T is mainly used for aligning stock data with news data). For example, the event “Sep 3, 2013 Microsoft agrees to buy Nokia’s mobile phone business for $7.2 billion.” is modeled as: (Actor = Microsoft, Action = buy, Object = Nokia’s mobile phone business, Time = Sep 3, 2013) Event Embedding"}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "8645a7ff78dc321e08dea6576c04f02a3ce158f9"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-8645a7ff78dc321e08dea6576c04f02a3ce158f9", "question": "Bank distress in the news: Describing events through deep learning", "context": "Videos serve to convey complex semantic information and ease the understanding of new knowledge. However, when mixed semantic meanings from different modalities (i.e., image, video, text) are involved, it is more difficult for a computer model to detect and classify the concepts (such as flood, storm, and animals). This paper presents a multimodal deep learning framework to improve video concept classification by leveraging recent advances in transfer learning and sequential deep learning models. Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN) models are then used to obtain the sequential semantics for both audio and textual models. The proposed framework is applied to a disaster-related video dataset that includes not only disaster scenes, but also the activities that took place during the disaster event. The experimental results show the effectiveness of the proposed framework."}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "a6df9a75a7a946cad8c32ee2a8c88d826a21430c"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-a6df9a75a7a946cad8c32ee2a8c88d826a21430c", "question": "Bank distress in the news: Describing events through deep learning", "context": "This is the first time New York University (NYU) participates in the event nugget (EN) evaluation of the Text Analysis Conference (TAC). We developed EN systems for both subtasks of event nugget, i.e, EN Task 1: Event Nugget Detection and EN Task 2: Event Nugget Detection and Coreference. The systems are mainly based on our recent research on deep learning for event detection (Nguyen and Grishman, 2015a; Nguyen and Grishman, 2016a). Due to the limited time we could devote to system development this year, we only ran the systems on the English evaluation data. However, we expect that the adaptation of the current systems to new languages can be done quickly. The development experiments show that although our current systems do not rely on complicated feature engineering, they significantly outperform the reported systems last year for the EN subtasks on the 2015 evaluation data."}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "6da97aa50c0c4f6d7473b607f872cd6bcb940c60"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-6da97aa50c0c4f6d7473b607f872cd6bcb940c60", "question": "Bank distress in the news: Describing events through deep learning", "context": "Deep learning is popular as an end-to-end framework extracting the prominent features and performing the classification also. In this paper, we extensively investigate deep networks as an alternate to feature encoding technique of lowlevel descriptors for emotion recognition on the benchmark EmoDB dataset. Fusion performance with such obtained encoded features with other available features is also investigated. Highest performance to date in the literature is observed."}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "09286005d0c0253995c970387dd222ae4acbc8f1"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-09286005d0c0253995c970387dd222ae4acbc8f1", "question": "Bank distress in the news: Describing events through deep learning", "context": "The problem of detecting events from content published on microblogs has garnered much interest in recent times. In this paper, we address the questions of what happens after the outbreak of an event in terms of how the event gradually progresses and attains each of its milestones, and how it eventually dissipates. We propose a model based approach to capture the gradual unfolding of an event over time. This enables the model to automatically produce entire timeline trajectories of events from the time of their outbreak to their disappearance. We apply our model on the Twitter messages collected about Ebola during the 2014 outbreak and obtain the progression timelines of several events that occurred during the outbreak. We also compare our model to several existing topic modeling and event detection baselines in literature to demonstrate its efficiency."}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "91cf9beb696cbb0818609614f4da7351262eac86"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-91cf9beb696cbb0818609614f4da7351262eac86", "question": "Bank distress in the news: Describing events through deep learning", "context": "In recent years, microblogs have become an important source for reporting real-world events. A real-world occurrence reported in microblogs is also called a social event. Social events may hold critical materials that describe the situations during a crisis. In real applications, such as crisis management and decision making, monitoring the critical events over social streams will enable watch officers to analyze a whole situation that is a composite event, and make the right decision based on the detailed contexts such as what is happening, where an event is happening, and who are involved. Although there has been significant research effort on detecting a target event in social networks based on a single source, in crisis, we often want to analyze the composite events contributed by different social users. So far, the problem of integrating ambiguous views from different users is not well investigated. To address this issue, we propose a novel framework to detect composite social events over streams, which fully exploits the information of social data over multiple dimensions. Specifically, we first propose a graphical model called location-time constrained topic (LTT) to capture the content, time, and location of social messages. Using LTT, a social message is represented as a probability distribution over a set of topics by inference, and the similarity between two messages is measured by the distance between their distributions. Then, the events are identified by conducting efficient similarity joins over social media streams. To accelerate the similarity join, we also propose a variable dimensional extendible hash over social streams. We have conducted extensive experiments to prove the high effectiveness and efficiency of the proposed approach."}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "f2e9f869a9fc1f07887866be5f70a37b6c31411b"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-f2e9f869a9fc1f07887866be5f70a37b6c31411b", "question": "Bank distress in the news: Describing events through deep learning", "context": "Stock trend prediction plays a critical role in seeking maximized profit from the stock investment. However, precise trend prediction is very difficult since the highly volatile and non-stationary nature of the stock market. Exploding information on the Internet together with the advancing development of natural language processing and text mining techniques have enabled investors to unveil market trends and volatility from online content. Unfortunately, the quality, trustworthiness, and comprehensiveness of online content related to stock market vary drastically, and a large portion consists of the low-quality news, comments, or even rumors. To address this challenge, we imitate the learning process of human beings facing such chaotic online news, driven by three principles: sequential content dependency, diverse influence, and effective and efficient learning. In this paper, to capture the first two principles, we designed a Hybrid Attention Networks(HAN) to predict the stock trend based on the sequence of recent related news. Moreover, we apply the self-paced learning mechanism to imitate the third principle. Extensive experiments on real-world stock market data demonstrate the effectiveness of our framework. A further simulation illustrates that a straightforward trading strategy based on our proposed framework can significantly increase the annualized return."}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "no"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-no", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "7d6a34508b091ba8cde8a403e26ec791325c60d1"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-7d6a34508b091ba8cde8a403e26ec791325c60d1", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "Smart Cities gained importance as a means of making ICT enabled services and applications available to the citizens, companies and authorities that are part of a city's system. It aims at increasing citizens' quality of life, and improving the efficiency and quality of the services provided by governing entities and businesses. This perspective requires an integrated vision of a city and of its infrastructures, in all its components. A Smart City can be taken according to six characteristics: All these domains raise new challenges in security and privacy, since users implicitly expect systems to be secure and privacy-preserving. One of the critical elements is which role(s) the city will take up as an actor within an increasingly complex value network. New players enter the market, actors shift their business strategies, roles change, different types of platforms emerge and vie for market dominance, technological developments create new threats and opportunities, etc. An element related to the trend of platformisation is cloud computing, which is increasingly helping the private sector to reduce cost, increase efficiency, and work smarter. One particular challenge relates to open data business models. Activities necessary for Public Sector Information provision can be identified. The development of efficient and effective e-government is a prerequisite. Transnational authentication systems for citizens and businesses, agreed frameworks for data privacy, and the sharing and collection of individual and business data, are key. Smart Cities need to be able to integrate themselves into national, regional and international infrastructures. Although the implementation aspects depend strongly on the authorities of these infrastructures, European wide recommendations and directives will definitely contribute to accelerate the deployment of Smart Cities. Health, inclusion and assisted living will play an essential role, since the demand for related services is rising, because ageing is changing disease composition. Requirements address a number of technologies, beyond the ones related to mobile and fixed networks. An integrated perspective on healthcare solutions for the near-to long-term can be foreseen, bridging a direct gap in between the health area and the technological development of communications (radio and network components). The needs for mobility in urban areas result into a number of problems, such as traffic congestion and energy consumption, which can be alleviated by exploiting Intelligent Transportation Systems and further adoption of vehicle-to-vehicle and vehicle-to-infrastructure communication networks. The information being managed in this area can be relevant to other domains, which increases its potential. An effective deployment …"}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "7b9ab27ad78899b6b284a17c38aa75fb0e1d1765"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-7b9ab27ad78899b6b284a17c38aa75fb0e1d1765", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "We describe an approach for how to design an essentially more scalable, flexible and resilient electric power infrastructure – one that encourages efficient use, integrates local generation, and manages demand through omnipresent awareness of energy availability and use over time. We are inspired by how the Internet has revolutionized communications infrastructure, by pushing intelligence to the edges while hiding the diversity of underlying technologies through well-defined interfaces. Any end device is a traffic source or sink and intelligent endpoints adapt their traffic to what the infrastructure can"}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "047eeb7fce304fdb2b41f3c4d0b393dd1137bdab"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-047eeb7fce304fdb2b41f3c4d0b393dd1137bdab", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "Mobile computing is emerging as the prime focus of next generation computing .One of the prime issues of mobile computing is to provide infrastructure support in terms of computing devices, seamless mobility, application middleware, data and user security, and user applications/services. Mobile commerce is one of the driving forces that has evinced enormous interest in mobile computing .The thought of conducting commerce on the go is what is driving the huge investments corporations are making in researching this area. This paper discusses the various challenges in providing infrastructure for wireless computing."}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "f1e0a619b6ad652b65b49f362ac9413e89291ad7"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-f1e0a619b6ad652b65b49f362ac9413e89291ad7", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "Previous efforts in electronic commerce (e-commerce) research in developing countries shows that there is an acute lack of theoretical frameworks and empirical evidence to understand how developing country firms realize electronic commerce benefits amidst their national constraints. This paper sets out to develop a theoretically abstracted but contextually grounded model of how developing country firms can orient their resources and realize these benefits amidst their national constraints. A review of e-commerce and strategy management literature to develop a resource – based model for e-commerce benefits was undertaken. The process-based model provides an understanding of how to identify, integrate, and reconfigure resources to achieve electronic commerce benefits; provides propositions that serves as theoretical platforms for future empirically grounded research on electronic commerce in developing country contexts and brings organizations closer to identifying and categorizing the strategic value of resources and the role managerial capabilities and intangible resources play in sustaining e-commerce benefits. Finally, our findings provides organizations the strategic options to address resources which have lost their value or have become less valuable to their strategic orientation in e-commerce adoption thereby serving as a starting point of examining e-commerce in developing countries through the theoretical lens of information systems and strategic management."}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "1c49c74521b4c9eae7a352a3b223b4213294c681"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-1c49c74521b4c9eae7a352a3b223b4213294c681", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "This paper puts the Internet of Things in a wider context: How it relates to the Future Internet overall, and where the business value lies so that it will become interesting for enterprises to invest in it. Real-World Awareness and Business Process Decomposition are the two major paradigms regarding future business value. The major application domains where the Internet of Things will play an important role and where there are concrete business opportunities are highlighted. But there are also many technical challenges that need to be addressed. These are listed and it is shown how they are tackled by existing research projects with industrial participation."}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "3716c4896944c3461477f845319ac09e3dfe3a10"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-3716c4896944c3461477f845319ac09e3dfe3a10", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "We designed eSports - a collaborative and synchronous video annotation platform, which is to be used in Internet scale cross-platform grid computing environment to facilitate computer supported cooperative work (CSCW) in education settings such as distance sport coaching, distance classroom etc. Different from traditional multimedia annotation systems, eSports provides the capabilities to collaboratively and synchronously play and archive real time live video, to take snapshots, to annotate video snapshots using whiteboard and to play back the video annotations synchronized with original video streams. eSports is designed based on the grid based collaboration paradigm $the shared event model using NaradaBrokering, which is a publish/subscribe based distributed message passing and event notification system. In addition to elaborate the design and implementation of eSports, we analyze the potential use cases of eSports under different education settings. We believed that eSports is very useful to improve the online collaborative coaching and education."}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "0b458ce6c0d6d7fd20499e5b64a46132d7c380f2"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-0b458ce6c0d6d7fd20499e5b64a46132d7c380f2", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "The Smart Grid, generally referred to as the next-generation power system, is considered as a revolutionary and evolutionary regime of existing power grids. More importantly, with the integration of advanced computing and communication technologies, the Smart Grid is expected to greatly enhance efficiency and reliability of future power systems with renewable energy resources, as well as distributed intelligence and demand response. Along with the silent features of the Smart Grid, cyber security emerges to be a critical issue because millions of electronic devices are inter-connected via communication networks throughout critical power facilities, which has an immediate impact on reliability of such a widespread infrastructure. In this paper, we present a comprehensive survey of cyber security issues for the Smart Grid. Specifically, we focus on reviewing and discussing security requirements, network vulnerabilities, attack countermeasures, secure communication protocols and architectures in the Smart Grid. We aim to provide a deep understanding of security vulnerabilities and solutions in the Smart Grid and shed light on future research directions for Smart Grid security. 2013 Elsevier B.V. All rights reserved."}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "ae6f25ce44d20c3917a62146082c55d2a62b7779"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-ae6f25ce44d20c3917a62146082c55d2a62b7779", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "The fifth generation of mobile technology (5G) will address the socio-economic demands and business contexts of 2020 and beyond. Leading operators from around the globe have defined their vision for 5G, considering customer and business contexts as well as potential technologies and migration issues in an initiative set up by the NGMN Alliance. While their concrete vision is described in the NGMN 5G whitepaper, this paper describes the key points, such as 5G design principles, 5G components, network slicing, 5G radio access technology and 5G interfacing options that have implications to the 5G architecture design."}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "33c5b066ebedd22675c55212232697060c7276ab"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-33c5b066ebedd22675c55212232697060c7276ab", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "Advances in both semiconductor and automotive industry are today enabling the next generation of vehicles with significant electronics content than ever before. Consumers can now avail vehicle offerings in the form of Electric and Hybrid Electric Vehicles (EV/HEV) that have improved fuel efficiency, provide enhanced driver-passenger comfort and experience through Advance Driver Assistance Systems (ADAS) and car infotainment systems, and more. Increasing electronics, software content, and connectivity drive two consumer concerns — “functional safety” and “security” — to the forefront. In this tutorial, we dissect these concerns from an end application perspective and translate the system level requirements and standards into semiconductor development requirements. We indicate both current and emerging practices, and touch upon areas requiring new or optimal design and electronic design automation (EDA) solutions. While functional safety is the primary focus for deep-dive in this tutorial, we also examine key facets of automotive security which is now emerging as a critical area for further understanding and standardization."}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "63c08ef4019bac59c8df18f27d32def8abf1890d"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-63c08ef4019bac59c8df18f27d32def8abf1890d", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "The aim of this paper is to study the implementation of online games to encourage public participation in urban planning. Its theoretical foundations are based on previous work in public participatory geographical information systems (PP GISs), play and games, with a special focus on serious games. Serious games aim to support learning processes in a new, more playful way. We developed the concept of playful public participation in urban planning, including playful elements such as storytelling, walking and moving, sketching, drawing, and games. A group of students designed an online serious public participatory game entitled NextCampus. The case study used in NextCampus was taken from the real-world question of a possible move of a university campus to a new location in the city of Hamburg, Germany. The development of the serious public participatory game NextCampus resulted in a physical prototype, user interface design, and a computational model of the game. The NextCampus game was tested with the help of two groups of urban planning students and presented to three external experts who provided valuable recommendations for further development. The critical comments questioned the level of complexity involved in such games. The positive comments included recognition of the potential for joy and the play-fulness a game like NextCampus could evoke. Public participatory online applications aim to attract citizens to discuss current issues related to their environment and to improve the process of public participation in general. An integration of geographic information systems (GISs) with public participatory tools represents one of the latest innovations in this area. ways of integrating the new applications into participatory processes and considers which new functionalities and technical characteristics could offer the most benefit to users. In the past, these technologies and other map-based applications were frequently criticized as being too complex for the majority of potential users (Steinmann, Krek, & Blaschke, 2004). New forms of collaboration and technical solutions emerged during the Web 2.0 era. For example , Google Maps and Google Earth can be used by lay users and non-experts without intense training. Recent research on collabo-rative mapping also known as ''geography without geographers'' prior work in PP GIS to include much wider, distributed participation (Hardy, 2008). Despite these new forms of collaboration and innovative technologies , Moody (2007) demonstrates that the use of GIS technology to involve citizens in participatory urban planning does not seem to empower citizens. An important factor in such findings …"}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "no"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-no", "question": "Pooled motion features for first-person videos", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "c67192cb7c82d2a0516b656909985823a5b2aba0"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-c67192cb7c82d2a0516b656909985823a5b2aba0", "question": "Pooled motion features for first-person videos", "context": "We present a method for converting first-person videos, for example, captured with a helmet camera during activities such as rock climbing or bicycling, into hyper-lapse videos, i.e., time-lapse videos with a smoothly moving camera. At high speed-up rates, simple frame sub-sampling coupled with existing video stabilization methods does not work, because the erratic camera shake present in first-person videos is amplified by the speed-up. Our algorithm first reconstructs the 3D input camera path as well as dense, per-frame proxy geometries. We then optimize a novel camera path for the output video that passes near the input cameras while ensuring that the virtual camera looks in directions that can be rendered well from the input. Finally, we generate the novel smoothed, time-lapse video by rendering, stitching, and blending appropriately selected source frames for each output frame. We present a number of results for challenging videos that cannot be processed using traditional techniques."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "748260579dc2fb789335a88ae3f63c114795d047"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-748260579dc2fb789335a88ae3f63c114795d047", "question": "Pooled motion features for first-person videos", "context": "In this work, we evaluate the performance of the popular dense trajectories approach on first-person action recognition datasets. A person moving around with a wearable camera will actively interact with humans and objects and also passively observe others interacting. Hence, in order to represent real-world scenarios, the dataset must contain actions from first-person perspective as well as third-person perspective. For this purpose, we introduce a new dataset which contains actions from both the perspectives captured using a head-mounted camera. We employ a motion pyramidal structure for grouping the dense trajectory features. The relative strengths of motion along the trajectories are used to compute different bag-of-words descriptors and concatenated to form a single descriptor for the action. The motion pyramidal approach performs better than the baseline improved trajectory descriptors. The method achieves 96.7% on the JPL interaction dataset and 61.8% on our NUS interaction dataset. The same is used to detect actions in long video sequences and achieves average precision of 0.79 on JPL interaction dataset."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "6674729287f2482eda9e836846d2a35e63ea401c"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-6674729287f2482eda9e836846d2a35e63ea401c", "question": "Pooled motion features for first-person videos", "context": "We propose a function-based temporal pooling method that captures the latent structure of the video sequence data - e.g., how frame-level features evolve over time in a video. We show how the parameters of a function that has been fit to the video data can serve as a robust new video representation. As a specific example, we learn a pooling function via ranking machines. By learning to rank the frame-level features of a video in chronological order, we obtain a new representation that captures the video-wide temporal dynamics of a video, suitable for action recognition. Other than ranking functions, we explore different parametric models that could also explain the temporal changes in videos. The proposed functional pooling methods, and rank pooling in particular, is easy to interpret and implement, fast to compute and effective in recognizing a wide variety of actions. We evaluate our method on various benchmarks for generic action, fine-grained action and gesture recognition. Results show that rank pooling brings an absolute improvement of 7-10 average pooling baseline. At the same time, rank pooling is compatible with and complementary to several appearance and local motion based methods and features, such as improved trajectories and deep learning features."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "1aad2da473888cb7ebc1bfaa15bfa0f1502ce005"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-1aad2da473888cb7ebc1bfaa15bfa0f1502ce005", "question": "Pooled motion features for first-person videos", "context": "This paper discusses the problem of recognizing interaction-level human activities from a first-person viewpoint. The goal is to enable an observer (e.g., a robot or a wearable camera) to understand 'what activity others are performing to it' from continuous video inputs. These include friendly interactions such as 'a person hugging the observer' as well as hostile interactions like 'punching the observer' or 'throwing objects to the observer', whose videos involve a large amount of camera ego-motion caused by physical interactions. The paper investigates multi-channel kernels to integrate global and local motion information, and presents a new activity learning/recognition methodology that explicitly considers temporal structures displayed in first-person activity videos. In our experiments, we not only show classification results with segmented videos, but also confirm that our new approach is able to detect activities from continuous videos reliably."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "3a1a9e15d42b67bfdee7761311aea9b699cd0d5f"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-3a1a9e15d42b67bfdee7761311aea9b699cd0d5f", "question": "Pooled motion features for first-person videos", "context": "JackIn is a new human-human communication framework for connecting two or more people. With first-person view video streaming from a person (called Body) wearing a transparent head-mounted display and a head-mounted camera, the other person (called Ghost) participates in the shared first-person view. With JackIn, people's activities can be shared and assistance or guidance can be given through other peoples expertise. This can be applied to daily activities such as cooking lessons, shopping navigation, education in craft-work or electrical work, and sharing experiences of sporting and live events. For a better viewing experience with frist-person view, we developed the out-of-body view in which first-person images are integrated to construct a scene around a Body, and a Ghost can virtually control the viewpoint to look around the space surrounding the Body. We also developed a tele-pointing gesture interface. We conducted an experiment to evaluate how effective this framework is and found that Ghosts can understand the spatial situation of the Body."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "93ce62fb04283efb253b512dc3f02b1d169ee7ed"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-93ce62fb04283efb253b512dc3f02b1d169ee7ed", "question": "Pooled motion features for first-person videos", "context": "3-D convolutional neural networks (3-D-convNets) have been very recently proposed for action recognition in videos, and promising results are achieved. However, existing 3-D-convNets has two “artificial” requirements that may reduce the quality of video analysis: 1) It requires a fixed-sized (e.g., 112  $\\times$ 112) input video; and 2) most of the 3-D-convNets require a fixed-length input (i.e., video shots with fixed number of frames). To tackle these issues, we propose an end-to-end pipeline named Two-stream 3-D-convNet Fusion, which can recognize human actions in videos of arbitrary size and length using multiple features. Specifically, we decompose a video into spatial and temporal shots. By taking a sequence of shots as input, each stream is implemented using a spatial temporal pyramid pooling (STPP) convNet with a long short-term memory (LSTM) or CNN-E model, softmax scores of which are combined by a late fusion. We devise the STPP convNet to extract equal-dimensional descriptions for each variable-size shot, and we adopt the LSTM/CNN-E model to learn a global description for the input video using these time-varying descriptions. With these advantages, our method should improve all 3-D CNN-based video analysis methods. We empirically evaluate our method for action recognition in videos and the experimental results show that our method outperforms the state-of-the-art methods (both 2-D and 3-D based) on three standard benchmark datasets (UCF101, HMDB51 and ACT datasets)."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "49941c788b9dd8a639b33b4208b32a740b8c7bf8"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-49941c788b9dd8a639b33b4208b32a740b8c7bf8", "question": "Pooled motion features for first-person videos", "context": "We present a novel algorithm for automatically applying constrainable, L1-optimal camera paths to generate stabilized videos by removing undesired motions. Our goal is to compute camera paths that are composed of constant, linear and parabolic segments mimicking the camera motions employed by professional cinematographers. To this end, our algorithm is based on a linear programming framework to minimize the first, second, and third derivatives of the resulting camera path. Our method allows for video stabilization beyond the conventional filtering of camera paths that only suppresses high frequency jitter. We incorporate additional constraints on the path of the camera directly in our algorithm, allowing for stabilized and retargeted videos. Our approach accomplishes this without the need of user interaction or costly 3D reconstruction of the scene, and works as a post-process for videos from any camera or from an online source."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "45392756fd0d437091d172e4cbbc37a66650555f"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-45392756fd0d437091d172e4cbbc37a66650555f", "question": "Pooled motion features for first-person videos", "context": "Given a short video we create a representation that captures a spectrum of looping videos with varying levels of dynamism, ranging from a static image to a highly animated loop. In such a progressively dynamic video, scene liveliness can be adjusted interactively using a slider control. Applications include background images and slideshows, where the desired level of activity may depend on personal taste or mood. The representation also provides a segmentation of the scene into independently looping regions, enabling interactive local adjustment over dynamism. For a landscape scene, this control might correspond to selective animation and deanimation of grass motion, water ripples, and swaying trees. Converting arbitrary video to looping content is a challenging research problem. Unlike prior work, we explore an optimization in which each pixel automatically determines its own looping period. The resulting nested segmentation of static and dynamic scene regions forms an extremely compact representation."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "070874b011f8eb2b18c8aa521ad0a7a932b4d9ad"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-070874b011f8eb2b18c8aa521ad0a7a932b4d9ad", "question": "Pooled motion features for first-person videos", "context": "Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results on four challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "6ec17e735cd9f7cb37485ab07b905a7895b0067d"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-6ec17e735cd9f7cb37485ab07b905a7895b0067d", "question": "Pooled motion features for first-person videos", "context": "This paper presents a video stabilization algorithm based on the extraction and tracking of scale invariant feature transform features through video frames. Implementation of SIFT operator is analyzed and adapted to be used in a feature-based motion estimation algorithm. SIFT features are extracted from video frames and then their trajectory is evaluated to estimate interframe motion. A modified version of iterative least squares method is adopted to avoid estimation errors and features are tracked as they appear in nearby frames to improve video stability. Intentional camera motion is eventually filtered with adaptive motion vector integration. Results confirm the effectiveness of the method."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "no"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-no", "question": "General transformations for GPU execution of tree traversals", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "36f06481eaae63522dfb61475602584997ebfee8"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-36f06481eaae63522dfb61475602584997ebfee8", "question": "General transformations for GPU execution of tree traversals", "context": "Modern graphics hardware architectures excel at compute-intensive tasks such as ray-triangle intersection, making them attractive target platforms for raytracing. To date, most GPU-based raytracers have relied upon uniform grid acceleration structures. In contrast, the kd-tree has gained widespread use in CPU-based raytracers and is regarded as the best general-purpose acceleration structure. We demonstrate two kd-tree traversal algorithms suitable for GPU implementation and integrate them into a streaming raytracer. We show that for scenes with many objects at different scales, our kd-tree algorithms are up to 8 times faster than a uniform grid. In addition, we identify load balancing and input data recirculation as two fundamental sources of inefficiency when raytracing on current graphics hardware."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "698b8181cd613a72adeac0d75252afe7f57a5180"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-698b8181cd613a72adeac0d75252afe7f57a5180", "question": "General transformations for GPU execution of tree traversals", "context": "We present two new parallel implementations of the tree-ensemble algorithms Random Forest (RF) and Extremely randomized trees (ERT) for emerging many-core platforms, e.g., contemporary graphics cards suitable for general-purpose computing (GPGPU). Random Forest and Extremely randomized trees are ensemble learners for classification and regression. They operate by constructing a multitude of decision trees at training time and outputting a prediction by comparing the outputs of the individual trees. Thanks to the inherent parallelism of the task, an obvious platform for its computation is to employ contemporary GPUs with a large number of processing cores. Previous parallel algorithms for Random Forests in the literature are either designed for traditional multi-core CPU platforms or early history GPUs with simpler hardware architecture and relatively few number of cores. The new parallel algorithms are designed for contemporary GPUs with a large number of cores and take into account aspects of the newer hardware architectures as memory hierarchy and thread scheduling. They are implemented using the C/C++ language and the CUDA interface for best possible performance on NVidia-based GPUs. An experimental study comparing with the most important previous solutions for CPU and GPU platforms shows significant improvement for the new implementations, often with several magnitudes."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "35bef4597f5e514359ff45bea31be8b8239effe1"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-35bef4597f5e514359ff45bea31be8b8239effe1", "question": "General transformations for GPU execution of tree traversals", "context": "The watershed transform is a popular image segmentation procedure from mathematical morphology used in many applications of computer vision. This paper proposes a novel parallel watershed procedure designed for GPU implementation. Our algorithm constructs paths of steepest descent and reduces these paths into direct pointers to catchment basin minima in logarithmic time, also crucially incorporating successful resolution of plateaux. Three implementation variants and their parameters are analysed through experiments on 2D and 3D images; a comparison against the state-of-the-art shows a runtime improvement of around 30%. For 3D images of 128 megavoxels execution times of approximately 1.5–2 seconds are achieved."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "2379c027e7376bb76978602a7b185dfa73a9cd35"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-2379c027e7376bb76978602a7b185dfa73a9cd35", "question": "General transformations for GPU execution of tree traversals", "context": "Graphics Processing Units (GPUs) have emerged as powerful accelerators for many regular algorithms that operate on dense arrays and matrices. In contrast, we know relatively little about using GPUs to accelerate highly irregular algorithms that operate on pointer-based data structures such as graphs. For the most part, research has focused on GPU implementations of graph analysis algorithms that do not modify the structure of the graph, such as algorithms for breadth-first search and strongly-connected components.\n In this paper, we describe a high-performance GPU implementation of an important graph algorithm used in compilers such as gcc and LLVM: Andersen-style inclusion-based points-to analysis. This algorithm is challenging to parallelize effectively on GPUs because it makes extensive modifications to the structure of the underlying graph and performs relatively little computation. In spite of this, our program, when executed on a 14 Streaming Multiprocessor GPU, achieves an average speedup of 7x compared to a sequential CPU implementation and outperforms a parallel implementation of the same algorithm running on 16 CPU cores.\n Our implementation provides general insights into how to produce high-performance GPU implementations of graph algorithms, and it highlights key differences between optimizing parallel programs for multicore CPUs and for GPUs."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "f4c5f7bdf3f7ce924cd42f26d2a9eb97ab8da4a3"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-f4c5f7bdf3f7ce924cd42f26d2a9eb97ab8da4a3", "question": "General transformations for GPU execution of tree traversals", "context": "We present an efficient method for volume rendering by raycasting on the CPU. We employ coherent packet traversal of an implicit bounding volume hierarchy, heuristically pruned using preintegrated transfer functions, to exploit empty or homogeneous space. We also detail SIMD optimizations for volumetric integration, trilinear interpolation, and gradient lighting. The resulting system performs well on low-end and laptop hardware, and can outperform out-of-core GPU methods by orders of magnitude when rendering large volumes without level-of-detail (LOD) on a workstation. We show that, while slower than GPU methods for low-resolution volumes, an optimized CPU renderer does not require LOD to achieve interactive performance on large data sets."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "50ed709761f57895b50346a8249814a6f66f6c89"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-50ed709761f57895b50346a8249814a6f66f6c89", "question": "General transformations for GPU execution of tree traversals", "context": "A GPU (Graphics Processing Unit) is a specialized processor for graphics processing. GPUs have the ability to perform high-speed parallel processing using its many processing cores. To utilize the powerful computing ability, GPUs are widely used for general purpose processing. The main contribution of this paper is to show a new template matching algorithm using pixel rearrangement. Template Matching is a technique for finding small parts of an image which match a template image. The feature of our proposed algorithm is that using pixel rearrangement, multiple low-resolution images are generated and template matching for the low-resolution images is performed to reduce the computing time. Also, we implemented our algorithm on a GPU system. The experimental results show that, for an input image with size of 4096 $\\times$ 4096 and a template image with size of 256 $\\times$ 256, our implementation can achieve a speedup factor of approximately 78 times over the conventional sequential implementation."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "67078d516a85204c016846e30c02e901ac16f142"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-67078d516a85204c016846e30c02e901ac16f142", "question": "General transformations for GPU execution of tree traversals", "context": "This paper presents a new GPU-based rasterization algorithm for Boolean operations that handles arbitary closed polygons. We construct an efficient data structure for interoperation of CPU and GPU and propose a fast GPU-based contour extraction method to ensure the performance of our algorithm. We then design a novel traversing strategy to achieve an error-free calculation of intersection point for correct Boolean operations. We finally give a detail evaluation and the results show that our algorithm has a higher performance than exsiting algorithms on processing polygons with large amount of vertices. key words: GPU, CPU, rasterization, Boolean operation, error-free"}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "371b07d65891b03eaae15c2865da2a6751a99bb8"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-371b07d65891b03eaae15c2865da2a6751a99bb8", "question": "General transformations for GPU execution of tree traversals", "context": "A novel algorithm is presented to compute the convex hull of a point set in ℝ3 using the graphics processing unit (GPU). By exploiting the relationship between the Voronoi diagram and the convex hull, the algorithm derives the approximation of the convex hull from the former. The other extreme vertices of the convex hull are then found by using a two-round checking in the digital and the continuous space successively. The algorithm does not need explicit locking or any other concurrency control mechanism, thus it can maximize the parallelism available on the modern GPU.\n The implementation using the CUDA programming model on NVIDIA GPUs is exact and efficient. The experiments show that it is up to an order of magnitude faster than other sequential convex hull implementations running on the CPU for inputs of millions of points. The works demonstrate that the GPU can be used to solve nontrivial computational geometry problems with significant performance benefit."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "1794435f6b541109ee9ea812d80d5b9add95aacd"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-1794435f6b541109ee9ea812d80d5b9add95aacd", "question": "General transformations for GPU execution of tree traversals", "context": "We introduce a technique for traversal of Merkle trees, and propose an efficient algorithm that generates a sequence of leaves along with their associated authentication paths. For one choice of parameters, and a total of N leaves, our technique requires a worst-case computational effort of 2 log N/loglog N hash function evaluations per output, and a total storage capacity of less than 1.5 log N/loglog N hash values. This is a simultaneous improvement both in space and time complexity over any previously published algorithm."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "bbe0f0b3e2d60c4f96d9d84f97dc8a9be4f72802"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-bbe0f0b3e2d60c4f96d9d84f97dc8a9be4f72802", "question": "General transformations for GPU execution of tree traversals", "context": "Graphics Processing Units (GPUs) offer tremendous computational power. CUDA (Compute Unified Device Architecture) provides a multi-threaded parallel programming model, facilitating high performance implementations of general-purpose computations. However, the explicitly managed memory hierarchy and multi-level parallel view make manual development of high-performance CUDA code rather complicated. Hence the automatic transformation of sequential input programs into efficient parallel CUDA programs is of considerable interest. This paper describes an automatic code transformation system that generates parallel CUDA code from input sequential C code, for regular (affine) programs. Using and adapting publicly available tools that have made polyhedral compiler optimization practically effective, we develop a C-to-CUDA transformation system that generates two-level parallel CUDA code that is optimized for efficient data access. The performance of automatically generated code is compared with manually optimized CUDA code for a number of benchmarks. The performance of the automatically generated CUDA code is quite close to hand-optimized CUDA code and considerably better than the benchmarks’ performance on a multicore CPU."}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "no"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-no", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "197c43315bdcec6785cb9834638140d9878ec131"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-197c43315bdcec6785cb9834638140d9878ec131", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "TacAir-Soar is an intelligent, rule-based system that generates believable “human-like” behavior for large scale, distributed military simulations. The innovation of the application is primarily a matter of scale and integration. The system is capable of executing most of the airborne missions that the United States military flies in fixed-wing aircraft. It accomplishes this by integrating a wide variety of intelligent capabilities, including real-time hierarchical execution of complex goals and plans, communication and coordination with humans and simulated entities, maintenance of situational awareness, and the ability to accept and respond to new orders while in flight. The system is currently deployed at the Oceana Naval Air Station WISSARD Lab and the Air Force Research Laboratory in Mesa, AZ. Its most dramatic use was in the Synthetic Theater of War 1997, which was an operational training exercise that ran for 48 continuous hours during which TacAir-Soar flew all U.S. fixed-wing aircraft."}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "29ab09179c9fc864b05fe853c8443f39ac1baaec"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-29ab09179c9fc864b05fe853c8443f39ac1baaec", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "This paper describes how language is grounded by a comprehension system called Lucia within a robotic agent called Rosie that can manipulate objects and navigate indoors. The whole system is built within the Soar cognitive architecture and uses Embodied Construction Grammar (ECG) as a formalism for describing linguistic knowledge. Grounding is performed using knowledge from the grammar itself, from the linguistic context, from the agent's perception, and from an ontology of long-term knowledge about object categories and properties and actions the agent can perform. The paper also describes a benchmark corpus of 200 sentences in this domain, along with test versions of the world model and ontology, and gold-standard meanings for each of the sentences. The benchmark is contained in the supplemental materials."}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "d041c7df26d2d212a6e37204f8615119aff56eed"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-d041c7df26d2d212a6e37204f8615119aff56eed", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "Human cognitive architecture includes a working memory of limited capacity and duration with partially separate visual and auditory channels, and an effectively infinite long-term memory holding many schemas that can vary in their degree of automation. These cognitive structures have evolved to handle information that varies in the extent to which elements can be processed successively in working memory or, because they interact, must be processed simultaneously imposing a heavy load on working memory. Cognitive load theory uses this combination of information and cognitive structures to guide instructional design. Several designs that rely heavily on visual working memory and its characteristics are discussed in this paper. Knowledge of human cognitive architecture is essential for instructional design, and visual cognition is a central aspect of human cognition. Not surprisingly, there are several instructional design effects that rely heavily on the manner in which humans visually process information. This paper discusses some relevant information structures, cognitive structures and instructional designs that rely on our knowledge of visual information processing. A. Information structures While considerable work by many researchers over several decades has been devoted to the organization of human cognitive architecture, far less effort has gone into investigating the information structures that must have driven the evolution of that architecture. Some work has been carried out by Sweller (1994) and Halford, Wilson and Phillips (1998). Sweller (1994) suggested that all information can be placed on a continuum according to the extent to which the elements that constitute the information interact. At one extreme, there is no interaction between the elements that need to be learned. They are independent. Element interactivity is low, or indeed, non-existent, and that means each element can be considered and learned serially without reference to any other element. Because elements at the low element interactivity end of the continuum do not interact with each other, there is no loss of understanding despite each element being learned individually and in isolation. Understanding is defined as the ability to process all elements that necessarily interact, simultaneously in working memory. Learning such material"}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "041326c202655cd60df276bf7a148f2ecddfc479"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-041326c202655cd60df276bf7a148f2ecddfc479", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "In this paper, we examine the motivations for research on cognitive architectures and review some candidates that have been explored in the literature. After this, we consider the capabilities that a cognitive architecture should support, some properties that it should exhibit related to representation, organization, performance, and learning, and some criteria for evaluating such architectures at the systems level. In closing, we discuss some open issues that should drive future research in this important area. 2008 Published by Elsevier B.V."}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "598eadcca8ac9365d188157d585e076dc2ef60d9"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-598eadcca8ac9365d188157d585e076dc2ef60d9", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "Existing cognitively based computational models for wayfinding focus primarily on the exploration of mental representations rather than the information needs for wayfinding. It is important to consider information needs because people trying to find their ways in unfamiliar environments do not have a previously acquired mental representation but depend on external information. The fundamental tenet of this work is that all information must be presented to the wayfinder at each decision point as “knowledge in the world.” Simulating people’s wayfinding behavior in a cognitively plausible way requires the integration of structures for information perception and cognition in the underlying model. In this work we use a cognizing agent to simulate people’s wayfinding processes in unfamiliar buildings. The agent-based model consists of two tiers: simulated states of the environment and simulated beliefs of the agent. The agent is modeled with state, an observation schema, a specific wayfinding strategy, and commonsense knowledge. The environment is modeled as a graph, where nodes represent decision points and edges represent lines of movement. The perceptual wayfinding model integrates the agent and its environment within a “Sense-Plan-Act” framework. It focuses on knowledge in the world to explain actions of the agent. The concepts of affordance and information are used to describe the kinds of knowledge the agent derives from the world by means of visual perception. Affordances are possibilities for action with reference to the agent. Information is necessary for the agent to decide which affordances to utilize. During the navigation process the agent accumulates beliefs about the environment by observing task-relevant affordances and information at decision points. The utilization of a “go-to” affordance leads the agent from one node to another where it is again provided with percepts. A successful navigation corresponds to the agent’s traversal from a start to a goal node. The proposed formal specifications of the agent-based model can be used to simulate people’s wayfinding behavior in spatial information and design systems in a cognitively plausible way. Such simulation helps to determine where and why people face wayfinding difficulties and what needs to be done to avoid them. The case of wayfinding in an airport, in which the signage in the airport is tested, is employed to demonstrate the perceptual wayfinding model."}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "da67375c8b6a250fbd5482bfbfce14f4eb7e506c"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-da67375c8b6a250fbd5482bfbfce14f4eb7e506c", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "This survey presents an overview of the autonomous development of mental capabilities in computational agents. It does so based on a characterization of cognitive systems as systems which exhibit adaptive, anticipatory, and purposive goal-directed behavior. We present a broad survey of the various paradigms of cognition, addressing cognitivist (physical symbol systems) approaches, emergent systems approaches, encompassing connectionist, dynamical, and enactive systems, and also efforts to combine the two in hybrid systems. We then review several cognitive architectures drawn from these paradigms. In each of these areas, we highlight the implications and attendant problems of adopting a developmental approach, both from phylogenetic and ontogenetic points of view. We conclude with a summary of the key architectural features that systems capable of autonomous development of mental capabilities should exhibit"}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "5e880d3bd1c4c4635ea7684df47109a33448b4c2"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-5e880d3bd1c4c4635ea7684df47109a33448b4c2", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "This paper describes an architecture that combines the complementary strengths of probabilistic graphical models and declarative programming to enable robots to represent and reason with qualitative and quantitative descriptions of uncertainty and domain knowledge. An action language is used for the architecture’s low-level (LL) and high-level (HL) system descriptions, and the HL definition of recorded history is expanded to allow prioritized defaults. For any given objective, tentative plans created in the HL using commonsense reasoning are implemented in the LL using probabilistic algorithms, and the corresponding observations are added to the HL history. Tight coupling between the levels helps automate the selection of relevant variables and the generation of policies in the LL for each HL action, and supports reasoning with violation of defaults, noisy observations and unreliable actions in complex domains. The architecture is evaluated in simulation and on robots moving objects in indoor domains."}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "e26cbabe8c60f1c62616917410f47ac2ad7d7609"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-e26cbabe8c60f1c62616917410f47ac2ad7d7609", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "OBJECTIVE\nThis paper explores the development of a rigorous computational model of driver behavior in a cognitive architecture--a computational framework with underlying psychological theories that incorporate basic properties and limitations of the human system.\n\n\nBACKGROUND\nComputational modeling has emerged as a powerful tool for studying the complex task of driving, allowing researchers to simulate driver behavior and explore the parameters and constraints of this behavior.\n\n\nMETHOD\nAn integrated driver model developed in the ACT-R (Adaptive Control of Thought-Rational) cognitive architecture is described that focuses on the component processes of control, monitoring, and decision making in a multilane highway environment.\n\n\nRESULTS\nThis model accounts for the steering profiles, lateral position profiles, and gaze distributions of human drivers during lane keeping, curve negotiation, and lane changing.\n\n\nCONCLUSION\nThe model demonstrates how cognitive architectures facilitate understanding of driver behavior in the context of general human abilities and constraints and how the driving domain benefits cognitive architectures by pushing model development toward more complex, realistic tasks.\n\n\nAPPLICATION\nThe model can also serve as a core computational engine for practical applications that predict and recognize driver behavior and distraction."}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "16524adee515692a50dd67a170b8e605e4b00b29"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-16524adee515692a50dd67a170b8e605e4b00b29", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "In his book “Conceptual Spaces The Geometry of Thought”, Peter Gärdenfors [1] presents a pioneering theory for representing conceptual knowledge, the basic construct of human thinking and reasoning [4]. The conceptual level is not seen as an alternative to traditional approaches of knowledge representation in artificial intelligence, namely symbolic or subsymbolic methods. Instead, it is meant to complement both approaches. The book is highly recommendable and worth reading, as it does not only tackle many fundamental problems of knowledge representation such as grounding [3], concept formation and similarity comparisons [2], but also outlines novel and enlightening ideas how to overcome these. The book introduces the notion of a conceptual space as a framework for representing knowledge at the conceptual level. It is motivated by contrasting it to other levels of knowledge representation: The highest level, the symbolic level, conceptualizes the human brain as a computing device. Knowledge is represented based on a language consisting of a set of symbols. Logical operations can be performed on these symbols to infer new knowledge. Human reasoning is modeled as a symbol manipulation process. Classical, symbolic artificial intelligence does not very well support central cognitive processes such as the acquisition or formation of new concepts and similarity comparisons. The lowest level, the subsymbolic knowledge representation, is oriented towards the neuro-biological structure of the human brain. Concepts are implicitly represented via activation patterns within the neural network. Learning is modeled by modifying the activation of neurons. Explicit representation of knowledge and concepts is not possible. At the intersection between the symbolic and the subsymbolic level, Gärdenfors introduces the conceptual level. The theory of conceptual spaces is based on semantic spaces with a geometric structure: A conceptual space is formed by a set of quality dimensions. One or several quality dimensions model one domain. An important example used throughout the book is the color domain represented by the quality dimensions hue, saturation and brightness. Conceptual spaces have a cognitive foundation because domains can be grounded in qualities perceivable by the human sensory apparatus. Concepts are represented as conceptual regions described by their properties on the quality dimensions. The geometric structure of conceptual spaces makes it possible to determine distances and therefore provides an inherent similarity measure by taking the distance in the conceptual space as indicator of the semantic similarity. The notion of similarity is an important construct for modeling categorization and concept formation. Using similarity for reasoning can also reflect well the vagueness typical for human reasoning. The strong focus on the cognitive foundation makes the book particularly valuable. It contains many challenging claims which are related to various disciplines by giving evidence from a wide range of literature. This shows the huge and highly interdisciplinary background of the author. Unfortunately, Gärdenfors describes his theory only at a very abstract level and forbears from describing algorithms for the formalization of his theory. The realization of a computational model for conceptual spaces bears many practical problems which still have to be solved. Moreover, no empirical evidence is given for his pioneering, sometimes revolutionary ideas. However, these shortcomings should be considered as challenges to solve in the future. The target audience of the book is highly interdisciplinary: since Gärdenfors tackles the problem of cognitive knowledge representation from a psychologic and computer science perspective as well as from a philosophic, neuroscience and linguistic point of view, this book is worth reading for researchers from many different areas. It is required reading for researchers in cognitive science or artificial intelligence interested in knowledge representation. The book has a clear structure and is very well written. The convincing examples throughout the book illustrate the findings very well and make it easy to understand. Therefore I would also deem Gärdenfors’ book to be suitable for students as introducing literature to various problem fields in cognitive science. It gives readers from related areas the chance to look beyond one’s own nose and get to know an interdisciplinary way of thinking. The book certainly meets the expectations of the highly interdisciplinary research area cognitive science."}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "ad943b2c2b859e46481308786c6aea9063dd49a9"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-ad943b2c2b859e46481308786c6aea9063dd49a9", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "The improved ground resolution of state-of-the-art synthetic aperture radar (SAR) sensors suggests utilizing SAR data for the analysis of urban areas. The appearance of buildings in SAR or interferometric SAR (InSAR) data is characterized by the consequences of the inherent oblique scene illumination, such as layover, occlusion by radar shadow, and multipath signal propagation. Therefore, particularly in dense built-up areas, building reconstruction is often impossible from a single SAR or InSAR measurement alone. But, the reconstruction quality can be significantly improved by a combined analysis of multi-aspect data. In this paper, two approaches are proposed to detect and reconstruct buildings of different size from multi-aspect high-resolution InSAR data sets. Both approaches focus on the recognition of buildings supported by knowledge-based analysis considering the mentioned SAR-specific effects observed in urban areas. Building features are extracted independently for each direction from the magnitude and phase information of the interferometric data. Initial primitives are segmented and afterward projected from slant-range into the world coordinate system. From the fused set of primitives of both flight directions, building hypotheses are generated. The first approach exploits the frequently observed lines of bright double-bounce scattering, which are used for building reconstruction in residential districts. In the case of larger buildings, such as industrial halls, often additional features of roof and facade elements are visible. Therefore, in a second approach, extended buildings are extracted by grouping primitives of different kinds. The two approaches are demonstrated in an urban environment for an InSAR data set, which has spatial resolution of about 30 cm and was taken from two orthogonal flight directions."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "no"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-no", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "090ab6c395c010ced449b540c425a6a2835647a6"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-090ab6c395c010ced449b540c425a6a2835647a6", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "Image super-resolution is a fundamental pre-processing technique for the machine vision applications of robotics and other mobile platforms. Inevitably, images captured by the mobile camera tend to emerge severe motion blur and this degradation will deteriorate the performance of current state-of-the-art super-resolution methods. In this paper, we propose a deep dual-branch convolution neural network (CNN) to generate a clear high-resolution image from a single natural image with severe blurs. Compared to off-the-shelf methods, our method, called DB-SRN, can remove the complex non-uniform motion blurs and restore useful texture details simultaneously. By sharing the features from modified residual blocks (ResBlocks), the dual-branch design can promote the performances of both tasks other while retaining network simplicity. Extensive experiments demonstrate that our method produces remarkable deblurred and super-resolved images in terms of quality and quantity with high computational efficiency."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "5e4b405202c92fd77a12f463ca1247a8b59fd935"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-5e4b405202c92fd77a12f463ca1247a8b59fd935", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "Novel low-density signature (LDS) structure is proposed for transmission and detection of symbol-synchronous communication over memoryless Gaussian channel. Given N as the processing gain, under this new arrangement, users' symbols are spread over N chips but virtually only dv < N chips that contain nonzero-values. The spread symbol is then so uniquely interleaved as the sampled, at chip rate, received signal contains the contribution from only dc < K number of users, where K denotes the total number of users in the system. Furthermore, a near-optimum chip-level iterative soft-in-soft-out (SISO) multiuser decoding (MUD), which is based on message passing algorithm (MPA) technique, is proposed to approximate optimum detection by efficiently exploiting the LDS structure. Given beta = K/N as the system loading, our simulation suggested that the proposed system alongside the proposed detection technique, in AWGN channel, can achieve an overall performance that is close to single-user performance, even when the system has 200% loading, i.e., when beta = 2. Its robustness against near-far effect and its performance behavior that is very similar to optimum detection are demonstrated in this paper. In addition, the complexity required for detection is now exponential to dc instead of K as in conventional code division multiple access (CDMA) structure employing optimum multiuser detector."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "0c3751db5a24c636c1aa8abfd9d63321b38cfce5"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-0c3751db5a24c636c1aa8abfd9d63321b38cfce5", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "Stochastic Gradient Descent (SGD) has become popular for solving large scale supervised machine learning optimization problems such as SVM, due to their strong theoretical guarantees. While the closely related Dual Coordinate Ascent (DCA) method has been implemented in various software packages, it has so far lacked good convergence analysis. This paper presents a new analysis of Stochastic Dual Coordinate Ascent (SDCA) showing that this class of methods enjoy strong theoretical guarantees that are comparable or better than SGD. This analysis justifies the effectiveness of SDCA for practical applications."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "0e9a674280b2dabe36e540c20ce5a7a9e10361f7"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-0e9a674280b2dabe36e540c20ce5a7a9e10361f7", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "Cognitive Radio (CR) is a next-generation wireless communication system that enables unlicensed users to exploit underutilized licensed spectrum to optimize the utilization of the overall radio spectrum. A Distributed Cognitive Radio Network (DCRN) is a distributed wireless network established by a number of unlicensed users in the absence of fixed network infrastructure such as a base station. Context awareness and intelligence are the capabilities that enable each unlicensed user to observe and carry out its own action as part of the joint action on its operating environment for network-wide performance enhancement. These capabilities can be applied in various application schemes in CR networks such as Dynamic Channel Selection (DCS), congestion control, and scheduling. In this paper, we apply Reinforcement Learning (RL), including single-agent and multi-agent approaches, to achieve context awareness and intelligence. Firstly, we show that the RL approach achieves a joint action that provides better network-wide performance in respect to DCS in DCRNs. The multi-agent approach is shown to provide higher levels of stability compared to the single-agent approach. Secondly, we show that RL achieves high level of fairness. Thirdly, we show the effects of network density and various essential parameters in RL on the network-wide performance."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "1b9d022273780c5b0b7522555bd0e2c626a38e77"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-1b9d022273780c5b0b7522555bd0e2c626a38e77", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "Underwater images are known to be strongly deteriorated by a combination of wavelength-dependent light attenuation and scattering. This results in complex color casts that depend both on the scene depth map and on the light spectrum. Color transfer, which is a technique of choice to counterbalance color casts, assumes stationary casts, defined by global parameters, and is therefore not directly applicable to the locally variable color casts encountered in underwater scenarios. To fill this gap, this paper introduces an original fusion-based strategy to exploit color transfer while tuning the color correction locally, as a function of the light attenuation level estimated from the red channel. The Dark Channel Prior (DCP) is then used to restore the color compensated image, by inverting the simplified Koschmieder light transmission model, as for outdoor dehazing. Our technique enhances image contrast in a quite effective manner and also supports accurate transmission map estimation. Our extensive experiments also show that our color correction strongly improves the effectiveness of local keypoints matching."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "549fc15ee760ceb7569c38888b21cee1c3806148"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-549fc15ee760ceb7569c38888b21cee1c3806148", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "Any motor action results from a dynamic interplay of various brain regions involved in different aspects of movement preparation and execution. Establishing a reliable model of how these areas interact is crucial for a better understanding of the mechanisms underlying motor function in both healthy subjects and patients. We used fMRI and dynamic causal modeling to reveal the specific excitatory and inhibitory influences within the human motor system for the generation of voluntary hand movements. We found an intrinsic balance of excitatory and inhibitory couplings among core motor regions within and across hemispheres. Neural coupling within this network was specifically modulated upon uni- and bimanual movements. During unimanual movements, connectivity towards the contralateral primary motor cortex was enhanced while neural coupling towards ipsilateral motor areas was reduced by both transcallosal inhibition and top-down modulation. Bimanual hand movements were associated with a symmetric facilitation of neural activity mediated by both increased intrahemispheric connectivity and enhanced transcallosal coupling of SMA and M1. The data suggest that especially the supplementary motor area represents a key structure promoting or suppressing activity in the cortical motor network driving uni- and bilateral hand movements. Our data demonstrate that fMRI in combination with DCM allows insights into intrinsic properties of the human motor system and task-dependent modulations thereof."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "c6c95c996037c00c62df1d3d2cfb3e010a317faf"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-c6c95c996037c00c62df1d3d2cfb3e010a317faf", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "This paper provides an overview of DC side fault tolerance issues of VSC based HVDC system and the need for fault tolerant converters. The working principle and DC fault ride through capability of recently introduced Alternate Arm Modular Multilevel Converter(AAMMC) has been discussed. The capacitor voltage balancing issues of AAMMC is analyzed and a novel scheme for balancing capacitor voltages of the wave shaping circuit is presented in this paper. The voltage balancing of capacitors of wave shaping circuits in the arm is done by introducing an overlap period during zero voltage period. Using the proposed scheme, the magnitude and direction of the current during the overlap period can be controlled by varying the switching pattern. It helps in charging or discharging of the submodule capacitors to bring them to their reference value. At the end of the overlap period, the arm current is brought to zero before opening the director switch so as to avoid the spike across the arm inductor. The efficacy of the proposed control scheme has been validated using simulation study done in PSCAD/EMTDC."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "35bc3b88d20098869a2e5cdb8cb83ed926627af0"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-35bc3b88d20098869a2e5cdb8cb83ed926627af0", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "With the recent advance of wearable devices and Internet of Things (IoTs), it becomes attractive to implement the Deep Convolutional Neural Networks (DCNNs) in embedded and portable systems. Currently, executing the software-based DCNNs requires high-performance servers, restricting the widespread deployment on embedded and mobile IoT devices. To overcome this obstacle, considerable research efforts have been made to develop highly-parallel and specialized DCNN accelerators using GPGPUs, FPGAs or ASICs.\n Stochastic Computing (SC), which uses a bit-stream to represent a number within [-1, 1] by counting the number of ones in the bit-stream, has high potential for implementing DCNNs with high scalability and ultra-low hardware footprint. Since multiplications and additions can be calculated using AND gates and multiplexers in SC, significant reductions in power (energy) and hardware footprint can be achieved compared to the conventional binary arithmetic implementations. The tremendous savings in power (energy) and hardware resources allow immense design space for enhancing scalability and robustness for hardware DCNNs.\n This paper presents SC-DCNN, the first comprehensive design and optimization framework of SC-based DCNNs, using a bottom-up approach. We first present the designs of function blocks that perform the basic operations in DCNN, including inner product, pooling, and activation function. Then we propose four designs of feature extraction blocks, which are in charge of extracting features from input feature maps, by connecting different basic function blocks with joint optimization. Moreover, the efficient weight storage methods are proposed to reduce the area and power (energy) consumption. Putting all together, with feature extraction blocks carefully selected, SC-DCNN is holistically optimized to minimize area and power (energy) consumption while maintaining high network accuracy. Experimental results demonstrate that the LeNet5 implemented in SC-DCNN consumes only 17 mm2 area and 1.53 W power, achieves throughput of 781250 images/s, area efficiency of 45946 images/s/mm2, and energy efficiency of 510734 images/J."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "5ad3c058535653b1c898302ffa42d5dccee542e3"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-5ad3c058535653b1c898302ffa42d5dccee542e3", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "In this paper we propose a convolutional neural network (CNN), which allows to identify corresponding patches of very high resolution (VHR) optical and SAR imagery of complex urban scenes. Instead of a siamese architecture as conventionally used in CNNs designed for image matching, we resort to a pseudo-siamese configuration with no interconnection between the two streams for SAR and optical imagery. The network is trained with automatically generated training data and does not resort to any hand-crafted features. First evaluations show that the network is able to predict corresponding patches with high accuracy, thus indicating great potential for further development to a generalized multi-sensor matching procedure."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "0690ba31424310a90028533218d0afd25a829c8d"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-0690ba31424310a90028533218d0afd25a829c8d", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called ”semantic image segmentation”). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our “DeepLab” system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the ’hole’ algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "no"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-no", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "5a778a2a32f35a96f4dfb0f22d1415eff321e7ad"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-5a778a2a32f35a96f4dfb0f22d1415eff321e7ad", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "Due to their rapid evolution, mobile devices demand for more dynamic and flexible networking services. A major challenges of future mobile networks is the increased mobile traffic. With the recent upcoming technologies of network programmability like Software-Defined Network (SDN), it may be integrated to create a new communication platform for Internet of Things (IoT). In this work, we present how to determine the effectiveness of an approach to build a new secured network architecture based on SDN and clusters. Our proposed scheme is a starting point for some experiments providing perspective over SDN deployment in a cluster environment. With this aim in mind, we suggest a routing protocol that manages routing tasks over Cluster-SDN. By using network virtualization and OpenFlow technologies to generate virtual nodes, we simulate a prototype system controlled by SDN. With our testbed, we are able to manage 500 things. We can analyze every OpenFlow messages and we have discovered that with a particular flow, the things can exchange information unlike the routing principle."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "90e3ec000125d579ec1724781410d4201be6d2a8"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-90e3ec000125d579ec1724781410d4201be6d2a8", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "This paper presents the study of different communication systems between IEC 61850 based distribution substation and distributed energy resources (DERs). Communication networks have been simulated for a typical distribution automation system (DAS) with DERs using OPNET software. The simulation study shows the performance of wired and wireless communication systems for different messages, such as GOOSE and measured (metered) values between DAS and DERs. A laboratory set-up has been implemented using commercial relay and communication devices for evaluating the performance of GOOSE messages, using wired and wireless physical medium. Finally, simulation and laboratory results are discussed in detail."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "90ed29e10e65c0fa13b64903eeba0fef1ef3cc60"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-90ed29e10e65c0fa13b64903eeba0fef1ef3cc60", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "In this paper, we consider a robust network control problem. We consider linear unstable and uncertain discrete time plants with a network between the sensors and controller and the controller and plant. We investigate two defining characteristics of network controlled systems and the impact of uncertainty on these. Namely, the minimum data rates required for the two networks and the tolerable data drop out in the form of packet losses. We are able to derive sufficient conditions in terms of the minimum data rate and minimum packet arrival rate to ensure stability of the closed loop system. I. I NTRODUCTION Recently, networked control systems (NCS) have gained great attention from both the control community and the network and communication community. When compared with classical feedback control system, networked control systems have many advantages. For example, they can reduce the system wiring, make the system easy to operate and maintain and later diagnose in case of malfunctioning, and increase system agility [16]. In spite of the great advantages that the networked control architecture brings, inserting a network in between the plant and the controller introduces many problems as well. For instance, zero-delayed sensing and actuation, perfect information and synchronization are no longer guaranteed in the new system architecture as only finite bandwidth is available and packet drops and delays may occur due to network traffic conditions. These must be revisited and analyzed before any practical networked control systems are built. In the past decade, many researchers have spent effort on those issues and a number of significant results were obtained and many are in progress. Many of the aforementioned issues are studied separately. Tatikonda [15] and Sahai [11] have presented some interesting results in the area of control under communication constraints. Specifically, Tatikonda gave a necessary and sufficient condition on the channel data rate such that a noiseless LTI system in the closed loop is asymptotically stable. He also gave rate results for stabilizing a noisy LTI system over the digital channel. Sahai proposed the notion of anytime capacity to deal with real time estimation and control for a networked control system. In our companion paper [13], the authors have considered various rate issues under finite bandwidth, packet drops and finite controls. The effect of pacekt loss and delay on state Control and Dynamical Systems, California Institute of Technology; Pasadena, CA 91125. Email:{shiling, epstein, murray }@cds.caltech.edu; Tel: (626) 395-2313, Fax: (626) 796-8914. Work supported in part by NSF ITR grant CCR-0326554. The authors are equally contributed to this work. estimation was studied by the work of Sinopoli, et. al. in [2]. It has further been investigated by many researchers including the present authors in [12] and [5]. One of the hallmarks of a good control system design is that the closed loop remain stable in the presence of uncertainty [3], [4]. While the researchers in [7] studied the problem of LQG control across a packet dropping networks, not many have considered the norm bounded uncertainty investigated in the present paper. We examine the impact of a norm bounded uncertainty on the network control system and provide sufficient conditions for stability in terms of the minimum data rates and packet arrival rates for the networks. The paper is organized as follows. In Section II, we present the mathematical model of the closed loop system and state our assumptions. In Section III, we state the sufficient conditions for closed loop stability for the case where a network connects the sensors to the controller. In Section IV, we state the sufficient stability conditions where in addition there is a network between the controller and the plant. For both sections we obtain results for scalar and general vector cases. Conclusions and future work are given in the last section. II. PROBLEM SET UP We consider linear discrete time systems with a norm bounded uncertainty in the A matrix. We will investigate two NCS that we will define by the type of networks embedded in the control loop. The first NCS considered has a network between the measurement sensors and the controller, with the controller then directly connected to the actuators/plant. The second NCS will also include a network between the controller and the actuators/plant. These two network types and depicted in Figures 1 and 2. The networks are defined in terms of their data rates and probability of dropping packets. We would consider any packet delays as losses, i. ., we do not use delayed packets for estimation or control. The following equations represent the closed loop system for NCS I (Figure 1). xk+1 = (A + ∆k)xk + Buk (1) yk = λkCxk (2) wherexk ∈ IR is the state of the system, uk ∈ IR is the control input,yk ∈ IR is the output of the system, and λk are Bernoulli i.i.d random variable with parameter λ, i.e., E[λk] = λ for all k. ∆k satisfies∆k ∆k ≤ KI for all k. We also assume the initial condition x0 ∈ IR is bounded. The matrix A is assumed to be unstable without loss of generality as for any matrix A, we can always do some state transformation to decompose the states into stable ones and"}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "9d68bcc77f953c3ae24047b8c83b7172646845d8"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-9d68bcc77f953c3ae24047b8c83b7172646845d8", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "This paper proposes a self-organized power allocation technique to solve the interference problem caused by a femtocell network operating in the same channel as an orthogonal frequency division multiple access cellular network. We model the femto network as a multi-agent system where the different femto base stations are the agents in charge of managing the radio resources to be allocated to their femtousers. We propose a form of real-time multi-agent reinforcement learning, known as decentralized Q-learning, to manage the interference generated to macro-users. By directly interacting with the surrounding environment in a distributed fashion, the multi-agent system is able to learn an optimal policy to solve the interference problem. Simulation results show that the introduction of the femto network increases the system capacity without decreasing the capacity of the macro network."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "70624f16968fe4f0c851398dbd46a1ebcce892ce"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-70624f16968fe4f0c851398dbd46a1ebcce892ce", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "In vehicular ad hoc network (VANET), vehicles equipped with computing, sensing, and communication capabilities can exchange information within a geographical area to distribute emergency messages and achieve safety system. Then how to enforce fine grained control of these messages and ensure the receiving messages coming from the claimed source in such a highly dynamic environments remains a key challenge that affects the quality of service. In this paper, we propose a hierarchical access control with authentication scheme for transmitted messages with security assurance over VANET. By extending ciphertext-policy attribute-based encryption (CP-ABE) with a hierarchical structure of multiple authorities, the scheme not only achieves scalability due to its hierarchical structure, but also inherits fine-grained access control on the transmitted messages. Also by exploiting attribute-based signature (ABS), the scheme can authorize the vehicles that can most appropriately deal with the message efficiently. The results of efficiency analysis and comparison with the related works show that the proposed scheme is efficient and scalable in dealing with access control and message authentication for data dissemination in VANET. & 2014 Elsevier B.V. All rights reserved."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "051853e79d6ebe49601348536ca4b14c5279cc97"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-051853e79d6ebe49601348536ca4b14c5279cc97", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "Microgrids are a key component in the evolution of the power grid. Microgrids are required to operate in both grid connected and standalone island mode using local sources of power. A major challenge in implementing microgrids is the communications and control to support transition from grid connected mode and operation in island mode. Here, we propose a secure communication architecture to support microgrid operation and control. A security model, including network, data, and attack models, is defined and a security protocol to address the real-time communication needs of microgrids is proposed. The implementation of the proposed security scheme is discussed and its performance evaluated using theoretical and co-simulation analysis, which shows it to be superior to existing protocols."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "392c43f9e521b829d9d5b7d072e4bd7f2bcfbe8a"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-392c43f9e521b829d9d5b7d072e4bd7f2bcfbe8a", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "This work is concerned with the control strategy for the parallel operation of distributed generation systems (DGS) in a standalone ac power supply. The proposed control method uses only low-bandwidth data communication signals between each generation system in addition to the locally measurable feedback signals. This is achieved by combining two control methods: droop control method and average power control method. The average power method with slow update rate is used in order to overcome the sensitivity about voltage and current measurement errors. In addition, a harmonic droop scheme for sharing harmonic content of the load currents is proposed based on the voltages and currents control algorithm. Experimental and simulation studies using two parallel three-phase pulsewidth modulation (PWM) inverters are presented to show the effectiveness of the proposed control."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "53bf890ddba4d6433e868c0a73a529243c23591c"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-53bf890ddba4d6433e868c0a73a529243c23591c", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "Network Service Providers should offer provisioning services with guaranteed Quality of Service (QoS), specifically adapted to the characteristics of the applications running on their network. In this paper we propose the Network Control Layer (NCL), a software framework solution based on Software Defined Networks (SDN), OpenFlow and Network as a Service (NaaS) paradigms. It addresses a major innovation area in the field of network control and management providing on-demand end-to-end network provisioning services with guaranteed QoS, based on the specific requirements of on-top running interactive applications. The NCL implementation is based on the OpenNaaS framework, and it includes mechanisms for network status monitoring and SDN switches configuration based on the interactive applications' QoS network requirements. We demonstrate NCL's utility in the context of control plane models making use of a practical use case."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "b1ca425cab859aa04259f0a093b7c948abd0e630"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-b1ca425cab859aa04259f0a093b7c948abd0e630", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "In this paper, we investigate a communication system in which unmanned aerial vehicles (UAVs) are used as relays between ground-based terminals and a network base station. We develop an algorithm for optimizing the performance of the ground-to-relay links through control of the UAV heading angle. To quantify link performance, we define the ergodic normalized transmission rate (ENTR) for the links between the ground nodes and the relay, and derive a closed-form expression for it in terms of the eigenvalues of the channel correlation matrix. We show that the ENTR can be approximated as a sinusoid with an offset that depends on the heading of the UAV. Using this observation, we develop a closed-form expression for the UAV heading that maximizes the uplink network data rate while keeping the rate of each individual link above a certain threshold. When the current UAV relay assignments cannot meet the minimum link requirements, we investigate the deployment and heading control problem for new UAV relays as they are added to the network, and propose a smart handoff algorithm that updates node and relay assignments as the topology of the network evolves."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "7826bc81ffed9c1f342616df264c92d6f732f4dd"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-7826bc81ffed9c1f342616df264c92d6f732f4dd", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "Autonomic communications aim to provide the quality-of-service in networks using self-management mechanisms. It inherits many characteristics from autonomic computing, in particular, when communication systems are running as specialized applications in software-defined networking (SDN) and network function virtualization (NFV)-enabled cloud environments. This paper surveys autonomic computing and communications in the context of software-driven networks, i.e., networks based on SDN/NFV concepts. Autonomic communications create new challenges in terms of security, operations, and business support. We discuss several goals, research challenges, and development issues on self-management mechanisms and architectures in software-driven networks. This paper covers multiple perspectives of autonomic communications in software-driven networks, such as automatic testing, integration, and deployment of network functions. We also focus on self-management and optimization, which make use of machine learning techniques."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "no"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-no", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "5c6b51bb44c9b2297733b58daaf26af01c98fe09"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-5c6b51bb44c9b2297733b58daaf26af01c98fe09", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "The paper systematically compares two feature extraction algorithms to mine product features commented on in customer reviews. The first approach [17] identifies candidate features by applying a set of POS patterns and pruning the candidate set based on the log likelihood ratio test. The second approach [11] applies association rule mining for identifying frequent features and a heuristic based on the presence of sentiment terms for identifying infrequent features. We evaluate the performance of the algorithms on five product specific document collections regarding consumer electronic devices. We perform an analysis of errors and discuss advantages and limitations of the algorithms."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "fa46ae777be8776a417a24e0b6c3f6076c5e578d"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-fa46ae777be8776a417a24e0b6c3f6076c5e578d", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "Detection and recognition of text from any natural scene image is challenging but essential extensively for extracting information from the image. In this paper, we propose an accurate and effective algorithm for detecting enhanced Maximally Stable Extremal Regions (MSERs) as main character candidates and these character candidates are filtered by stroke width variation for removing regions where the stroke width exhibits too much variation. For the detection of text regions, firstly some preprocessing is applied to the natural image and then after detecting MSERs, an intersection of canny edge and MSER region is produced to locate regions that are even more likely to belong to text. Finally, the selected text region is taken as an input of a novel Optical Character Recognition (OCR) technique to make the text editable and usable. The evaluation results substantiates 77.47% of the f-measure on the ICDAR 2011 dataset which is better than the previous performance 76.22%."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "caec97674544a4948a1b0ec2b9f6c624b87b647b"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-caec97674544a4948a1b0ec2b9f6c624b87b647b", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "This paper analyzes, compares, and contrasts technical challenges, methods, and the performance of text detection and recognition research in color imagery. It summarizes the fundamental problems and enumerates factors that should be considered when addressing these problems. Existing techniques are categorized as either stepwise or integrated and sub-problems are highlighted including text localization, verification, segmentation and recognition. Special issues associated with the enhancement of degraded text and the processing of video text, multi-oriented, perspectively distorted and multilingual text are also addressed. The categories and sub-categories of text are illustrated, benchmark datasets are enumerated, and the performance of the most representative approaches is compared. This review provides a fundamental comparison and analysis of the remaining problems in the field."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "7f44973b8cb78be47d55d335f40a54aa00ef814c"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-7f44973b8cb78be47d55d335f40a54aa00ef814c", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "This paper proposes a novel method for offline text-independent writer identification by using convolutional neural network (CNN) and joint Bayesian, which consists of two stages, i.e. feature extraction and writer identification. In the stage of feature extraction, since a large number of data is essential to train an effective CNN model with high generalizability and the amount of handwriting is limited in writer identification, a data augmentation technique is first developed to generate thousands of handwriting images for each writer. Then a deep CNN network is designed to extract discriminative features to represent the properties of different writing styles, which is trained by using the generated handwriting images. In the stage of writer identification, the training dataset is used to train the CNN model for feature extraction and the joint Bayesian technique is employed to accomplish the task of writer identification based on the extracted CNN features. The proposed method is tested on two standard benchmark datasets, i.e. ICDAR2013 and CVL dataset. Experimental results demonstrate that the proposed method gets the best performance compared to the state-of-the-art approaches."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "e78a8ff7b0adac4f6255dd999342c85f6a28e2f0"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-e78a8ff7b0adac4f6255dd999342c85f6a28e2f0", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "This paper presents an OCR method for degraded character recognition applied to a reference number (RN) of 15 printed characters of an invoice document produced by dot-matrix printer. First, the paper deals with the problem of the reference number localization and extraction, in which the characters tops or bottoms are or not touched with a printed reference line of the electrical bill. In case of touched RN, the extracted characters are severely degraded leading to missing parts in the characters tops or bottoms. Secondly, a combined recognition method based on the complementary similarity measure (CSM) method and MLP-based classifier is used. The CSM is used to accept or reject an incoming character. In case of acceptation, the CSM acts as a feature extractor and produces a feature vector of ten component features. The MLP is then trained using these feature vectors. The use of the CSM as a feature extractor tends to make the MLP very powerful and very well suited for rejection. Experimental results on electrical bills show the ability of the model to yield relevant and robust recognition on severely degraded printed characters."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "5040c699cc3a02d8dd2eecfdb20e5690432ad7a5"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-5040c699cc3a02d8dd2eecfdb20e5690432ad7a5", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "As the number of customer reviews grows very rapidly, it is essential to summarize useful opinions for buyers, sellers and producers. One key step of opinion mining is feature extraction. Most existing research focus on finding explicit features, only a few attempts have been made to extract implicit features. Nearly all existing research only concentrate on product features, few has paid attention to other features that relates to sellers, services and logistics. Therefore in this paper, we propose a novel co-occurrence association-based method, which aims to extract implicit features in customer reviews and provide more comprehensive and fine-grained mining results."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "a79f43246bed540084ca2d1fcf99a68c69820747"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-a79f43246bed540084ca2d1fcf99a68c69820747", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "Text detection and localization in natural scene images is important for content-based image analysis. This problem is challenging due to the complex background, the non-uniform illumination, the variations of text font, size and line orientation. In this paper, we present a hybrid approach to robustly detect and localize texts in natural scene images. A text region detector is designed to estimate the text existing confidence and scale information in image pyramid, which help segment candidate text components by local binarization. To efficiently filter out the non-text components, a conditional random field (CRF) model considering unary component properties and binary contextual component relationships with supervised parameter learning is proposed. Finally, text components are grouped into text lines/words with a learning-based energy minimization method. Since all the three stages are learning-based, there are very few parameters requiring manual tuning. Experimental results evaluated on the ICDAR 2005 competition dataset show that our approach yields higher precision and recall performance compared with state-of-the-art methods. We also evaluated our approach on a multilingual image dataset with promising results."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "08fddf1865e48a1adc21d4875396a754711f0a28"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-08fddf1865e48a1adc21d4875396a754711f0a28", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "Machine learning for text classification is the cor nerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is essential to make the learning task efficient and m ore accurate. This paper presents an empirical comparison of twelve feature selection methods (e.g . Information Gain) evaluated on a benchmark of 229 text classification problem instances that w ere gathered from Reuters, TREC, OHSUMED, etc. The results are analyzed from multiple goal p ers ectives—accuracy, F-measure, precision, and recall—since each is appropriate in different si tuat ons. The results reveal that a new feature selection me tric we call ‘Bi-Normal Separation’ (BNS), outperformed the others by a substantial margin in most situations. This margin widened in tasks with high class skew, which is rampant in text clas sification problems and is particularly challenging for induction algorithms. A new evaluation methodology is offered that focus es on the needs of the data mining practitioner faced with a single dataset who seeks to choose one (or a pair of) metrics that are most likely to yield the best performance. From this perspect iv , BNS was the top single choice for all goals except precision, for which Information Gain yielded the best result most often. This analysis also revealed, for example, that Informati on Gain and Chi-Squared have correlated failures, and so they work poorly together. When c hoosing optimal pairs of metrics for each of the four performance goals, BNS is consistently a membe r of the pair—e.g., for greatest recall, the pair BNS + F1-measure yielded the best performance on the greatest number of tasks by a considerable margin."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "68e47fea6d61acbf0b058a963e42228a4e3f07af"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-68e47fea6d61acbf0b058a963e42228a4e3f07af", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "Programming community-based question-answering (PCQA) websites such as Stack Overflow enable programmers to find working solutions to their questions. Despite detailed posting guidelines, duplicate questions that have been answered are frequently created. To tackle this problem, Stack Overflow provides a mechanism for reputable users to manually mark duplicate questions. This is a laborious effort, and leads to many duplicate questions remain undetected. Existing duplicate detection methodologies from traditional community based question-answering (CQA) websites are difficult to be adopted directly to PCQA, as PCQA posts often contain source code which is linguistically very different from natural languages. In this paper, we propose a methodology designed for the PCQA domain to detect duplicate questions. We model the detection as a classification problem over question pairs. To extract features for question pairs, our methodology leverages continuous word vectors from the deep learning literature, topic model features and phrases pairs that co-occur frequently in duplicate questions mined using machine translation systems. These features capture semantic similarities between questions and produce a strong performance for duplicate detection. Experiments on a range of real-world datasets demonstrate that our method works very well; in some cases over 30% improvement compared to state-of-the-art benchmarks. As a product of one of the proposed features, the association score feature, we have mined a set of associated phrases from duplicate questions on Stack Overflow and open the dataset to the public."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "0621213a012d169cb7c2930354c6489d6a89baf8"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-0621213a012d169cb7c2930354c6489d6a89baf8", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "In this paper, we define and study a novel text mining problem, which we refer to as Comparative Text Mining (CTM). Given a set of comparable text collections, the task of comparative text mining is to discover any latent common themes across all collections as well as summarize the similarity and differences of these collections along each common theme. This general problem subsumes many interesting applications, including business intelligence and opinion summarization. We propose a generative probabilistic mixture model for comparative text mining. The model simultaneously performs cross-collection clustering and within-collection clustering, and can be applied to an arbitrary set of comparable text collections. The model can be estimated efficiently using the Expectation-Maximization (EM) algorithm. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method also based on a mixture model. Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model."}
