{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "no"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-no", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "802b80852996d87dc16082b86f6e77115eb6c9a6"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-802b80852996d87dc16082b86f6e77115eb6c9a6", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "In this article, a methodology to extract Flash EEPROM memory contents is presented. Samples are first backside prepared to expose the tunnel oxide of floating gate transistors. Then, a Scanning Electron Microscope (SEM) in the so called Passive Voltage Contrast (PVC) mode allows distinguishing ‘0’ and ‘1’ bit values stored in individual memory cell. Using SEM operator-free acquisition and standard image processing technique we demonstrate the possible automating of such technique over a full memory. The presented fast, efficient and low cost technique is successfully implemented on 0.35μm technology node microcontrollers and on a 0.21μm smart card type integrated circuit. The technique is at least two orders of magnitude faster than state-of-the-art Scanning Probe Microscopy (SPM) methods. Without adequate protection an adversary could obtain the full memory array content within minutes. The technique is a first step for reverse engineering secure embedded systems."}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "a0fd813b9218813e1b020d03a3099de7677dd145"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-a0fd813b9218813e1b020d03a3099de7677dd145", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "Manipulation in micro or nanoscale with robotic manipulators under observation of electron microscopes is a widely used strategy for fabrication of nanodevices and nanoscale material property characterization. These types of manipulation systems can handle the relatively larger scale of objects. However, the complexity of manipulation increases highly for 3D manipulation. Since the manipulation system consists of multiple components including manipulator, microscope, and also some end-effector tools, a proper offline visualization of the system is necessary for operation. Therefore, we propose a web-based virtual interface between the user and the actual manipulator operated under digital microscope initially. It gives the operator 3D positional feedback from the virtual model by mapping data read during remote operation. The same interface is used for remote operation of the manipulator within the SEM chamber and a manipulation task is performed."}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "5de3ba76eeead6a6ee3295220080ee881f84bd27"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-5de3ba76eeead6a6ee3295220080ee881f84bd27", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "The objective of this paper is to propose a new homography-based approach to image-based visual tracking and servoing. The visual tracking algorithm proposed in the paper is based on a new efficient second-order minimization method. Theoretical analysis and comparative experiments with other tracking approaches show that the proposed method has a higher convergence rate than standard first-order minimization techniques. Therefore, it is well adapted to real-time robotic applications. The output of the visual tracking is a homography linking the current and the reference image of a planar target. Using the homography, a task function isomorphic to the camera pose has been designed. A new image-based control law is proposed which does not need any measure of the 3D structure of the observed target (e.g. the normal to the plane). The theoretical proof of the existence of the isomorphism between the task function and the camera pose and the theoretical proof of the stability of the control law are provided. The experimental results, obtained with a 6 d.o.f. robot, show the advantages of the proposed method with respect to the existing approaches. KEY WORDS—visual tracking, visual servoing, efficient second-order minimization, homography-based control law"}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "4ee0ad8e256523256c8d21790189388ed4beca7e"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-4ee0ad8e256523256c8d21790189388ed4beca7e", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "PRNU-based techniques guarantee a good forgery detection performance irrespective of the specific type of forgery. The presence or absence of the camera PRNU pattern is detected by a correlation test. Given the very low power of the PRNU signal, however, the correlation must be averaged over a pretty large window, reducing the algorithm's ability to reveal small forgeries. To improve resolution, we estimate correlation with a spatially adaptive filtering technique, with weights computed over a suitable pilot image. Implementation efficiency is achieved by resorting to the recently proposed guided filters. Experiments prove that the proposed filtering strategy allows for a much better detection performance in the case of small forgeries."}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "c6dac8aca55dc7326e5cb996b386db5bce4da46e"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-c6dac8aca55dc7326e5cb996b386db5bce4da46e", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "Point-of-Interest (POI) recommendation has received increasing attention in Location-based Social Networks (LBSNs). It involves user behavior analysis, movement pattern model and trajectory sequence prediction, in order to recommend personalized services to target user. Existing POI recommendation methods are confronted with three problems: (1) they only consider the location information of users' check-ins, which causes data sparsity; (2) they fail to consider the order of users' visited locations, which is valuable to reflect the interest or preference of users; (3) users cannot be recommended the suitable services when they move into the new place. To address the above issues, we propose a semantical pattern and preference-aware service mining method called SEM-PPA to make full use of the semantic information of locations for personalized POI recommendation. In SEM-PPA, we firstly propose a novel algorithm to classify the locations into different types for location identification; then we construct the user model for each user from four aspects, which are location trajectory, semantic trajectory, location popularity and user familiarity; in addition, a potential friends discovery algorithm based on movement pattern is proposed. Finally, we conduct extensive experiments to evaluate the recommendation accuracy and recommendation effectiveness on two real-life datasets from GeoLife and Beijing POI. Experimental results show that SEM-PPA can achieve better recommendation performance in particular for sparse data and recommendation accuracy in comparison with other methods."}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "343cc181987202cf4b98e61738d0b310927c1fcf"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-343cc181987202cf4b98e61738d0b310927c1fcf", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "The substrate integrated waveguide (SIW) technique makes it possible that a complete circuit including planar circuitry, transitions, and rectangular waveguides are fabricated in planar form using a standard printed circuit board or other planar processing techniques. In this paper, guided wave and modes characteristics of such an SIW periodic structure are studied in detail for the first time. A numerical multimode calibration procedure is proposed and developed with a commercial software package on the basis of a full-wave finite-element method for the accurate extraction of complex propagation constants of the SIW structure. Two different lengths of the SIW are numerically simulated under multimode excitation. By means of our proposed technique, the complex propagation constant of each SIW mode can accurately be extracted and the electromagnetic bandstop phenomena of periodic structures are also investigated. Experiments are made to validate our proposed technique. Simple design rules are provided and discussed."}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "a65476a4fadd112b64f519a6f51a71f6077ed0ae"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-a65476a4fadd112b64f519a6f51a71f6077ed0ae", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "We report a monolithic wafer-level fabrication method for a hemispherical reflector coupled light-emitting-diode (LED) array using isotropic etching of silicon. These neural stimulators collect the backside as well as the front side emission of the μ-LEDs and thus provide higher intensity, which is imperative for opsin expressions in optogenetics experiments. Aluminum was used as the reflective layer and the planarization of polymer on the reflector cavity was done using polydimethylsiloxane (PDMS). The lateral and vertical profiles of silicon etching were measured and the light intensity increase due to the reflector was investigated. It was found that the intensity increases by a minimum of 49% and maximum of 65% when coupling a reflector with the μ-LEDs."}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "d8354a6d188a9bfca01586a5467670650f3e3a8a"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-d8354a6d188a9bfca01586a5467670650f3e3a8a", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "The next generation of implantable high-power neuroprosthetic devices such as visual prostheses and brain computer interfaces are going to be powered by transcutaneous inductive power links formed between a pair of printed spiral coils (PSC) that are batch-fabricated using micromachining technology. Optimizing the power efficiency of the wireless link is imperative to minimize the size of the external energy source, heating dissipation in the tissue, and interference with other devices. Previous design methodologies for coils made of 1-D filaments are not comprehensive and accurate enough to consider all geometrical aspects of PSCs with planar 3-D conductors as well as design constraints imposed by implantable device application and fabrication technology. We have outlined the theoretical foundation of optimal power transmission efficiency in an inductive link, and combined it with semi-empirical models to predict parasitic components in PSCs. We have used this foundation to devise an iterative PSC design methodology that starts with a set of realistic design constraints and ends with the optimal PSC pair geometries. We have executed this procedure on two design examples at 1 and 5 MHz achieving power transmission efficiencies of 41.2% and 85.8%, respectively, at 10-mm spacing. All results are verified with simulations using a commercial field solver (HFSS) as well as measurements using PSCs fabricated on printed circuit boards."}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "58a64cc6a1dd8269ab19b9de271e202ab3e6de92"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-58a64cc6a1dd8269ab19b9de271e202ab3e6de92", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "We present in this paper the development of a high-resolution projection micro-stereolithography (P SL) process by using the Digital Micromirror Device (DMDTM, Texas Instruments) as a dynamic mask. This unique technology provides a parallel fabrication of complex three-dimensional (3D) microstructures used for micro electro-mechanical systems (MEMS). Based on the understanding of underlying mechanisms, a process model has been developed with all critical parameters obtained from the experimental measurement. By coupling the experimental measurement and the process model, the photon-induced curing behavior of the resin has been quantitatively studied. The role o erty of the r n d ©"}
{"metadata": {"dataset": "scidocs", "query_id": "01273bd34dacfe9ef887b320f36934d2f9fa9b34", "doc_id": "122374a3baf1e0efde03301226344a2d728eafc3"}, "id": "scidocs-01273bd34dacfe9ef887b320f36934d2f9fa9b34-122374a3baf1e0efde03301226344a2d728eafc3", "question": "Image-Guided Nanopositioning Scheme for SEM", "context": "PURPOSE\nThe purpose of this study is to investigate the feasibility of increasing the system spatial resolution and scanning speed of Hologic Selenia Dimensions digital breast tomosynthesis (DBT) scanner by replacing the rotating mammography x-ray tube with a specially designed carbon nanotube (CNT) x-ray source array, which generates all the projection images needed for tomosynthesis reconstruction by electronically activating individual x-ray sources without any mechanical motion. The stationary digital breast tomosynthesis (s-DBT) design aims to (i) increase the system spatial resolution by eliminating image blurring due to x-ray tube motion and (ii) reduce the scanning time. Low spatial resolution and long scanning time are the two main technical limitations of current DBT technology.\n\n\nMETHODS\nA CNT x-ray source array was designed and evaluated against a set of targeted system performance parameters. Simulations were performed to determine the maximum anode heat load at the desired focal spot size and to design the electron focusing optics. Field emission current from CNT cathode was measured for an extended period of time to determine the stable life time of CNT cathode for an expected clinical operation scenario. The source array was manufactured, tested, and integrated with a Selenia scanner. An electronic control unit was developed to interface the source array with the detection system and to scan and regulate x-ray beams. The performance of the s-DBT system was evaluated using physical phantoms.\n\n\nRESULTS\nThe spatially distributed CNT x-ray source array comprised 31 individually addressable x-ray sources covering a 30 angular span with 1 pitch and an isotropic focal spot size of 0.6 mm at full width at half-maximum. Stable operation at 28 kV(peak) anode voltage and 38 mA tube current was demonstrated with extended lifetime and good source-to-source consistency. For the standard imaging protocol of 15 views over 14, 100 mAs dose, and 2 × 2 detector binning, the projection resolution along the scanning direction increased from 4.0 cycles/mm [at 10% modulation-transfer-function (MTF)] in DBT to 5.1 cycles/mm in s-DBT at magnification factor of 1.08. The improvement is more pronounced for faster scanning speeds, wider angular coverage, and smaller detector pixel sizes. The scanning speed depends on the detector, the number of views, and the imaging dose. With 240 ms detector readout time, the s-DBT system scanning time is 6.3 s for a 15-view, 100 mAs scan regardless of the angular coverage. The scanning speed can be reduced to less than 4 s when detectors become faster. Initial phantom studies showed good quality reconstructed images.\n\n\nCONCLUSIONS\nA prototype s-DBT scanner has been developed and evaluated by retrofitting the Selenia rotating gantry DBT scanner with a spatially distributed CNT x-ray source array. Preliminary results show that it improves system spatial resolution substantially by eliminating image blur due to x-ray focal spot motion. The scanner speed of s-DBT system is independent of angular coverage and can be increased with faster detector without image degration. The accelerated lifetime measurement demonstrated the long term stability of CNT x-ray source array with typical clinical operation lifetime over 3 years."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "no"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-no", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "af77547dc79c67367675e76f28c3bbf3032a9a12"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-af77547dc79c67367675e76f28c3bbf3032a9a12", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "In mobile cloud computing, mobile devices can rely on cloud computing and information storage resource to perform computationally intensive operations such as searching, data mining, and multimedia processing. In addition to providing traditional computation services, mobile cloud also enhances the operation of traditional ad hoc network by treating mobile devices as service nodes, e.g., sensing services. The sensed information, such as location coordinates, health related information, should be processed and stored in a secure fashion to protect user's privacy in the cloud. To this end, we present a new mobile cloud data processing framework through trust management and private data isolation. Finally, an implementation pilot for improving teenagers' driving safety, which is called FocusDrive, is presented to demonstrate the solution."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "f9823bc7eec44a9a6cd7b629c8f6430fe82877fd"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-f9823bc7eec44a9a6cd7b629c8f6430fe82877fd", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "Cloud computing provides convenient on-demand network access to a shared pool of configurable computing resources. The resources can be rapidly deployed with great efficiency and minimal management overhead. Cloud is an insecure computing platform from the view point of the cloud users, the system must design mechanisms that not only protect sensitive information by enabling computations with encrypted data, but also protect users from malicious behaviours by enabling the validation of the computation result. In this paper, we propose a new data encoding scheme called layered interleaving, designed for time-sensitive packet recovery in the presence of bursty loss. It is high-speed data recovery scheme with minimal loss probability and using a forward error correction scheme to handle bursty loss. The proposed approach is highly efficient in recovering the singleton losses almost immediately and from bursty data losses."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "de33698f7b2264bf7313f43d1de8c2d19e2a2f7a"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-de33698f7b2264bf7313f43d1de8c2d19e2a2f7a", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "Abstract. Mobile cloud computing has emerged aiming at assisting mobile devices in processing computationally or data intensive tasks using cloud resources. This paper presents an optimization approach for utilizing cloud services for mobile client in mobile cloud, which considers the benefit of both mobile device users and cloud datacenters. The mobile cloud service provisioning optimization is conducted in parallel under the deadline, budget and energy expenditure constraint. Mobile cloud provider runs multiple VMs to execute the jobs for mobile device users, the cloud providers want to maximize the revenue and minimize the electrical cost. The mobile device user gives the suitable payment to the cloud datacenter provider for available cloud resources for optimize the benefit. The paper proposes a distributed optimization algorithm for utilizing cloud services for mobile devices. The experiment is to test convergence of the proposed algorithm and also compare it with other related work. The experiments study the impacts of job arrival rate, deadline and mobility speeds on energy consumption ratio, execution success ratio, resource allocation efficiency and cost. The experiment shows that the proposed algorithm outperforms other related work in terms of some performance metrics such as allocation efficiency."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "4c9774c5e57a4b7535eb19f6584f75c8b9c2cdcc"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-4c9774c5e57a4b7535eb19f6584f75c8b9c2cdcc", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "Cloud computing is an emerging computing model in which resources of the computing communications are provided as services over the Internet. Privacy and security of cloud storage services are very important and become a challenge in cloud computing due to loss of control over data and its dependence on the cloud computing provider. While there is a huge amount of transferring data in cloud system, the risk of accessing data by attackers raises. Considering the problem of building a secure cloud storage service, current scheme is proposed which is based on combination of RSA and AES encryption methods to share the data among users in a secure cloud system. The proposed method allows providing difficulty for attackers as well as reducing the time of information transmission between user and cloud data storage."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "541c3ab3ce75594c403126413b9c866fa7fba57a"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-541c3ab3ce75594c403126413b9c866fa7fba57a", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "Cloud computing has paved a way for resource-constrained mobile devices to speed up their computing tasks and to expand their storage capacity. However, cloud computing is not necessary a panacea for all mobile applications. The high network latency to cloud data centers may not be ideal for delay-sensitive applications while storing everything on public clouds risks users' security and privacy. In this paper, we discuss two preliminary ideas, one for mobile application offloading and the other for mobile storage expansion, by leveraging the edge intelligence offered by fog computing to help mobile applications. Preliminary experiments conducted based on implemented prototypes show that fog computing can provide an effective and sometimes better alternative to help mobile applications."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "ec1df457a2be681227f79de3ce932fccb65ee2bb"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-ec1df457a2be681227f79de3ce932fccb65ee2bb", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "The dynamic mobility and limitations in computational power, battery resources, and memory availability are main bottlenecks in fully harnessing mobile devices as data mining platforms. Therefore, the mobile devices are augmented with cloud resources in mobile edge cloud computing (MECC) environments to seamlessly execute data mining tasks. The MECC infrastructures provide compute, network, and storage services in one-hop wireless distance from mobile devices to minimize the latency in communication as well as provide localized computations to reduce the burden on federated cloud systems. However, when and how to offload the computation is a hard problem. In this paper, we present an opportunistic computation offloading scheme to efficiently execute data mining tasks in MECC environments. The scheme provides the suitable execution mode after analyzing the amount of unprocessed data, privacy configurations, contextual information, and available on-board local resources (memory, CPU, and battery power). We develop a mobile application for online activity recognition and evaluate the proposed scheme using the event data stream of 5 million activities collected from 12 users for 15 days. The experiments show significant improvement in execution time and battery power consumption resulting in 98% data reduction."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "43d40cf9251ba497f6ea3957bfc3c189fd11d421"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-43d40cf9251ba497f6ea3957bfc3c189fd11d421", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "In modern societies, the number of mobile users has dramatically risen in recent years. In this paper, an efficient authentication scheme for distributed mobile cloud computing services is proposed. The proposed scheme provides security and convenience for mobile users to access multiple mobile cloud computing services from multiple service providers using only a single private key. The security strength of the proposed scheme is based on bilinear pairing cryptosystem and dynamic nonce generation. In addition, the scheme supports mutual authentication, key exchange, user anonymity, and user untraceability. From system implementation point of view, verification tables are not required for the trusted smart card generator (SCG) service and cloud computing service providers when adopting the proposed scheme. In consequence, this scheme reduces the usage of memory spaces on these corresponding service providers. In one mobile user authentication session, only the targeted cloud service provider needs to interact with the service requestor (user). The trusted SCG serves as the secure key distributor for distributed cloud service providers and mobile clients. In the proposed scheme, the trusted SCG service is not involved in individual user authentication process. With this design, our scheme reduces authentication processing time required by communication and computation between cloud service providers and traditional trusted third party service. Formal security proof and performance analyses are conducted to show that the scheme is both secure and efficient."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "be98ba27ffdaa99834d12a1aa9c905c7bc6848c1"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-be98ba27ffdaa99834d12a1aa9c905c7bc6848c1", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "Cloud computing is an emerging computing paradigm that enables users to store their data in a cloud server to enjoy scalable and on-demand services. Nevertheless, it also brings many security issues, since cloud service providers (CSPs) are not in the same trusted domain as users. To protect data privacy against untrusted CSPs, existing solutions apply cryptographic methods (e.g., encryption mechanisms) and provide decryption keys only to authorized users. However, sharing cloud data among authorized users at a fine-grained level is still a challenging issue, especially when dealing with dynamic user groups. In this paper, we propose a secure and efficient fine-grained access control and data sharing scheme for dynamic user groups by: 1) defining and enforcing access policies based on the attributes of the data; 2) permitting the key generation center to efficiently update user credentials for dynamic user groups; and 3) allowing some expensive computation tasks to be performed by untrusted CSPs without requiring any delegation key. Specifically, we first design an efficient revocable attribute-based encryption (ABE) scheme with the property of ciphertext delegation by exploiting and uniquely combining techniques of identity-based encryption, ABE, subset-cover framework, and ciphertext encoding mechanism. We then present a fine-grained access control and data sharing system for on-demand services with dynamic user groups in the cloud. The experimental data show that our proposed scheme is more efficient and scalable than the state-of-the-art solution."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "9a6abed922749b4680465c5200bf5aefd26306e3"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-9a6abed922749b4680465c5200bf5aefd26306e3", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "Cloud computing enables cost-effective, self-service, and elastic hosting of applications in the Cloud. Applications may be partially or completely moved to the Cloud. When hosting or moving the database layer to the Cloud, challenges such as avoidance of disclosure of critical data have to be faced. The main challenges are handling different levels of confidentiality and satisfying security and privacy requirements. We provide reusable solutions in the form of patterns."}
{"metadata": {"dataset": "scidocs", "query_id": "012e396b02aa584cb74a65ae14af355e7c897858", "doc_id": "67ab675999e4b11904743070e295bc22476080fe"}, "id": "scidocs-012e396b02aa584cb74a65ae14af355e7c897858-67ab675999e4b11904743070e295bc22476080fe", "question": "Efficient and secure data storage operations for mobile cloud computing", "context": "The inherently limited processing power and battery lifetime of mobile phones hinder the possible execution of computationally intensive applications like content-based video analysis or 3D modeling. Offloading of computationally intensive application parts from the mobile platform into a remote cloud infrastructure or nearby idle computers addresses this problem. This paper presents our Mobile Augmentation Cloud Services (MACS) middleware which enables adaptive extension of Android application execution from a mobile client into the cloud. Applications are developed by using the standard Android development pattern. The middleware does the heavy lifting of adaptive application partitioning, resource monitoring and computation offloading. These elastic mobile applications can run as usual mobile application, but they can also reach transparently remote computing resources. Two prototype applications using the MACS middleware demonstrate the benefits of the approach. The evaluation shows that applications, which involve complicated algorithms and large computations, can benefit from offloading with around 95% energy savings and significant performance gains compared to local execution only."}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "no"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-no", "question": "Bank distress in the news: Describing events through deep learning", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "048e82ac9c88c458a50cc6289662e9cb2ecb4fc9"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-048e82ac9c88c458a50cc6289662e9cb2ecb4fc9", "question": "Bank distress in the news: Describing events through deep learning", "context": "State-of-the-art event encoding approaches rely on sentence or phrase level labeling, which are both time consuming and infeasible to extend to large scale text corpora and emerging domains. Using a multiple instance learning approach, we take advantage of the fact that while labels at the sentence level are difficult to obtain, they are relatively easy to gather at the document level. This enables us to view the problems of event detection and extraction in a unified manner. Using distributed representations of text, we develop a multiple instance formulation that simultaneously classifies news articles and extracts sentences indicative of events without any engineered features. We evaluate our model in its ability to detect news articles about civil unrest events (from Spanish text) across ten Latin American countries and identify the key sentences pertaining to these events. Our model, trained without annotated sentence labels, yields performance that is competitive with selected state-of-the-art models for event detection and sentence identification. Additionally, qualitative experimental results show that the extracted event-related sentences are informative and enhance various downstream applications such as article summarization, visualization, and event encoding."}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "09f83b83fd3b0114c2c902212101152c2d2d1259"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-09f83b83fd3b0114c2c902212101152c2d2d1259", "question": "Bank distress in the news: Describing events through deep learning", "context": ""}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "8f67ff7d7a4fc72d87f82ae340dba4365b7ea664"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-8f67ff7d7a4fc72d87f82ae340dba4365b7ea664", "question": "Bank distress in the news: Describing events through deep learning", "context": "Designing appropriate features for acoustic event recognition tasks is an active field of research. Expressive features should both improve the performance of the tasks and also be interpret-able. Currently, heuristically designed features based on the domain knowledge requires tremendous effort in hand-crafting, while features extracted through deep network are difficult for human to interpret. In this work, we explore the experience guided learning method for designing acoustic features. This is a novel hybrid approach combining both domain knowledge and purely data driven feature designing. Based on the procedure of log Mel-filter banks, we design a filter bank learning layer. We concatenate this layer with a convolutional neural network (CNN) model. After training the network, the weight of the filter bank learning layer is extracted to facilitate the design of acoustic features. We smooth the trained weight of the learning layer and re-initialize it in filter bank learning layer as audio feature extractor. For the environmental sound recognition task based on the Urbansound8K dataset [1], the experience guided learning leads to a 2% accuracy improvement compared with the fixed feature extractors (the log Mel-filter bank). The shape of the new filter banks are visualized and explained to prove the effectiveness of the feature design process."}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "4938e8c8c9ea3d351d283181819af5e5801efbed"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-4938e8c8c9ea3d351d283181819af5e5801efbed", "question": "Bank distress in the news: Describing events through deep learning", "context": "Neural Tensor Network for Learning Event Embeddings Event Representation E = (O1, P, O2, T), where P is the action, O1 is the actor, O2 is the object and T is the timestamp (T is mainly used for aligning stock data with news data). For example, the event “Sep 3, 2013 Microsoft agrees to buy Nokia’s mobile phone business for $7.2 billion.” is modeled as: (Actor = Microsoft, Action = buy, Object = Nokia’s mobile phone business, Time = Sep 3, 2013) Event Embedding"}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "8645a7ff78dc321e08dea6576c04f02a3ce158f9"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-8645a7ff78dc321e08dea6576c04f02a3ce158f9", "question": "Bank distress in the news: Describing events through deep learning", "context": "Videos serve to convey complex semantic information and ease the understanding of new knowledge. However, when mixed semantic meanings from different modalities (i.e., image, video, text) are involved, it is more difficult for a computer model to detect and classify the concepts (such as flood, storm, and animals). This paper presents a multimodal deep learning framework to improve video concept classification by leveraging recent advances in transfer learning and sequential deep learning models. Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN) models are then used to obtain the sequential semantics for both audio and textual models. The proposed framework is applied to a disaster-related video dataset that includes not only disaster scenes, but also the activities that took place during the disaster event. The experimental results show the effectiveness of the proposed framework."}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "a6df9a75a7a946cad8c32ee2a8c88d826a21430c"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-a6df9a75a7a946cad8c32ee2a8c88d826a21430c", "question": "Bank distress in the news: Describing events through deep learning", "context": "This is the first time New York University (NYU) participates in the event nugget (EN) evaluation of the Text Analysis Conference (TAC). We developed EN systems for both subtasks of event nugget, i.e, EN Task 1: Event Nugget Detection and EN Task 2: Event Nugget Detection and Coreference. The systems are mainly based on our recent research on deep learning for event detection (Nguyen and Grishman, 2015a; Nguyen and Grishman, 2016a). Due to the limited time we could devote to system development this year, we only ran the systems on the English evaluation data. However, we expect that the adaptation of the current systems to new languages can be done quickly. The development experiments show that although our current systems do not rely on complicated feature engineering, they significantly outperform the reported systems last year for the EN subtasks on the 2015 evaluation data."}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "6da97aa50c0c4f6d7473b607f872cd6bcb940c60"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-6da97aa50c0c4f6d7473b607f872cd6bcb940c60", "question": "Bank distress in the news: Describing events through deep learning", "context": "Deep learning is popular as an end-to-end framework extracting the prominent features and performing the classification also. In this paper, we extensively investigate deep networks as an alternate to feature encoding technique of lowlevel descriptors for emotion recognition on the benchmark EmoDB dataset. Fusion performance with such obtained encoded features with other available features is also investigated. Highest performance to date in the literature is observed."}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "09286005d0c0253995c970387dd222ae4acbc8f1"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-09286005d0c0253995c970387dd222ae4acbc8f1", "question": "Bank distress in the news: Describing events through deep learning", "context": "The problem of detecting events from content published on microblogs has garnered much interest in recent times. In this paper, we address the questions of what happens after the outbreak of an event in terms of how the event gradually progresses and attains each of its milestones, and how it eventually dissipates. We propose a model based approach to capture the gradual unfolding of an event over time. This enables the model to automatically produce entire timeline trajectories of events from the time of their outbreak to their disappearance. We apply our model on the Twitter messages collected about Ebola during the 2014 outbreak and obtain the progression timelines of several events that occurred during the outbreak. We also compare our model to several existing topic modeling and event detection baselines in literature to demonstrate its efficiency."}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "91cf9beb696cbb0818609614f4da7351262eac86"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-91cf9beb696cbb0818609614f4da7351262eac86", "question": "Bank distress in the news: Describing events through deep learning", "context": "In recent years, microblogs have become an important source for reporting real-world events. A real-world occurrence reported in microblogs is also called a social event. Social events may hold critical materials that describe the situations during a crisis. In real applications, such as crisis management and decision making, monitoring the critical events over social streams will enable watch officers to analyze a whole situation that is a composite event, and make the right decision based on the detailed contexts such as what is happening, where an event is happening, and who are involved. Although there has been significant research effort on detecting a target event in social networks based on a single source, in crisis, we often want to analyze the composite events contributed by different social users. So far, the problem of integrating ambiguous views from different users is not well investigated. To address this issue, we propose a novel framework to detect composite social events over streams, which fully exploits the information of social data over multiple dimensions. Specifically, we first propose a graphical model called location-time constrained topic (LTT) to capture the content, time, and location of social messages. Using LTT, a social message is represented as a probability distribution over a set of topics by inference, and the similarity between two messages is measured by the distance between their distributions. Then, the events are identified by conducting efficient similarity joins over social media streams. To accelerate the similarity join, we also propose a variable dimensional extendible hash over social streams. We have conducted extensive experiments to prove the high effectiveness and efficiency of the proposed approach."}
{"metadata": {"dataset": "scidocs", "query_id": "01d208b33561362f7714f714d3bc4a1f7aa1637c", "doc_id": "f2e9f869a9fc1f07887866be5f70a37b6c31411b"}, "id": "scidocs-01d208b33561362f7714f714d3bc4a1f7aa1637c-f2e9f869a9fc1f07887866be5f70a37b6c31411b", "question": "Bank distress in the news: Describing events through deep learning", "context": "Stock trend prediction plays a critical role in seeking maximized profit from the stock investment. However, precise trend prediction is very difficult since the highly volatile and non-stationary nature of the stock market. Exploding information on the Internet together with the advancing development of natural language processing and text mining techniques have enabled investors to unveil market trends and volatility from online content. Unfortunately, the quality, trustworthiness, and comprehensiveness of online content related to stock market vary drastically, and a large portion consists of the low-quality news, comments, or even rumors. To address this challenge, we imitate the learning process of human beings facing such chaotic online news, driven by three principles: sequential content dependency, diverse influence, and effective and efficient learning. In this paper, to capture the first two principles, we designed a Hybrid Attention Networks(HAN) to predict the stock trend based on the sequence of recent related news. Moreover, we apply the self-paced learning mechanism to imitate the third principle. Extensive experiments on real-world stock market data demonstrate the effectiveness of our framework. A further simulation illustrates that a straightforward trading strategy based on our proposed framework can significantly increase the annualized return."}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "no"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-no", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "7d6a34508b091ba8cde8a403e26ec791325c60d1"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-7d6a34508b091ba8cde8a403e26ec791325c60d1", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "Smart Cities gained importance as a means of making ICT enabled services and applications available to the citizens, companies and authorities that are part of a city's system. It aims at increasing citizens' quality of life, and improving the efficiency and quality of the services provided by governing entities and businesses. This perspective requires an integrated vision of a city and of its infrastructures, in all its components. A Smart City can be taken according to six characteristics: All these domains raise new challenges in security and privacy, since users implicitly expect systems to be secure and privacy-preserving. One of the critical elements is which role(s) the city will take up as an actor within an increasingly complex value network. New players enter the market, actors shift their business strategies, roles change, different types of platforms emerge and vie for market dominance, technological developments create new threats and opportunities, etc. An element related to the trend of platformisation is cloud computing, which is increasingly helping the private sector to reduce cost, increase efficiency, and work smarter. One particular challenge relates to open data business models. Activities necessary for Public Sector Information provision can be identified. The development of efficient and effective e-government is a prerequisite. Transnational authentication systems for citizens and businesses, agreed frameworks for data privacy, and the sharing and collection of individual and business data, are key. Smart Cities need to be able to integrate themselves into national, regional and international infrastructures. Although the implementation aspects depend strongly on the authorities of these infrastructures, European wide recommendations and directives will definitely contribute to accelerate the deployment of Smart Cities. Health, inclusion and assisted living will play an essential role, since the demand for related services is rising, because ageing is changing disease composition. Requirements address a number of technologies, beyond the ones related to mobile and fixed networks. An integrated perspective on healthcare solutions for the near-to long-term can be foreseen, bridging a direct gap in between the health area and the technological development of communications (radio and network components). The needs for mobility in urban areas result into a number of problems, such as traffic congestion and energy consumption, which can be alleviated by exploiting Intelligent Transportation Systems and further adoption of vehicle-to-vehicle and vehicle-to-infrastructure communication networks. The information being managed in this area can be relevant to other domains, which increases its potential. An effective deployment …"}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "7b9ab27ad78899b6b284a17c38aa75fb0e1d1765"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-7b9ab27ad78899b6b284a17c38aa75fb0e1d1765", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "We describe an approach for how to design an essentially more scalable, flexible and resilient electric power infrastructure – one that encourages efficient use, integrates local generation, and manages demand through omnipresent awareness of energy availability and use over time. We are inspired by how the Internet has revolutionized communications infrastructure, by pushing intelligence to the edges while hiding the diversity of underlying technologies through well-defined interfaces. Any end device is a traffic source or sink and intelligent endpoints adapt their traffic to what the infrastructure can"}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "047eeb7fce304fdb2b41f3c4d0b393dd1137bdab"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-047eeb7fce304fdb2b41f3c4d0b393dd1137bdab", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "Mobile computing is emerging as the prime focus of next generation computing .One of the prime issues of mobile computing is to provide infrastructure support in terms of computing devices, seamless mobility, application middleware, data and user security, and user applications/services. Mobile commerce is one of the driving forces that has evinced enormous interest in mobile computing .The thought of conducting commerce on the go is what is driving the huge investments corporations are making in researching this area. This paper discusses the various challenges in providing infrastructure for wireless computing."}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "f1e0a619b6ad652b65b49f362ac9413e89291ad7"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-f1e0a619b6ad652b65b49f362ac9413e89291ad7", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "Previous efforts in electronic commerce (e-commerce) research in developing countries shows that there is an acute lack of theoretical frameworks and empirical evidence to understand how developing country firms realize electronic commerce benefits amidst their national constraints. This paper sets out to develop a theoretically abstracted but contextually grounded model of how developing country firms can orient their resources and realize these benefits amidst their national constraints. A review of e-commerce and strategy management literature to develop a resource – based model for e-commerce benefits was undertaken. The process-based model provides an understanding of how to identify, integrate, and reconfigure resources to achieve electronic commerce benefits; provides propositions that serves as theoretical platforms for future empirically grounded research on electronic commerce in developing country contexts and brings organizations closer to identifying and categorizing the strategic value of resources and the role managerial capabilities and intangible resources play in sustaining e-commerce benefits. Finally, our findings provides organizations the strategic options to address resources which have lost their value or have become less valuable to their strategic orientation in e-commerce adoption thereby serving as a starting point of examining e-commerce in developing countries through the theoretical lens of information systems and strategic management."}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "1c49c74521b4c9eae7a352a3b223b4213294c681"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-1c49c74521b4c9eae7a352a3b223b4213294c681", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "This paper puts the Internet of Things in a wider context: How it relates to the Future Internet overall, and where the business value lies so that it will become interesting for enterprises to invest in it. Real-World Awareness and Business Process Decomposition are the two major paradigms regarding future business value. The major application domains where the Internet of Things will play an important role and where there are concrete business opportunities are highlighted. But there are also many technical challenges that need to be addressed. These are listed and it is shown how they are tackled by existing research projects with industrial participation."}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "3716c4896944c3461477f845319ac09e3dfe3a10"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-3716c4896944c3461477f845319ac09e3dfe3a10", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "We designed eSports - a collaborative and synchronous video annotation platform, which is to be used in Internet scale cross-platform grid computing environment to facilitate computer supported cooperative work (CSCW) in education settings such as distance sport coaching, distance classroom etc. Different from traditional multimedia annotation systems, eSports provides the capabilities to collaboratively and synchronously play and archive real time live video, to take snapshots, to annotate video snapshots using whiteboard and to play back the video annotations synchronized with original video streams. eSports is designed based on the grid based collaboration paradigm $the shared event model using NaradaBrokering, which is a publish/subscribe based distributed message passing and event notification system. In addition to elaborate the design and implementation of eSports, we analyze the potential use cases of eSports under different education settings. We believed that eSports is very useful to improve the online collaborative coaching and education."}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "0b458ce6c0d6d7fd20499e5b64a46132d7c380f2"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-0b458ce6c0d6d7fd20499e5b64a46132d7c380f2", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "The Smart Grid, generally referred to as the next-generation power system, is considered as a revolutionary and evolutionary regime of existing power grids. More importantly, with the integration of advanced computing and communication technologies, the Smart Grid is expected to greatly enhance efficiency and reliability of future power systems with renewable energy resources, as well as distributed intelligence and demand response. Along with the silent features of the Smart Grid, cyber security emerges to be a critical issue because millions of electronic devices are inter-connected via communication networks throughout critical power facilities, which has an immediate impact on reliability of such a widespread infrastructure. In this paper, we present a comprehensive survey of cyber security issues for the Smart Grid. Specifically, we focus on reviewing and discussing security requirements, network vulnerabilities, attack countermeasures, secure communication protocols and architectures in the Smart Grid. We aim to provide a deep understanding of security vulnerabilities and solutions in the Smart Grid and shed light on future research directions for Smart Grid security. 2013 Elsevier B.V. All rights reserved."}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "ae6f25ce44d20c3917a62146082c55d2a62b7779"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-ae6f25ce44d20c3917a62146082c55d2a62b7779", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "The fifth generation of mobile technology (5G) will address the socio-economic demands and business contexts of 2020 and beyond. Leading operators from around the globe have defined their vision for 5G, considering customer and business contexts as well as potential technologies and migration issues in an initiative set up by the NGMN Alliance. While their concrete vision is described in the NGMN 5G whitepaper, this paper describes the key points, such as 5G design principles, 5G components, network slicing, 5G radio access technology and 5G interfacing options that have implications to the 5G architecture design."}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "33c5b066ebedd22675c55212232697060c7276ab"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-33c5b066ebedd22675c55212232697060c7276ab", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "Advances in both semiconductor and automotive industry are today enabling the next generation of vehicles with significant electronics content than ever before. Consumers can now avail vehicle offerings in the form of Electric and Hybrid Electric Vehicles (EV/HEV) that have improved fuel efficiency, provide enhanced driver-passenger comfort and experience through Advance Driver Assistance Systems (ADAS) and car infotainment systems, and more. Increasing electronics, software content, and connectivity drive two consumer concerns — “functional safety” and “security” — to the forefront. In this tutorial, we dissect these concerns from an end application perspective and translate the system level requirements and standards into semiconductor development requirements. We indicate both current and emerging practices, and touch upon areas requiring new or optimal design and electronic design automation (EDA) solutions. While functional safety is the primary focus for deep-dive in this tutorial, we also examine key facets of automotive security which is now emerging as a critical area for further understanding and standardization."}
{"metadata": {"dataset": "scidocs", "query_id": "0210b3fe6f7173c86936b5dd9261bc0be0c45652", "doc_id": "63c08ef4019bac59c8df18f27d32def8abf1890d"}, "id": "scidocs-0210b3fe6f7173c86936b5dd9261bc0be0c45652-63c08ef4019bac59c8df18f27d32def8abf1890d", "question": "Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits", "context": "The aim of this paper is to study the implementation of online games to encourage public participation in urban planning. Its theoretical foundations are based on previous work in public participatory geographical information systems (PP GISs), play and games, with a special focus on serious games. Serious games aim to support learning processes in a new, more playful way. We developed the concept of playful public participation in urban planning, including playful elements such as storytelling, walking and moving, sketching, drawing, and games. A group of students designed an online serious public participatory game entitled NextCampus. The case study used in NextCampus was taken from the real-world question of a possible move of a university campus to a new location in the city of Hamburg, Germany. The development of the serious public participatory game NextCampus resulted in a physical prototype, user interface design, and a computational model of the game. The NextCampus game was tested with the help of two groups of urban planning students and presented to three external experts who provided valuable recommendations for further development. The critical comments questioned the level of complexity involved in such games. The positive comments included recognition of the potential for joy and the play-fulness a game like NextCampus could evoke. Public participatory online applications aim to attract citizens to discuss current issues related to their environment and to improve the process of public participation in general. An integration of geographic information systems (GISs) with public participatory tools represents one of the latest innovations in this area. ways of integrating the new applications into participatory processes and considers which new functionalities and technical characteristics could offer the most benefit to users. In the past, these technologies and other map-based applications were frequently criticized as being too complex for the majority of potential users (Steinmann, Krek, & Blaschke, 2004). New forms of collaboration and technical solutions emerged during the Web 2.0 era. For example , Google Maps and Google Earth can be used by lay users and non-experts without intense training. Recent research on collabo-rative mapping also known as ''geography without geographers'' prior work in PP GIS to include much wider, distributed participation (Hardy, 2008). Despite these new forms of collaboration and innovative technologies , Moody (2007) demonstrates that the use of GIS technology to involve citizens in participatory urban planning does not seem to empower citizens. An important factor in such findings …"}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "no"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-no", "question": "Pooled motion features for first-person videos", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "c67192cb7c82d2a0516b656909985823a5b2aba0"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-c67192cb7c82d2a0516b656909985823a5b2aba0", "question": "Pooled motion features for first-person videos", "context": "We present a method for converting first-person videos, for example, captured with a helmet camera during activities such as rock climbing or bicycling, into hyper-lapse videos, i.e., time-lapse videos with a smoothly moving camera. At high speed-up rates, simple frame sub-sampling coupled with existing video stabilization methods does not work, because the erratic camera shake present in first-person videos is amplified by the speed-up. Our algorithm first reconstructs the 3D input camera path as well as dense, per-frame proxy geometries. We then optimize a novel camera path for the output video that passes near the input cameras while ensuring that the virtual camera looks in directions that can be rendered well from the input. Finally, we generate the novel smoothed, time-lapse video by rendering, stitching, and blending appropriately selected source frames for each output frame. We present a number of results for challenging videos that cannot be processed using traditional techniques."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "748260579dc2fb789335a88ae3f63c114795d047"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-748260579dc2fb789335a88ae3f63c114795d047", "question": "Pooled motion features for first-person videos", "context": "In this work, we evaluate the performance of the popular dense trajectories approach on first-person action recognition datasets. A person moving around with a wearable camera will actively interact with humans and objects and also passively observe others interacting. Hence, in order to represent real-world scenarios, the dataset must contain actions from first-person perspective as well as third-person perspective. For this purpose, we introduce a new dataset which contains actions from both the perspectives captured using a head-mounted camera. We employ a motion pyramidal structure for grouping the dense trajectory features. The relative strengths of motion along the trajectories are used to compute different bag-of-words descriptors and concatenated to form a single descriptor for the action. The motion pyramidal approach performs better than the baseline improved trajectory descriptors. The method achieves 96.7% on the JPL interaction dataset and 61.8% on our NUS interaction dataset. The same is used to detect actions in long video sequences and achieves average precision of 0.79 on JPL interaction dataset."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "6674729287f2482eda9e836846d2a35e63ea401c"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-6674729287f2482eda9e836846d2a35e63ea401c", "question": "Pooled motion features for first-person videos", "context": "We propose a function-based temporal pooling method that captures the latent structure of the video sequence data - e.g., how frame-level features evolve over time in a video. We show how the parameters of a function that has been fit to the video data can serve as a robust new video representation. As a specific example, we learn a pooling function via ranking machines. By learning to rank the frame-level features of a video in chronological order, we obtain a new representation that captures the video-wide temporal dynamics of a video, suitable for action recognition. Other than ranking functions, we explore different parametric models that could also explain the temporal changes in videos. The proposed functional pooling methods, and rank pooling in particular, is easy to interpret and implement, fast to compute and effective in recognizing a wide variety of actions. We evaluate our method on various benchmarks for generic action, fine-grained action and gesture recognition. Results show that rank pooling brings an absolute improvement of 7-10 average pooling baseline. At the same time, rank pooling is compatible with and complementary to several appearance and local motion based methods and features, such as improved trajectories and deep learning features."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "1aad2da473888cb7ebc1bfaa15bfa0f1502ce005"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-1aad2da473888cb7ebc1bfaa15bfa0f1502ce005", "question": "Pooled motion features for first-person videos", "context": "This paper discusses the problem of recognizing interaction-level human activities from a first-person viewpoint. The goal is to enable an observer (e.g., a robot or a wearable camera) to understand 'what activity others are performing to it' from continuous video inputs. These include friendly interactions such as 'a person hugging the observer' as well as hostile interactions like 'punching the observer' or 'throwing objects to the observer', whose videos involve a large amount of camera ego-motion caused by physical interactions. The paper investigates multi-channel kernels to integrate global and local motion information, and presents a new activity learning/recognition methodology that explicitly considers temporal structures displayed in first-person activity videos. In our experiments, we not only show classification results with segmented videos, but also confirm that our new approach is able to detect activities from continuous videos reliably."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "3a1a9e15d42b67bfdee7761311aea9b699cd0d5f"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-3a1a9e15d42b67bfdee7761311aea9b699cd0d5f", "question": "Pooled motion features for first-person videos", "context": "JackIn is a new human-human communication framework for connecting two or more people. With first-person view video streaming from a person (called Body) wearing a transparent head-mounted display and a head-mounted camera, the other person (called Ghost) participates in the shared first-person view. With JackIn, people's activities can be shared and assistance or guidance can be given through other peoples expertise. This can be applied to daily activities such as cooking lessons, shopping navigation, education in craft-work or electrical work, and sharing experiences of sporting and live events. For a better viewing experience with frist-person view, we developed the out-of-body view in which first-person images are integrated to construct a scene around a Body, and a Ghost can virtually control the viewpoint to look around the space surrounding the Body. We also developed a tele-pointing gesture interface. We conducted an experiment to evaluate how effective this framework is and found that Ghosts can understand the spatial situation of the Body."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "93ce62fb04283efb253b512dc3f02b1d169ee7ed"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-93ce62fb04283efb253b512dc3f02b1d169ee7ed", "question": "Pooled motion features for first-person videos", "context": "3-D convolutional neural networks (3-D-convNets) have been very recently proposed for action recognition in videos, and promising results are achieved. However, existing 3-D-convNets has two “artificial” requirements that may reduce the quality of video analysis: 1) It requires a fixed-sized (e.g., 112  $\\times$ 112) input video; and 2) most of the 3-D-convNets require a fixed-length input (i.e., video shots with fixed number of frames). To tackle these issues, we propose an end-to-end pipeline named Two-stream 3-D-convNet Fusion, which can recognize human actions in videos of arbitrary size and length using multiple features. Specifically, we decompose a video into spatial and temporal shots. By taking a sequence of shots as input, each stream is implemented using a spatial temporal pyramid pooling (STPP) convNet with a long short-term memory (LSTM) or CNN-E model, softmax scores of which are combined by a late fusion. We devise the STPP convNet to extract equal-dimensional descriptions for each variable-size shot, and we adopt the LSTM/CNN-E model to learn a global description for the input video using these time-varying descriptions. With these advantages, our method should improve all 3-D CNN-based video analysis methods. We empirically evaluate our method for action recognition in videos and the experimental results show that our method outperforms the state-of-the-art methods (both 2-D and 3-D based) on three standard benchmark datasets (UCF101, HMDB51 and ACT datasets)."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "49941c788b9dd8a639b33b4208b32a740b8c7bf8"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-49941c788b9dd8a639b33b4208b32a740b8c7bf8", "question": "Pooled motion features for first-person videos", "context": "We present a novel algorithm for automatically applying constrainable, L1-optimal camera paths to generate stabilized videos by removing undesired motions. Our goal is to compute camera paths that are composed of constant, linear and parabolic segments mimicking the camera motions employed by professional cinematographers. To this end, our algorithm is based on a linear programming framework to minimize the first, second, and third derivatives of the resulting camera path. Our method allows for video stabilization beyond the conventional filtering of camera paths that only suppresses high frequency jitter. We incorporate additional constraints on the path of the camera directly in our algorithm, allowing for stabilized and retargeted videos. Our approach accomplishes this without the need of user interaction or costly 3D reconstruction of the scene, and works as a post-process for videos from any camera or from an online source."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "45392756fd0d437091d172e4cbbc37a66650555f"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-45392756fd0d437091d172e4cbbc37a66650555f", "question": "Pooled motion features for first-person videos", "context": "Given a short video we create a representation that captures a spectrum of looping videos with varying levels of dynamism, ranging from a static image to a highly animated loop. In such a progressively dynamic video, scene liveliness can be adjusted interactively using a slider control. Applications include background images and slideshows, where the desired level of activity may depend on personal taste or mood. The representation also provides a segmentation of the scene into independently looping regions, enabling interactive local adjustment over dynamism. For a landscape scene, this control might correspond to selective animation and deanimation of grass motion, water ripples, and swaying trees. Converting arbitrary video to looping content is a challenging research problem. Unlike prior work, we explore an optimization in which each pixel automatically determines its own looping period. The resulting nested segmentation of static and dynamic scene regions forms an extremely compact representation."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "070874b011f8eb2b18c8aa521ad0a7a932b4d9ad"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-070874b011f8eb2b18c8aa521ad0a7a932b4d9ad", "question": "Pooled motion features for first-person videos", "context": "Recently dense trajectories were shown to be an efficient video representation for action recognition and achieved state-of-the-art results on a variety of datasets. This paper improves their performance by taking into account camera motion to correct them. To estimate camera motion, we match feature points between frames using SURF descriptors and dense optical flow, which are shown to be complementary. These matches are, then, used to robustly estimate a homography with RANSAC. Human motion is in general different from camera motion and generates inconsistent matches. To improve the estimation, a human detector is employed to remove these matches. Given the estimated camera motion, we remove trajectories consistent with it. We also use this estimation to cancel out camera motion from the optical flow. This significantly improves motion-based descriptors, such as HOF and MBH. Experimental results on four challenging action datasets (i.e., Hollywood2, HMDB51, Olympic Sports and UCF50) significantly outperform the current state of the art."}
{"metadata": {"dataset": "scidocs", "query_id": "0229829e9a1eed5769a2b5eccddcaa7cd9460b92", "doc_id": "6ec17e735cd9f7cb37485ab07b905a7895b0067d"}, "id": "scidocs-0229829e9a1eed5769a2b5eccddcaa7cd9460b92-6ec17e735cd9f7cb37485ab07b905a7895b0067d", "question": "Pooled motion features for first-person videos", "context": "This paper presents a video stabilization algorithm based on the extraction and tracking of scale invariant feature transform features through video frames. Implementation of SIFT operator is analyzed and adapted to be used in a feature-based motion estimation algorithm. SIFT features are extracted from video frames and then their trajectory is evaluated to estimate interframe motion. A modified version of iterative least squares method is adopted to avoid estimation errors and features are tracked as they appear in nearby frames to improve video stability. Intentional camera motion is eventually filtered with adaptive motion vector integration. Results confirm the effectiveness of the method."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "no"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-no", "question": "General transformations for GPU execution of tree traversals", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "36f06481eaae63522dfb61475602584997ebfee8"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-36f06481eaae63522dfb61475602584997ebfee8", "question": "General transformations for GPU execution of tree traversals", "context": "Modern graphics hardware architectures excel at compute-intensive tasks such as ray-triangle intersection, making them attractive target platforms for raytracing. To date, most GPU-based raytracers have relied upon uniform grid acceleration structures. In contrast, the kd-tree has gained widespread use in CPU-based raytracers and is regarded as the best general-purpose acceleration structure. We demonstrate two kd-tree traversal algorithms suitable for GPU implementation and integrate them into a streaming raytracer. We show that for scenes with many objects at different scales, our kd-tree algorithms are up to 8 times faster than a uniform grid. In addition, we identify load balancing and input data recirculation as two fundamental sources of inefficiency when raytracing on current graphics hardware."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "698b8181cd613a72adeac0d75252afe7f57a5180"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-698b8181cd613a72adeac0d75252afe7f57a5180", "question": "General transformations for GPU execution of tree traversals", "context": "We present two new parallel implementations of the tree-ensemble algorithms Random Forest (RF) and Extremely randomized trees (ERT) for emerging many-core platforms, e.g., contemporary graphics cards suitable for general-purpose computing (GPGPU). Random Forest and Extremely randomized trees are ensemble learners for classification and regression. They operate by constructing a multitude of decision trees at training time and outputting a prediction by comparing the outputs of the individual trees. Thanks to the inherent parallelism of the task, an obvious platform for its computation is to employ contemporary GPUs with a large number of processing cores. Previous parallel algorithms for Random Forests in the literature are either designed for traditional multi-core CPU platforms or early history GPUs with simpler hardware architecture and relatively few number of cores. The new parallel algorithms are designed for contemporary GPUs with a large number of cores and take into account aspects of the newer hardware architectures as memory hierarchy and thread scheduling. They are implemented using the C/C++ language and the CUDA interface for best possible performance on NVidia-based GPUs. An experimental study comparing with the most important previous solutions for CPU and GPU platforms shows significant improvement for the new implementations, often with several magnitudes."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "35bef4597f5e514359ff45bea31be8b8239effe1"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-35bef4597f5e514359ff45bea31be8b8239effe1", "question": "General transformations for GPU execution of tree traversals", "context": "The watershed transform is a popular image segmentation procedure from mathematical morphology used in many applications of computer vision. This paper proposes a novel parallel watershed procedure designed for GPU implementation. Our algorithm constructs paths of steepest descent and reduces these paths into direct pointers to catchment basin minima in logarithmic time, also crucially incorporating successful resolution of plateaux. Three implementation variants and their parameters are analysed through experiments on 2D and 3D images; a comparison against the state-of-the-art shows a runtime improvement of around 30%. For 3D images of 128 megavoxels execution times of approximately 1.5–2 seconds are achieved."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "2379c027e7376bb76978602a7b185dfa73a9cd35"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-2379c027e7376bb76978602a7b185dfa73a9cd35", "question": "General transformations for GPU execution of tree traversals", "context": "Graphics Processing Units (GPUs) have emerged as powerful accelerators for many regular algorithms that operate on dense arrays and matrices. In contrast, we know relatively little about using GPUs to accelerate highly irregular algorithms that operate on pointer-based data structures such as graphs. For the most part, research has focused on GPU implementations of graph analysis algorithms that do not modify the structure of the graph, such as algorithms for breadth-first search and strongly-connected components.\n In this paper, we describe a high-performance GPU implementation of an important graph algorithm used in compilers such as gcc and LLVM: Andersen-style inclusion-based points-to analysis. This algorithm is challenging to parallelize effectively on GPUs because it makes extensive modifications to the structure of the underlying graph and performs relatively little computation. In spite of this, our program, when executed on a 14 Streaming Multiprocessor GPU, achieves an average speedup of 7x compared to a sequential CPU implementation and outperforms a parallel implementation of the same algorithm running on 16 CPU cores.\n Our implementation provides general insights into how to produce high-performance GPU implementations of graph algorithms, and it highlights key differences between optimizing parallel programs for multicore CPUs and for GPUs."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "f4c5f7bdf3f7ce924cd42f26d2a9eb97ab8da4a3"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-f4c5f7bdf3f7ce924cd42f26d2a9eb97ab8da4a3", "question": "General transformations for GPU execution of tree traversals", "context": "We present an efficient method for volume rendering by raycasting on the CPU. We employ coherent packet traversal of an implicit bounding volume hierarchy, heuristically pruned using preintegrated transfer functions, to exploit empty or homogeneous space. We also detail SIMD optimizations for volumetric integration, trilinear interpolation, and gradient lighting. The resulting system performs well on low-end and laptop hardware, and can outperform out-of-core GPU methods by orders of magnitude when rendering large volumes without level-of-detail (LOD) on a workstation. We show that, while slower than GPU methods for low-resolution volumes, an optimized CPU renderer does not require LOD to achieve interactive performance on large data sets."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "50ed709761f57895b50346a8249814a6f66f6c89"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-50ed709761f57895b50346a8249814a6f66f6c89", "question": "General transformations for GPU execution of tree traversals", "context": "A GPU (Graphics Processing Unit) is a specialized processor for graphics processing. GPUs have the ability to perform high-speed parallel processing using its many processing cores. To utilize the powerful computing ability, GPUs are widely used for general purpose processing. The main contribution of this paper is to show a new template matching algorithm using pixel rearrangement. Template Matching is a technique for finding small parts of an image which match a template image. The feature of our proposed algorithm is that using pixel rearrangement, multiple low-resolution images are generated and template matching for the low-resolution images is performed to reduce the computing time. Also, we implemented our algorithm on a GPU system. The experimental results show that, for an input image with size of 4096 $\\times$ 4096 and a template image with size of 256 $\\times$ 256, our implementation can achieve a speedup factor of approximately 78 times over the conventional sequential implementation."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "67078d516a85204c016846e30c02e901ac16f142"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-67078d516a85204c016846e30c02e901ac16f142", "question": "General transformations for GPU execution of tree traversals", "context": "This paper presents a new GPU-based rasterization algorithm for Boolean operations that handles arbitary closed polygons. We construct an efficient data structure for interoperation of CPU and GPU and propose a fast GPU-based contour extraction method to ensure the performance of our algorithm. We then design a novel traversing strategy to achieve an error-free calculation of intersection point for correct Boolean operations. We finally give a detail evaluation and the results show that our algorithm has a higher performance than exsiting algorithms on processing polygons with large amount of vertices. key words: GPU, CPU, rasterization, Boolean operation, error-free"}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "371b07d65891b03eaae15c2865da2a6751a99bb8"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-371b07d65891b03eaae15c2865da2a6751a99bb8", "question": "General transformations for GPU execution of tree traversals", "context": "A novel algorithm is presented to compute the convex hull of a point set in ℝ3 using the graphics processing unit (GPU). By exploiting the relationship between the Voronoi diagram and the convex hull, the algorithm derives the approximation of the convex hull from the former. The other extreme vertices of the convex hull are then found by using a two-round checking in the digital and the continuous space successively. The algorithm does not need explicit locking or any other concurrency control mechanism, thus it can maximize the parallelism available on the modern GPU.\n The implementation using the CUDA programming model on NVIDIA GPUs is exact and efficient. The experiments show that it is up to an order of magnitude faster than other sequential convex hull implementations running on the CPU for inputs of millions of points. The works demonstrate that the GPU can be used to solve nontrivial computational geometry problems with significant performance benefit."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "1794435f6b541109ee9ea812d80d5b9add95aacd"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-1794435f6b541109ee9ea812d80d5b9add95aacd", "question": "General transformations for GPU execution of tree traversals", "context": "We introduce a technique for traversal of Merkle trees, and propose an efficient algorithm that generates a sequence of leaves along with their associated authentication paths. For one choice of parameters, and a total of N leaves, our technique requires a worst-case computational effort of 2 log N/loglog N hash function evaluations per output, and a total storage capacity of less than 1.5 log N/loglog N hash values. This is a simultaneous improvement both in space and time complexity over any previously published algorithm."}
{"metadata": {"dataset": "scidocs", "query_id": "027e7780dbda48d99f3654e77b4a63063224950e", "doc_id": "bbe0f0b3e2d60c4f96d9d84f97dc8a9be4f72802"}, "id": "scidocs-027e7780dbda48d99f3654e77b4a63063224950e-bbe0f0b3e2d60c4f96d9d84f97dc8a9be4f72802", "question": "General transformations for GPU execution of tree traversals", "context": "Graphics Processing Units (GPUs) offer tremendous computational power. CUDA (Compute Unified Device Architecture) provides a multi-threaded parallel programming model, facilitating high performance implementations of general-purpose computations. However, the explicitly managed memory hierarchy and multi-level parallel view make manual development of high-performance CUDA code rather complicated. Hence the automatic transformation of sequential input programs into efficient parallel CUDA programs is of considerable interest. This paper describes an automatic code transformation system that generates parallel CUDA code from input sequential C code, for regular (affine) programs. Using and adapting publicly available tools that have made polyhedral compiler optimization practically effective, we develop a C-to-CUDA transformation system that generates two-level parallel CUDA code that is optimized for efficient data access. The performance of automatically generated code is compared with manually optimized CUDA code for a number of benchmarks. The performance of the automatically generated CUDA code is quite close to hand-optimized CUDA code and considerably better than the benchmarks’ performance on a multicore CPU."}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "no"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-no", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "197c43315bdcec6785cb9834638140d9878ec131"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-197c43315bdcec6785cb9834638140d9878ec131", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "TacAir-Soar is an intelligent, rule-based system that generates believable “human-like” behavior for large scale, distributed military simulations. The innovation of the application is primarily a matter of scale and integration. The system is capable of executing most of the airborne missions that the United States military flies in fixed-wing aircraft. It accomplishes this by integrating a wide variety of intelligent capabilities, including real-time hierarchical execution of complex goals and plans, communication and coordination with humans and simulated entities, maintenance of situational awareness, and the ability to accept and respond to new orders while in flight. The system is currently deployed at the Oceana Naval Air Station WISSARD Lab and the Air Force Research Laboratory in Mesa, AZ. Its most dramatic use was in the Synthetic Theater of War 1997, which was an operational training exercise that ran for 48 continuous hours during which TacAir-Soar flew all U.S. fixed-wing aircraft."}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "29ab09179c9fc864b05fe853c8443f39ac1baaec"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-29ab09179c9fc864b05fe853c8443f39ac1baaec", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "This paper describes how language is grounded by a comprehension system called Lucia within a robotic agent called Rosie that can manipulate objects and navigate indoors. The whole system is built within the Soar cognitive architecture and uses Embodied Construction Grammar (ECG) as a formalism for describing linguistic knowledge. Grounding is performed using knowledge from the grammar itself, from the linguistic context, from the agent's perception, and from an ontology of long-term knowledge about object categories and properties and actions the agent can perform. The paper also describes a benchmark corpus of 200 sentences in this domain, along with test versions of the world model and ontology, and gold-standard meanings for each of the sentences. The benchmark is contained in the supplemental materials."}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "d041c7df26d2d212a6e37204f8615119aff56eed"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-d041c7df26d2d212a6e37204f8615119aff56eed", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "Human cognitive architecture includes a working memory of limited capacity and duration with partially separate visual and auditory channels, and an effectively infinite long-term memory holding many schemas that can vary in their degree of automation. These cognitive structures have evolved to handle information that varies in the extent to which elements can be processed successively in working memory or, because they interact, must be processed simultaneously imposing a heavy load on working memory. Cognitive load theory uses this combination of information and cognitive structures to guide instructional design. Several designs that rely heavily on visual working memory and its characteristics are discussed in this paper. Knowledge of human cognitive architecture is essential for instructional design, and visual cognition is a central aspect of human cognition. Not surprisingly, there are several instructional design effects that rely heavily on the manner in which humans visually process information. This paper discusses some relevant information structures, cognitive structures and instructional designs that rely on our knowledge of visual information processing. A. Information structures While considerable work by many researchers over several decades has been devoted to the organization of human cognitive architecture, far less effort has gone into investigating the information structures that must have driven the evolution of that architecture. Some work has been carried out by Sweller (1994) and Halford, Wilson and Phillips (1998). Sweller (1994) suggested that all information can be placed on a continuum according to the extent to which the elements that constitute the information interact. At one extreme, there is no interaction between the elements that need to be learned. They are independent. Element interactivity is low, or indeed, non-existent, and that means each element can be considered and learned serially without reference to any other element. Because elements at the low element interactivity end of the continuum do not interact with each other, there is no loss of understanding despite each element being learned individually and in isolation. Understanding is defined as the ability to process all elements that necessarily interact, simultaneously in working memory. Learning such material"}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "041326c202655cd60df276bf7a148f2ecddfc479"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-041326c202655cd60df276bf7a148f2ecddfc479", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "In this paper, we examine the motivations for research on cognitive architectures and review some candidates that have been explored in the literature. After this, we consider the capabilities that a cognitive architecture should support, some properties that it should exhibit related to representation, organization, performance, and learning, and some criteria for evaluating such architectures at the systems level. In closing, we discuss some open issues that should drive future research in this important area. 2008 Published by Elsevier B.V."}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "598eadcca8ac9365d188157d585e076dc2ef60d9"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-598eadcca8ac9365d188157d585e076dc2ef60d9", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "Existing cognitively based computational models for wayfinding focus primarily on the exploration of mental representations rather than the information needs for wayfinding. It is important to consider information needs because people trying to find their ways in unfamiliar environments do not have a previously acquired mental representation but depend on external information. The fundamental tenet of this work is that all information must be presented to the wayfinder at each decision point as “knowledge in the world.” Simulating people’s wayfinding behavior in a cognitively plausible way requires the integration of structures for information perception and cognition in the underlying model. In this work we use a cognizing agent to simulate people’s wayfinding processes in unfamiliar buildings. The agent-based model consists of two tiers: simulated states of the environment and simulated beliefs of the agent. The agent is modeled with state, an observation schema, a specific wayfinding strategy, and commonsense knowledge. The environment is modeled as a graph, where nodes represent decision points and edges represent lines of movement. The perceptual wayfinding model integrates the agent and its environment within a “Sense-Plan-Act” framework. It focuses on knowledge in the world to explain actions of the agent. The concepts of affordance and information are used to describe the kinds of knowledge the agent derives from the world by means of visual perception. Affordances are possibilities for action with reference to the agent. Information is necessary for the agent to decide which affordances to utilize. During the navigation process the agent accumulates beliefs about the environment by observing task-relevant affordances and information at decision points. The utilization of a “go-to” affordance leads the agent from one node to another where it is again provided with percepts. A successful navigation corresponds to the agent’s traversal from a start to a goal node. The proposed formal specifications of the agent-based model can be used to simulate people’s wayfinding behavior in spatial information and design systems in a cognitively plausible way. Such simulation helps to determine where and why people face wayfinding difficulties and what needs to be done to avoid them. The case of wayfinding in an airport, in which the signage in the airport is tested, is employed to demonstrate the perceptual wayfinding model."}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "da67375c8b6a250fbd5482bfbfce14f4eb7e506c"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-da67375c8b6a250fbd5482bfbfce14f4eb7e506c", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "This survey presents an overview of the autonomous development of mental capabilities in computational agents. It does so based on a characterization of cognitive systems as systems which exhibit adaptive, anticipatory, and purposive goal-directed behavior. We present a broad survey of the various paradigms of cognition, addressing cognitivist (physical symbol systems) approaches, emergent systems approaches, encompassing connectionist, dynamical, and enactive systems, and also efforts to combine the two in hybrid systems. We then review several cognitive architectures drawn from these paradigms. In each of these areas, we highlight the implications and attendant problems of adopting a developmental approach, both from phylogenetic and ontogenetic points of view. We conclude with a summary of the key architectural features that systems capable of autonomous development of mental capabilities should exhibit"}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "5e880d3bd1c4c4635ea7684df47109a33448b4c2"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-5e880d3bd1c4c4635ea7684df47109a33448b4c2", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "This paper describes an architecture that combines the complementary strengths of probabilistic graphical models and declarative programming to enable robots to represent and reason with qualitative and quantitative descriptions of uncertainty and domain knowledge. An action language is used for the architecture’s low-level (LL) and high-level (HL) system descriptions, and the HL definition of recorded history is expanded to allow prioritized defaults. For any given objective, tentative plans created in the HL using commonsense reasoning are implemented in the LL using probabilistic algorithms, and the corresponding observations are added to the HL history. Tight coupling between the levels helps automate the selection of relevant variables and the generation of policies in the LL for each HL action, and supports reasoning with violation of defaults, noisy observations and unreliable actions in complex domains. The architecture is evaluated in simulation and on robots moving objects in indoor domains."}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "e26cbabe8c60f1c62616917410f47ac2ad7d7609"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-e26cbabe8c60f1c62616917410f47ac2ad7d7609", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "OBJECTIVE\nThis paper explores the development of a rigorous computational model of driver behavior in a cognitive architecture--a computational framework with underlying psychological theories that incorporate basic properties and limitations of the human system.\n\n\nBACKGROUND\nComputational modeling has emerged as a powerful tool for studying the complex task of driving, allowing researchers to simulate driver behavior and explore the parameters and constraints of this behavior.\n\n\nMETHOD\nAn integrated driver model developed in the ACT-R (Adaptive Control of Thought-Rational) cognitive architecture is described that focuses on the component processes of control, monitoring, and decision making in a multilane highway environment.\n\n\nRESULTS\nThis model accounts for the steering profiles, lateral position profiles, and gaze distributions of human drivers during lane keeping, curve negotiation, and lane changing.\n\n\nCONCLUSION\nThe model demonstrates how cognitive architectures facilitate understanding of driver behavior in the context of general human abilities and constraints and how the driving domain benefits cognitive architectures by pushing model development toward more complex, realistic tasks.\n\n\nAPPLICATION\nThe model can also serve as a core computational engine for practical applications that predict and recognize driver behavior and distraction."}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "16524adee515692a50dd67a170b8e605e4b00b29"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-16524adee515692a50dd67a170b8e605e4b00b29", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "In his book “Conceptual Spaces The Geometry of Thought”, Peter Gärdenfors [1] presents a pioneering theory for representing conceptual knowledge, the basic construct of human thinking and reasoning [4]. The conceptual level is not seen as an alternative to traditional approaches of knowledge representation in artificial intelligence, namely symbolic or subsymbolic methods. Instead, it is meant to complement both approaches. The book is highly recommendable and worth reading, as it does not only tackle many fundamental problems of knowledge representation such as grounding [3], concept formation and similarity comparisons [2], but also outlines novel and enlightening ideas how to overcome these. The book introduces the notion of a conceptual space as a framework for representing knowledge at the conceptual level. It is motivated by contrasting it to other levels of knowledge representation: The highest level, the symbolic level, conceptualizes the human brain as a computing device. Knowledge is represented based on a language consisting of a set of symbols. Logical operations can be performed on these symbols to infer new knowledge. Human reasoning is modeled as a symbol manipulation process. Classical, symbolic artificial intelligence does not very well support central cognitive processes such as the acquisition or formation of new concepts and similarity comparisons. The lowest level, the subsymbolic knowledge representation, is oriented towards the neuro-biological structure of the human brain. Concepts are implicitly represented via activation patterns within the neural network. Learning is modeled by modifying the activation of neurons. Explicit representation of knowledge and concepts is not possible. At the intersection between the symbolic and the subsymbolic level, Gärdenfors introduces the conceptual level. The theory of conceptual spaces is based on semantic spaces with a geometric structure: A conceptual space is formed by a set of quality dimensions. One or several quality dimensions model one domain. An important example used throughout the book is the color domain represented by the quality dimensions hue, saturation and brightness. Conceptual spaces have a cognitive foundation because domains can be grounded in qualities perceivable by the human sensory apparatus. Concepts are represented as conceptual regions described by their properties on the quality dimensions. The geometric structure of conceptual spaces makes it possible to determine distances and therefore provides an inherent similarity measure by taking the distance in the conceptual space as indicator of the semantic similarity. The notion of similarity is an important construct for modeling categorization and concept formation. Using similarity for reasoning can also reflect well the vagueness typical for human reasoning. The strong focus on the cognitive foundation makes the book particularly valuable. It contains many challenging claims which are related to various disciplines by giving evidence from a wide range of literature. This shows the huge and highly interdisciplinary background of the author. Unfortunately, Gärdenfors describes his theory only at a very abstract level and forbears from describing algorithms for the formalization of his theory. The realization of a computational model for conceptual spaces bears many practical problems which still have to be solved. Moreover, no empirical evidence is given for his pioneering, sometimes revolutionary ideas. However, these shortcomings should be considered as challenges to solve in the future. The target audience of the book is highly interdisciplinary: since Gärdenfors tackles the problem of cognitive knowledge representation from a psychologic and computer science perspective as well as from a philosophic, neuroscience and linguistic point of view, this book is worth reading for researchers from many different areas. It is required reading for researchers in cognitive science or artificial intelligence interested in knowledge representation. The book has a clear structure and is very well written. The convincing examples throughout the book illustrate the findings very well and make it easy to understand. Therefore I would also deem Gärdenfors’ book to be suitable for students as introducing literature to various problem fields in cognitive science. It gives readers from related areas the chance to look beyond one’s own nose and get to know an interdisciplinary way of thinking. The book certainly meets the expectations of the highly interdisciplinary research area cognitive science."}
{"metadata": {"dataset": "scidocs", "query_id": "02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f", "doc_id": "ad943b2c2b859e46481308786c6aea9063dd49a9"}, "id": "scidocs-02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f-ad943b2c2b859e46481308786c6aea9063dd49a9", "question": "A Gentle Introduction to Soar, an Architecture for Human Cognition.", "context": "The improved ground resolution of state-of-the-art synthetic aperture radar (SAR) sensors suggests utilizing SAR data for the analysis of urban areas. The appearance of buildings in SAR or interferometric SAR (InSAR) data is characterized by the consequences of the inherent oblique scene illumination, such as layover, occlusion by radar shadow, and multipath signal propagation. Therefore, particularly in dense built-up areas, building reconstruction is often impossible from a single SAR or InSAR measurement alone. But, the reconstruction quality can be significantly improved by a combined analysis of multi-aspect data. In this paper, two approaches are proposed to detect and reconstruct buildings of different size from multi-aspect high-resolution InSAR data sets. Both approaches focus on the recognition of buildings supported by knowledge-based analysis considering the mentioned SAR-specific effects observed in urban areas. Building features are extracted independently for each direction from the magnitude and phase information of the interferometric data. Initial primitives are segmented and afterward projected from slant-range into the world coordinate system. From the fused set of primitives of both flight directions, building hypotheses are generated. The first approach exploits the frequently observed lines of bright double-bounce scattering, which are used for building reconstruction in residential districts. In the case of larger buildings, such as industrial halls, often additional features of roof and facade elements are visible. Therefore, in a second approach, extended buildings are extracted by grouping primitives of different kinds. The two approaches are demonstrated in an urban environment for an InSAR data set, which has spatial resolution of about 30 cm and was taken from two orthogonal flight directions."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "no"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-no", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "090ab6c395c010ced449b540c425a6a2835647a6"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-090ab6c395c010ced449b540c425a6a2835647a6", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "Image super-resolution is a fundamental pre-processing technique for the machine vision applications of robotics and other mobile platforms. Inevitably, images captured by the mobile camera tend to emerge severe motion blur and this degradation will deteriorate the performance of current state-of-the-art super-resolution methods. In this paper, we propose a deep dual-branch convolution neural network (CNN) to generate a clear high-resolution image from a single natural image with severe blurs. Compared to off-the-shelf methods, our method, called DB-SRN, can remove the complex non-uniform motion blurs and restore useful texture details simultaneously. By sharing the features from modified residual blocks (ResBlocks), the dual-branch design can promote the performances of both tasks other while retaining network simplicity. Extensive experiments demonstrate that our method produces remarkable deblurred and super-resolved images in terms of quality and quantity with high computational efficiency."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "5e4b405202c92fd77a12f463ca1247a8b59fd935"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-5e4b405202c92fd77a12f463ca1247a8b59fd935", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "Novel low-density signature (LDS) structure is proposed for transmission and detection of symbol-synchronous communication over memoryless Gaussian channel. Given N as the processing gain, under this new arrangement, users' symbols are spread over N chips but virtually only dv < N chips that contain nonzero-values. The spread symbol is then so uniquely interleaved as the sampled, at chip rate, received signal contains the contribution from only dc < K number of users, where K denotes the total number of users in the system. Furthermore, a near-optimum chip-level iterative soft-in-soft-out (SISO) multiuser decoding (MUD), which is based on message passing algorithm (MPA) technique, is proposed to approximate optimum detection by efficiently exploiting the LDS structure. Given beta = K/N as the system loading, our simulation suggested that the proposed system alongside the proposed detection technique, in AWGN channel, can achieve an overall performance that is close to single-user performance, even when the system has 200% loading, i.e., when beta = 2. Its robustness against near-far effect and its performance behavior that is very similar to optimum detection are demonstrated in this paper. In addition, the complexity required for detection is now exponential to dc instead of K as in conventional code division multiple access (CDMA) structure employing optimum multiuser detector."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "0c3751db5a24c636c1aa8abfd9d63321b38cfce5"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-0c3751db5a24c636c1aa8abfd9d63321b38cfce5", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "Stochastic Gradient Descent (SGD) has become popular for solving large scale supervised machine learning optimization problems such as SVM, due to their strong theoretical guarantees. While the closely related Dual Coordinate Ascent (DCA) method has been implemented in various software packages, it has so far lacked good convergence analysis. This paper presents a new analysis of Stochastic Dual Coordinate Ascent (SDCA) showing that this class of methods enjoy strong theoretical guarantees that are comparable or better than SGD. This analysis justifies the effectiveness of SDCA for practical applications."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "0e9a674280b2dabe36e540c20ce5a7a9e10361f7"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-0e9a674280b2dabe36e540c20ce5a7a9e10361f7", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "Cognitive Radio (CR) is a next-generation wireless communication system that enables unlicensed users to exploit underutilized licensed spectrum to optimize the utilization of the overall radio spectrum. A Distributed Cognitive Radio Network (DCRN) is a distributed wireless network established by a number of unlicensed users in the absence of fixed network infrastructure such as a base station. Context awareness and intelligence are the capabilities that enable each unlicensed user to observe and carry out its own action as part of the joint action on its operating environment for network-wide performance enhancement. These capabilities can be applied in various application schemes in CR networks such as Dynamic Channel Selection (DCS), congestion control, and scheduling. In this paper, we apply Reinforcement Learning (RL), including single-agent and multi-agent approaches, to achieve context awareness and intelligence. Firstly, we show that the RL approach achieves a joint action that provides better network-wide performance in respect to DCS in DCRNs. The multi-agent approach is shown to provide higher levels of stability compared to the single-agent approach. Secondly, we show that RL achieves high level of fairness. Thirdly, we show the effects of network density and various essential parameters in RL on the network-wide performance."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "1b9d022273780c5b0b7522555bd0e2c626a38e77"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-1b9d022273780c5b0b7522555bd0e2c626a38e77", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "Underwater images are known to be strongly deteriorated by a combination of wavelength-dependent light attenuation and scattering. This results in complex color casts that depend both on the scene depth map and on the light spectrum. Color transfer, which is a technique of choice to counterbalance color casts, assumes stationary casts, defined by global parameters, and is therefore not directly applicable to the locally variable color casts encountered in underwater scenarios. To fill this gap, this paper introduces an original fusion-based strategy to exploit color transfer while tuning the color correction locally, as a function of the light attenuation level estimated from the red channel. The Dark Channel Prior (DCP) is then used to restore the color compensated image, by inverting the simplified Koschmieder light transmission model, as for outdoor dehazing. Our technique enhances image contrast in a quite effective manner and also supports accurate transmission map estimation. Our extensive experiments also show that our color correction strongly improves the effectiveness of local keypoints matching."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "549fc15ee760ceb7569c38888b21cee1c3806148"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-549fc15ee760ceb7569c38888b21cee1c3806148", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "Any motor action results from a dynamic interplay of various brain regions involved in different aspects of movement preparation and execution. Establishing a reliable model of how these areas interact is crucial for a better understanding of the mechanisms underlying motor function in both healthy subjects and patients. We used fMRI and dynamic causal modeling to reveal the specific excitatory and inhibitory influences within the human motor system for the generation of voluntary hand movements. We found an intrinsic balance of excitatory and inhibitory couplings among core motor regions within and across hemispheres. Neural coupling within this network was specifically modulated upon uni- and bimanual movements. During unimanual movements, connectivity towards the contralateral primary motor cortex was enhanced while neural coupling towards ipsilateral motor areas was reduced by both transcallosal inhibition and top-down modulation. Bimanual hand movements were associated with a symmetric facilitation of neural activity mediated by both increased intrahemispheric connectivity and enhanced transcallosal coupling of SMA and M1. The data suggest that especially the supplementary motor area represents a key structure promoting or suppressing activity in the cortical motor network driving uni- and bilateral hand movements. Our data demonstrate that fMRI in combination with DCM allows insights into intrinsic properties of the human motor system and task-dependent modulations thereof."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "c6c95c996037c00c62df1d3d2cfb3e010a317faf"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-c6c95c996037c00c62df1d3d2cfb3e010a317faf", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "This paper provides an overview of DC side fault tolerance issues of VSC based HVDC system and the need for fault tolerant converters. The working principle and DC fault ride through capability of recently introduced Alternate Arm Modular Multilevel Converter(AAMMC) has been discussed. The capacitor voltage balancing issues of AAMMC is analyzed and a novel scheme for balancing capacitor voltages of the wave shaping circuit is presented in this paper. The voltage balancing of capacitors of wave shaping circuits in the arm is done by introducing an overlap period during zero voltage period. Using the proposed scheme, the magnitude and direction of the current during the overlap period can be controlled by varying the switching pattern. It helps in charging or discharging of the submodule capacitors to bring them to their reference value. At the end of the overlap period, the arm current is brought to zero before opening the director switch so as to avoid the spike across the arm inductor. The efficacy of the proposed control scheme has been validated using simulation study done in PSCAD/EMTDC."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "35bc3b88d20098869a2e5cdb8cb83ed926627af0"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-35bc3b88d20098869a2e5cdb8cb83ed926627af0", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "With the recent advance of wearable devices and Internet of Things (IoTs), it becomes attractive to implement the Deep Convolutional Neural Networks (DCNNs) in embedded and portable systems. Currently, executing the software-based DCNNs requires high-performance servers, restricting the widespread deployment on embedded and mobile IoT devices. To overcome this obstacle, considerable research efforts have been made to develop highly-parallel and specialized DCNN accelerators using GPGPUs, FPGAs or ASICs.\n Stochastic Computing (SC), which uses a bit-stream to represent a number within [-1, 1] by counting the number of ones in the bit-stream, has high potential for implementing DCNNs with high scalability and ultra-low hardware footprint. Since multiplications and additions can be calculated using AND gates and multiplexers in SC, significant reductions in power (energy) and hardware footprint can be achieved compared to the conventional binary arithmetic implementations. The tremendous savings in power (energy) and hardware resources allow immense design space for enhancing scalability and robustness for hardware DCNNs.\n This paper presents SC-DCNN, the first comprehensive design and optimization framework of SC-based DCNNs, using a bottom-up approach. We first present the designs of function blocks that perform the basic operations in DCNN, including inner product, pooling, and activation function. Then we propose four designs of feature extraction blocks, which are in charge of extracting features from input feature maps, by connecting different basic function blocks with joint optimization. Moreover, the efficient weight storage methods are proposed to reduce the area and power (energy) consumption. Putting all together, with feature extraction blocks carefully selected, SC-DCNN is holistically optimized to minimize area and power (energy) consumption while maintaining high network accuracy. Experimental results demonstrate that the LeNet5 implemented in SC-DCNN consumes only 17 mm2 area and 1.53 W power, achieves throughput of 781250 images/s, area efficiency of 45946 images/s/mm2, and energy efficiency of 510734 images/J."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "5ad3c058535653b1c898302ffa42d5dccee542e3"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-5ad3c058535653b1c898302ffa42d5dccee542e3", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "In this paper we propose a convolutional neural network (CNN), which allows to identify corresponding patches of very high resolution (VHR) optical and SAR imagery of complex urban scenes. Instead of a siamese architecture as conventionally used in CNNs designed for image matching, we resort to a pseudo-siamese configuration with no interconnection between the two streams for SAR and optical imagery. The network is trained with automatically generated training data and does not resort to any hand-crafted features. First evaluations show that the network is able to predict corresponding patches with high accuracy, thus indicating great potential for further development to a generalized multi-sensor matching procedure."}
{"metadata": {"dataset": "scidocs", "query_id": "030ff7012b92b805a60976f8dbd6a08c1cecebe6", "doc_id": "0690ba31424310a90028533218d0afd25a829c8d"}, "id": "scidocs-030ff7012b92b805a60976f8dbd6a08c1cecebe6-0690ba31424310a90028533218d0afd25a829c8d", "question": "DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation", "context": "Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called ”semantic image segmentation”). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our “DeepLab” system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the ’hole’ algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "no"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-no", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "5a778a2a32f35a96f4dfb0f22d1415eff321e7ad"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-5a778a2a32f35a96f4dfb0f22d1415eff321e7ad", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "Due to their rapid evolution, mobile devices demand for more dynamic and flexible networking services. A major challenges of future mobile networks is the increased mobile traffic. With the recent upcoming technologies of network programmability like Software-Defined Network (SDN), it may be integrated to create a new communication platform for Internet of Things (IoT). In this work, we present how to determine the effectiveness of an approach to build a new secured network architecture based on SDN and clusters. Our proposed scheme is a starting point for some experiments providing perspective over SDN deployment in a cluster environment. With this aim in mind, we suggest a routing protocol that manages routing tasks over Cluster-SDN. By using network virtualization and OpenFlow technologies to generate virtual nodes, we simulate a prototype system controlled by SDN. With our testbed, we are able to manage 500 things. We can analyze every OpenFlow messages and we have discovered that with a particular flow, the things can exchange information unlike the routing principle."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "90e3ec000125d579ec1724781410d4201be6d2a8"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-90e3ec000125d579ec1724781410d4201be6d2a8", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "This paper presents the study of different communication systems between IEC 61850 based distribution substation and distributed energy resources (DERs). Communication networks have been simulated for a typical distribution automation system (DAS) with DERs using OPNET software. The simulation study shows the performance of wired and wireless communication systems for different messages, such as GOOSE and measured (metered) values between DAS and DERs. A laboratory set-up has been implemented using commercial relay and communication devices for evaluating the performance of GOOSE messages, using wired and wireless physical medium. Finally, simulation and laboratory results are discussed in detail."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "90ed29e10e65c0fa13b64903eeba0fef1ef3cc60"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-90ed29e10e65c0fa13b64903eeba0fef1ef3cc60", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "In this paper, we consider a robust network control problem. We consider linear unstable and uncertain discrete time plants with a network between the sensors and controller and the controller and plant. We investigate two defining characteristics of network controlled systems and the impact of uncertainty on these. Namely, the minimum data rates required for the two networks and the tolerable data drop out in the form of packet losses. We are able to derive sufficient conditions in terms of the minimum data rate and minimum packet arrival rate to ensure stability of the closed loop system. I. I NTRODUCTION Recently, networked control systems (NCS) have gained great attention from both the control community and the network and communication community. When compared with classical feedback control system, networked control systems have many advantages. For example, they can reduce the system wiring, make the system easy to operate and maintain and later diagnose in case of malfunctioning, and increase system agility [16]. In spite of the great advantages that the networked control architecture brings, inserting a network in between the plant and the controller introduces many problems as well. For instance, zero-delayed sensing and actuation, perfect information and synchronization are no longer guaranteed in the new system architecture as only finite bandwidth is available and packet drops and delays may occur due to network traffic conditions. These must be revisited and analyzed before any practical networked control systems are built. In the past decade, many researchers have spent effort on those issues and a number of significant results were obtained and many are in progress. Many of the aforementioned issues are studied separately. Tatikonda [15] and Sahai [11] have presented some interesting results in the area of control under communication constraints. Specifically, Tatikonda gave a necessary and sufficient condition on the channel data rate such that a noiseless LTI system in the closed loop is asymptotically stable. He also gave rate results for stabilizing a noisy LTI system over the digital channel. Sahai proposed the notion of anytime capacity to deal with real time estimation and control for a networked control system. In our companion paper [13], the authors have considered various rate issues under finite bandwidth, packet drops and finite controls. The effect of pacekt loss and delay on state Control and Dynamical Systems, California Institute of Technology; Pasadena, CA 91125. Email:{shiling, epstein, murray }@cds.caltech.edu; Tel: (626) 395-2313, Fax: (626) 796-8914. Work supported in part by NSF ITR grant CCR-0326554. The authors are equally contributed to this work. estimation was studied by the work of Sinopoli, et. al. in [2]. It has further been investigated by many researchers including the present authors in [12] and [5]. One of the hallmarks of a good control system design is that the closed loop remain stable in the presence of uncertainty [3], [4]. While the researchers in [7] studied the problem of LQG control across a packet dropping networks, not many have considered the norm bounded uncertainty investigated in the present paper. We examine the impact of a norm bounded uncertainty on the network control system and provide sufficient conditions for stability in terms of the minimum data rates and packet arrival rates for the networks. The paper is organized as follows. In Section II, we present the mathematical model of the closed loop system and state our assumptions. In Section III, we state the sufficient conditions for closed loop stability for the case where a network connects the sensors to the controller. In Section IV, we state the sufficient stability conditions where in addition there is a network between the controller and the plant. For both sections we obtain results for scalar and general vector cases. Conclusions and future work are given in the last section. II. PROBLEM SET UP We consider linear discrete time systems with a norm bounded uncertainty in the A matrix. We will investigate two NCS that we will define by the type of networks embedded in the control loop. The first NCS considered has a network between the measurement sensors and the controller, with the controller then directly connected to the actuators/plant. The second NCS will also include a network between the controller and the actuators/plant. These two network types and depicted in Figures 1 and 2. The networks are defined in terms of their data rates and probability of dropping packets. We would consider any packet delays as losses, i. ., we do not use delayed packets for estimation or control. The following equations represent the closed loop system for NCS I (Figure 1). xk+1 = (A + ∆k)xk + Buk (1) yk = λkCxk (2) wherexk ∈ IR is the state of the system, uk ∈ IR is the control input,yk ∈ IR is the output of the system, and λk are Bernoulli i.i.d random variable with parameter λ, i.e., E[λk] = λ for all k. ∆k satisfies∆k ∆k ≤ KI for all k. We also assume the initial condition x0 ∈ IR is bounded. The matrix A is assumed to be unstable without loss of generality as for any matrix A, we can always do some state transformation to decompose the states into stable ones and"}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "9d68bcc77f953c3ae24047b8c83b7172646845d8"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-9d68bcc77f953c3ae24047b8c83b7172646845d8", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "This paper proposes a self-organized power allocation technique to solve the interference problem caused by a femtocell network operating in the same channel as an orthogonal frequency division multiple access cellular network. We model the femto network as a multi-agent system where the different femto base stations are the agents in charge of managing the radio resources to be allocated to their femtousers. We propose a form of real-time multi-agent reinforcement learning, known as decentralized Q-learning, to manage the interference generated to macro-users. By directly interacting with the surrounding environment in a distributed fashion, the multi-agent system is able to learn an optimal policy to solve the interference problem. Simulation results show that the introduction of the femto network increases the system capacity without decreasing the capacity of the macro network."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "70624f16968fe4f0c851398dbd46a1ebcce892ce"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-70624f16968fe4f0c851398dbd46a1ebcce892ce", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "In vehicular ad hoc network (VANET), vehicles equipped with computing, sensing, and communication capabilities can exchange information within a geographical area to distribute emergency messages and achieve safety system. Then how to enforce fine grained control of these messages and ensure the receiving messages coming from the claimed source in such a highly dynamic environments remains a key challenge that affects the quality of service. In this paper, we propose a hierarchical access control with authentication scheme for transmitted messages with security assurance over VANET. By extending ciphertext-policy attribute-based encryption (CP-ABE) with a hierarchical structure of multiple authorities, the scheme not only achieves scalability due to its hierarchical structure, but also inherits fine-grained access control on the transmitted messages. Also by exploiting attribute-based signature (ABS), the scheme can authorize the vehicles that can most appropriately deal with the message efficiently. The results of efficiency analysis and comparison with the related works show that the proposed scheme is efficient and scalable in dealing with access control and message authentication for data dissemination in VANET. & 2014 Elsevier B.V. All rights reserved."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "051853e79d6ebe49601348536ca4b14c5279cc97"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-051853e79d6ebe49601348536ca4b14c5279cc97", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "Microgrids are a key component in the evolution of the power grid. Microgrids are required to operate in both grid connected and standalone island mode using local sources of power. A major challenge in implementing microgrids is the communications and control to support transition from grid connected mode and operation in island mode. Here, we propose a secure communication architecture to support microgrid operation and control. A security model, including network, data, and attack models, is defined and a security protocol to address the real-time communication needs of microgrids is proposed. The implementation of the proposed security scheme is discussed and its performance evaluated using theoretical and co-simulation analysis, which shows it to be superior to existing protocols."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "392c43f9e521b829d9d5b7d072e4bd7f2bcfbe8a"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-392c43f9e521b829d9d5b7d072e4bd7f2bcfbe8a", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "This work is concerned with the control strategy for the parallel operation of distributed generation systems (DGS) in a standalone ac power supply. The proposed control method uses only low-bandwidth data communication signals between each generation system in addition to the locally measurable feedback signals. This is achieved by combining two control methods: droop control method and average power control method. The average power method with slow update rate is used in order to overcome the sensitivity about voltage and current measurement errors. In addition, a harmonic droop scheme for sharing harmonic content of the load currents is proposed based on the voltages and currents control algorithm. Experimental and simulation studies using two parallel three-phase pulsewidth modulation (PWM) inverters are presented to show the effectiveness of the proposed control."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "53bf890ddba4d6433e868c0a73a529243c23591c"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-53bf890ddba4d6433e868c0a73a529243c23591c", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "Network Service Providers should offer provisioning services with guaranteed Quality of Service (QoS), specifically adapted to the characteristics of the applications running on their network. In this paper we propose the Network Control Layer (NCL), a software framework solution based on Software Defined Networks (SDN), OpenFlow and Network as a Service (NaaS) paradigms. It addresses a major innovation area in the field of network control and management providing on-demand end-to-end network provisioning services with guaranteed QoS, based on the specific requirements of on-top running interactive applications. The NCL implementation is based on the OpenNaaS framework, and it includes mechanisms for network status monitoring and SDN switches configuration based on the interactive applications' QoS network requirements. We demonstrate NCL's utility in the context of control plane models making use of a practical use case."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "b1ca425cab859aa04259f0a093b7c948abd0e630"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-b1ca425cab859aa04259f0a093b7c948abd0e630", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "In this paper, we investigate a communication system in which unmanned aerial vehicles (UAVs) are used as relays between ground-based terminals and a network base station. We develop an algorithm for optimizing the performance of the ground-to-relay links through control of the UAV heading angle. To quantify link performance, we define the ergodic normalized transmission rate (ENTR) for the links between the ground nodes and the relay, and derive a closed-form expression for it in terms of the eigenvalues of the channel correlation matrix. We show that the ENTR can be approximated as a sinusoid with an offset that depends on the heading of the UAV. Using this observation, we develop a closed-form expression for the UAV heading that maximizes the uplink network data rate while keeping the rate of each individual link above a certain threshold. When the current UAV relay assignments cannot meet the minimum link requirements, we investigate the deployment and heading control problem for new UAV relays as they are added to the network, and propose a smart handoff algorithm that updates node and relay assignments as the topology of the network evolves."}
{"metadata": {"dataset": "scidocs", "query_id": "031a0f18d46b8e006eb4262233f7734fe4505c21", "doc_id": "7826bc81ffed9c1f342616df264c92d6f732f4dd"}, "id": "scidocs-031a0f18d46b8e006eb4262233f7734fe4505c21-7826bc81ffed9c1f342616df264c92d6f732f4dd", "question": "AntNet: Distributed Stigmergetic Control for Communications Networks", "context": "Autonomic communications aim to provide the quality-of-service in networks using self-management mechanisms. It inherits many characteristics from autonomic computing, in particular, when communication systems are running as specialized applications in software-defined networking (SDN) and network function virtualization (NFV)-enabled cloud environments. This paper surveys autonomic computing and communications in the context of software-driven networks, i.e., networks based on SDN/NFV concepts. Autonomic communications create new challenges in terms of security, operations, and business support. We discuss several goals, research challenges, and development issues on self-management mechanisms and architectures in software-driven networks. This paper covers multiple perspectives of autonomic communications in software-driven networks, such as automatic testing, integration, and deployment of network functions. We also focus on self-management and optimization, which make use of machine learning techniques."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "no"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-no", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "5c6b51bb44c9b2297733b58daaf26af01c98fe09"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-5c6b51bb44c9b2297733b58daaf26af01c98fe09", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "The paper systematically compares two feature extraction algorithms to mine product features commented on in customer reviews. The first approach [17] identifies candidate features by applying a set of POS patterns and pruning the candidate set based on the log likelihood ratio test. The second approach [11] applies association rule mining for identifying frequent features and a heuristic based on the presence of sentiment terms for identifying infrequent features. We evaluate the performance of the algorithms on five product specific document collections regarding consumer electronic devices. We perform an analysis of errors and discuss advantages and limitations of the algorithms."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "fa46ae777be8776a417a24e0b6c3f6076c5e578d"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-fa46ae777be8776a417a24e0b6c3f6076c5e578d", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "Detection and recognition of text from any natural scene image is challenging but essential extensively for extracting information from the image. In this paper, we propose an accurate and effective algorithm for detecting enhanced Maximally Stable Extremal Regions (MSERs) as main character candidates and these character candidates are filtered by stroke width variation for removing regions where the stroke width exhibits too much variation. For the detection of text regions, firstly some preprocessing is applied to the natural image and then after detecting MSERs, an intersection of canny edge and MSER region is produced to locate regions that are even more likely to belong to text. Finally, the selected text region is taken as an input of a novel Optical Character Recognition (OCR) technique to make the text editable and usable. The evaluation results substantiates 77.47% of the f-measure on the ICDAR 2011 dataset which is better than the previous performance 76.22%."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "caec97674544a4948a1b0ec2b9f6c624b87b647b"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-caec97674544a4948a1b0ec2b9f6c624b87b647b", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "This paper analyzes, compares, and contrasts technical challenges, methods, and the performance of text detection and recognition research in color imagery. It summarizes the fundamental problems and enumerates factors that should be considered when addressing these problems. Existing techniques are categorized as either stepwise or integrated and sub-problems are highlighted including text localization, verification, segmentation and recognition. Special issues associated with the enhancement of degraded text and the processing of video text, multi-oriented, perspectively distorted and multilingual text are also addressed. The categories and sub-categories of text are illustrated, benchmark datasets are enumerated, and the performance of the most representative approaches is compared. This review provides a fundamental comparison and analysis of the remaining problems in the field."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "7f44973b8cb78be47d55d335f40a54aa00ef814c"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-7f44973b8cb78be47d55d335f40a54aa00ef814c", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "This paper proposes a novel method for offline text-independent writer identification by using convolutional neural network (CNN) and joint Bayesian, which consists of two stages, i.e. feature extraction and writer identification. In the stage of feature extraction, since a large number of data is essential to train an effective CNN model with high generalizability and the amount of handwriting is limited in writer identification, a data augmentation technique is first developed to generate thousands of handwriting images for each writer. Then a deep CNN network is designed to extract discriminative features to represent the properties of different writing styles, which is trained by using the generated handwriting images. In the stage of writer identification, the training dataset is used to train the CNN model for feature extraction and the joint Bayesian technique is employed to accomplish the task of writer identification based on the extracted CNN features. The proposed method is tested on two standard benchmark datasets, i.e. ICDAR2013 and CVL dataset. Experimental results demonstrate that the proposed method gets the best performance compared to the state-of-the-art approaches."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "e78a8ff7b0adac4f6255dd999342c85f6a28e2f0"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-e78a8ff7b0adac4f6255dd999342c85f6a28e2f0", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "This paper presents an OCR method for degraded character recognition applied to a reference number (RN) of 15 printed characters of an invoice document produced by dot-matrix printer. First, the paper deals with the problem of the reference number localization and extraction, in which the characters tops or bottoms are or not touched with a printed reference line of the electrical bill. In case of touched RN, the extracted characters are severely degraded leading to missing parts in the characters tops or bottoms. Secondly, a combined recognition method based on the complementary similarity measure (CSM) method and MLP-based classifier is used. The CSM is used to accept or reject an incoming character. In case of acceptation, the CSM acts as a feature extractor and produces a feature vector of ten component features. The MLP is then trained using these feature vectors. The use of the CSM as a feature extractor tends to make the MLP very powerful and very well suited for rejection. Experimental results on electrical bills show the ability of the model to yield relevant and robust recognition on severely degraded printed characters."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "5040c699cc3a02d8dd2eecfdb20e5690432ad7a5"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-5040c699cc3a02d8dd2eecfdb20e5690432ad7a5", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "As the number of customer reviews grows very rapidly, it is essential to summarize useful opinions for buyers, sellers and producers. One key step of opinion mining is feature extraction. Most existing research focus on finding explicit features, only a few attempts have been made to extract implicit features. Nearly all existing research only concentrate on product features, few has paid attention to other features that relates to sellers, services and logistics. Therefore in this paper, we propose a novel co-occurrence association-based method, which aims to extract implicit features in customer reviews and provide more comprehensive and fine-grained mining results."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "a79f43246bed540084ca2d1fcf99a68c69820747"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-a79f43246bed540084ca2d1fcf99a68c69820747", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "Text detection and localization in natural scene images is important for content-based image analysis. This problem is challenging due to the complex background, the non-uniform illumination, the variations of text font, size and line orientation. In this paper, we present a hybrid approach to robustly detect and localize texts in natural scene images. A text region detector is designed to estimate the text existing confidence and scale information in image pyramid, which help segment candidate text components by local binarization. To efficiently filter out the non-text components, a conditional random field (CRF) model considering unary component properties and binary contextual component relationships with supervised parameter learning is proposed. Finally, text components are grouped into text lines/words with a learning-based energy minimization method. Since all the three stages are learning-based, there are very few parameters requiring manual tuning. Experimental results evaluated on the ICDAR 2005 competition dataset show that our approach yields higher precision and recall performance compared with state-of-the-art methods. We also evaluated our approach on a multilingual image dataset with promising results."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "08fddf1865e48a1adc21d4875396a754711f0a28"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-08fddf1865e48a1adc21d4875396a754711f0a28", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "Machine learning for text classification is the cor nerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is essential to make the learning task efficient and m ore accurate. This paper presents an empirical comparison of twelve feature selection methods (e.g . Information Gain) evaluated on a benchmark of 229 text classification problem instances that w ere gathered from Reuters, TREC, OHSUMED, etc. The results are analyzed from multiple goal p ers ectives—accuracy, F-measure, precision, and recall—since each is appropriate in different si tuat ons. The results reveal that a new feature selection me tric we call ‘Bi-Normal Separation’ (BNS), outperformed the others by a substantial margin in most situations. This margin widened in tasks with high class skew, which is rampant in text clas sification problems and is particularly challenging for induction algorithms. A new evaluation methodology is offered that focus es on the needs of the data mining practitioner faced with a single dataset who seeks to choose one (or a pair of) metrics that are most likely to yield the best performance. From this perspect iv , BNS was the top single choice for all goals except precision, for which Information Gain yielded the best result most often. This analysis also revealed, for example, that Informati on Gain and Chi-Squared have correlated failures, and so they work poorly together. When c hoosing optimal pairs of metrics for each of the four performance goals, BNS is consistently a membe r of the pair—e.g., for greatest recall, the pair BNS + F1-measure yielded the best performance on the greatest number of tasks by a considerable margin."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "68e47fea6d61acbf0b058a963e42228a4e3f07af"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-68e47fea6d61acbf0b058a963e42228a4e3f07af", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "Programming community-based question-answering (PCQA) websites such as Stack Overflow enable programmers to find working solutions to their questions. Despite detailed posting guidelines, duplicate questions that have been answered are frequently created. To tackle this problem, Stack Overflow provides a mechanism for reputable users to manually mark duplicate questions. This is a laborious effort, and leads to many duplicate questions remain undetected. Existing duplicate detection methodologies from traditional community based question-answering (CQA) websites are difficult to be adopted directly to PCQA, as PCQA posts often contain source code which is linguistically very different from natural languages. In this paper, we propose a methodology designed for the PCQA domain to detect duplicate questions. We model the detection as a classification problem over question pairs. To extract features for question pairs, our methodology leverages continuous word vectors from the deep learning literature, topic model features and phrases pairs that co-occur frequently in duplicate questions mined using machine translation systems. These features capture semantic similarities between questions and produce a strong performance for duplicate detection. Experiments on a range of real-world datasets demonstrate that our method works very well; in some cases over 30% improvement compared to state-of-the-art benchmarks. As a product of one of the proposed features, the association score feature, we have mined a set of associated phrases from duplicate questions on Stack Overflow and open the dataset to the public."}
{"metadata": {"dataset": "scidocs", "query_id": "033897d56c7cf4f084dec1fad072f1a6aca65c6e", "doc_id": "0621213a012d169cb7c2930354c6489d6a89baf8"}, "id": "scidocs-033897d56c7cf4f084dec1fad072f1a6aca65c6e-0621213a012d169cb7c2930354c6489d6a89baf8", "question": "Feature Extraction and Duplicate Detection for Text Mining : A Survey", "context": "In this paper, we define and study a novel text mining problem, which we refer to as Comparative Text Mining (CTM). Given a set of comparable text collections, the task of comparative text mining is to discover any latent common themes across all collections as well as summarize the similarity and differences of these collections along each common theme. This general problem subsumes many interesting applications, including business intelligence and opinion summarization. We propose a generative probabilistic mixture model for comparative text mining. The model simultaneously performs cross-collection clustering and within-collection clustering, and can be applied to an arbitrary set of comparable text collections. The model can be estimated efficiently using the Expectation-Maximization (EM) algorithm. We evaluate the model on two different text data sets (i.e., a news article data set and a laptop review data set), and compare it with a baseline clustering method also based on a mixture model. Experiment results show that the model is quite effective in discovering the latent common themes across collections and performs significantly better than our baseline mixture model."}
{"metadata": {"dataset": "scidocs", "query_id": "03725753e46ee9b13cbdfa78c9b62700d4cc2956", "doc_id": "no"}, "id": "scidocs-03725753e46ee9b13cbdfa78c9b62700d4cc2956-no", "question": "BRAND: A robust appearance and depth descriptor for RGB-D images", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "03725753e46ee9b13cbdfa78c9b62700d4cc2956", "doc_id": "1af5293e7e270d35be5eca28cd904d1e8fc9219c"}, "id": "scidocs-03725753e46ee9b13cbdfa78c9b62700d4cc2956-1af5293e7e270d35be5eca28cd904d1e8fc9219c", "question": "BRAND: A robust appearance and depth descriptor for RGB-D images", "context": "This paper deals with a new low level feature that is extracted from the images and can be used for indexing and retrieval. This feature is called “Color and Edge Directivity Descriptor” and incorporates color and texture information in a histogram. CEDD size is limited to 54 bytes per image, rendering this descriptor suitable for use in large image databases. One of the most important attribute of the CEDD is the low computational power needed for its extraction, in comparison with the needs of the most MPEG-7 descriptors. The objective measure called ANMRR is used to evaluate the performance of the proposed feature. An online demo that implements the proposed feature in an image retrieval system is available at: http://orpheus.ee.duth.gr/image_retrieval."}
{"metadata": {"dataset": "scidocs", "query_id": "03725753e46ee9b13cbdfa78c9b62700d4cc2956", "doc_id": "383e3b9e408f57052bbbc430b8e6b60c0e31f7ef"}, "id": "scidocs-03725753e46ee9b13cbdfa78c9b62700d4cc2956-383e3b9e408f57052bbbc430b8e6b60c0e31f7ef", "question": "BRAND: A robust appearance and depth descriptor for RGB-D images", "context": "We propose a new deep learning architecture for the tasks of semantic segmentation and depth prediction from RGBD images. We revise the state of art based on the RGB and depth feature fusion, where both modalities are assumed to be available at train and test time. We propose a new architecture where the feature fusion is replaced with a common deep representation. Combined with an encoder-decoder type of the network, the architecture can jointly learn models for semantic segmentation and depth estimation based on their common representation. This representation, inspired by multi-view learning, offers several important advantages, such as using one modality available at test time to reconstruct the missing modality. In the RGB-D case, this enables the cross-modality scenarios, such as using depth data for semantically segmentation and the RGB images for depth estimation. We demonstrate the effectiveness of the proposed network on two publicly available RGB-D datasets. The experimental results show that the proposed method works well in both semantic segmentation and depth estimation tasks."}
{"metadata": {"dataset": "scidocs", "query_id": "03725753e46ee9b13cbdfa78c9b62700d4cc2956", "doc_id": "1b1337a166cdcf6ee51a70cb23f291c36e9eee34"}, "id": "scidocs-03725753e46ee9b13cbdfa78c9b62700d4cc2956-1b1337a166cdcf6ee51a70cb23f291c36e9eee34", "question": "BRAND: A robust appearance and depth descriptor for RGB-D images", "context": "RGB-D scanning of indoor environments is important for many applications, including real estate, interior design, and virtual reality. However, it is still challenging to register RGB-D images from a hand-held camera over a long video sequence into a globally consistent 3D model. Current methods often can lose tracking or drift and thus fail to reconstruct salient structures in large environments (e.g., parallel walls in different rooms). To address this problem, we propose a fine-to-coarse global registration algorithm that leverages robust registrations at finer scales to seed detection and enforcement of new correspondence and structural constraints at coarser scales. To test global registration algorithms, we provide a benchmark with 10,401 manually-clicked point correspondences in 25 scenes from the SUN3D dataset. During experiments with this benchmark, we find that our fine-to-coarse algorithm registers long RGB-D sequences better than previous methods."}
{"metadata": {"dataset": "scidocs", "query_id": "03725753e46ee9b13cbdfa78c9b62700d4cc2956", "doc_id": "4e348a6bb29f7ac5514ba52d503417424153223c"}, "id": "scidocs-03725753e46ee9b13cbdfa78c9b62700d4cc2956-4e348a6bb29f7ac5514ba52d503417424153223c", "question": "BRAND: A robust appearance and depth descriptor for RGB-D images", "context": "Semantic segmentation has made encouraging progress due to the success of deep convolutional networks in recent years. Meanwhile, depth sensors become prevalent nowadays, so depth maps can be acquired more easily. However, there are few studies that focus on the RGB-D semantic segmentation task. Exploiting the depth information effectiveness to improve performance is a challenge. In this paper, we propose a novel solution named LDFNet, which incorporates Luminance, Depth and Color information by a fusion-based network. It includes a sub-network to process depth maps and employs luminance images to assist the depth information in processes. LDFNet outperforms the other state-of-art systems on the Cityscapes dataset, and its inference speed is faster than most of the existing networks. The experimental results show the effectiveness of the proposed multi-modal fusion network and its potential for practical applications."}
{"metadata": {"dataset": "scidocs", "query_id": "03725753e46ee9b13cbdfa78c9b62700d4cc2956", "doc_id": "8ac74b8679cf073e4205ad9463d01db7e6b5838c"}, "id": "scidocs-03725753e46ee9b13cbdfa78c9b62700d4cc2956-8ac74b8679cf073e4205ad9463d01db7e6b5838c", "question": "BRAND: A robust appearance and depth descriptor for RGB-D images", "context": "In this paper, we present RKD-SLAM, a robust keyframe-based dense SLAM approach for an RGB-D camera that can robustly handle fast motion and dense loop closure, and run without time limitation in a moderate size scene. It not only can be used to scan high-quality 3D models, but also can satisfy the demand of VR and AR applications. First, we combine color and depth information to construct a very fast keyframe-based tracking method on a CPU, which can work robustly in challenging cases (e.g. fast camera motion and complex loops). For reducing accumulation error, we also introduce a very efficient incremental bundle adjustment (BA) algorithm, which can greatly save unnecessary computation and perform local and global BA in a unified optimization framework. An efficient keyframe-based depth representation and fusion method is proposed to generate and timely update the dense 3D surface with online correction according to the refined camera poses of keyframes through BA. The experimental results and comparisons on a variety of challenging datasets and TUM RGB-D benchmark demonstrate the effectiveness of the proposed system."}
{"metadata": {"dataset": "scidocs", "query_id": "03725753e46ee9b13cbdfa78c9b62700d4cc2956", "doc_id": "601c9ac5859021c5c1321adeb38b177ebad346f0"}, "id": "scidocs-03725753e46ee9b13cbdfa78c9b62700d4cc2956-601c9ac5859021c5c1321adeb38b177ebad346f0", "question": "BRAND: A robust appearance and depth descriptor for RGB-D images", "context": " A novel and effective salient color names (SCNCD) based color descriptor is proposed for person re-identification;  Background information is exploited to enrich the feature representation which is of good robustness against background interference and partial occlusions;  To tackle different types of illumination changes, color names distribution and color histograms are fused in four color spaces. Experimental Results"}
{"metadata": {"dataset": "scidocs", "query_id": "03725753e46ee9b13cbdfa78c9b62700d4cc2956", "doc_id": "dbd66f601b325404ff3cdd7b9a1a282b2da26445"}, "id": "scidocs-03725753e46ee9b13cbdfa78c9b62700d4cc2956-dbd66f601b325404ff3cdd7b9a1a282b2da26445", "question": "BRAND: A robust appearance and depth descriptor for RGB-D images", "context": "We introduce T-LESS, a new public dataset for estimating the 6D pose, i.e. translation and rotation, of texture-less rigid objects. The dataset features thirty industry-relevant objects with no significant texture and no discriminative color or reflectance properties. The objects exhibit symmetries and mutual similarities in shape and/or size. Compared to other datasets, a unique property is that some of the objects are parts of others. The dataset includes training and test images that were captured with three synchronized sensors, specifically a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera. There are approximately 39K training and 10K test images from each sensor. Additionally, two types of 3D models are provided for each object, i.e. a manually created CAD model and a semi-automatically reconstructed one. Training images depict individual objects against a black background. Test images originate from twenty test scenes having varying complexity, which increases from simple scenes with several isolated objects to very challenging ones with multiple instances of several objects and with a high amount of clutter and occlusion. The images were captured from a systematically sampled view sphere around the object/scene, and are annotated with accurate ground truth 6D poses of all modeled objects. Initial evaluation results indicate that the state of the art in 6D object pose estimation has ample room for improvement, especially in difficult cases with significant occlusion. The T-LESS dataset is available online at cmp:felk:cvut:cz/t-less."}
{"metadata": {"dataset": "scidocs", "query_id": "03725753e46ee9b13cbdfa78c9b62700d4cc2956", "doc_id": "baf8ace34b363e123a115ffdf0eac4f39fd4f199"}, "id": "scidocs-03725753e46ee9b13cbdfa78c9b62700d4cc2956-baf8ace34b363e123a115ffdf0eac4f39fd4f199", "question": "BRAND: A robust appearance and depth descriptor for RGB-D images", "context": "The ability to classify objects is fundamental for robots. Besides knowledge about their visual appearance, captured by the RGB channel, robots heavily need also depth information to make sense of the world. While the use of deep networks on RGB robot images has benefited from the plethora of results obtained on databases like ImageNet, using convnets on depth images requires mapping them into three-dimensional channels. This transfer learning procedure makes them processable by pretrained deep architectures. Current mappings are based on heuristic assumptions over preprocessing steps and on what depth properties should be most preserved, resulting often in cumbersome data visualizations, and in suboptimal performance in terms of generality and recognition results. Here, we take an alternative route and we attempt instead to learn an optimal colorization mapping for any given pretrained architecture, using as training data a reference RGB-D database. We propose a deep network architecture, exploiting the residual paradigm, that learns how to map depth data to three channel images. A qualitative analysis of the images obtained with this approach clearly indicates that learning the optimal mapping preserves the richness of depth information better than current hand-crafted approaches. Experiments on the Washington, JHUIT-50 and BigBIRD public benchmark databases, using CaffeNet, VGG-16, GoogleNet, and ResNet50 clearly showcase the power of our approach, with gains in performance of up to 16% compared to state of the art competitors on the depth channel only, leading to top performances when dealing with RGB-D data."}
{"metadata": {"dataset": "scidocs", "query_id": "03725753e46ee9b13cbdfa78c9b62700d4cc2956", "doc_id": "9360ce51ec055c05fd0384343792c58363383952"}, "id": "scidocs-03725753e46ee9b13cbdfa78c9b62700d4cc2956-9360ce51ec055c05fd0384343792c58363383952", "question": "BRAND: A robust appearance and depth descriptor for RGB-D images", "context": "In this paper we address the problem of semantic labeling of indoor scenes on RGB-D data. With the availability of RGB-D cameras, it is expected that additional depth measurement will improve the accuracy. Here we investigate a solution how to incorporate complementary depth information into a semantic segmentation framework by making use of convolutional neural networks (CNNs). Recently encoder-decoder type fully convolutional CNN architectures have achieved a great success in the field of semantic segmentation. Motivated by this observation we propose an encoder-decoder type network, where the encoder part is composed of two branches of networks that simultaneously extract features from RGB and depth images and fuse depth features into the RGB feature maps as the network goes deeper. Comprehensive experimental evaluations demonstrate that the proposed fusion-based architecture achieves competitive results with the state-of-the-art methods on the challenging SUN RGB-D benchmark obtaining 76.27% global accuracy, 48.30% average class accuracy and 37.29% average intersectionover-union score."}
{"metadata": {"dataset": "scidocs", "query_id": "03725753e46ee9b13cbdfa78c9b62700d4cc2956", "doc_id": "c5fe87291747c39f56bf0cbe4499cca77ae91351"}, "id": "scidocs-03725753e46ee9b13cbdfa78c9b62700d4cc2956-c5fe87291747c39f56bf0cbe4499cca77ae91351", "question": "BRAND: A robust appearance and depth descriptor for RGB-D images", "context": "Intrinsic image decomposition refers to recover the albedo and shading from images, which is an ill-posed problem in signal processing. As realistic labeled data are severely lacking, it is difficult to apply learning methods in this issue. In this letter, we propose using a synthesized dataset to facilitate the solving of this problem. A physically based renderer is used to generate color images and their underlying ground-truth albedo and shading from three-dimensional models. Additionally, we render a Kinect-like noisy depth map for each instance. We utilize this synthetic dataset to train a deep neural network for intrinsic image decomposition and further fine-tune it for real-world images. Our model supports both RGB and RGB-D as input, and it employs both high-level and low-level features to avoid blurry outputs. Experimental results verify the effectiveness of our model on realistic images."}
{"metadata": {"dataset": "scidocs", "query_id": "038ee3d9e0a739752f4a270548ab8c97ed024633", "doc_id": "no"}, "id": "scidocs-038ee3d9e0a739752f4a270548ab8c97ed024633-no", "question": "Fast Vehicle Detection with Lateral Convolutional Neural Network", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "038ee3d9e0a739752f4a270548ab8c97ed024633", "doc_id": "b329e8dc2f97ee604df17b6fa15484363ccb52ab"}, "id": "scidocs-038ee3d9e0a739752f4a270548ab8c97ed024633-b329e8dc2f97ee604df17b6fa15484363ccb52ab", "question": "Fast Vehicle Detection with Lateral Convolutional Neural Network", "context": "Deep learning has emerged as a hot topic due to extensive application and high accuracy. In this paper this efficient method is used for vehicle detection and classification. We extract visual features from the activation of a deep convolutional network, large-scale sparse learning and other distinguishing features in order to compare their accuracy. When compared to the leading methods in the challenging ImageNet dataset, our deep learning approach obtains highly competitive results. Through the experiments with in short of training data, the features extracted by deep learning method outperform those generated by traditional approaches."}
{"metadata": {"dataset": "scidocs", "query_id": "038ee3d9e0a739752f4a270548ab8c97ed024633", "doc_id": "a06805d4de16df54395e1700ec51797e5a65eb64"}, "id": "scidocs-038ee3d9e0a739752f4a270548ab8c97ed024633-a06805d4de16df54395e1700ec51797e5a65eb64", "question": "Fast Vehicle Detection with Lateral Convolutional Neural Network", "context": "Pedestrian detection is a highly debated issue in the scientific community due to its outstanding importance for a large number of applications, especially in the fields of automotive safety, robotics and surveillance. In spite of the widely varying methods developed in recent years, pedestrian detection is still an open challenge whose accuracy and robustness has to be improved. Therefore, in this paper, we focus on improving the classification component in the pedestrian detection task on the Daimler stereo vision data set by adopting two approaches: 1) by combining three image modalities (intensity, depth and flow) to feed a unique convolutional neural network (CNN) and 2) by fusing the results of three independent CNNs."}
{"metadata": {"dataset": "scidocs", "query_id": "038ee3d9e0a739752f4a270548ab8c97ed024633", "doc_id": "04fa47f1d3983bacfea1e3c838cf868f9b73dc58"}, "id": "scidocs-038ee3d9e0a739752f4a270548ab8c97ed024633-04fa47f1d3983bacfea1e3c838cf868f9b73dc58", "question": "Fast Vehicle Detection with Lateral Convolutional Neural Network", "context": "In this paper, we present a novel face detection approach based on a convolutional neural architecture, designed to robustly detect highly variable face patterns, rotated up to /spl plusmn/20 degrees in image plane and turned up to /spl plusmn/60 degrees, in complex real world images. The proposed system automatically synthesizes simple problem-specific feature extractors from a training set of face and nonface patterns, without making any assumptions or using any hand-made design concerning the features to extract or the areas of the face pattern to analyze. The face detection procedure acts like a pipeline of simple convolution and subsampling modules that treat the raw input image as a whole. We therefore show that an efficient face detection system does not require any costly local preprocessing before classification of image areas. The proposed scheme provides very high detection rate with a particularly low level of false positives, demonstrated on difficult test sets, without requiring the use of multiple networks for handling difficult cases. We present extensive experimental results illustrating the efficiency of the proposed approach on difficult test sets and including an in-depth sensitivity analysis with respect to the degrees of variability of the face patterns."}
{"metadata": {"dataset": "scidocs", "query_id": "038ee3d9e0a739752f4a270548ab8c97ed024633", "doc_id": "671697cf84dfbe53a1cb0bed29b9f649c653bbc5"}, "id": "scidocs-038ee3d9e0a739752f4a270548ab8c97ed024633-671697cf84dfbe53a1cb0bed29b9f649c653bbc5", "question": "Fast Vehicle Detection with Lateral Convolutional Neural Network", "context": "Multispectral pedestrian detection is essential for around-the-clock applications, e.g., surveillance and autonomous driving. We deeply analyze Faster R-CNN for multispectral pedestrian detection task and then model it into a convolutional network (ConvNet) fusion problem. Further, we discover that ConvNet-based pedestrian detectors trained by color or thermal images separately provide complementary information in discriminating human instances. Thus there is a large potential to improve pedestrian detection by using color and thermal images in DNNs simultaneously. We carefully design four ConvNet fusion architectures that integrate two-branch ConvNets on different DNNs stages, all of which yield better performance compared with the baseline detector. Our experimental results on KAIST pedestrian benchmark show that the Halfway Fusion model that performs fusion on the middle-level convolutional features outperforms the baseline method by 11% and yields a missing rate 3.5% lower than the other proposed architectures."}
{"metadata": {"dataset": "scidocs", "query_id": "038ee3d9e0a739752f4a270548ab8c97ed024633", "doc_id": "0f12c93d685ec82d23f2c43d555e7687f80e5b7c"}, "id": "scidocs-038ee3d9e0a739752f4a270548ab8c97ed024633-0f12c93d685ec82d23f2c43d555e7687f80e5b7c", "question": "Fast Vehicle Detection with Lateral Convolutional Neural Network", "context": "The detection of small road hazards, such as lost cargo, is a vital capability for self-driving cars. We tackle this challenging and rarely addressed problem with a vision system that leverages appearance, contextual as well as geometric cues. To utilize the appearance and contextual cues, we propose a new deep learning-based obstacle detection framework. Here a variant of a fully convolutional network is proposed to predict a pixel-wise semantic labeling of (i) free-space, (ii) on-road unexpected obstacles, and (iii) background. The geometric cues are exploited using a state-of-the-art detection approach that predicts obstacles from stereo input images via model-based statistical hypothesis tests. We present a principled Bayesian framework to fuse the semantic and stereo-based detection results. The mid-level Stixel representation is used to describe obstacles in a flexible, compact and robust manner. We evaluate our new obstacle detection system on the Lost and Found dataset, which includes very challenging scenes with obstacles of only 5 cm height. Overall, we report a major improvement over the state-of-the-art, with a performance gain of 27.4%. In particular, we achieve a detection rate of over 90% for distances of up to 50 m. Our system operates at 22 Hz on our self-driving platform."}
{"metadata": {"dataset": "scidocs", "query_id": "038ee3d9e0a739752f4a270548ab8c97ed024633", "doc_id": "f9fafd8ea1190ffbc2757eed0f0a8bbff610c43e"}, "id": "scidocs-038ee3d9e0a739752f4a270548ab8c97ed024633-f9fafd8ea1190ffbc2757eed0f0a8bbff610c43e", "question": "Fast Vehicle Detection with Lateral Convolutional Neural Network", "context": "Detection of non-motorized road users, such as cyclists and pedestrians, is a challenging problem in collision warning/collision avoidance (CW/CA) systems as direct information (e.g. location, speed, and class) cannot be obtained from such users. In this paper, we propose a fusion of LIDAR data and a deep learning-based computer vision algorithm, to substantially improve the detection of regions of interest (ROIs) and subsequent identification of road users. Experimental results on the KITTI object detection benchmark quantify the effectiveness of incorporating LIDAR data with region-based deep convolutional networks. Thus our work provides another step towards the goal of designing safe and smart transportation systems of the future."}
{"metadata": {"dataset": "scidocs", "query_id": "038ee3d9e0a739752f4a270548ab8c97ed024633", "doc_id": "d532777f2386766e7c93c0bf4257d0a359e91f6b"}, "id": "scidocs-038ee3d9e0a739752f4a270548ab8c97ed024633-d532777f2386766e7c93c0bf4257d0a359e91f6b", "question": "Fast Vehicle Detection with Lateral Convolutional Neural Network", "context": "In this thesis, the deep learning method convolutional neural networks (CNNs) has been used in an attempt to solve two classification problems, namely traffic sign recognition and Alzheimer’s disease detection. The two datasets used are from the German Traffic Sign Recognition Benchmark (GTSRB) and the Alzheimer’s Disease Neuroimaging Initiative (ADNI). The final test results on the traffic sign dataset generated a classification accuracy of 98.81 %, almost as high as human performance on the same dataset, 98.84 %. Different parameter settings of the selected CNN structure have also been tested in order to see their impact on the classification accuracy. Trying to distinguish between MRI images of healthy brains and brains afflicted with Alzheimer’s disease gained only about 65 % classification accuracy. These results show that the convolutional neural network approach is very promising for classifying traffic signs, but more work needs to be done when working with the more complex problem of detecting Alzheimer’s disease."}
{"metadata": {"dataset": "scidocs", "query_id": "038ee3d9e0a739752f4a270548ab8c97ed024633", "doc_id": "45d2f411151a4a9c05cd0f1fba0746570dbd7708"}, "id": "scidocs-038ee3d9e0a739752f4a270548ab8c97ed024633-45d2f411151a4a9c05cd0f1fba0746570dbd7708", "question": "Fast Vehicle Detection with Lateral Convolutional Neural Network", "context": "Recent studies have indicated that the architectures of convolutional neural networks (CNNs) tailored for computer vision may not be best suited to image steganalysis. In this letter, we report a CNN architecture that takes into account knowledge of steganalysis. In the detailed architecture, we take absolute values of elements in the feature maps generated from the first convolutional layer to facilitate and improve statistical modeling in the subsequent layers; to prevent overfitting, we constrain the range of data values with the saturation regions of hyperbolic tangent (TanH) at early stages of the networks and reduce the strength of modeling using 1×1 convolutions in deeper layers. Although it learns from only one type of noise residual, the proposed CNN is competitive in terms of detection performance compared with the SRM with ensemble classifiers on the BOSSbase for detecting S-UNIWARD and HILL. The results have implied that well-designed CNNs have the potential to provide a better detection performance in the future."}
{"metadata": {"dataset": "scidocs", "query_id": "038ee3d9e0a739752f4a270548ab8c97ed024633", "doc_id": "54eed22ff377dcb0472c8de454b1261988c4a9ac"}, "id": "scidocs-038ee3d9e0a739752f4a270548ab8c97ed024633-54eed22ff377dcb0472c8de454b1261988c4a9ac", "question": "Fast Vehicle Detection with Lateral Convolutional Neural Network", "context": "Traffic safety is a severe problem around the world. Many road accidents are normally related with the driver's unsafe driving behavior, e.g. eating while driving. In this work, we propose a vision-based solution to recognize the driver's behavior based on convolutional neural networks. Specifically, given an image, skin-like regions are extracted by Gaussian Mixture Model, which are passed to a deep convolutional neural networks model, namely R*CNN, to generate action labels. The skin-like regions are able to provide abundant semantic information with sufficient discriminative capability. Also, R*CNN is able to select the most informative regions from candidates to facilitate the final action recognition. We tested the proposed methods on Southeast University Driving-posture Dataset and achieve mean Average Precision(mAP) of 97.76% on the dataset which prove the proposed method is effective in drivers's action recognition."}
{"metadata": {"dataset": "scidocs", "query_id": "038ee3d9e0a739752f4a270548ab8c97ed024633", "doc_id": "c4df3bbcc011e4f2a53ae0885ac484bd15e40824"}, "id": "scidocs-038ee3d9e0a739752f4a270548ab8c97ed024633-c4df3bbcc011e4f2a53ae0885ac484bd15e40824", "question": "Fast Vehicle Detection with Lateral Convolutional Neural Network", "context": "A system for real-time traffic sign detection is described in this paper. The system uses restricted Hough transform for circumferences in order to detect circular signs, and for straight lines for triangular ones. Some results obtained from a set of real road images captured under both normal and adverse weather conditions are presented as well in order to illustrate the robustness of the detection system. The average processing time is 30 ms per frame, what makes the system a good approach to work in real time conditions."}
{"metadata": {"dataset": "scidocs", "query_id": "03bd09f62445ee68095f20000342c1c76b57d7c9", "doc_id": "no"}, "id": "scidocs-03bd09f62445ee68095f20000342c1c76b57d7c9-no", "question": "Square Root SAM: Simultaneous Localization and Mapping via Square Root Information Smoothing", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "03bd09f62445ee68095f20000342c1c76b57d7c9", "doc_id": "74d15b56e6d19a7d527e1c2626c76dd1418d798c"}, "id": "scidocs-03bd09f62445ee68095f20000342c1c76b57d7c9-74d15b56e6d19a7d527e1c2626c76dd1418d798c", "question": "Square Root SAM: Simultaneous Localization and Mapping via Square Root Information Smoothing", "context": "It is widely believed that in order to solve large scale global optimization problems an appropriate mixture of local approximation and global exploration is necessary. Local approximation, if first order information on the objective function is available, is efficiently performed by means of local optimization methods. Unfortunately, global exploration, in absence of some kind of global information on the problem, is a “blind” procedure, aimed at placing observations as evenly as possible in the search domain. Often this procedure reduces to uniform random sampling (like in Multistart algorithms, or in techniques based on clustering). In this paper we propose a new framework for global exploration which tries to guide random exploration towards the region of attraction of low-level local optima. The main idea originated by the use of smoothing techniques (based on gaussian convolutions): the possibility of applying a smoothing transformation not to the objective function but to the result of local searches seems to have never been explored yet. Although an exact smoothing of the results of local searches is impossible to implement, in this paper we propose a computational approximation scheme which has proven to be very efficient and (maybe more important) extremely robust in solving large scale global optimization problems with huge numbers of local optima. ∗Email: b.addis@ing.unifi.it Dip. Sistemi e Informatica Università di Firenze †Email: locatell@di.unito.it Dip. Informatica Università di Torino ‡Email: schoen@ing.unifi.it Dip. Sistemi e Informatica Università di Firenze"}
{"metadata": {"dataset": "scidocs", "query_id": "03bd09f62445ee68095f20000342c1c76b57d7c9", "doc_id": "fcd05a3d018c6711d6da3cb4b5aa1abac132e53a"}, "id": "scidocs-03bd09f62445ee68095f20000342c1c76b57d7c9-fcd05a3d018c6711d6da3cb4b5aa1abac132e53a", "question": "Square Root SAM: Simultaneous Localization and Mapping via Square Root Information Smoothing", "context": "Virtual reality applications prefer real walking to provide highly immersive presence than other locomotive methods. Mapping-based techniques are very effective for supporting real walking in small physical workspaces while exploring large virtual scenes. However, the existing methods for computing real walking maps suffer from poor quality due to distortion. In this paper, we present a novel divide-and-conquer method, called Smooth Assembly Mapping (SAM), to compute real walking maps with low isometric distortion for large-scale virtual scenes. First, the input virtual scene is decomposed into a set of smaller local patches. Then, a group of local patches is mapped together into a real workspace by minimizing a low isometric distortion energy with smoothness constraints between the adjacent patches. All local patches are mapped and assembled one by one to obtain a complete map. Finally, a global optimization is adopted to further reduce the distortion throughout the entire map. Our method easily handles teleportation technique by computing maps of individual regions and assembling them with teleporter conformity constraints. A large number of experiments, including formative user studies and comparisons, have shown that our method succeeds in generating high-quality real walking maps from large-scale virtual scenes to small real workspaces and is demonstrably superior to state-of-the-art methods."}
{"metadata": {"dataset": "scidocs", "query_id": "03bd09f62445ee68095f20000342c1c76b57d7c9", "doc_id": "7c38d99373d68e8206878aa6f49fce6e0d4cfc6a"}, "id": "scidocs-03bd09f62445ee68095f20000342c1c76b57d7c9-7c38d99373d68e8206878aa6f49fce6e0d4cfc6a", "question": "Square Root SAM: Simultaneous Localization and Mapping via Square Root Information Smoothing", "context": "The autocorrelation is often used in signal processing as a tool for finding repeating patterns in a signal. In image processing, there are various image analysis techniques that use the autocorrelation of an image in a broad range of applications from texture analysis to grain density estimation. This paper provides an extensive review of two recently introduced and related frameworks for image representation based on autocorrelation, namely Patch Autocorrelation Features (PAF) and Translation and Rotation Invariant Patch Autocorrelation Features (TRIPAF). The PAF approach stores a set of features obtained by comparing pairs of patches from an image. More precisely, each feature is the euclidean distance between a particular pair of patches. The proposed approach is successfully evaluated in a series of handwritten digit recognition experiments on the popular MNIST data set. However, the PAF approach has limited applications, because it is not invariant to affine transformations. More recently, the PAF approach was extended to become invariant to image transformations, including (but not limited to) translation and rotation changes. In the TRIPAF framework, several features are extracted from each image patch. Based on these features, a vector of similarity values is computed between each pair of patches. Then, the similarity vectors are clustered together such that the spatial offset between the patches of each pair is roughly the same. Finally, the mean and the standard deviation of each similarity value are computed for each group of similarity vectors. These statistics are concatenated to obtain the TRIPAF feature vector. The TRIPAF vector essentially records information about the repeating patterns within an image at various spatial offsets. After presenting the two approaches, several optical character recognition and texture classification experiments are conducted to evaluate the two approaches. Results are reported on the MNIST (98.93%), the Brodatz (96.51%), and the UIUCTex (98.31%) data sets. Both PAF and TRIPAF are fast to compute and produce compact representations in practice, while reaching accuracy levels similar to other state-of-the-art methods."}
{"metadata": {"dataset": "scidocs", "query_id": "03bd09f62445ee68095f20000342c1c76b57d7c9", "doc_id": "f62a8c59de7bb20da5030ed09a8892dd353f3a50"}, "id": "scidocs-03bd09f62445ee68095f20000342c1c76b57d7c9-f62a8c59de7bb20da5030ed09a8892dd353f3a50", "question": "Square Root SAM: Simultaneous Localization and Mapping via Square Root Information Smoothing", "context": "In a visualization task, every nonlinear projection method needs to make a compromise between trustworthiness and continuity. In a trustworthy projection the visualized proximities hold in the original data as well, whereas a continuous projection visualizes all proximities of the original data. We show experimentally that one of the multidimensional scaling methods, curvilinear components analysis, is good at maximizing trustworthiness. We then extend it to focus on local proximities both in the input and output space, and to explicitly make a user-tunable parameterized compromise between trustworthiness and continuity. The new method compares favorably to alternative nonlinear projection methods."}
{"metadata": {"dataset": "scidocs", "query_id": "03bd09f62445ee68095f20000342c1c76b57d7c9", "doc_id": "6c1ccc66420136488cf34c1ffe707afefd8b00b9"}, "id": "scidocs-03bd09f62445ee68095f20000342c1c76b57d7c9-6c1ccc66420136488cf34c1ffe707afefd8b00b9", "question": "Square Root SAM: Simultaneous Localization and Mapping via Square Root Information Smoothing", "context": "We consider brightness/contrast-invariant and rotation-discriminating template matching that searches an image to analyze A for a query image Q. We propose to use the complex coefficients of the discrete Fourier transform of the radial projections to compute new rotation-invariant local features. These coefficients can be efficiently obtained via FFT. We classify templates in “stable” and “unstable” ones and argue that any local feature-based template matching may fail to find unstable templates. We extract several stable sub-templates of Q and find them in A by comparing the features. The matchings of the sub-templates are combined using the Hough transform. As the features of A are computed only once, the algorithm can find quickly many different sub-templates in A, and it is suitable for: finding many query images in A; multi-scale searching and partial occlusion-robust template matching."}
{"metadata": {"dataset": "scidocs", "query_id": "03bd09f62445ee68095f20000342c1c76b57d7c9", "doc_id": "83009e2fa390a5935b88d1d91d2948de890ee1dd"}, "id": "scidocs-03bd09f62445ee68095f20000342c1c76b57d7c9-83009e2fa390a5935b88d1d91d2948de890ee1dd", "question": "Square Root SAM: Simultaneous Localization and Mapping via Square Root Information Smoothing", "context": "The generalized Hough transform (GHT) is widely used for detecting or locating objects under similarity transformation. However, a weakness of the traditional GHT is its large storage requirement and time-consuming computational complexity due to the 4-D parameter space voting strategy. In this paper, a polygon-invariant GHT (PI-GHT) algorithm, as a novel scale- and rotation-invariant template matching method, is presented for high-speed object vision-based positioning. To demonstrate the performance of PI-GHT, several experiments were carried out to compare this novel algorithm with the other five popular matching methods. Experimental results show that the computational effort required by PI-GHT is smaller than that of the common methods due to the similarity transformations applied to the scale- and rotation-invariant triangle features. Moreover, the proposed PI-GHT maintains inherent robustness against partial occlusion, noise, and nonlinear illumination changes, because the local triangle features are based on the gradient directions of edge points. Consequently, PI-GHT is implemented in packaging equipment for radio frequency identification devices at an average time of 4.13 ms and 97.06% matching rate, to solder paste printing at average time nearly 5 ms with 99.87%. PI-GHT is applied to LED manufacturing equipment to locate multiobjects at least five times improvement in speed with a 96% matching rate."}
{"metadata": {"dataset": "scidocs", "query_id": "03bd09f62445ee68095f20000342c1c76b57d7c9", "doc_id": "e15f910f7cc2d785c3cefd37dc7ae99e46bf031f"}, "id": "scidocs-03bd09f62445ee68095f20000342c1c76b57d7c9-e15f910f7cc2d785c3cefd37dc7ae99e46bf031f", "question": "Square Root SAM: Simultaneous Localization and Mapping via Square Root Information Smoothing", "context": "Spatial localization is a key determinant of cellular fate and behavior, but methods for spatially resolved, transcriptome-wide gene expression profiling across complex tissues are lacking. RNA staining methods assay only a small number of transcripts, whereas single-cell RNA-seq, which measures global gene expression, separates cells from their native spatial context. Here we present Seurat, a computational strategy to infer cellular localization by integrating single-cell RNA-seq data with in situ RNA patterns. We applied Seurat to spatially map 851 single cells from dissociated zebrafish (Danio rerio) embryos and generated a transcriptome-wide map of spatial patterning. We confirmed Seurat's accuracy using several experimental approaches, then used the strategy to identify a set of archetypal expression patterns and spatial markers. Seurat correctly localizes rare subpopulations, accurately mapping both spatially restricted and scattered groups. Seurat will be applicable to mapping cellular localization within complex patterned tissues in diverse systems."}
{"metadata": {"dataset": "scidocs", "query_id": "03bd09f62445ee68095f20000342c1c76b57d7c9", "doc_id": "3d23be971f17ac490ba276de93f587b5203fccfc"}, "id": "scidocs-03bd09f62445ee68095f20000342c1c76b57d7c9-3d23be971f17ac490ba276de93f587b5203fccfc", "question": "Square Root SAM: Simultaneous Localization and Mapping via Square Root Information Smoothing", "context": "We propose a new method for the localization of a Micro Aerial Vehicle (MAV) with respect to a ground robot. We solve the problem of registering the 3D maps computed by the robots using different sensors: a dense 3D reconstruction from the MAV monocular camera is aligned with the map computed from the depth sensor on the ground robot. Once aligned, the dense reconstruction from the MAV is used to augment the map computed by the ground robot, by extending it with the information conveyed by the aerial views. The overall approach is novel, as it builds on recent developments in live dense reconstruction from moving cameras to address the problem of air-ground localization. The core of our contribution is constituted by a novel algorithm integrating dense reconstructions from monocular views, Monte Carlo localization, and an iterative pose refinement. In spite of the radically different vantage points from which the maps are acquired, the proposed method achieves high accuracy whereas appearance-based, state-of-the-art approaches fail. Experimental validation in indoor and outdoor scenarios reported an accuracy in position estimation of 0.08 meters and real time performance. This demonstrates that our new approach effectively overcomes the limitations imposed by the difference in sensors and vantage points that negatively affect previous techniques relying on matching visual features."}
{"metadata": {"dataset": "scidocs", "query_id": "03bd09f62445ee68095f20000342c1c76b57d7c9", "doc_id": "34e5eeb48da470e8b91693cc5d322959d79f1470"}, "id": "scidocs-03bd09f62445ee68095f20000342c1c76b57d7c9-34e5eeb48da470e8b91693cc5d322959d79f1470", "question": "Square Root SAM: Simultaneous Localization and Mapping via Square Root Information Smoothing", "context": "Estimating positions of world points from features observed in images is a key problem in 3D reconstruction, image mosaicking, simultaneous localization and mapping and structure from motion. We consider a special instance in which there is a dominant ground plane G viewed from a parallel viewing plane S above it. Such instances commonly arise, for example, in aerial photography. Consider a world point g ∈ G and its worst case reconstruction uncertainty ε(g,S) obtained by merging all possible views of g chosen from S. We first show that one can pick two views sp and sq such that the uncertainty ε(g, {sp, sq}) obtained using only these two views is almost as good as (i.e. within a small constant factor of) ε(g,S). Next, we extend the result to the entire ground plane G and show that one can pick a small subset of S ′ ⊆ S (which grows only linearly with the area of G) and still obtain a constant factor approximation, for every point g ∈ G, to the minimum worst case estimate obtained by merging all views in S. Our results provide a view selection mechanism with provable performance guarantees which can drastically increase the speed of scene reconstruction algorithms. In addition to theoretical results, we demonstrate their effectiveness in an application where aerial imagery is used for monitoring farms and orchards."}
{"metadata": {"dataset": "scidocs", "query_id": "03bd09f62445ee68095f20000342c1c76b57d7c9", "doc_id": "d1d9acd4a55c9d742a8b6736928711c3cfbe6526"}, "id": "scidocs-03bd09f62445ee68095f20000342c1c76b57d7c9-d1d9acd4a55c9d742a8b6736928711c3cfbe6526", "question": "Square Root SAM: Simultaneous Localization and Mapping via Square Root Information Smoothing", "context": "In this paper, we propose a Scale and Rotation Invariant Implicit Shape Model (SRIISM), and develop a local feature matching based system using the model to accurately locate and identify large numbers of object instances in an image. Due to repeated instances and cluttered background, conventional methods for multiple object instance identification suffer from poor identification results. In the proposed SRIISM, we model the joint distribution of object centers, scale, and orientation computed from local feature matches in Hough voting, which is not only invariant to scale changes and rotation of objects, but also robust to false feature matches. In the multiple object instance identification system using SRIISM, we apply a fast 4D bin search method in Hough space with complexity O(n), where n is the number of feature matches, in order to segment and locate each instance. Futhermore, we apply maximum likelihood estimation (MLE) for accurate object pose detection. In the evaluation, we created datasets simulating various industrial applications such as pick-and-place and inventory management. Experiment results on the datasets show that our method outperforms conventional methods in both accuracy (5%-30% gain) and speed (2x speed up)."}
{"metadata": {"dataset": "scidocs", "query_id": "03d23160e7066e5adab0d55779287e3c4982b9d5", "doc_id": "no"}, "id": "scidocs-03d23160e7066e5adab0d55779287e3c4982b9d5-no", "question": "Analysis of human faces using a measurement-based skin reflectance model", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "03d23160e7066e5adab0d55779287e3c4982b9d5", "doc_id": "46af4e3272fe3dbc7ee648400fb049ae6d3689cd"}, "id": "scidocs-03d23160e7066e5adab0d55779287e3c4982b9d5-46af4e3272fe3dbc7ee648400fb049ae6d3689cd", "question": "Analysis of human faces using a measurement-based skin reflectance model", "context": "We present a method to acquire the reflectance field of a human face and use these measurements to render the face under arbitrary changes in lighting and viewpoint. We first acquire images of the face from a small set of viewpoints under a dense sampling of incident illumination directions using a light stage. We then construct a reflectance function image for each observed image pixel from its values over the space of illumination directions. From the reflectance functions, we can directly generate images of the face from the original viewpoints in any form of sampled or computed illumination. To change the viewpoint, we use a model of skin reflectance to estimate the appearance of the reflectance functions for novel viewpoints. We demonstrate the technique with synthetic renderings of a person's face under novel illumination and viewpoints."}
{"metadata": {"dataset": "scidocs", "query_id": "03d23160e7066e5adab0d55779287e3c4982b9d5", "doc_id": "4d2bd65a4b24679d4c440afc6fe88f13229e644d"}, "id": "scidocs-03d23160e7066e5adab0d55779287e3c4982b9d5-4d2bd65a4b24679d4c440afc6fe88f13229e644d", "question": "Analysis of human faces using a measurement-based skin reflectance model", "context": "The goal of face detection is to lo­cate all regions that contain a face. This pa­per has a simple face detection procedure which has two major steps, first to segment skin region from an image, and second, to decide these regions contain human face or not. Our procedure is based on skin color segmentation and human face features (knowledge-based approach). In this paper, we used RGB, YCbCr, CEILAB (L*a*b) and HSV color models for skin color segmentation. These color models with thresholds, help to remove non skin like pixels from an image. We tested each skin region, that skin region is actually represents a human face or not, by using human face features based on knowledge of geometrical properties of human face. The experiment result shows that, the algorithm gives hopeful results. At last, we concluded this paper and proposed future work."}
{"metadata": {"dataset": "scidocs", "query_id": "03d23160e7066e5adab0d55779287e3c4982b9d5", "doc_id": "002aaf4412f91d0828b79511f35c0863a1a32c47"}, "id": "scidocs-03d23160e7066e5adab0d55779287e3c4982b9d5-002aaf4412f91d0828b79511f35c0863a1a32c47", "question": "Analysis of human faces using a measurement-based skin reflectance model", "context": "We present a real-time face tracker in this paper The system has achieved a rate of 30% frameshecond using an HP-9000 workstation with a framegrabber and a Canon VC-CI camera. It can track a person 'sface while the person moves freely (e.g., walks, jumps, sits down and stands up) in a room. Three types of models have been employed in developing the system. First, we present a stochastic model to characterize skin-color distributions of human faces. The information provided by the model is sufJicient for tracking a human face in various poses and views. This model is adaptable to different people and different lighting conditions in real-time. Second, a motion model e's used to estimate image motion and to predict search window. Third, a camera model is used toprediet and to compensate for camera motion. The system can be applied to tele-conferencing and many HCI applications including lip-reading and gaze tracking. The principle in developing this system can be extended to other tracking problems such as tracking the human hand."}
{"metadata": {"dataset": "scidocs", "query_id": "03d23160e7066e5adab0d55779287e3c4982b9d5", "doc_id": "3146fabd5631a7d1387327918b184103d06c2211"}, "id": "scidocs-03d23160e7066e5adab0d55779287e3c4982b9d5-3146fabd5631a7d1387327918b184103d06c2211", "question": "Analysis of human faces using a measurement-based skin reflectance model", "context": "Person-independent and pose-invariant estimation of eye-gaze is important for situation analysis and for automated video annotation. We propose a fast cascade regression based method that first estimates the location of a dense set of markers and their visibility, then reconstructs face shape by fitting a part-based 3D model. Next, the reconstructed 3D shape is used to estimate a canonical view of the eyes for 3D gaze estimation. The model operates in a feature space that naturally encodes local ordinal properties of pixel intensities leading to photometric invariant estimation of gaze. To evaluate the algorithm in comparison with alternative approaches, three publicly-available databases were used, Boston University Head Tracking, Multi-View Gaze and CAVE Gaze datasets. Precision for head pose and gaze averaged 4 degrees or less for pitch, yaw, and roll. The algorithm outperformed alternative methods in both datasets."}
{"metadata": {"dataset": "scidocs", "query_id": "03d23160e7066e5adab0d55779287e3c4982b9d5", "doc_id": "a3983077d951e29278a6139fe6dfb41ade9b1df1"}, "id": "scidocs-03d23160e7066e5adab0d55779287e3c4982b9d5-a3983077d951e29278a6139fe6dfb41ade9b1df1", "question": "Analysis of human faces using a measurement-based skin reflectance model", "context": "Human pose estimation in real-time is a challenging problem in computer vision. In this paper, we present a novel approach to recover a 3D human pose in real-time from a single depth human silhouette using Principal Direction Analysis (PDA) on each recognized body part. In our work, the human body parts are first recognized from a depth human body silhouette via the trained Random Forests (RFs). On each recognized body part which is presented as a set of 3D points cloud, PDA is applied to estimate the principal direction of the body part. Finally, a 3D human pose gets recovered by mapping the principal directional vector to each body part of a 3D human body model which is created with a set of super-quadrics linked by the kinematic chains. In our experiments, we have performed quantitative and qualitative evaluations of the proposed 3D human pose reconstruction methodology. Our evaluation results show that the proposed approach performs reliably on a sequence of unconstrained poses and achieves an average reconstruction error of 7.46 degree in a few key joint angles. Our 3D pose recovery methodology should be applicable to many areas such as human computer interactions and human activity recognition."}
{"metadata": {"dataset": "scidocs", "query_id": "03d23160e7066e5adab0d55779287e3c4982b9d5", "doc_id": "fef0b51c865cc72b64bfafd6a1bf3539c3c1d290"}, "id": "scidocs-03d23160e7066e5adab0d55779287e3c4982b9d5-fef0b51c865cc72b64bfafd6a1bf3539c3c1d290", "question": "Analysis of human faces using a measurement-based skin reflectance model", "context": "Liveness detection is a way to detect whether the person is live or not during submission of his/her biometric trait. It is mandatory in order to prevent face spoofing attacks. Therefore, in this paper, we proposed a robust face structure analysis mechanism to detect the liveness by exploiting face shape information. 3D structure/shape of the face is measured on the basis of disparity map between left and right image taken by stereoscopic vision. A gradient-based eight neighbour feature extraction technique has been proposed to extract unique features from these disparity images. It produces minimal computational cost by taking subset of the overall image. We have applied linear discriminant analysis (LDA), C-means algorithms on these features while principal component analysis (PCA) is applied on raw disparity images. We have achieved a recognition rate of 91.6%, 97.5% and 98.3% using PCA, LDA and C-means respectively, which strengthened the confidence of our proposed feature extraction technique."}
{"metadata": {"dataset": "scidocs", "query_id": "03d23160e7066e5adab0d55779287e3c4982b9d5", "doc_id": "2f813885f3ac7be62894b182fa3c5d7a8226a480"}, "id": "scidocs-03d23160e7066e5adab0d55779287e3c4982b9d5-2f813885f3ac7be62894b182fa3c5d7a8226a480", "question": "Analysis of human faces using a measurement-based skin reflectance model", "context": "Face detection is one of the challenging problems in image processing. This report proposes a novel technique for detecting faces in color images using skin color model algorithm combined with skin likely-hood, skin Segmentation, Morphological operation and Template matching. Color images with skin color in the chromatic and pure color space YCrCb, which separates luminance and chrominance components. A Gaussian probability density is estimated from skin samples, collected from different ethnic groups, via the maximum-likelihood criterion. Adaptive thresholding for segmentation to localize the faces within the detected skin regions. Further, mathematical morphological operators are used to remove noisy regions and fill holes in the skin-color region, so that candidate human face regions can be extracted. These systems can achieve high detection accuracy, high detection speed and reduce the false detecting rate. The two methodology used for the Skin Segmentation is YCbCr and RGB Model. In YCbCr model both the skin colour and texture of the image can be used to identify the particular object in the image,where as in RGB model only the skin color has to be used for identification of the person.Hence the YCbCr model is better than the RGB model . From our analysis we conclude that the new approach in modeling skin color can achieve good detection success rate. The algorithm gives computationally a very efficient as well as an accurate approach for skin detection. The performance of different color space may be dependent on the method used to model the color for skin pixel. Keywords—Gaussian probability, YCrCb,RGB Model"}
{"metadata": {"dataset": "scidocs", "query_id": "03d23160e7066e5adab0d55779287e3c4982b9d5", "doc_id": "2d33c06612b8e85ba1b36cb42ef7d26ca22091f3"}, "id": "scidocs-03d23160e7066e5adab0d55779287e3c4982b9d5-2d33c06612b8e85ba1b36cb42ef7d26ca22091f3", "question": "Analysis of human faces using a measurement-based skin reflectance model", "context": "Human face aging is irreversible process causing changes in human face characteristics such us hair whitening, muscles drop and wrinkles. Due to the importance of human face aging in biometrics systems, age estimation became an attractive area for researchers. This paper presents a novel method to estimate the age from face images, using binarized statistical image features (BSIF) and local binary patterns (LBP) histograms as features performed by support vector regression (SVR) and kernel ridge regression (KRR). We applied our method on FG-NET and PAL datasets. Our proposed method has shown superiority to that of the state-of-the-art methods when using the whole PAL database."}
{"metadata": {"dataset": "scidocs", "query_id": "03d23160e7066e5adab0d55779287e3c4982b9d5", "doc_id": "16f940b4b5da79072d64a77692a876627092d39c"}, "id": "scidocs-03d23160e7066e5adab0d55779287e3c4982b9d5-16f940b4b5da79072d64a77692a876627092d39c", "question": "Analysis of human faces using a measurement-based skin reflectance model", "context": "This paper presents a framework to automatically measure the intensity of naturally occurring facial actions. Naturalistic expressions are non-posed spontaneous actions. The facial action coding system (FACS) is the gold standard technique for describing facial expressions, which are parsed as comprehensive, nonoverlapping action units (Aus). AUs have intensities ranging from absent to maximal on a six-point metric (i.e., 0 to 5). Despite the efforts in recognizing the presence of non-posed action units, measuring their intensity has not been studied comprehensively. In this paper, we develop a framework to measure the intensity of AU12 (lip corner puller) and AU6 (cheek raising) in videos captured from infant-mother live face-to-face communications. The AU12 and AU6 are the most challenging case of infant's expressions (e.g., low facial texture in infant's face). One of the problems in facial image analysis is the large dimensionality of the visual data. Our approach for solving this problem is to utilize the spectral regression technique to project high dimensionality facial images into a low dimensionality space. Represented facial images in the low dimensional space are utilized to train support vector machine classifiers to predict the intensity of action units. Analysis of 18 minutes of captured video of non-posed facial expressions of several infants and mothers shows significant agreement between a human FACS coder and our approach, which makes it an efficient approach for automated measurement of the intensity of non-posed facial action units."}
{"metadata": {"dataset": "scidocs", "query_id": "03d23160e7066e5adab0d55779287e3c4982b9d5", "doc_id": "46d29ee2b97362299ef83c06ffc4461906f1ccda"}, "id": "scidocs-03d23160e7066e5adab0d55779287e3c4982b9d5-46d29ee2b97362299ef83c06ffc4461906f1ccda", "question": "Analysis of human faces using a measurement-based skin reflectance model", "context": "Eye gaze is an important non-verbal cue for human affect analysis. Recent gaze estimation work indicated that information from the full face region can benefit performance. Pushing this idea further, we propose an appearance-based method that, in contrast to a long-standing line of work in computer vision, only takes the full face image as input. Our method encodes the face image using a convolutional neural network with spatial weights applied on the feature maps to flexibly suppress or enhance information in different facial regions. Through extensive evaluation, we show that our full-face method significantly outperforms the state of the art for both 2D and 3D gaze estimation, achieving improvements of up to 14.3% on MPIIGaze and 27.7% on EYEDIAP for person-independent 3D gaze estimation. We further show that this improvement is consistent across different illumination conditions and gaze directions and particularly pronounced for the most challenging extreme head poses."}
{"metadata": {"dataset": "scidocs", "query_id": "0416f5d1564d1f2a597acac04e81b02b2eff67d2", "doc_id": "no"}, "id": "scidocs-0416f5d1564d1f2a597acac04e81b02b2eff67d2-no", "question": "A High Performance CRF Model for Clothes Parsing", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0416f5d1564d1f2a597acac04e81b02b2eff67d2", "doc_id": "e0a051ad689963e33eaa854ed4bf849b91240f34"}, "id": "scidocs-0416f5d1564d1f2a597acac04e81b02b2eff67d2-e0a051ad689963e33eaa854ed4bf849b91240f34", "question": "A High Performance CRF Model for Clothes Parsing", "context": "We present techniques for automatically parsing existing sewing patterns and converting them into 3D garment models. Our parser takes a sewing pattern in PDF format as input and starts by extracting the set of panels and styling elements (e.g. darts, pleats and hemlines) contained in the pattern. It then applies a combination of machine learning and integer programming to infer how the panels must be stitched together to form the garment. Our system includes an interactive garment simulator that takes the parsed result and generates the corresponding 3D model. Our fully automatic approach correctly parses 68% of the sewing patterns in our collection. Most of the remaining patterns contain only a few errors that can be quickly corrected within the garment simulator. Finally we present two applications that take advantage of our collection of parsed sewing patterns. Our garment hybrids application lets users smoothly interpolate multiple garments in the 2D space of patterns. Our sketch-based search application allows users to navigate the pattern collection by drawing the shape of panels."}
{"metadata": {"dataset": "scidocs", "query_id": "0416f5d1564d1f2a597acac04e81b02b2eff67d2", "doc_id": "5906297bd4108376a032cb4c610d3e2926750d47"}, "id": "scidocs-0416f5d1564d1f2a597acac04e81b02b2eff67d2-5906297bd4108376a032cb4c610d3e2926750d47", "question": "A High Performance CRF Model for Clothes Parsing", "context": "This paper aims at developing an integrated system for clothing co-parsing (CCP), in order to jointly parse a set of clothing images (unsegmented but annotated with tags) into semantic configurations. A novel data-driven system consisting of two phases of inference is proposed. The first phase, referred as “image cosegmentation,” iterates to extract consistent regions on images and jointly refines the regions over all images by employing the exemplar-SVM technique [1]. In the second phase (i.e., “region colabeling”), we construct a multiimage graphical model by taking the segmented regions as vertices, and incorporating several contexts of clothing configuration (e.g., item locations and mutual interactions). The joint label assignment can be solved using the efficient Graph Cuts algorithm. In addition to evaluate our framework on the Fashionista dataset [2], we construct a dataset called the SYSU-Clothes dataset consisting of 2098 high-resolution street fashion photos to demonstrate the performance of our system. We achieve 90.29%/88.23% segmentation accuracy and 65.52%/63.89% recognition rate on the Fashionista and the SYSU-Clothes datasets, respectively, which are superior compared with the previous methods. Furthermore, we apply our method on a challenging task, i.e., cross-domain clothing retrieval: given user photo depicting a clothing image, retrieving the same clothing items from online shopping stores based on the fine-grained parsing results."}
{"metadata": {"dataset": "scidocs", "query_id": "0416f5d1564d1f2a597acac04e81b02b2eff67d2", "doc_id": "60e7c95e555ddfdd31b4f98f8c02bc59b925dafb"}, "id": "scidocs-0416f5d1564d1f2a597acac04e81b02b2eff67d2-60e7c95e555ddfdd31b4f98f8c02bc59b925dafb", "question": "A High Performance CRF Model for Clothes Parsing", "context": "Inspired by the success of MRF models for solving object segmentation problems, we formulate the binarization problem in this framework. We represent the pixels in a document image as random variables in an MRF, and introduce a new energy (or cost) function on these variables. Each variable takes a foreground or background label, and the quality of the binarization (or labelling) is determined by the value of the energy function. We minimize the energy function, i.e. find the optimal binarization, using an iterative graph cut scheme. Our model is robust to variations in foreground and background colours as we use a Gaussian Mixture Model in the energy function. In addition, our algorithm is efficient to compute, and adapts to a variety of document images. We show results on word images from the challenging ICDAR 2003 dataset, and compare our performance with previously reported methods. Our approach shows significant improvement in pixel level accuracy as well as OCR accuracy."}
{"metadata": {"dataset": "scidocs", "query_id": "0416f5d1564d1f2a597acac04e81b02b2eff67d2", "doc_id": "165ef2b5f86b9b2c68b652391db5ece8c5a0bc7e"}, "id": "scidocs-0416f5d1564d1f2a597acac04e81b02b2eff67d2-165ef2b5f86b9b2c68b652391db5ece8c5a0bc7e", "question": "A High Performance CRF Model for Clothes Parsing", "context": "Recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neural networks (CNNs). We show how to improve semantic segmentation through the use of contextual information, specifically, we explore 'patch-patch' context between image regions, and 'patch-background' context. For learning from the patch-patch context, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied to avoid repeated expensive CRF inference for back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance. Our experimental results set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an intersection-overunion score of 78:0 on the challenging PASCAL VOC 2012 dataset."}
{"metadata": {"dataset": "scidocs", "query_id": "0416f5d1564d1f2a597acac04e81b02b2eff67d2", "doc_id": "33526226231cce669317ece44e6af262b8395dd9"}, "id": "scidocs-0416f5d1564d1f2a597acac04e81b02b2eff67d2-33526226231cce669317ece44e6af262b8395dd9", "question": "A High Performance CRF Model for Clothes Parsing", "context": "Deep convolutional neural networks (CNN) have achieved great success. On the other hand, modeling structural information has been proved critical in many vision problems. It is of great interest to integrate them effectively. In a classical neural network, there is no message passing between neurons in the same layer. In this paper, we propose a CRF-CNN framework which can simultaneously model structural information in both output and hidden feature layers in a probabilistic way, and it is applied to human pose estimation. A message passing scheme is proposed, so that in various layers each body joint receives messages from all the others in an efficient way. Such message passing can be implemented with convolution between features maps in the same layer, and it is also integrated with feedforward propagation in neural networks. Finally, a neural network implementation of endto-end learning CRF-CNN is provided. Its effectiveness is demonstrated through experiments on two benchmark datasets."}
{"metadata": {"dataset": "scidocs", "query_id": "0416f5d1564d1f2a597acac04e81b02b2eff67d2", "doc_id": "29c34a034f6f35915a141dac98cabf625bea2b3c"}, "id": "scidocs-0416f5d1564d1f2a597acac04e81b02b2eff67d2-29c34a034f6f35915a141dac98cabf625bea2b3c", "question": "A High Performance CRF Model for Clothes Parsing", "context": "Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003). CRFs are log-linear, allowing the incorporation of arbitrary features into the model. To train on u labeled data, we requireunsupervisedestimation methods for log-linear models; few exist. We describe a novel approach,contrastive estimation . We show that the new technique can be intuitively understood as exploiting implicit negative evidence and is computationally efficient. Applied to a sequence labeling problem—POS tagging given a tagging dictionary and unlabeled text—contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features."}
{"metadata": {"dataset": "scidocs", "query_id": "0416f5d1564d1f2a597acac04e81b02b2eff67d2", "doc_id": "b025235adf05e92ba7df7da1ced3c1d3b569d65e"}, "id": "scidocs-0416f5d1564d1f2a597acac04e81b02b2eff67d2-b025235adf05e92ba7df7da1ced3c1d3b569d65e", "question": "A High Performance CRF Model for Clothes Parsing", "context": "For the challenging semantic image segmentation task the most efficient models have traditionally combined the structured modelling capabilities of Conditional Random Fields (CRFs) with the feature extraction power of CNNs. In more recent works however, CRF post-processing has fallen out of favour. We argue that this is mainly due to the slow training and inference speeds of CRFs, as well as the difficulty of learning the internal CRF parameters. To overcome both issues we propose to add the assumption of conditional independence to the framework of fully-connected CRFs. This allows us to reformulate the inference in terms of convolutions, which can be implemented highly efficiently on GPUs. Doing so speeds up inference and training by two orders of magnitude. All parameters of the convolutional CRFs can easily be optimized using backpropagation. To facilitating further CRF research we make our implementation publicly available. Please visit: https://github.com/MarvinTeichmann/ConvCRF"}
{"metadata": {"dataset": "scidocs", "query_id": "0416f5d1564d1f2a597acac04e81b02b2eff67d2", "doc_id": "cfec3fb4352ebb004b0aaf8b0a3b9869f23e7765"}, "id": "scidocs-0416f5d1564d1f2a597acac04e81b02b2eff67d2-cfec3fb4352ebb004b0aaf8b0a3b9869f23e7765", "question": "A High Performance CRF Model for Clothes Parsing", "context": "In our daily life, how to match clothing well is always a troublesome problem especially when we are shopping online to select a pair of matched pieces of clothing from tens of thousands available selections. To help common customers overcome selection issues, recent studies in the recommender system area have started to infer the fashion matching results automatically. The traditional fashion recommendation is normally achieved by considering visual similarity of clothing items or/and item co-purchase history from existing shopping transactions. Due to the high complexity of visual features and the lack of historical item purchase records, most of the existing work is unlikely to make an efficient and accurate recommendation. To address the problem, in this paper, we propose a new model called Discrete Supervised Fashion Coordinates Hashing. Its main objective is to learn meaningful yet compact high-level features of clothing items, which are represented as binary hash codes. In detail, this learning process is supervised by a clothing matching matrix, which is initially constructed based on limited known matching pairs and subsequently on the self-augmented ones. The proposed model jointly learns the intrinsic matching patterns from the matching matrix and the binary representations from the clothing items’ images, where the visual feature of each clothing item is discretized into a fixed-length binary vector. The binary representation learning significantly reduces the memory cost and accelerates the recommendation speed. The experiments compared with several state-of-the-art approaches have evidenced the superior performance of the proposed approach on efficient fashion recommendation."}
{"metadata": {"dataset": "scidocs", "query_id": "0416f5d1564d1f2a597acac04e81b02b2eff67d2", "doc_id": "317eaeb73757c131bd1cf920a87c0c54f9b0dfe8"}, "id": "scidocs-0416f5d1564d1f2a597acac04e81b02b2eff67d2-317eaeb73757c131bd1cf920a87c0c54f9b0dfe8", "question": "A High Performance CRF Model for Clothes Parsing", "context": "Funding information National Natural Science Foundation of China, Grant/Award Number: 61732015; National Key R&D Program of China, Grant/Award Number: 2017YFB1002600; Key Research and Development Program of Zhejiang Province, Grant/Award Number: 2018C01090; National Natural Science Foundation of China, Grant/Award Number: 61472355 Abstract We present a fast and automatic method to fit a given 3D garment onto a human model with various shapes and poses, without using a reference human model. Our approach uses a novel skeleton-based error metric to find the pose that best fits the input garment. Specifically, we first generate the skeleton of the given human model and its corresponding skinning weights. Then, we iteratively rotate each bone to find its best position to fit the garment. After that, we rig the surface of the human model according to the transformations of the skeleton. Potential penetrations are resolved using collision handling and physically based simulation. Finally, we restore the human model back to the original pose in order to obtain the desired fitting result. Our experiment results show that besides its efficiency and automation, our method is about two orders of magnitudes faster than existing approaches, and it can handle various garments, including jacket, trousers, skirt, a suit of clothing, and even multilayered clothing."}
{"metadata": {"dataset": "scidocs", "query_id": "0416f5d1564d1f2a597acac04e81b02b2eff67d2", "doc_id": "18b534c7207a1376fa92e87fe0d2cfb358d98c51"}, "id": "scidocs-0416f5d1564d1f2a597acac04e81b02b2eff67d2-18b534c7207a1376fa92e87fe0d2cfb358d98c51", "question": "A High Performance CRF Model for Clothes Parsing", "context": "We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F 1) is better than that of earlylexicalizedPCFG models, and surprisingly close to the current state-of-theart. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize. In the early 1990s, as probabilistic methods swept NLP, parsing work revived the investigation of probabilistic context-free grammars ( PCFGs) (Booth and Thomson, 1973; Baker, 1979). However, early results on the utility ofPCFGs for parse disambiguation and language modeling were somewhat disappointing. A conviction arose that lexicalizedPCFGs (where head words annotate phrasal nodes) were the key tool for high performancePCFG parsing. This approach was congruent with the great success of word n-gram models in speech recognition, and drew strength from a broader interest in lexicalized grammars, as well as demonstrations that lexical dependencies were a key tool for resolving ambiguities such asPPattachments (Ford et al., 1982; Hindle and Rooth, 1993). In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (Magerman, 1995; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001). However, several results have brought into question how large a role lexicalization plays in such parsers. Johnson (1998) showed that the performance of anunlexicalizedPCFGover the Penn treebank could be improved enormously simply by annotating each node by its parent category. The Penn treebank coveringPCFGis a poor tool for parsing because the context-freedom assumptions it embodies are far too strong, and weakening them in this way makes the model much better. More recently, Gildea (2001) discusses how taking the bilexical probabilities out of a good current lexicalized PCFG parser hurts performance hardly at all: by at most 0.5% for test text from the same domain as the training data, and not at all for test text from a different domain. 1 But it is precisely these bilexical dependencies that backed the intuition that lexicalized PCFGs should be very successful, for example in Hindle and Rooth’s demonstration fromPPattachment. We take this as a reflection of the fundamental sparseness of the lexical dependency information available in the Penn Treebank. As a speech person would say, one million words of training data just isn’t enough. Even for topics central to the treebank’s Wall Street Journal text, such as stocks, many very plausible dependencies occur only once, for example stocks stabilized, while many others occur not at all, for example stocks skyrocketed .2 The best-performing lexicalized PCFGs have increasingly made use of subcategorization 3 of the 1There are minor differences, but all the current best-known lexicalized PCFGs employ bothmonolexicalstatistics, which describe the phrasal categories of arguments and adjuncts t hat appear around a head lexical item, and bilexicalstatistics, or dependencies, which describe the likelihood of a head word tak ing as a dependent a phrase headed by a certain other word. 2This observation motivates various classor similaritybased approaches to combating sparseness, and this remains a promising avenue of work, but success in this area has proven somewhat elusive, and, at any rate, current lexicalized PCFGs do simply use exact word matches if available, and interpola te with syntactic category-based estimates when they are not. 3In this paper we use the term subcategorizationin the original general sense of Chomsky (1965), for where a syntactic ca tcategories appearing in the Penn treebank. Charniak (2000) shows the value his parser gains from parentannotation of nodes, suggesting that this information is at least partly complementary to information derivable from lexicalization, and Collins (1999) uses a range of linguistically motivated and carefully hand-engineered subcategorizations to break down wrong context-freedom assumptions of the naive Penn treebank covering PCFG, such as differentiating “baseNPs” from noun phrases with phrasal modifiers, and distinguishing sentences with empty subjects from those where there is an overt subject NP. While he gives incomplete experimental results as to their efficacy, we can assume that these features were incorporated because of beneficial effects on parsing that were complementary to lexicalization. In this paper, we show that the parsing performance that can be achieved by an unlexicalized PCFG is far higher than has previously been demonstrated, and is, indeed, much higher than community wisdom has thought possible. We describe several simple, linguistically motivated annotations which do much to close the gap between a vanilla PCFG and state-of-the-art lexicalized models. Specifically, we construct anunlexicalizedPCFG which outperforms the lexicalized PCFGs of Magerman (1995) and Collins (1996) (though not more recent models, such as Charniak (1997) or Collins (1999)). One benefit of this result is a much-strengthened lower bound on the capacity of an unlexicalized PCFG. To the extent that no such strong baseline has been provided, the community has tended to greatly overestimate the beneficial effect of lexicalization in probabilistic parsing, rather than looking critically at where lexicalized probabilities are both neededto make the right decision and availablein the training data. Secondly, this result affirms the value of linguistic analysis for feature discovery. The result has other uses and advantages: an unlexicalized PCFGis easier to interpret, reason about, and improve than the more complex lexicalized models. The grammar representation is much more compact, no longer requiring large structures that store lexicalized probabilities. The parsing algorithms have lower asymptotic complexity4 and have much smaller grammar egory is divided into several subcategories, for example di viding verb phrases into finite and non-finite verb phrases, rath e than in the modern restricted usage where the term refers onl y to the syntactic argument frames of predicators. 4O(n3) vs. O(n5) for a naive implementation, or vs. O(n4) if using the clever approach of Eisner and Satta (1999). constants. An unlexicalizedPCFG parser is much simpler to build and optimize, including both standard code optimization techniques and the investigation of methods for search space pruning (Caraballo and Charniak, 1998; Charniak et al., 1998). It is not our goal to argue against the use of lexicalized probabilities in high-performance probabilistic parsing. It has been comprehensively demonstrated that lexical dependencies are useful in resolving major classes of sentence ambiguities, and a parser should make use of such information where possible. We focus here on using unlexicalized, tructural context because we feel that this information has been underexploited and underappreciated. We see this investigation as only one part of the foundation for state-of-the-art parsing which employsboth lexical and structural conditioning. 1 Experimental Setup To facilitate comparison with previous work, we trained our models on sections 2–21 of the WSJsection of the Penn treebank. We used the first 20 files (393 sentences) of section 22 as a development set (devset ). This set is small enough that there is noticeable variance in individual results, but it allowed rapid search for good features via continually reparsing the devset in a partially manual hill-climb. All of section 23 was used as a test set for the final model. For each model, input trees were annotated or transformed in some way, as in Johnson (1998). Given a set of transformed trees, we viewed the local trees as grammar rewrite rules in the standard way, and used (unsmoothed) maximum-likelihood estimates for rule probabilities. 5 To parse the grammar, we used a simple array-based Java implementation of a generalizedCKY parser, which, for our final best model, was able to exhaustively parse all sentences in section 23 in 1GB of memory, taking approximately 3 sec for average length sentences. 6 5The tagging probabilities were smoothed to accommodate unknown words. The quantityP(tag|word) was estimated as follows: words were split into one of several categories wordclass, based on capitalization, suffix, digit, and other character features. For each of these categories, we took th e maximum-likelihood estimate of P(tag|wordclass). This distribution was used as a prior against which observed tagging s, if any, were taken, givingP(tag|word) = [c(tag, word) + κ P(tag|wordclass)]/[c(word)+κ]. This was then inverted to give P(word|tag). The quality of this tagging model impacts all numbers; for example the raw treebank grammar’s devset F 1 is 72.62 with it and 72.09 without it. 6The parser is available for download as open source at: http://nlp.stanford.edu/downloads/lex-parser.shtml"}
{"metadata": {"dataset": "scidocs", "query_id": "045a50ec31973fee15ff967f18e016fae77fd1f3", "doc_id": "no"}, "id": "scidocs-045a50ec31973fee15ff967f18e016fae77fd1f3-no", "question": "Characterizing cloud computing hardware reliability", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "045a50ec31973fee15ff967f18e016fae77fd1f3", "doc_id": "01e0a7cdbf9851a30f7dc31dc79adc2a7bde1c9f"}, "id": "scidocs-045a50ec31973fee15ff967f18e016fae77fd1f3-01e0a7cdbf9851a30f7dc31dc79adc2a7bde1c9f", "question": "Characterizing cloud computing hardware reliability", "context": "This paper considers current paradigms in computing and outlines the most important aspects concerning their reliability. The Fog computing paradigm as a non-trivial extension of the Cloud is considered and the reliability of the networks of smart devices are discussed. Combining the reliability requirements of grid and cloud paradigms with the reliability requirements of networks of sensor and actuators it follows that designing a reliable Fog computing platform is feasible."}
{"metadata": {"dataset": "scidocs", "query_id": "045a50ec31973fee15ff967f18e016fae77fd1f3", "doc_id": "e12083f6b62753e7e46fa466efd9f126b3310132"}, "id": "scidocs-045a50ec31973fee15ff967f18e016fae77fd1f3-e12083f6b62753e7e46fa466efd9f126b3310132", "question": "Characterizing cloud computing hardware reliability", "context": "Cloud computing provides support for hosting client's application. Cloud is a distributed platform that provides hardware, software and network resources to both execute consumer's application and also to store and mange user's data. Cloud is also used to execute scientific workflow applications that are in general complex in nature when compared to other applications. Since cloud is a distributed platform, it is more prone to errors and failures. In such an environment, avoiding a failure is difficult and identifying the source of failure is also complex. Because of this, fault tolerance mechanisms are implemented on the cloud platform. This ensures that even if there are failures in the environment, critical data of the client is not lost and user's application running on cloud is not affected in any manner. Fault tolerance mechanisms also help in improving the cloud's performance by proving the services to the users as required on demand. In this paper a survey of existing fault tolerance mechanisms for the cloud platform are discussed. This paper also discusses the failures, fault tolerant clustering methods and fault tolerant models that are specific for scientific workflow applications."}
{"metadata": {"dataset": "scidocs", "query_id": "045a50ec31973fee15ff967f18e016fae77fd1f3", "doc_id": "1bc6ea86a8ed0a80406404693c675f11f6b8e454"}, "id": "scidocs-045a50ec31973fee15ff967f18e016fae77fd1f3-1bc6ea86a8ed0a80406404693c675f11f6b8e454", "question": "Characterizing cloud computing hardware reliability", "context": "Cloud computing provides cost-efficient opportunities for enterprises by offering a variety of dynamic, scalable, and shared services. Usually, cloud providers provide assurances by specifying technical and functional descriptions in Service Level Agreements (SLAs) for the services they offer. The descriptions in SLAs are not consistent among the cloud providers even though they offer services with similar functionality. Therefore, customers are not sure whether they can identify a trustworthy cloud provider only based on its SLA. To support the customers in reliably identifying trustworthy cloud providers, we propose a multi-faceted Trust Management (TM) system architecture for a cloud computing marketplace. This system provides means to identify the trustworthy cloud providers in terms of different attributes (e.g., security, performance, compliance) assessed by multiple sources and roots of trust information."}
{"metadata": {"dataset": "scidocs", "query_id": "045a50ec31973fee15ff967f18e016fae77fd1f3", "doc_id": "8cafc53716505314c49fe46fe44dfa9f5542fc66"}, "id": "scidocs-045a50ec31973fee15ff967f18e016fae77fd1f3-8cafc53716505314c49fe46fe44dfa9f5542fc66", "question": "Characterizing cloud computing hardware reliability", "context": "Cloud computing based delivery model has been adopted by end-users and enterprises to reduce IT costs and complexities. The ability to offload user software and data to cloud data centers has raised many security and privacy concerns over the cloud computing model. Significant research efforts have focused on hyper visor security and low-layer operating system implementations in cloud data centers. Unfortunately, the role of cloud carrier in the security and privacy of user software and data has not been well studied. Cloud carrier represents the wide area network that provides the connectivity and transport of cloud services between cloud consumers and cloud providers. In this paper, we present a risk assessment framework to study the security risk of the cloud carrier between cloud consumers and cloud providers. The risk assessment framework leverages the National Vulnerability Database (NVD) to examine the security vulnerabilities of operating systems of routers within the cloud carrier. This framework provides quantifiable security metrics for cloud carrier, which enables cloud consumers to establish the quality of security services among cloud providers. Such security metric information is very useful in the Service Level Agreement (SLA) negotiation between a cloud consumer and a cloud provider. It can be also be used to build a tool to verify SLA compliance. Furthermore, we implement this framework for the cloud carriers of Amazon Web Services and Windows Azure Platform. Our experiments show that the security risks of cloud carriers on these two commercial clouds are significantly different. This finding provides guidance for a network provider to improve the security of cloud carriers."}
{"metadata": {"dataset": "scidocs", "query_id": "045a50ec31973fee15ff967f18e016fae77fd1f3", "doc_id": "385800aa5a2c4c84ad476df05721bf0ad7bf5df0"}, "id": "scidocs-045a50ec31973fee15ff967f18e016fae77fd1f3-385800aa5a2c4c84ad476df05721bf0ad7bf5df0", "question": "Characterizing cloud computing hardware reliability", "context": "Cloud computing is the concept implemented to decipher the Daily Computing Problems. Cloud computing is basically virtual pool of resources and it provides these resources to users via internet. Cloud computing is the internet based development and used in computer technology. The prevalent problem associated with cloud computing is data privacy, security, anonymity and reliability etc. But the most important between them is security and how cloud provider assures it. The work plan here is to eliminate the concerns regarding data privacy using encryption algorithms to enhance the security in cloud. Have discussed about cloud computing security mechanisms and presented the comparative study of several algorithms."}
{"metadata": {"dataset": "scidocs", "query_id": "045a50ec31973fee15ff967f18e016fae77fd1f3", "doc_id": "5b02cf69f2f9efe0cb61c922974748d10d1506af"}, "id": "scidocs-045a50ec31973fee15ff967f18e016fae77fd1f3-5b02cf69f2f9efe0cb61c922974748d10d1506af", "question": "Characterizing cloud computing hardware reliability", "context": "Recent years have seen the massive migration of enterprise applications to the cloud. One of the challenges posed by cloud applications is Quality-of-Service (QoS) management, which is the problem of allocating resources to the application to guarantee a service level along dimensions such as performance, availability and reliability. This paper aims at supporting research in this area by providing a survey of the state of the art of QoS modeling approaches suitable for cloud systems. We also review and classify their early application to some decision-making problems arising in cloud QoS management."}
{"metadata": {"dataset": "scidocs", "query_id": "045a50ec31973fee15ff967f18e016fae77fd1f3", "doc_id": "f9823bc7eec44a9a6cd7b629c8f6430fe82877fd"}, "id": "scidocs-045a50ec31973fee15ff967f18e016fae77fd1f3-f9823bc7eec44a9a6cd7b629c8f6430fe82877fd", "question": "Characterizing cloud computing hardware reliability", "context": "Cloud computing provides convenient on-demand network access to a shared pool of configurable computing resources. The resources can be rapidly deployed with great efficiency and minimal management overhead. Cloud is an insecure computing platform from the view point of the cloud users, the system must design mechanisms that not only protect sensitive information by enabling computations with encrypted data, but also protect users from malicious behaviours by enabling the validation of the computation result. In this paper, we propose a new data encoding scheme called layered interleaving, designed for time-sensitive packet recovery in the presence of bursty loss. It is high-speed data recovery scheme with minimal loss probability and using a forward error correction scheme to handle bursty loss. The proposed approach is highly efficient in recovering the singleton losses almost immediately and from bursty data losses."}
{"metadata": {"dataset": "scidocs", "query_id": "045a50ec31973fee15ff967f18e016fae77fd1f3", "doc_id": "51f3ba2fc98fb2d0f75a090d08f91d7840a7b57c"}, "id": "scidocs-045a50ec31973fee15ff967f18e016fae77fd1f3-51f3ba2fc98fb2d0f75a090d08f91d7840a7b57c", "question": "Characterizing cloud computing hardware reliability", "context": "Cloud environments offer low-cost computing resources as a subscription-based service. These resources are elastically scalable and dynamically provisioned. Furthermore, cloud providers have also pioneered new pricing models like spot instances that are cost-effective. As a result, scientific workflows are increasingly adopting cloud computing. However, spot instances are terminated when the market price exceeds the users bid price. Likewise, cloud is not a utopian environment. Failures are inevitable in such large complex distributed systems. It is also well studied that cloud resources experience fluctuations in the delivered performance. These challenges make fault tolerance an important criterion in workflow scheduling. This article presents an adaptive, just-in-time scheduling algorithm for scientific workflows. This algorithm judiciously uses both spot and on-demand instances to reduce cost and provide fault tolerance. The proposed scheduling algorithm also consolidates resources to further minimize execution time and cost. Extensive simulations show that the proposed heuristics are fault tolerant and are effective, especially under short deadlines, providing robust schedules with minimal makespan and cost."}
{"metadata": {"dataset": "scidocs", "query_id": "045a50ec31973fee15ff967f18e016fae77fd1f3", "doc_id": "523cf537aa1050efdcf0befe1d851b363afa0396"}, "id": "scidocs-045a50ec31973fee15ff967f18e016fae77fd1f3-523cf537aa1050efdcf0befe1d851b363afa0396", "question": "Characterizing cloud computing hardware reliability", "context": "Cloud computing is the concept implemented to decipher the Daily Computing Problems. Cloud computing is basically virtual pool of resources and it provides these resources to users via internet. Cloud computing is the internet based development and used in computer technology. The prevalent problem associated with cloud computing is data privacy, security, anonymity and reliability etc. But the most important between them is security and how cloud provider assures it. To secure the Cloud means secure the treatments (calculations) and storage (databases hosted by the Cloud provider). In this paper we analyses different security issues to cloud and different cryptographic algorithms adoptable to better security for the cloud. Keywords— Cloud Computing, Cryptographic Algorithm, Internet, Security Algorithms, Security Attacks, Security Issue"}
{"metadata": {"dataset": "scidocs", "query_id": "045a50ec31973fee15ff967f18e016fae77fd1f3", "doc_id": "286dc0c3992f5b9c78209827117546888371b818"}, "id": "scidocs-045a50ec31973fee15ff967f18e016fae77fd1f3-286dc0c3992f5b9c78209827117546888371b818", "question": "Characterizing cloud computing hardware reliability", "context": "Cloud computing presents a new model for IT service delivery and it typically involves over-a-network, on-demand, self-service access, which is dynamically scalable and elastic, utilising pools of often virtualized resources. Through these features, cloud computing has the potential to improve the way businesses and IT operate by offering fast start-up, flexibility, scalability and cost efficiency. Even though cloud computing provides compelling benefits and cost-effective options for IT hosting and expansion, new risks and opportunities for security exploits are introduced. Standards, policies and controls are therefore of the essence to assist management in protecting and safeguarding systems and data. Management should understand and analyse cloud computing risks in order to protect systems and data from security exploits. The focus of this paper is on mitigation for cloud computing security risks as a fundamental step towards ensuring secure cloud computing environments."}
{"metadata": {"dataset": "scidocs", "query_id": "0491b1a097378701dbbab2ce9dcc2e109a95d97e", "doc_id": "no"}, "id": "scidocs-0491b1a097378701dbbab2ce9dcc2e109a95d97e-no", "question": "A Dark Side of the Cannula Injections: How Arterial Wall Perforations and Emboli Occur", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0491b1a097378701dbbab2ce9dcc2e109a95d97e", "doc_id": "a374d6b042b8a0f1bb09c0c19d9cf8f9a203e5e5"}, "id": "scidocs-0491b1a097378701dbbab2ce9dcc2e109a95d97e-a374d6b042b8a0f1bb09c0c19d9cf8f9a203e5e5", "question": "A Dark Side of the Cannula Injections: How Arterial Wall Perforations and Emboli Occur", "context": "Vascular complications after hyaluronic acid (HA) filling of the chin have rarely been reported. In this report, two cases of vascular occlusion after HA augmentation of the mentum are presented. The first case involved local skin necrosis that resulted from a massive microcirculatory embolism and/or external compression of the chin skin microvasculature. The second case involved vascular compromise in the tongue that resulted from HA injection in the chin. The diagnosis was established on the basis of interventional angiography findings. Concerning the pathogenesis, we hypothesized that the filler embolus flowed into the branches of the deep lingual artery through the rich vascular anastomoses among the submental, sublingual, and deep lingual arteries, after being accidentally injected into the submental artery (or its branches). Level of Evidence V This journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com/00266."}
{"metadata": {"dataset": "scidocs", "query_id": "0491b1a097378701dbbab2ce9dcc2e109a95d97e", "doc_id": "766b55aa9c8915a9f786f0fd9f0e79f2e9bf57dc"}, "id": "scidocs-0491b1a097378701dbbab2ce9dcc2e109a95d97e-766b55aa9c8915a9f786f0fd9f0e79f2e9bf57dc", "question": "A Dark Side of the Cannula Injections: How Arterial Wall Perforations and Emboli Occur", "context": "Hyaluronic acid filler injection is widely used for soft tissue augmentation. However, there can be disastrous complications by direct vascular embolization. We present a case of ischemic oculomotor nerve palsy and skin necrosis after hyaluronic acid filler injection on glabellar.blepharoptosis, exotropia and diplopia developed suddenly after the injection, and skin necrosis gradually occurred. Symptoms and signs of oculomotor nerve palsy continuously improved with steroid therapy. Skin defects healed with minimal scars through intensive wound care.Percutaneous filler injection of periorbital areas should be performed carefully by experienced surgeons, and the possibility of embolization should be considered promptly if symptoms develop."}
{"metadata": {"dataset": "scidocs", "query_id": "0491b1a097378701dbbab2ce9dcc2e109a95d97e", "doc_id": "e107087aeaee9b98b17f229e96392aed1781bdbe"}, "id": "scidocs-0491b1a097378701dbbab2ce9dcc2e109a95d97e-e107087aeaee9b98b17f229e96392aed1781bdbe", "question": "A Dark Side of the Cannula Injections: How Arterial Wall Perforations and Emboli Occur", "context": "Dorsal nasal augmentation is an essential part of injection rhinoplasty on the Asian nose. Aesthetic physicians require detailed knowledge of the nasal anatomy to accurately and safely inject filler. One hundred and thirty-five histological cross sections were examined from 45 longitudinal strips of soft tissue harvested from the midline of the nose, beginning from the glabella to the nasal tip. Muscles and nasal cartilage were used as landmarks for vascular identification. At the nasal tip, a midline longitudinal columellar artery with a diameter of 0.21 ± 0.09 mm was noted in 14 cadavers (31.1 %). At the infratip, subcutaneous tissue contained cavernous tissue similar to that of the nasal mucosa. The feeding arteries of these dilated veins formed arteriovenous shunts, into which retrograde injection of filler may be possible. All of the nasal arteries present were identified as subcutaneous arteries. They coursed mainly in the superficial layer of the subcutaneous tissues, with smaller branches forming subdermal plexuses. A substantial arterial anastomosis occurred at the supratip region, in which the artery lay in the middle of the subcutaneous tissue at the level of the major alar cartilages. These arteries had a diameter ranging between 0.4 and 0.9 mm and were found in 29 of 45 specimens (64.4 %). This was at the level midway between the rhinion above the supratip and the infratip. This anastomotic artery also crossed the midline at the rhinion superficial to the origin of the procerus on the lower end of the nasal bone. Here the arterial diameter ranged between 0.1 and 0.3 mm, which was not large enough to cause arterial emboli. Fascicular cross sections of the nasalis muscle directly covered the entire upper lateral cartilage. The subdermal tissue contained few layers of fat cells along with the occasional small artery. The procerus arose from the nasal bone and was continuous with the nasalis in 16 cadavers (35.6 %). There was fatty areolar tissue between the procerus and the periosteal layer and no significant arteries present. The procerus ascended beyond the brow to insert into the frontalis muscle with very few cutaneous insertions. The supratrochlear vessels and accompanying nerve were occasionally found on the surface of the frontalis muscle. Most nasal arteries found in the midline are subcutaneous arteries. Filler should be injected deeply to avoid vascular injury leading to compromised perfusion at the dorsum or filler emboli at the nasal tip. This journal requires that the authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com/00266 ."}
{"metadata": {"dataset": "scidocs", "query_id": "0491b1a097378701dbbab2ce9dcc2e109a95d97e", "doc_id": "697d28f8c1074b67ac6300ca6ca46c8f913380da"}, "id": "scidocs-0491b1a097378701dbbab2ce9dcc2e109a95d97e-697d28f8c1074b67ac6300ca6ca46c8f913380da", "question": "A Dark Side of the Cannula Injections: How Arterial Wall Perforations and Emboli Occur", "context": "BACKGROUND\nEmbolia cutis medicamentosa (ECM) is a rare phenomenon attributed to intra-arterial drug injection. Glabellar filler injections can result in potentially devastating visual loss from inadvertent retrograde arteriolar embolization due to the extensive vasculature within the upper face. The minimum amount of filler necessary to potentiate this complication has not yet been reported.\n\n\nOBJECTIVES\nWe aim to determine the volume of filler necessary to occupy the supratrochlear artery from the glabella to the bifurcation of the ophthalmic and central retinal arteries. We specifically examine the volume of the supratrochlear artery from the glabella to orbital apex.\n\n\nMETHODS\nThe study was approved by Duke University Institutional Review Board and involved surgical dissection of six fresh tissue cadaver heads (12 hemifaces). The arterial system in each cadaver head was injected with latex for visualization. The supratrochlear arteries were isolated anteriorly from the glabella to the orbital apex posteriorly. Intra-orbital vessel radius and length were measured. The vessel volume was calculated by water displacement of the intra-arterial latex.\n\n\nRESULTS\nThe vessel volumes ranged from 0.04 to 0.12 mL. The average vessel volume was calculated as 0.085 mL, the average length as 51.75 mm, and the average radius as 0.72 mm.\n\n\nCONCLUSIONS\nVascular occlusion from filler injections can lead to devastating visual consequences due to inadvertent retrograde intra-arterial embolization. Our findings indicate that the average entire volume of the supratrochlear artery from the glabella to the orbital apex is 0.085 mL. Injectors should be aware that a bolus of this critical volume may lead to a significant adverse outcome."}
{"metadata": {"dataset": "scidocs", "query_id": "0491b1a097378701dbbab2ce9dcc2e109a95d97e", "doc_id": "605a105608bb7aff888360879c53ed02ac116577"}, "id": "scidocs-0491b1a097378701dbbab2ce9dcc2e109a95d97e-605a105608bb7aff888360879c53ed02ac116577", "question": "A Dark Side of the Cannula Injections: How Arterial Wall Perforations and Emboli Occur", "context": "Facial fillers are becoming increasingly popular as aesthetic procedures to temporarily reduce the depth of wrinkles or to contour faces. However, even in the hands of very experienced injectors, there is always a small possibility of vascular complications like intra-arterial injection of filler substance. We present a case report of a patient who developed features of vascular obstruction in right infraorbital artery and tell-tale signs of impending skin necrosis, after hyaluronic acid filler injection by an experienced injector. The diagnosis of a vascular complication was made quickly with the help of clinical features like blanching, livedo reticularis, and poor capillary refill. Patient was treated promptly with \"high-dose pulsed hyaluronidase protocol\" comprising three 1,000-unit pulses of hyaluronidase, administered hourly. There was no further increase in size of the involved area after the first dose of hyaluronidase. All of the involved area, along with 1 cm overlapping in uninvolved skin area, was injected during each injection pulse, using a combination of cannula and needle. Complete reperfusion and good capillary filling were achieved after completion of 3 pulses, and these were taken as the end-point of high-dose pulsed hyaluronidase treatment. Immediate skin changes after filler injections, as well as after hyaluronidase injections and during the 3-week recovery period, were documented with photographs and clinical notes. Involved skin was found to have been fully recovered from this vascular episode, thus indicating that complete recovery of the ischemic skin changes secondary to possible intra-arterial injection could be achieved using high-dose pulsed hyaluronidase protocol."}
{"metadata": {"dataset": "scidocs", "query_id": "0491b1a097378701dbbab2ce9dcc2e109a95d97e", "doc_id": "5841bf263cfd388a7af631f0f85fc6fa07dca945"}, "id": "scidocs-0491b1a097378701dbbab2ce9dcc2e109a95d97e-5841bf263cfd388a7af631f0f85fc6fa07dca945", "question": "A Dark Side of the Cannula Injections: How Arterial Wall Perforations and Emboli Occur", "context": "SUMMARY\nForeign body granulomas occur at certain rates with all injectable dermal fillers. They have to be distinguished from early implant nodules, which usually appear 2 to 4 weeks after injection. In general, foreign body granulomas appear after a latent period of several months at all injected sites at the same time. If diagnosed early and treated correctly, they can be diminished within a few weeks. The treatment of choice of this hyperactive granulation tissue is the intralesional injection of corticosteroid crystals (triamcinolone, betamethasone, or prednisolone), which may be repeated in 4-week cycles until the right dose is found. To lower the risk of skin atrophy, corticosteroids can be combined with antimitotic drugs such as 5-fluorouracil and pulsed lasers. Because foreign body granulomas grow fingerlike into the surrounding tissue, surgical excision should be the last option. Surgery or drainage is indicated to treat normal lumps and cystic foreign body granulomas with little tissue ingrowth. In most patients, a foreign body granuloma is a single event during a lifetime, often triggered by a systemic bacterial infection."}
{"metadata": {"dataset": "scidocs", "query_id": "0491b1a097378701dbbab2ce9dcc2e109a95d97e", "doc_id": "77febaeb483dc8d145a2c897f743ee46b11266ad"}, "id": "scidocs-0491b1a097378701dbbab2ce9dcc2e109a95d97e-77febaeb483dc8d145a2c897f743ee46b11266ad", "question": "A Dark Side of the Cannula Injections: How Arterial Wall Perforations and Emboli Occur", "context": "Injection-induced necrosis is a rare but dreaded consequence of soft tissue augmentation with filler agents. It usually occurs as a result of injection of filler directly into an artery, but can also result from compression or injury. We provide recommendations on the use of hyaluronidase when vascular compromise is suspected. Consensus recommendations were developed by thorough discussion and debate amongst the authors at a roundtable meeting on Wednesday June 18, 2014 in Las Vegas, NV as well as significant ongoing written and verbal communications amongst the authors in the months prior to journal submission. All authors are experienced tertiary care providers. A prompt diagnosis and immediate treatment with high doses of hyaluronidase (at least 200 U) are critically important. It is not felt necessary to do a skin test in cases of impending necrosis. Some experts recommend dilution with saline to increase dispersion or lidocaine to aid vasodilation. Additional hyaluronidase should be injected if improvement is not seen within 60 minutes. A warm compress also aids vasodilation, and massage has been shown to help. Some experts advocate the use of nitroglycerin paste, although this area is controversial. Introducing an oral aspirin regimen should help prevent further clot formation due to vascular compromise. In our experience, patients who are diagnosed promptly and treated within 24 hours will usually have the best outcomes."}
{"metadata": {"dataset": "scidocs", "query_id": "0491b1a097378701dbbab2ce9dcc2e109a95d97e", "doc_id": "00da506d8b50ba47313feb642c0caef2352080bd"}, "id": "scidocs-0491b1a097378701dbbab2ce9dcc2e109a95d97e-00da506d8b50ba47313feb642c0caef2352080bd", "question": "A Dark Side of the Cannula Injections: How Arterial Wall Perforations and Emboli Occur", "context": "Soft tissue filler injections are the second most common non-surgical procedure performed by the plastic surgeon. Embolization of intravascular material after facial injection is a rare but terrifying outcome due to the high likelihood of long-term sequela such as blindness and cerebrovascular accident. The literature is replete with examples of permanent blindness caused by injection with autologous fat, soft tissue fillers such as hyaluronic acid, PLLA, calcium hydroxyl-apatite, and even corticosteroid suspensions. However, missing from the discussion is an effective treatment algorithm that can be quickly and safely followed by injecting physicians in the case of an intravascular injection with impending blindness. In this report, we present the case of a 64-year-old woman who suffered from blindness and hemiparesis after facial cosmetic injections performed by a family physician. We use this case to create awareness that this complication has become more common as the number of injectors and patients seeking these treatments have increased exponentially over the past few years. We share in this study our experience with the incorporation of a “blindness safety kit” in each of our offices to promptly initiate treatment in someone with embolization and impending blindness. The kit contains a step-by-step protocol to follow in the event of arterial embolization of filler material associated with ocular pain and impending loss of vision. This journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com/00266 ."}
{"metadata": {"dataset": "scidocs", "query_id": "0491b1a097378701dbbab2ce9dcc2e109a95d97e", "doc_id": "44035f565cd8cc78d5698b072d5cb2aef12ac49a"}, "id": "scidocs-0491b1a097378701dbbab2ce9dcc2e109a95d97e-44035f565cd8cc78d5698b072d5cb2aef12ac49a", "question": "A Dark Side of the Cannula Injections: How Arterial Wall Perforations and Emboli Occur", "context": "With limited downtime and immediate results, facial filler injections are becoming an ever more popular alternative to surgical rejuvenation of the face. The results, and the complications, can be impressive. To maximize safety during injections, the authors have outlined general injection principles followed by pertinent anatomy within six different facial danger zones. Bearing in mind the depth and the location of the vasculature within each zone, practitioners can tailor their injection techniques to prevent vessel injury and avoid cannulation."}
{"metadata": {"dataset": "scidocs", "query_id": "0491b1a097378701dbbab2ce9dcc2e109a95d97e", "doc_id": "407ee40d3f6168411e4b66eec003e416f6195466"}, "id": "scidocs-0491b1a097378701dbbab2ce9dcc2e109a95d97e-407ee40d3f6168411e4b66eec003e416f6195466", "question": "A Dark Side of the Cannula Injections: How Arterial Wall Perforations and Emboli Occur", "context": "BACKGROUND\nThe purposes of this study were to determine the morphological features and conceptualize the anatomical definition of the angular artery (AA) as an aid to practical operations in the clinical field.\n\n\nMATERIALS AND METHODS\nThirty-one hemifaces from 17 Korean cadavers and 26 hemifaces from 13 Thai cadavers were dissected.\n\n\nRESULTS\nThe topography of the AA was classified into 4 types according to its course: Type I (persistent pattern), in which the AA traverses the lateral side of the nose (11%); Type II (detouring pattern), in which the AA traverses the cheek and tear trough area (18%); Type III (alternative pattern), in which the AA traverses the medial canthal area through a branch of the ophthalmic artery (22.8%); and Type IV (latent pattern), in which the AA is absent (26.3%).\n\n\nCONCLUSION\nThe findings of this study will contribute toward improved outcomes for cosmetic surgery involving the injection of facial filler by enhancing the understanding of AA anatomy."}
{"metadata": {"dataset": "scidocs", "query_id": "04ca5de59edbdd49a9c0502c58331524d220bc8c", "doc_id": "no"}, "id": "scidocs-04ca5de59edbdd49a9c0502c58331524d220bc8c-no", "question": "Communication Efficient Distributed Machine Learning with the Parameter Server", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "04ca5de59edbdd49a9c0502c58331524d220bc8c", "doc_id": "2f7678f96837afbc1f58680ad844c35ffa52b0c1"}, "id": "scidocs-04ca5de59edbdd49a9c0502c58331524d220bc8c-2f7678f96837afbc1f58680ad844c35ffa52b0c1", "question": "Communication Efficient Distributed Machine Learning with the Parameter Server", "context": "As Machine Learning (ML) applications embrace greater data size and model complexity, practitioners turn to distributed clusters to satisfy the increased computational and memory demands. Effective use of clusters for ML programs requires considerable expertise in writing distributed code, but existing highlyabstracted frameworks like Hadoop that pose low bar-ed frameworks like Hadoop that pose low barriers to distributed-programming have not, in practice, matched the performance seen in highly specialized and advanced ML implementations. The recent Parameter Server (PS) paradigm is a middle ground between these extremes, allowing easy conversion of single-machine parallel ML programs into distributed ones, while maintaining high throughput through relaxed “consistency models” that allow asynchronous (and, hence, inconsistent) parameter reads. However, due to insufficient theoretical study, it is not clear which of these consistency models can really ensure correct ML algorithm output; at the same time, there remain many theoreticallymotivated but undiscovered opportunities to maximize computational throughput. Inspired by this challenge, we study both the theoretical guarantees and empirical behavior of iterative-convergent ML algorithms in existing PS consistency models. We then use the gleaned insights to improve a consistency model using an “eager” PS communication mechanism, and implement it as a new PS system that enables ML programs to reach their solution more quickly."}
{"metadata": {"dataset": "scidocs", "query_id": "04ca5de59edbdd49a9c0502c58331524d220bc8c", "doc_id": "13cf0ec38d349f1c83f0e6c26e89c85222955f5f"}, "id": "scidocs-04ca5de59edbdd49a9c0502c58331524d220bc8c-13cf0ec38d349f1c83f0e6c26e89c85222955f5f", "question": "Communication Efficient Distributed Machine Learning with the Parameter Server", "context": "Due to the rapid growth of data and the ever increasing model complexity, which often manifests itself in the large number of model parameters, today, many important machine learning problems cannot be efficiently solved by a single machine. Distributed optimization and inference is becoming more and more inevitable for solving large scale machine learning problems in both academia and industry. However, obtaining an efficient distributed implementation of an algorithm, is far from trivial. Both intensive computational workloads and the volume of data communication demand careful design of distributed computation systems and distributed machine learning algorithms. In this thesis, we focus on the co-design of distributed computing systems and distributed optimization algorithms that are specialized for large machine learning problems. In the first part, we propose two distributed computing frameworks: Parameter Server, a distributed machine learning framework that features efficient data communication between the machines; MXNet, a multi-language library that aims to simplify the development of deep neural network algorithms. We have witnessed the wide adoption of the two proposed systems in the past two years. They have enabled and will continue to enable more people to harness the power of distributed computing to design efficient large-scale machine learning applications. In the second part, we examine a number of distributed optimization problems in machine learning, leveraging the two computing platforms. We present new methods to accelerate the training process, such as data partitioning with better locality properties, communication friendly optimization methods, and more compact statistical models. We implement the new algorithms on the two systems and test on large scale real data sets. We successfully demonstrate that careful co-design of computing systems and learning algorithms can greatly accelerate large scale distributed machine learning."}
{"metadata": {"dataset": "scidocs", "query_id": "04ca5de59edbdd49a9c0502c58331524d220bc8c", "doc_id": "7ca3f9400049a860ded736840e345da3cd8ce150"}, "id": "scidocs-04ca5de59edbdd49a9c0502c58331524d220bc8c-7ca3f9400049a860ded736840e345da3cd8ce150", "question": "Communication Efficient Distributed Machine Learning with the Parameter Server", "context": "This work introduces a novel, modular, layered web based platform for managing machine learning experiments on grid-based High Performance Computing infrastructures. The coupling of the communication services offered by the grid, with an administration layer and conventional web server programming, via a data synchronization utility, leads to the straightforward development of a web-based user interface that allows the monitoring and managing of diverse online distributed computing applications. It also introduces an experiment generation and monitoring tool particularly suitable for investigating machine learning in game playing. The platform is demonstrated with experiments for two different games."}
{"metadata": {"dataset": "scidocs", "query_id": "04ca5de59edbdd49a9c0502c58331524d220bc8c", "doc_id": "4414a35093721c9f8aa73cd26bae895879de84d2"}, "id": "scidocs-04ca5de59edbdd49a9c0502c58331524d220bc8c-4414a35093721c9f8aa73cd26bae895879de84d2", "question": "Communication Efficient Distributed Machine Learning with the Parameter Server", "context": "We present a Communication-efficient Surrogate Likelihood (CSL) framework for solving distributed statistical inference problems. CSL provides a communication-efficient surrogate to the global likelihood that can be used for low-dimensional estimation, high-dimensional regularized estimation and Bayesian inference. For low-dimensional estimation, CSL provably improves upon naive averaging schemes and facilitates the construction of confidence intervals. For high-dimensional regularized estimation, CSL leads to a minimax-optimal estimator with controlled communication cost. For Bayesian inference, CSL can be used to form a communication-efficient quasi-posterior distribution that converges to the true posterior. This quasi-posterior procedure significantly improves the computational efficiency of MCMC algorithms even in a non-distributed setting. We present both theoretical analysis and experiments to explore the properties of the CSL approximation."}
{"metadata": {"dataset": "scidocs", "query_id": "04ca5de59edbdd49a9c0502c58331524d220bc8c", "doc_id": "02de9d7b2c76a11896902c79b329a3034fc572b6"}, "id": "scidocs-04ca5de59edbdd49a9c0502c58331524d220bc8c-02de9d7b2c76a11896902c79b329a3034fc572b6", "question": "Communication Efficient Distributed Machine Learning with the Parameter Server", "context": "Emerging paradigms like High Performance Data Analytics (HPDA) and Deep Learning (DL) pose at least two new design challenges for existing MPI runtimes. First, these paradigms require an efficient support for communicating unusually large messages across processes. And second, the communication buffers used by HPDA applications and DL frameworks generally reside on a GPU's memory. In this context, we observe that conventional MPI runtimes have been optimized over decades to achieve lowest possible communication latency for relatively smaller message sizes (up-to 1 Megabyte) and that too for CPU memory buffers. With the advent of CUDA-Aware MPI runtimes, a lot of research has been conducted to improve performance of GPU buffer based communication. However, little exists in current state of the art that deals with very large message communication of GPU buffers. In this paper, we investigate these new challenges by analyzing the performance bottlenecks in existing CUDA-Aware MPI runtimes like MVAPICH2-GDR, and propose hierarchical collective designs to improve communication latency of the MPI_Bcast primitive by exploiting a new communication library called NCCL. To the best of our knowledge, this is the first work that addresses these new requirements where GPU buffers are used for communication with message sizes surpassing hundreds of megabytes. We highlight the design challenges for our work along with the details of design and implementation. In addition, we provide a comprehensive performance evaluation using a Micro-benchmark and a CUDA-Aware adaptation of Microsoft CNTK DL framework. We report up to 47% improvement in training time for CNTK using the proposed hierarchical MPI_Bcast design."}
{"metadata": {"dataset": "scidocs", "query_id": "04ca5de59edbdd49a9c0502c58331524d220bc8c", "doc_id": "2645a136bb1f0af81a526f04a1c9eb2b28dccb1b"}, "id": "scidocs-04ca5de59edbdd49a9c0502c58331524d220bc8c-2645a136bb1f0af81a526f04a1c9eb2b28dccb1b", "question": "Communication Efficient Distributed Machine Learning with the Parameter Server", "context": "We study two communication-efficient algorithms for distributed statistical optimization on large-scale data. The first algorithm is an averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves mean-squared error that decays as O(N-1 + (N/m)-2). Whenever m ≤ √N, this guarantee matches the best possible rate achievable by a centralized algorithm with access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap. Requiring only a single round of communication, it has mean-squared error that decays as O(N-1 + (N/m)-3), and so is more robust to the amount of parallelization."}
{"metadata": {"dataset": "scidocs", "query_id": "04ca5de59edbdd49a9c0502c58331524d220bc8c", "doc_id": "3157ed1fbad482520ca87045b308446d8adbdedb"}, "id": "scidocs-04ca5de59edbdd49a9c0502c58331524d220bc8c-3157ed1fbad482520ca87045b308446d8adbdedb", "question": "Communication Efficient Distributed Machine Learning with the Parameter Server", "context": "Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10–100× as compared to synchronized stochastic gradient descent."}
{"metadata": {"dataset": "scidocs", "query_id": "04ca5de59edbdd49a9c0502c58331524d220bc8c", "doc_id": "ffa25b893ea72ffb077158eb750df827d154b5b6"}, "id": "scidocs-04ca5de59edbdd49a9c0502c58331524d220bc8c-ffa25b893ea72ffb077158eb750df827d154b5b6", "question": "Communication Efficient Distributed Machine Learning with the Parameter Server", "context": "State-of-the-art deep learning systems rely on iterative distributed training to tackle the increasing complexity of models and input data. The iteration time in these communication-heavy systems depends on the computation time, communication time and the extent of overlap of computation and communication. In this work, we identify a shortcoming in systems with graph representation for computation, such as TensorFlow and PyTorch, that result in high variance in iteration time — random order of received parameters across workers. We develop a system, TicTac, to improve the iteration time by fixing this issue in distributed deep learning with Parameter Servers while guaranteeing near-optimal overlap of communication and computation. TicTac identifies and enforces an order of network transfers which improves the iteration time using prioritization. Our system is implemented over TensorFlow and requires no changes to the model or developer inputs. TicTac improves the throughput by up to 37.7% in inference and 19.2% in training, while also reducing straggler effect by up to 2.3×. Our code is publicly available."}
{"metadata": {"dataset": "scidocs", "query_id": "04ca5de59edbdd49a9c0502c58331524d220bc8c", "doc_id": "080aebd2cc1019f17e78496354c37195560b0697"}, "id": "scidocs-04ca5de59edbdd49a9c0502c58331524d220bc8c-080aebd2cc1019f17e78496354c37195560b0697", "question": "Communication Efficient Distributed Machine Learning with the Parameter Server", "context": "MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines."}
{"metadata": {"dataset": "scidocs", "query_id": "04ca5de59edbdd49a9c0502c58331524d220bc8c", "doc_id": "edc2e4e6308d7dfce586cb8a4441c704f8f8d41b"}, "id": "scidocs-04ca5de59edbdd49a9c0502c58331524d220bc8c-edc2e4e6308d7dfce586cb8a4441c704f8f8d41b", "question": "Communication Efficient Distributed Machine Learning with the Parameter Server", "context": "In this paper, we present two new communication-efficient methods for distributed minimization of an average of functions. The first algorithm is an inexact variant of the DANE algorithm [20] that allows any local algorithm to return an approximate solution to a local subproblem. We show that such a strategy does not affect the theoretical guarantees of DANE significantly. In fact, our approach can be viewed as a robustification strategy since the method is substantially better behaved than DANE on data partition arising in practice. It is well known that DANE algorithm does not match the communication complexity lower bounds. To bridge this gap, we propose an accelerated variant of the first method, called AIDE, that not only matches the communication lower bounds but can also be implemented using a purely first-order oracle. Our empirical results show that AIDE is superior to other communication efficient algorithms in settings that naturally arise in machine learning applications."}
{"metadata": {"dataset": "scidocs", "query_id": "04e4034344bda5c97015ea634e6eb1b65ef3a898", "doc_id": "no"}, "id": "scidocs-04e4034344bda5c97015ea634e6eb1b65ef3a898-no", "question": "Agile Team Perceptions of Productivity Factors", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "04e4034344bda5c97015ea634e6eb1b65ef3a898", "doc_id": "6426be25b02086a78a5dd5505c077c5d4205f275"}, "id": "scidocs-04e4034344bda5c97015ea634e6eb1b65ef3a898-6426be25b02086a78a5dd5505c077c5d4205f275", "question": "Agile Team Perceptions of Productivity Factors", "context": "In software development, people have a fundamental role as the basis for a project’s success. Regarding agile methodologies, this factor is increased by the need of self-organized teams, which is related to its member’s personality and the relationships between them. This paper evaluates how the member’s personality types and social relations influence the outcome of Scrum teams, based on MBTI and sociometry. As a result it was possible to identify how psychological profiles influence the quality, productivity and achievement of goals defined in the Sprint Backlog."}
{"metadata": {"dataset": "scidocs", "query_id": "04e4034344bda5c97015ea634e6eb1b65ef3a898", "doc_id": "08ae3f221339feb8230ae15647df51e7a5b5a13a"}, "id": "scidocs-04e4034344bda5c97015ea634e6eb1b65ef3a898-08ae3f221339feb8230ae15647df51e7a5b5a13a", "question": "Agile Team Perceptions of Productivity Factors", "context": "The better the software development community becomes at creating software, the more software the world seems to demand. Although there is a large body of research about measuring and investigating productivity from an organizational point of view, there is a paucity of research about how software developers, those at the front-line of software construction, think about, assess and try to improve their productivity. To investigate software developers' perceptions of software development productivity, we conducted two studies: a survey with 379 professional software developers to help elicit themes and an observational study with 11 professional software developers to investigate emergent themes in more detail. In both studies, we found that developers perceive their days as productive when they complete many or big tasks without significant interruptions or context switches. Yet, the observational data we collected shows our participants performed significant task and activity switching while still feeling productive. We analyze such apparent contradictions in our findings and use the analysis to propose ways to better support software developers in a retrospection and improvement of their productivity through the development of new tools and the sharing of best practices."}
{"metadata": {"dataset": "scidocs", "query_id": "04e4034344bda5c97015ea634e6eb1b65ef3a898", "doc_id": "81b1991bfc9e32a3af7bd00bb84a3b4a60007f19"}, "id": "scidocs-04e4034344bda5c97015ea634e6eb1b65ef3a898-81b1991bfc9e32a3af7bd00bb84a3b4a60007f19", "question": "Agile Team Perceptions of Productivity Factors", "context": "Scrum Teams use lightweight tools like Story Points, the Burn down chart, and Team Velocity. While essential, these tools alone provide insufficient information to maintain a high energy state that yields Hyper productivity. More data is required, but data collection itself can slow Teams. This effect must be avoided when productivity is the primary marker of success. Here we describe nine metrics that can develop and sustain Hyper productive Teams -- Velocity, Work Capacity, Focus Factor, Percentage of Adopted Work, Percentage of Found Work, Accuracy of Estimation, Accuracy of Forecast, Targeted Value Increase, Success at Scale, and the Win/Loss Record of the Team. The unique contribution of this paper is to demonstrate how a light touch and lightweight strategy can be used to compare Teams with different Story Point reference scales."}
{"metadata": {"dataset": "scidocs", "query_id": "04e4034344bda5c97015ea634e6eb1b65ef3a898", "doc_id": "b58b3a1dd84fe44f91510df00905a1ed33c1525c"}, "id": "scidocs-04e4034344bda5c97015ea634e6eb1b65ef3a898-b58b3a1dd84fe44f91510df00905a1ed33c1525c", "question": "Agile Team Perceptions of Productivity Factors", "context": "Since the late seventies, efforts to catalog factors that influences productivity, as well as actions to improve it, has been a huge concern for both academy and software development industry. Despite numerous studies, software organizations still do not know which the most significant factors are and what to do with it. Several studies present the factors in a very superficial way, some others address only the related factors or there are those that describe only a single factor. Actions to deal with the factors are spread and frequently were not mapped. Through a literature review, this paper presents a consolidated view of the main factors that have affected productivity over the years, and the strategies to deal with these factors nowadays. This research aims to support software development industry on the selection of their strategies to improve productivity by maximizing the positive factors and minimizing or avoiding the impact of the negative ones."}
{"metadata": {"dataset": "scidocs", "query_id": "04e4034344bda5c97015ea634e6eb1b65ef3a898", "doc_id": "d7b9dde9a7d304b378079049a0c2af40454a13bb"}, "id": "scidocs-04e4034344bda5c97015ea634e6eb1b65ef3a898-d7b9dde9a7d304b378079049a0c2af40454a13bb", "question": "Agile Team Perceptions of Productivity Factors", "context": "Agile software development practices such as eXtreme Programming (XP) and SCRUM have increasingly been adopted to respond to the challenges of volatile business environments, where the markets and technologies evolve rapidly and present the unexpected. In spite of the encouraging results so far, little is known about how agile practices affect communication. This article presents the results from a study which examined the impact of XP and SCRUM practices on communication within software development teams and within the focal organization. The research was carried out as a case study in F-Secure where two agile software development projects were compared from the communication perspective. The goal of the study is to increase the understanding of communication in the context of agile software development: internally among the developers and project leaders and in the interface between the development team and stakeholders (i.e. customers, testers, other development teams). The study shows that agile practices improve both informal and formal communication. However, it further indicates that, in larger development situations involving multiple external stakeholders, a mismatch of adequate communication mechanisms can sometimes even hinder the communication. The study highlights the fact that hurdles and improvements in the communication process can both affect the feature requirements and task subtask dependencies as described in coordination theory. While the use of SCRUM and some XP practices facilitate team and organizational communication of the dependencies between product features and working tasks, the use of agile practices requires that the team and organization use also additional plan-driven practices to ensure the efficiency of external communication between all the actors of software development."}
{"metadata": {"dataset": "scidocs", "query_id": "04e4034344bda5c97015ea634e6eb1b65ef3a898", "doc_id": "64290c658d2f1c47ad4fd8757a87ac6c9a708f89"}, "id": "scidocs-04e4034344bda5c97015ea634e6eb1b65ef3a898-64290c658d2f1c47ad4fd8757a87ac6c9a708f89", "question": "Agile Team Perceptions of Productivity Factors", "context": "\" This article uses an ecological approach to analyze factors in the effectiveness of work teams--small groups of interdependent individuals who share responsibility for outcomes for their organizations. Applications include advice and involvement, as in quality control circles and committees; production and service, as in assembly groups and sales teams; projects and development, as in engineering and research groups; and action and negotiation, as in sports teams and combat units. An analytic framework depicts team effectiveness as interdependent with organizational context, boundaries, and team development. Key context factors include (a) organizational culture, (b) technology and task design, (c) mission clarity, (d) autonomy, (e) rewards, ( f ) performance feedback, (g) training/consultation, and (h) physical environment. Team boundaries may mediate the impact of organizational context on team development. Current research leaves unanswered questions but suggests that effectiveness depends on organizational context and boundaries as much as on internal processes. Issues are raised for research and practice. The terms work team and work group appear often in today's discussions of organizations. Some experts claim that to be effective modern firms need to use small teams for an increasing variety of jobs. For instance, in an article subtitled \"The Team as Hero,\" Reich (1987) wrote, If we are to compete in today's world, we must begin to celebrate collective entrepreneurship, endeavors in which the whole of the effort is greater than the sum of individual contributions. We need to honor our teams more, our aggressive leaders and maverick geniuses less. (p. 78) Work teams occupy a pivotal role in what has been described as a management transformation (Walton, 1985), paradigm shift (Ketehum, 1984), and corporate renaissance (Kanter, 1983). In this management revolution, Peters (1988) advised that organizations use \"multi-function teams for all development activities\" (p. 210) and \"organize every function into tento thirty-person, largely self-managing teams\" (p. 296). Tornatzky (1986) pointed to new technologies that allow small work groups to take responsibility for whole products. Hackman (1986) predicted that, \"organizations in the future will rely heavily on member self-management\" (p. 90). Building blocks of such organizations are self-regulating work teams. But University of Tennessee University of Wisconsin--Eau Claire University o f Tennessee far from being revolutionary, work groups are traditional; \"the problem before us is not to invent more tools, but to use the ones we have\" (Kanter, 1983, p. 64). In this article, we explore applications of work teams and propose an analytic framework for team effectiveness. Work teams are defined as interdependent collections of individuals who share responsibility for specific outcomes for their organizations. In what follows, we first identify applications of work teams and then offer a framework for analyzing team effectiveness. Its facets make up topics of subsequent sections: organizational context, boundaries, and team development. We close with issues for research and practice. A p p l i c a t i o n s o f W o r k T e a m s Two watershed events called attention to the benefits of applying work teams beyond sports and mih'tary settings: the Hawthorne studies (Homans, 1950) and European experiments with autonomous work groups (Kelly, 1982). Enthusiasm has alternated with disenchantment (Bramel & Friend, 1987), but the 1980s have brought a resurgence of interest. Unfortunately, we have little evidence on how widely work teams are used or whether their use is expanding. Pasmore, Francis, Haldeman, and Shani (1982) reported that introduction of autonomous work groups was the most common intervention in 134 experiments in manufacturing firms. Production teams number among four broad categories of work team applications: (a) advice and involvement, (b) production and service, (c) projects and development, and (d) action and negotiation. Advice and Involvement Decision-making committees traditional in management now are expanding to first-line employees. Quality control (QC) circles and employee involvement groups have been common in the 1980s, often as vehicles for employee participation ( Cole, 1982 ). Perhaps several hundred thousand U.S. employees belong to QC circles (Ledford, Lawler, & Mohrman, 1988), usually first-line manufacturing employees who meet to identify opportunities for improvement. Some make and carry out proposals, but most have restricted scopes of activity and little working time, perhaps a few hours each month (Thompson, 1982). Employee involvement groups operate similarly, exploring ways to improve customer service (Peterfreund, 1982). 120 February 1990 • American Psychologist Copyright 1990 by the American Psyc2aological A~mciafion, Inc. 0003-066X/90/$00.75 Vol. 45, No. 2, 120-133 QC circles and employee involvement groups at times may have been implemented poorly (Shea, 1986), but they have been used extensively in some companies"}
{"metadata": {"dataset": "scidocs", "query_id": "04e4034344bda5c97015ea634e6eb1b65ef3a898", "doc_id": "d4065b447872f7f77676adf51adf8abe6c8b3e5d"}, "id": "scidocs-04e4034344bda5c97015ea634e6eb1b65ef3a898-d4065b447872f7f77676adf51adf8abe6c8b3e5d", "question": "Agile Team Perceptions of Productivity Factors", "context": "Scrum was designed to achieve a hyperproductive state where productivity increases 5-10 times over industry averages and many collocated teams have achieved this effect. The question for this paper is whether distributed, offshore teams can consistently achieve the hyperproductive state. In particular, can a team establish a localized velocity and then maintain or increase that velocity when distributing teams across continents. Since 2006, Xebia started projects with half Dutch and half Indian team members. After establishing localized hyperproductivity, they move the Indian members of the team to India and show increasing velocity with fully distributed teams. After running XP engineering practices inside many distributed Scrum projects, Xebia has systematically productized a model very similar to the SirsiDynix model (J. Sutherland, 2006) for high performance, distributed, offshore teams with outstanding quality."}
{"metadata": {"dataset": "scidocs", "query_id": "04e4034344bda5c97015ea634e6eb1b65ef3a898", "doc_id": "2218865804788c714410ce03ccad195393daf515"}, "id": "scidocs-04e4034344bda5c97015ea634e6eb1b65ef3a898-2218865804788c714410ce03ccad195393daf515", "question": "Agile Team Perceptions of Productivity Factors", "context": "Agile software development continually measures both our product and the process used to create it, to allow improvement. With increased popularity, more risk-averse groups are being drawn to agile, bringing with them modes of evaluation incompatible with agile values and principles. These outmoded metrics drive dysfunctional behaviors which threaten the integrity of an emerging agile culture. This paper collects some of the current thinking on appropriate agile metrics, and proposes simple tools for use by teams or organizations. The intention of these tools is to foster dialogue about the appropriateness of metrics in a given context, and thereby to encourage measurements more congruent with the objectives of agile teamwork"}
{"metadata": {"dataset": "scidocs", "query_id": "04e4034344bda5c97015ea634e6eb1b65ef3a898", "doc_id": "b9eaf65730ae6fbb81c8c86145a6c92f6d406efc"}, "id": "scidocs-04e4034344bda5c97015ea634e6eb1b65ef3a898-b9eaf65730ae6fbb81c8c86145a6c92f6d406efc", "question": "Agile Team Perceptions of Productivity Factors", "context": "T understand the impact of social capital on knowledge integration and performance within digitally enabled teams, we studied 46 teams who had a history and a future working together. All three dimensions of their social capital (structural, relational, and cognitive) were measured prior to the team performing two tasks in a controlled setting, one face-to-face and the other through a lean digital network. Structural and cognitive capital were more important to knowledge integration when teams communicated through lean digital networks than when they communicated face-to-face; relational capital directly impacted knowledge integration equally, regardless of the communication media used by the team. Knowledge integration, in turn, impacted team decision quality, suggesting that social capital influences team performance in part by increasing a team’s ability to integrate knowledge. These results suggest that team history may be necessary but not sufficient for teams to overcome the problems with the use of lean digital networks as a communication environment. However, team history may present a window of opportunity for social capital to develop, which in turn allows teams to perform just as well as in either communication environment."}
{"metadata": {"dataset": "scidocs", "query_id": "04e4034344bda5c97015ea634e6eb1b65ef3a898", "doc_id": "e930f44136bcc8cf32370c4caaf0734af0fa0d51"}, "id": "scidocs-04e4034344bda5c97015ea634e6eb1b65ef3a898-e930f44136bcc8cf32370c4caaf0734af0fa0d51", "question": "Agile Team Perceptions of Productivity Factors", "context": "Individual and work characteristics are used in telecommuting plans; however, their impact on telecommuting success is not well known. We studied how employee tenure, work experience, communication skills, task interdependence, work output measurability, and task variety impact telecommuter productivity, performance, and satisfaction after taking into account the impact of communication technologies. Data collected from 89 North American telecommuters suggest that in addition to the richness of the media, work experience, communication skills, and task interdependence impact telecommuting success. These characteristics are practically identifiable and measurable; therefore, we expect our findings to help managers convert increasing telecommuting adoption rates to well-defined and measurable gains."}
{"metadata": {"dataset": "scidocs", "query_id": "0553dbcc91c98d5e068f6532f0b071a7d219d67e", "doc_id": "no"}, "id": "scidocs-0553dbcc91c98d5e068f6532f0b071a7d219d67e-no", "question": "An empirical task analysis of warehouse order picking using head-mounted displays", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0553dbcc91c98d5e068f6532f0b071a7d219d67e", "doc_id": "f174f8bfedfcd64ab740d1ff6160c05f2588cada"}, "id": "scidocs-0553dbcc91c98d5e068f6532f0b071a7d219d67e-f174f8bfedfcd64ab740d1ff6160c05f2588cada", "question": "An empirical task analysis of warehouse order picking using head-mounted displays", "context": "Order picking is one of the most important process steps in logistics. Because of their flexibility human beings cannot be replaced by machines. But if workers in order picking systems are equipped with a head-mounted display, Augmented Reality can improve the information visualization.\n In this paper the development of such a system -- called Pick-by-Vision - is presented. The system is evaluated in a user study performed in a real storage environment. Important logistics figures as well as subjective figures were measured. The results show that a Pick-by-Vision system can improve considerably industrial order picking processes."}
{"metadata": {"dataset": "scidocs", "query_id": "0553dbcc91c98d5e068f6532f0b071a7d219d67e", "doc_id": "a5c51fdbb4dfd15a90c56521790eaec1f2a3b6dc"}, "id": "scidocs-0553dbcc91c98d5e068f6532f0b071a7d219d67e-a5c51fdbb4dfd15a90c56521790eaec1f2a3b6dc", "question": "An empirical task analysis of warehouse order picking using head-mounted displays", "context": "Order picking has long been identified as the most labour-intensive and costly activity for almost every warehouse; the cost of order picking is estimated to be as much as 55% of the total warehouse operating expense. Any underperformance in order picking can lead to unsatisfactory service and high operational cost for its warehouse, and consequently for the whole supply chain. In order to operate efficiently, the orderpicking process needs to be robustly designed and optimally controlled. This paper gives a literature overview on typical decision problems in design and control of manual order-picking processes. We focus on optimal (internal) layout design, storage assignment methods, routing methods, order batching and zoning. The research in this area has grown rapidly recently. Still, combinations of the above areas have hardly been explored. Order-picking system developments in practice lead to promising new research directions."}
{"metadata": {"dataset": "scidocs", "query_id": "0553dbcc91c98d5e068f6532f0b071a7d219d67e", "doc_id": "348e64727356683dd6582b746e81d66d1b6f8e42"}, "id": "scidocs-0553dbcc91c98d5e068f6532f0b071a7d219d67e-348e64727356683dd6582b746e81d66d1b6f8e42", "question": "An empirical task analysis of warehouse order picking using head-mounted displays", "context": "The order batching problem (OBP) is the problem of determining the number of orders to be picked together in one picking tour. Although various objectives may arise in practice, minimizing the average throughput time of a random order is a common concern. In this paper, we consider the OBP for a 2-block rectangular warehouse with the assumptions that orders arrive according to a Poisson process and the method used for routing the orderpickers is the well-known S-shape heuristic. We first elaborate on the first and second moment of the order-picker's travel time. Then we use these moments to estimate the average throughput time of a random order. This enables us to estimate the optimal picking batch size. Results from simulation show that the method provides a high accuracy level. Furthermore, the method is rather simple and can be easily applied in practice."}
{"metadata": {"dataset": "scidocs", "query_id": "0553dbcc91c98d5e068f6532f0b071a7d219d67e", "doc_id": "2c1bfdd311742ec077cdc6e5367e34cfe0acc4c0"}, "id": "scidocs-0553dbcc91c98d5e068f6532f0b071a7d219d67e-2c1bfdd311742ec077cdc6e5367e34cfe0acc4c0", "question": "An empirical task analysis of warehouse order picking using head-mounted displays", "context": "The paper focuses on the analysis and optimization of production warehouses, proposing a novel approach to reduce inefficiencies which employs three lean manufacturing tools in an integrated and iterative framework. The proposed approach integrates the Unified Modeling Language (UML) – providing a detailed description of the warehouse logistics – the Value Stream Mapping (VSM) tool – identifying non-value adding activities – and a mathematical formulation of the so-called Genba Shikumi philosophy – ranking such system anomalies and assessing how they affect the warehouse. The subsequent reapplication of the VSM produces a complete picture of the reengineered warehouse, and using the UML tool allows describing in detail the updated system. By applying the presented methodology to the warehouse of an Italian interior design producer, we show that it represents a useful tool to systematically and dynamically improve the warehouse management. Indeed, the application of the approach to the company leads to an innovative proposal for the warehouse analysis and optimization: a warehouse management system that leads to increased profitability and quality as well as to reduced errors. 2014 Elsevier B.V. All rights reserved."}
{"metadata": {"dataset": "scidocs", "query_id": "0553dbcc91c98d5e068f6532f0b071a7d219d67e", "doc_id": "dd2f411593178a9d9a59537cf2ddc3f91371f15a"}, "id": "scidocs-0553dbcc91c98d5e068f6532f0b071a7d219d67e-dd2f411593178a9d9a59537cf2ddc3f91371f15a", "question": "An empirical task analysis of warehouse order picking using head-mounted displays", "context": "Optical see-through head-mounted displays (OST-HMD) feature an unhindered and instantaneous view of the surgery site and can enable a mixed reality experience for surgeons during procedures. In this paper, we present a systematic approach to identify the criteria for evaluation of OST-HMD technologies for specific clinical scenarios, which benefit from using an object-anchored 2D-display visualizing medical information. Criteria for evaluating the performance of OST-HMDs for visualization of medical information and its usage are identified and proposed. These include text readability, contrast perception, task load, frame rate, and system lag. We choose to compare three commercially available OST-HMDs, which are representatives of currently available head-mounted display technologies. A multi-user study and an offline experiment are conducted to evaluate their performance. Statistical analysis demonstrates that Microsoft HoloLens performs best among the three tested OST-HMDs, in terms of contrast perception, task load, and frame rate, while ODG R-7 offers similar text readability. The integration of indoor localization and fiducial tracking on the HoloLens provides significantly less system lag in a relatively motionless scenario. With ever more OST-HMDs appearing on the market, the proposed criteria could be used in the evaluation of their suitability for mixed reality surgical intervention. Currently, Microsoft HoloLens may be more suitable than ODG R-7 and Epson Moverio BT-200 for clinical usability in terms of the evaluated criteria. To the best of our knowledge, this is the first paper that presents a methodology and conducts experiments to evaluate and compare OST-HMDs for their use as object-anchored 2D-display during interventions."}
{"metadata": {"dataset": "scidocs", "query_id": "0553dbcc91c98d5e068f6532f0b071a7d219d67e", "doc_id": "0d645b2254be58a788e811da7d12beed5d19ea3c"}, "id": "scidocs-0553dbcc91c98d5e068f6532f0b071a7d219d67e-0d645b2254be58a788e811da7d12beed5d19ea3c", "question": "An empirical task analysis of warehouse order picking using head-mounted displays", "context": "A new method for extracting planar polygonal rooftops in monocular aerial imagery is proposed. Structural features are extracted and hierarchically related using perceptual grouping techniques. Top-down feature veri cation is used so that features, and links between the features, are veri ed with local information in the image and weighed in a graph. Cycles in the graph correspond to possible building rooftop hypotheses. Virtual features are hypothesized for the perceptual completion of partially occluded rooftops. Extraction of the \\best\" grouping of features into a building rooftop hypothesis is posed as a graph search problem. The maximally weighted, independent set of cycles in the graph is extracted as the nal set of roof"}
{"metadata": {"dataset": "scidocs", "query_id": "0553dbcc91c98d5e068f6532f0b071a7d219d67e", "doc_id": "c2df80c76550306231978b0e8269ba4af46146a2"}, "id": "scidocs-0553dbcc91c98d5e068f6532f0b071a7d219d67e-c2df80c76550306231978b0e8269ba4af46146a2", "question": "An empirical task analysis of warehouse order picking using head-mounted displays", "context": "This thesis evaluates how a head mounted display (HMD) can be used to increase usability compared to existing computer programs that are used during maintenance work on vehicles. Problems identified during a case study in a vehicle workshop are first described. As an attempt to solve some of the identified problems a prototype application using a HMD was developed. The prototype application aids the user during troubleshooting of systems on the vehicle by leading the mechanic with textual information and augmented reality (AR). Assessment of the prototype application was done by comparing it to the existing computer program and measuring error rate and time to completion for a predefined task. Usability was also measured using the System Usability Scale. The assessment showed that HMDs can provide higher usability in terms of efficiency and satisfaction. Furthermore, the thesis describes and discusses other possibilities and limitations that usage of HMDs and AR can lead to that were identified both from theory and during implementation."}
{"metadata": {"dataset": "scidocs", "query_id": "0553dbcc91c98d5e068f6532f0b071a7d219d67e", "doc_id": "bb3a3836652c8a581e0ce92fdb3a7cd884efec40"}, "id": "scidocs-0553dbcc91c98d5e068f6532f0b071a7d219d67e-bb3a3836652c8a581e0ce92fdb3a7cd884efec40", "question": "An empirical task analysis of warehouse order picking using head-mounted displays", "context": "Recent research indicates that users consistently underestimate depth judgments to Augmented Reality (AR) graphics when viewed through optical see-through displays. However, to our knowledge, little work has examined how AR graphics may affect depth judgments of real world objects that have been overlaid or annotated with AR graphics. This study begins a preliminary analysis whether AR graphics have directional effects on users' depth perception of real-world objects, as might be experienced in vehicle driving scenarios (e.g., as viewed via an optical see-through head-up display or HUD). Twenty-four participants were asked to judge the depth of a physical pedestrian proxy figure moving towards them at a constant rate of 1 meter/second. Participants were shown an initial target location that varied in distance from 11 to 20 m and were then asked to press a button to indicate when the moving target was perceived to be at the previously specified target location. Each participant experienced three different display conditions: no AR visual display (control), a conformal AR graphic overlaid on the pedestrian via a HUD, and the same graphic presented on a tablet physically located on the pedestrian. Participants completed 10 trials (one for each target distance between 11 and 20 inclusive) per display condition for a total of 30 trials per participant. The judged distance from the correct location was recorded, and after each trial, participants' confidence in determining the correct distance was captured. Across all conditions, participants underestimated the distance of the physical object consistent with existing literature. Greater variability was observed in the accuracy of distance judgments under the AR HUD condition relative to the other two display conditions. In addition, participant confidence levels were considerably lower in the AR HUD condition."}
{"metadata": {"dataset": "scidocs", "query_id": "0553dbcc91c98d5e068f6532f0b071a7d219d67e", "doc_id": "a16e2a64fea3ec99dd34bdc955e299921a4e6b83"}, "id": "scidocs-0553dbcc91c98d5e068f6532f0b071a7d219d67e-a16e2a64fea3ec99dd34bdc955e299921a4e6b83", "question": "An empirical task analysis of warehouse order picking using head-mounted displays", "context": "In the supply chain, a warehouse is an essential component for linking the chain partners. It is necessary to allocate warehouse resources efficiently and effectively to enhance the productivity and reduce the operation costs of the warehouse. Therefore, warehouse management systems (WMSs) have been developed for handling warehouse resources and monitoring warehouse operations. However, it is difficult to update daily operations of inventory level, locations of forklifts and stock keeping units (SKUs) in realtime by using the bar-code-based or manual-based warehouse management systems. In this paper, RFID technology is adopted to facilitate the collection and sharing of data in a warehouse. Tests are performed for evaluating the reading performance of both the active and passive RFID apparatus. With the help of the testing results, the efficient radio frequency cover ranges of the readers are examined for formulating a radio frequency identification case-based logistics resource management system (R-LRMS). The capabilities of R-LRMS are demonstrated in GSL Limited. Three objectives are achieved: (i) a simplification of RFID adoption procedure, (ii) an improvement in the visibility of warehouse operations and (iii) an enhancement of the productivity of the warehouse. The successful case example proved the feasibility of R-LRMS in real working practice. 2008 Elsevier Ltd. All rights reserved."}
{"metadata": {"dataset": "scidocs", "query_id": "0553dbcc91c98d5e068f6532f0b071a7d219d67e", "doc_id": "7a6a54e8cf488ccb30b12e4bb54e40fe6176fbb0"}, "id": "scidocs-0553dbcc91c98d5e068f6532f0b071a7d219d67e-7a6a54e8cf488ccb30b12e4bb54e40fe6176fbb0", "question": "An empirical task analysis of warehouse order picking using head-mounted displays", "context": "The paper presents a clustering based approach for consumers’ classification in representative categories characterized by typical load profiles. For determination of the consumption categories, every customer must be characterized by the following primary information: daily (monthly) energy consumption, average load and peak load. The used database contains the daily load curves corresponding to the small customers from a rural distribution system from Romania. The obtained results it demonstrated that the proposed approach can be used with the success in building specific tariff structures of the customers or in the optimal operation and planning of distribution systems."}
{"metadata": {"dataset": "scidocs", "query_id": "0588053b2cde6414e542c656023ade147397f597", "doc_id": "no"}, "id": "scidocs-0588053b2cde6414e542c656023ade147397f597-no", "question": "Improve Chinese Word Embeddings by Exploiting Internal Structure", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0588053b2cde6414e542c656023ade147397f597", "doc_id": "a5134affedeeac45a1123167e0ecbf52aa96cc1e"}, "id": "scidocs-0588053b2cde6414e542c656023ade147397f597-a5134affedeeac45a1123167e0ecbf52aa96cc1e", "question": "Improve Chinese Word Embeddings by Exploiting Internal Structure", "context": "This paper considers the problem of learning Chinese word embeddings. In contrast to English, a Chinese word is usually composed of characters, and most of the characters themselves can be further divided into components such as radicals. While characters and radicals contain rich information and are capable of indicating semantic meanings of words, they have not been fully exploited by existing word embedding methods. In this work, we propose multi-granularity embedding (MGE) for Chinese words. The key idea is to make full use of such word-character-radical composition, and enrich word embeddings by further incorporating finer-grained semantics from characters and radicals. Quantitative evaluation demonstrates the superiority of MGE in word similarity computation and analogical reasoning. Qualitative analysis further shows its capability to identify finer-grained semantic meanings of words."}
{"metadata": {"dataset": "scidocs", "query_id": "0588053b2cde6414e542c656023ade147397f597", "doc_id": "12a97799334e3a455e278f2a995a93a6e0c034bf"}, "id": "scidocs-0588053b2cde6414e542c656023ade147397f597-12a97799334e3a455e278f2a995a93a6e0c034bf", "question": "Improve Chinese Word Embeddings by Exploiting Internal Structure", "context": "This paper proposes an embedding matching approach to Chinese word segmentation, which generalizes the traditional sequence labeling framework and takes advantage of distributed representations. The training and prediction algorithms have linear-time complexity. Based on the proposed model, a greedy segmenter is developed and evaluated on benchmark corpora. Experiments show that our greedy segmenter achieves improved results over previous neural network-based word segmenters, and its performance is competitive with state-of-the-art methods, despite its simple feature set and the absence of external resources for training."}
{"metadata": {"dataset": "scidocs", "query_id": "0588053b2cde6414e542c656023ade147397f597", "doc_id": "56de5dbb4e2e81f5d09622bf278b22a6ef6df168"}, "id": "scidocs-0588053b2cde6414e542c656023ade147397f597-56de5dbb4e2e81f5d09622bf278b22a6ef6df168", "question": "Improve Chinese Word Embeddings by Exploiting Internal Structure", "context": "Cross-lingual word embeddings are representations for vocabularies of two or more languages in one common continuous vector space and are widely used in various natural language processing tasks. A state-of-the-art way to generate cross-lingual word embeddings is to learn a linear mapping, with an assumption that the vector representations of similar words in different languages are related by a linear relationship. However, this assumption does not always hold true, especially for substantially different languages. We therefore propose to use kernel canonical correlation analysis to capture a non-linear relationship between word embeddings of two languages. By extensively evaluating the learned word embeddings on three tasks (word similarity, cross-lingual dictionary induction, and cross-lingual document classification) across five language pairs, we demonstrate that our proposed approach achieves essentially better performances than previous linear methods on all of the three tasks, especially for language pairs with substantial typological difference."}
{"metadata": {"dataset": "scidocs", "query_id": "0588053b2cde6414e542c656023ade147397f597", "doc_id": "0d3950fa9967d74825d7685be052ed55b231347c"}, "id": "scidocs-0588053b2cde6414e542c656023ade147397f597-0d3950fa9967d74825d7685be052ed55b231347c", "question": "Improve Chinese Word Embeddings by Exploiting Internal Structure", "context": "We present neural network-based joint models for Chinese word segmentation, POS tagging and dependency parsing. Our models are the first neural approaches for fully joint Chinese analysis that is known to prevent the error propagation problem of pipeline models. Although word embeddings play a key role in dependency parsing, they cannot be applied directly to the joint task in the previous work. To address this problem, we propose embeddings of character strings, in addition to words. Experiments show that our models outperform existing systems in Chinese word segmentation and POS tagging, and perform preferable accuracies in dependency parsing. We also explore bi-LSTM models with fewer features."}
{"metadata": {"dataset": "scidocs", "query_id": "0588053b2cde6414e542c656023ade147397f597", "doc_id": "0718237a30408609554a0e2b90d35e37d54b1959"}, "id": "scidocs-0588053b2cde6414e542c656023ade147397f597-0718237a30408609554a0e2b90d35e37d54b1959", "question": "Improve Chinese Word Embeddings by Exploiting Internal Structure", "context": "Recent literature has shown a wide variety of benefits to mapping traditional onehot representations of words and phrases to lower-dimensional real-valued vectors known as word embeddings. Traditionally, most word embedding algorithms treat each word as the finest meaningful semantic granularity and perform embedding by learning distinct embedding vectors for each word. Contrary to this line of thought, technical domains such as scientific and medical literature compose words from subword structures such as prefixes, suffixes, and root-words as well as compound words. Treating individual words as the finest-granularity unit discards meaningful shared semantic structure between words sharing substructures. This not only leads to poor embeddings for text corpora that have longtail distributions, but also heuristic methods for handling out-of-vocabulary words. In this paper we propose SubwordMine, an entropybased subword mining algorithm that is fast, unsupervised, and fully data-driven. We show that this allows for great cross-domain performance in identifying semantically meaningful subwords. We then investigate utilizing the mined subwords within the FastText embedding model and compare performance of the learned representations in a downstream language modeling task."}
{"metadata": {"dataset": "scidocs", "query_id": "0588053b2cde6414e542c656023ade147397f597", "doc_id": "13317a497f4dc5f62a15dbdc135dd3ea293474df"}, "id": "scidocs-0588053b2cde6414e542c656023ade147397f597-13317a497f4dc5f62a15dbdc135dd3ea293474df", "question": "Improve Chinese Word Embeddings by Exploiting Internal Structure", "context": "Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations. Yet while ‘multi-sense’ methods have been proposed and tested on artificial wordsimilarity tasks, we don’t know if they improve real natural language understanding tasks. In this paper we introduce a multisense embedding model based on Chinese Restaurant Processes that achieves state of the art performance on matching human word similarity judgments, and propose a pipelined architecture for incorporating multi-sense embeddings into language un-"}
{"metadata": {"dataset": "scidocs", "query_id": "0588053b2cde6414e542c656023ade147397f597", "doc_id": "405872b6c6c1a53c2ede41ccb7c9de6d4207d9be"}, "id": "scidocs-0588053b2cde6414e542c656023ade147397f597-405872b6c6c1a53c2ede41ccb7c9de6d4207d9be", "question": "Improve Chinese Word Embeddings by Exploiting Internal Structure", "context": "Current character-based approaches are not robust for cross domain Chin ese word segmentation. In this paper, we alleviate this problem by deriving a novel enhanced ch aracterbased generative model with a new abstract aggregate candidate-feature, which indicates if th given candidate prefers the corresponding position-tag of the longest dictionary matching wo rd. Since the distribution of the proposed feature is invariant across domains, our m del thus possesses better generalization ability. Open tests on CIPS-SIGHAN-2010 show that the enhance d gen rative model achieves robust cross-domain performance for various OOV cov erage rates and obtains the best performance on three out of four domains. The enhanced gen erative model is then further integrated with a discriminative model which also utilizes dictionary information . This integrated model is shown to be either superior or comparable to all other models repo rted in the literatur e on every domain of this task."}
{"metadata": {"dataset": "scidocs", "query_id": "0588053b2cde6414e542c656023ade147397f597", "doc_id": "db8aa4263d89b459ea1380bf8ec8f272880fe2f4"}, "id": "scidocs-0588053b2cde6414e542c656023ade147397f597-db8aa4263d89b459ea1380bf8ec8f272880fe2f4", "question": "Improve Chinese Word Embeddings by Exploiting Internal Structure", "context": "We approach the task of named entity recognition (NER) for Chinese by representing each entity with a composition of its traits: the token-entity, its characters, and its characters’ main radicals. Character and radical-level information for each entity are included to provide additional relationships that might not be strictly captured within a token-entity’s word embedding during training. We learn using neural networks that are some combination of the following traits: unidirectional or bidirectional; single or multi-layer; simple, gated recurrent unit (GRU), or long short term memory (LSTM) celled. We achieve a maximum not-O token-level F1 score of 76 and entity-level F1 of 70."}
{"metadata": {"dataset": "scidocs", "query_id": "0588053b2cde6414e542c656023ade147397f597", "doc_id": "41e5f9984ec71218416c54304bcb9a99c27b4938"}, "id": "scidocs-0588053b2cde6414e542c656023ade147397f597-41e5f9984ec71218416c54304bcb9a99c27b4938", "question": "Improve Chinese Word Embeddings by Exploiting Internal Structure", "context": "In this paper, we build a Chinese text corrector which can correct spelling mistakes precisely in Chinese texts. Our motivation is inspired by the recently proposed seq2seq model which consider the text corrector as a sequence learning problem. To begin with, we propose a biased-decoding method to improve the bilingual evaluation understudy (BLEU) score of our model. Secondly, we adopt a more reasonable OOV token scheme, which enhances the robustness of our correction mechanism. Moreover, to test the performance of our proposed model thoroughly, we establish a corpus which includes 600,000 sentences from news data of Sogou Labs. Experiments show that our corrector model can achieve better corrector results based on the corpus."}
{"metadata": {"dataset": "scidocs", "query_id": "0588053b2cde6414e542c656023ade147397f597", "doc_id": "0db01127c3a919df8891d85c270109894c67e3f8"}, "id": "scidocs-0588053b2cde6414e542c656023ade147397f597-0db01127c3a919df8891d85c270109894c67e3f8", "question": "Improve Chinese Word Embeddings by Exploiting Internal Structure", "context": "We present an unsupervised, language agnostic approach for exploiting morphological regularities present in high dimensional vector spaces. We propose a novel method for generating embeddings of words from their morphological variants using morphological transformation operators. We evaluate this approach on MSR word analogy test set (Mikolov et al., 2013d) with an accuracy of 85% which is 12% higher than the previous best known system."}
{"metadata": {"dataset": "scidocs", "query_id": "05b32985bf72fe15383bb4bba14beb43e438e937", "doc_id": "no"}, "id": "scidocs-05b32985bf72fe15383bb4bba14beb43e438e937-no", "question": "An LTCC-based 35-GHz substrate-integrated-waveguide bandpass filter", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "05b32985bf72fe15383bb4bba14beb43e438e937", "doc_id": "2cb6d78e822ca7fd0e29670ec7e26e37ae3d3e8f"}, "id": "scidocs-05b32985bf72fe15383bb4bba14beb43e438e937-2cb6d78e822ca7fd0e29670ec7e26e37ae3d3e8f", "question": "An LTCC-based 35-GHz substrate-integrated-waveguide bandpass filter", "context": "This paper presents a novel compact low-temperature cofired ceramic (LTCC) bandpass filter (BPF) with wide stopband and high selectivity. The proposed circuit consists of two coupled λ<sub>g</sub>/4 transmission-line resonators. A special coupling region is selected to realize a novel discriminating coupling scheme for generating a transmission zero (TZ) at the third harmonic frequency. The mechanism is analyzed and the design guideline is described. The source-load coupling is introduced to generate two TZs near the passband and one in the stopband. Thus, wide stopband can be obtained without extra circuits. Due to the LTCC multilayer structures, the filter size is 0.058 λ<sub>g</sub>×0.058 λ<sub>g</sub>×0.011 λ<sub>g</sub>, or 2.63 mm × 2.61 mm × 0.5 mm. The simulated and measured results of the demonstrated LTCC BPF are presented to validate the proposed design."}
{"metadata": {"dataset": "scidocs", "query_id": "05b32985bf72fe15383bb4bba14beb43e438e937", "doc_id": "54797270f673a2efc9c4b86b5de3befa59b13bd2"}, "id": "scidocs-05b32985bf72fe15383bb4bba14beb43e438e937-54797270f673a2efc9c4b86b5de3befa59b13bd2", "question": "An LTCC-based 35-GHz substrate-integrated-waveguide bandpass filter", "context": "Substrate integrated waveguide (SIW) technology provides an attractive solution to the integration of planar and nonplanar circuits by using a planar circuit fabrication process. However, it is usually difficult to implement the negative coupling structure required for the design of compact canonical folded elliptic or quasi-elliptic cross-coupled bandpass filter on the basis of a single-layer SIW. In this paper, a special planar negative coupling scheme including a magnetic coupling post-wall iris and a balanced microstrip line with a pair of metallic via-holes is studied in detail. Two -band fourth-degree cross-coupled bandpass filters without and with source-load coupling using the negative coupling structures are then proposed and designed. The two novel SIW filters having the same center frequency of 20.5 GHz and respective passband width of 700 and 800 MHz are implemented on a single-layer Rogers RT/Duroid 5880 substrate with thickness of 0.508 mm. Measured results of those filters, which exhibit a high selectivity, and a minimum in-band insertion loss of approximately 0.9 and 1.0 dB, respectively, agree well with simulated results."}
{"metadata": {"dataset": "scidocs", "query_id": "05b32985bf72fe15383bb4bba14beb43e438e937", "doc_id": "fe6ade6920b212bd32af011bc6efa94b66107e1f"}, "id": "scidocs-05b32985bf72fe15383bb4bba14beb43e438e937-fe6ade6920b212bd32af011bc6efa94b66107e1f", "question": "An LTCC-based 35-GHz substrate-integrated-waveguide bandpass filter", "context": "A laminated waveguide magic-T with imbedded Chebyshev filter response is developed in a multilayer low-temperature co-fired ceramic technology by vertically stacked rectangular cavity resonators with highly symmetric coupling structures. The cavities provide even- and odd-symmetric field distributions by simultaneously resonating TE102 and TE201 modes at the center frequency of the magic-T. Thus, the in-phase and out-of-phase responses of the magic-T function are accomplished according to the induced mode. Meanwhile, the filter frequency response is realized by the cascaded cavities with proper coupling strengths according to the filter specification. With the degenerate, but orthogonal cavity modes, a fewer number of cavities are required and the circuit size is further reduced. A third-order bandpass magic-T is designed and fabricated to operate at 24 GHz with 6% fractional bandwidth. The highly symmetric structure of the implemented magic-T provides a low in-band magnitude imbalance ( ±0.25 dB) and phase imbalance (0°-6°). The sum and difference ports also provide an isolation greater than 30 dB in the operation frequency range. Measurement and simulated results are in good agreement, both validating the design concept."}
{"metadata": {"dataset": "scidocs", "query_id": "05b32985bf72fe15383bb4bba14beb43e438e937", "doc_id": "f0eace9bfe72c2449f76461ad97c4042d2a7141b"}, "id": "scidocs-05b32985bf72fe15383bb4bba14beb43e438e937-f0eace9bfe72c2449f76461ad97c4042d2a7141b", "question": "An LTCC-based 35-GHz substrate-integrated-waveguide bandpass filter", "context": "In this letter, a novel antenna-in-package (AiP) technology at W-band has been proposed. This technology is presented for solving the special case that the metallic package should be used to accommodate high mechanical strength. By taking advantages of the multilayer low temperature co-fired ceramic (LTCC) technology, the radiation efficiency of the antenna can be maintained. Meanwhile, high mechanical strength and shielding performance are achieved. A prototype of AiP has been designed. The prototype constitutes integrated LTCC antenna, low-loss feeder, and metallic package with a tapered horn aperture. This LTCC feeder is realized by laminated waveguide (LWG). An LWG cavity that is buried in LTCC is employed to broaden the antenna impedance bandwidth. Electromagnetic (EM) simulations and measurements of antenna performances agree well over the whole frequency range of interest. The proposed prototype achieves a -10-dB impedance bandwidth of 10 GHz from 88 to 98 GHz and a peak gain of 12.3 dBi at 89 GHz."}
{"metadata": {"dataset": "scidocs", "query_id": "05b32985bf72fe15383bb4bba14beb43e438e937", "doc_id": "4e74cadb44acfe373940f0b151c41ef3a02b9b0c"}, "id": "scidocs-05b32985bf72fe15383bb4bba14beb43e438e937-4e74cadb44acfe373940f0b151c41ef3a02b9b0c", "question": "An LTCC-based 35-GHz substrate-integrated-waveguide bandpass filter", "context": "This paper proposes a quasi-elliptic filter with slot coupling and nonadjacent cross coupling based on the substrate integrated waveguide (SIW) cavity. The slots etched on the top metal plane of SIW cavity are used to produce electrical coupling, and the cross coupling is realized by the microstrip transmission line above the SIW cavity. The coupling strength is mainly controlled by the width and height of the slot. The length of the open-ended microstrip line controls the sign of the cross coupling. The cross coupling with different signs are used in the filter to produce a pair of transmission zeros (TZs) at both sides of the passband. In order to prove the validity, a fourth-order SIW quasi-elliptic filter with TZsat both sides of the passband is fabricated in a two-layer printed circuit board. The measured insertion loss at a center frequency of 3.7 GHz is 1.1 dB. The return loss within the passband is below -18 dB with a fractional bandwidth of 16%. The measured results are in good agreement with the simulated results."}
{"metadata": {"dataset": "scidocs", "query_id": "05b32985bf72fe15383bb4bba14beb43e438e937", "doc_id": "bd5c67bf72da28a5f4bc06e58555683505d10ef1"}, "id": "scidocs-05b32985bf72fe15383bb4bba14beb43e438e937-bd5c67bf72da28a5f4bc06e58555683505d10ef1", "question": "An LTCC-based 35-GHz substrate-integrated-waveguide bandpass filter", "context": "An eleven-stage broadband SIW filter has been designed, fabricated and measured. The measured results show that the SIW filter can be operated from 9.25 GHz to 11.25 GHz with good selectivity performance. The filter has a small size, low weight, and can be integrated with other SIW components without any transitions. It's suitable for the design of microwave and millimeter wave integrated circuits"}
{"metadata": {"dataset": "scidocs", "query_id": "05b32985bf72fe15383bb4bba14beb43e438e937", "doc_id": "64af3319e9ed4e459e80b469923106887d28313d"}, "id": "scidocs-05b32985bf72fe15383bb4bba14beb43e438e937-64af3319e9ed4e459e80b469923106887d28313d", "question": "An LTCC-based 35-GHz substrate-integrated-waveguide bandpass filter", "context": "A microstrip bandpass filter using stepped-impedance resonators is designed in low-temperature co-fired ceramic technology for dual-band applications at 2.4 and 5.2 GHz. New coupling schemes are proposed to replace the normal counterparts. It is found that the new coupling scheme for the interstages can enhance the layout compactness of the bandpass filter; while the new coupling scheme at the input and output can improve the performance of the bandpass filter. To validate the design and analysis, a prototype of the bandpass filter was fabricated and measured. It is shown that the measured and simulated performances are in good agreement. The prototype of the bandpass filter achieved insertion loss of 1.25 and 1.87 dB, S11 of -29 and -40 dB, and bandwidth of 21% and 12.7% at 2.4 and 5.2 GHz, respectively. The bandpass filter is further studied for a single-package solution of dual-band radio transceivers. The bandpass filter is, therefore, integrated into a ceramic ball grid array package. The integration is analyzed with an emphasis on the connection of the bandpass filter to the antenna and to the transceiver die"}
{"metadata": {"dataset": "scidocs", "query_id": "05b32985bf72fe15383bb4bba14beb43e438e937", "doc_id": "6d778ec897507edb08df3b58ee33a5517bb895f3"}, "id": "scidocs-05b32985bf72fe15383bb4bba14beb43e438e937-6d778ec897507edb08df3b58ee33a5517bb895f3", "question": "An LTCC-based 35-GHz substrate-integrated-waveguide bandpass filter", "context": "In this paper microstrip antenna for 60 GHz on LTCC is presented. Special techniques are used to satisfy the antenna specification in wide bandwidth (7 GHz, from 57 GHz to 64 GHz) and high gain (15 dBi). An increase in bandwidth is achieved by using aperture the coupling and the multilayer structure. Gain is increased by using parasitic patches which increase the total antenna aperture. In paper was presented 60 GHz parasitic microstrip antenna with two layers of parasitic patches. High gain is obtained as the result of the optimization. However, presented return loss results can not be treated as wide bandwidth."}
{"metadata": {"dataset": "scidocs", "query_id": "05b32985bf72fe15383bb4bba14beb43e438e937", "doc_id": "e3ff5de6f800fde3368eb78c61463c6cb3302f59"}, "id": "scidocs-05b32985bf72fe15383bb4bba14beb43e438e937-e3ff5de6f800fde3368eb78c61463c6cb3302f59", "question": "An LTCC-based 35-GHz substrate-integrated-waveguide bandpass filter", "context": "A compact slow-wave millimeter-wave bandpass filter for wireless communication systems is developed and proposed in this paper. The filter is based on an open-loop resonator that is composed of a microstrip line with both ends loaded with folded open stubs. The folded arms of the open stubs are utilized to increase the loading capacitance to ground. In addition, the folded arms of the open stubs are also used to introduce cross couplings. The cross couplings can create transmission zeros at the edges of the desired passband. As a result, the filter can exhibit a passband with sharp rejection skirts. The filter is designed to have a fractional bandwidth of about 3.3% at a center frequency of 30.0 GHz. The filter design is realized using Rogers RT6002 substrate with a dielectric constant of 2.94 and a thickness of 0.127 mm. The filter design is successfully fabricated using the printed circuit board (PCB) technology. The fabricated filter is measured and an excellent agreement between the simulated and measured results is achieved. The fabricated filter has the advantages of compact size and excellent performance."}
{"metadata": {"dataset": "scidocs", "query_id": "05b32985bf72fe15383bb4bba14beb43e438e937", "doc_id": "aaaea1314570b6b692ff3cce3715ec9dada7c7aa"}, "id": "scidocs-05b32985bf72fe15383bb4bba14beb43e438e937-aaaea1314570b6b692ff3cce3715ec9dada7c7aa", "question": "An LTCC-based 35-GHz substrate-integrated-waveguide bandpass filter", "context": "A low-cost, fully-integrated antenna-in-package solution for 60 GHz phased-array systems is demonstrated. Sixteen patch antennas are integrated into a 28 mm × 28 mm ball grid array together with a flip-chip attached transmitter or receiver IC. The packages have been implemented using low temperature co-fired ceramic technology. 60 GHz interconnects, including flip-chip transitions and via structures, are optimized using full-wave simulation. Anechoic chamber measurement has shown ~ 5 dBi unit antenna gain across all four IEEE 802.15.3c channels, achieving excellent model-to-hardware correlation. The packaged transmitter and receiver ICs, mounted on evaluation boards, have demonstrated beam-steered, non-line-of-sight links with data rates up to 5.3 Gb/s."}
{"metadata": {"dataset": "scidocs", "query_id": "05dba74b1ecf7e40b2a904e2d797768ef79832d3", "doc_id": "no"}, "id": "scidocs-05dba74b1ecf7e40b2a904e2d797768ef79832d3-no", "question": "Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "05dba74b1ecf7e40b2a904e2d797768ef79832d3", "doc_id": "223dcd0e44532fc02444709e61327432c74fe46d"}, "id": "scidocs-05dba74b1ecf7e40b2a904e2d797768ef79832d3-223dcd0e44532fc02444709e61327432c74fe46d", "question": "Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases", "context": "This paper is based on the work carried out in the framework of the Verbmobil project, which is a limited-domain speech translation task (German-English). In the final evaluation, the statistical approach was found to perform best among five competing approaches. In this paper, we will further investigate the used statistical translation models. A shortcoming of the single-word based model is that it does not take contextual information into account for the translation decisions. We will present a translation model that is based on bilingual phrases to explicitly model the local context. We will show that this model performs better than the single-word based model. We will compare monotone and non-monotone search for this model and we will investigate the benefit of using the sum criterion instead of the maximum approximation."}
{"metadata": {"dataset": "scidocs", "query_id": "05dba74b1ecf7e40b2a904e2d797768ef79832d3", "doc_id": "7262bc3674c4c063526eaf4d2dcf54eecea7bf77"}, "id": "scidocs-05dba74b1ecf7e40b2a904e2d797768ef79832d3-7262bc3674c4c063526eaf4d2dcf54eecea7bf77", "question": "Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases", "context": "We describe PARANMT-50M, a dataset of more than 50 million English-English sentential paraphrase pairs. We generated the pairs automatically by using neural machine translation to translate the nonEnglish side of a large parallel corpus, following Wieting et al. (2017). Our hope is that PARANMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks. To show its utility, we use PARANMT-50M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation.1"}
{"metadata": {"dataset": "scidocs", "query_id": "05dba74b1ecf7e40b2a904e2d797768ef79832d3", "doc_id": "a0498059093527323c29e32547c51c29e8849006"}, "id": "scidocs-05dba74b1ecf7e40b2a904e2d797768ef79832d3-a0498059093527323c29e32547c51c29e8849006", "question": "Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases", "context": "tra Statistical machine translation systems are usually trained on large amounts of bilingual text and monolingual text. In this paper, we propose a method to perform domain adaptation for statistical machine translation, where in-domain bilingual corpora do not exist. This method first uses out-of-domain corpora to train a baseline system and then uses in-domain translation dictionaries and in-domain monolingual corpora to improve the indomain performance. We propose an algorithm to combine these different resources in a unified framework. Experimental results indicate that our method achieves absolute improvements of 8.16 and 3.36 BLEU scores on Chinese to English translation and English to French translation respectively, as compared with the baselines using only out-ofdomain corpora."}
{"metadata": {"dataset": "scidocs", "query_id": "05dba74b1ecf7e40b2a904e2d797768ef79832d3", "doc_id": "555e1d6ecc7af031f29b0225bdca06d4a6da77ed"}, "id": "scidocs-05dba74b1ecf7e40b2a904e2d797768ef79832d3-555e1d6ecc7af031f29b0225bdca06d4a6da77ed", "question": "Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases", "context": "We describe a method for incorporating syntactic information in statistical machine translation systems. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system. The goal of this step is to recover an underlying word order that is closer to the target language word-order than the original string. The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system. We describe experiments on translation from German to English, showing an improvement from 25.2% Bleu score for a baseline system to 26.8% Bleu score for the system with reordering, a statistically significant improvement."}
{"metadata": {"dataset": "scidocs", "query_id": "05dba74b1ecf7e40b2a904e2d797768ef79832d3", "doc_id": "c9ab6adcf149c740b03ef3759260719af4f7ce07"}, "id": "scidocs-05dba74b1ecf7e40b2a904e2d797768ef79832d3-c9ab6adcf149c740b03ef3759260719af4f7ce07", "question": "Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases", "context": "This paper describes the University of Edinburgh’s (UEDIN) phrase-based submissions to the translation and medical translation shared tasks of the 2014 Workshop on Statistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized representations, specifically automatic word clusters for translations out of English, ii) using unsupervised character-based models to translate unknown words in RussianEnglish and Hindi-English pairs, iii) synthesizing Hindi data from closely-related Urdu data, and iv) building huge language on the common crawl corpus."}
{"metadata": {"dataset": "scidocs", "query_id": "05dba74b1ecf7e40b2a904e2d797768ef79832d3", "doc_id": "0157dcd6122c20b5afc359a799b2043453471f7f"}, "id": "scidocs-05dba74b1ecf7e40b2a904e2d797768ef79832d3-0157dcd6122c20b5afc359a799b2043453471f7f", "question": "Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases", "context": "Dictionaries and phrase tables are the basis of modern statistical machine translation systems. This paper develops a method that can automate the process of generating and extending dictionaries and phrase tables. Our method can translate missing word and phrase entries by learning language structures based on large monolingual data and mapping between languages from small bilingual data. It uses distributed representation of words and learns a linear mapping between vector spaces of languages. Despite its simplicity, our method is surprisingly effective: we can achieve almost 90% precision@5 for translation of words between English and Spanish. This method makes little assumption about the languages, so it can be used to extend and refine dictionaries and translation tables for any language pairs."}
{"metadata": {"dataset": "scidocs", "query_id": "05dba74b1ecf7e40b2a904e2d797768ef79832d3", "doc_id": "4323ea81a04766acc38287254e0c3ff30985dbe5"}, "id": "scidocs-05dba74b1ecf7e40b2a904e2d797768ef79832d3-4323ea81a04766acc38287254e0c3ff30985dbe5", "question": "Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases", "context": "Since sentences in patent texts are long, they are difficult to translate by a machine. Although statistical machine translation is one of the major streams of the field, long patent sentences are difficult to translate not using syntactic analysis. We propose the combination of a rule based method and a statistical method. It is a rule based machine translation (RMT) with a statistical based post editor (SPE). The evaluation by the NIST score shows RMT+SPE is more accurate than RMT only. Manual checks, however, show the outputs of RMT+SPE often have strange expressions in the target language. So we propose a new evaluation measure NMG (normalized mean grams). Although NMG is based on n-gram, it counts the number of words in the longest word sequence matches between the test sentence and the target language reference corpus. We use two reference corpora. One is the reference translation only the other is a large scaled target language corpus. In the former case, RMT+SPE wins in the later case, RMT wins."}
{"metadata": {"dataset": "scidocs", "query_id": "05dba74b1ecf7e40b2a904e2d797768ef79832d3", "doc_id": "0f16ab376632ee83f2a3af21e96ebb925a8ac8b8"}, "id": "scidocs-05dba74b1ecf7e40b2a904e2d797768ef79832d3-0f16ab376632ee83f2a3af21e96ebb925a8ac8b8", "question": "Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases", "context": "Statistical machine translation (SMT) performance suffers when models are trained on only small amounts of parallel data. The learned models typically have both low accuracy (incorrect translations and feature scores) and low coverage (high out-of-vocabulary rates). In this work, we use an additional data resource, comparable corpora, to improve both. Beginning with a small bitext and corresponding phrase-based SMT model, we improve coverage by using bilingual lexicon induction techniques to learn new translations from comparable corpora. Then, we supplement the model’s feature space with translation scores estimated over comparable corpora in order to improve accuracy. We observe improvements between 0.5 and 1.7 BLEU translating Tamil, Telugu, Bengali, Malayalam, Hindi, and Urdu into English."}
{"metadata": {"dataset": "scidocs", "query_id": "05dba74b1ecf7e40b2a904e2d797768ef79832d3", "doc_id": "7768f88efe03b9735f79462c0f89aa04a074f107"}, "id": "scidocs-05dba74b1ecf7e40b2a904e2d797768ef79832d3-7768f88efe03b9735f79462c0f89aa04a074f107", "question": "Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases", "context": "Neural machine translation (NMT) conducts end-to-end translation with a source language encoder and a target language decoder, making promising translation performance. However, as a newly emerged approach, the method has some limitations. An NMT system usually has to apply a vocabulary of certain size to avoid the time-consuming training and decoding, thus it causes a serious out-of-vocabulary problem. Furthermore, the decoder lacks a mechanism to guarantee all the source words to be translated and usually favors short translations, resulting in fluent but inadequate translations. In order to solve the above problems, we incorporate statistical machine translation (SMT) features, such as a translation model and an n-gram language model, with the NMT model under the log-linear framework. Our experiments show that the proposed method significantly improves the translation quality of the state-of-the-art NMT system on Chinese-toEnglish translation tasks. Our method produces a gain of up to 2.33 BLEU score on NIST open test sets."}
{"metadata": {"dataset": "scidocs", "query_id": "05dba74b1ecf7e40b2a904e2d797768ef79832d3", "doc_id": "40ccca414291f5585fefb22cb9d3e841ef63fc27"}, "id": "scidocs-05dba74b1ecf7e40b2a904e2d797768ef79832d3-40ccca414291f5585fefb22cb9d3e841ef63fc27", "question": "Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases", "context": "A phrase-based statistical machine translation approach the alignment template approach is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source-channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German-English speech Verbmobil task, we analyze the effect of various system components. On the French-English Canadian Hansards task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese-English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems."}
{"metadata": {"dataset": "scidocs", "query_id": "063c6ae786c34d3722c6d9060df6339e246bbc3b", "doc_id": "no"}, "id": "scidocs-063c6ae786c34d3722c6d9060df6339e246bbc3b-no", "question": "Texture Synthesis Using Convolutional Neural Networks", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "063c6ae786c34d3722c6d9060df6339e246bbc3b", "doc_id": "09be020a9738464799740602d7cf3273c1416c6a"}, "id": "scidocs-063c6ae786c34d3722c6d9060df6339e246bbc3b-09be020a9738464799740602d7cf3273c1416c6a", "question": "Texture Synthesis Using Convolutional Neural Networks", "context": "Procedural texture generation enables the creation of more rich and detailed virtual environments without the help of an artist. However, finding a flexible generative model of real world textures remains an open problem. We present a novel Convolutional Neural Network based texture model consisting of two summary statistics (the Gramian and Translation Gramian matrices), as well as spectral constraints. We investigate the Fourier Transform or Window Fourier Transform in applying spectral constraints, and find that the Window Fourier Transform improved the quality of the generated textures. We demonstrate the efficacy of our system by comparing generated output with that of related state of the art systems."}
{"metadata": {"dataset": "scidocs", "query_id": "063c6ae786c34d3722c6d9060df6339e246bbc3b", "doc_id": "356827905c70ef763e3aa373f966fe6d8cf753f9"}, "id": "scidocs-063c6ae786c34d3722c6d9060df6339e246bbc3b-356827905c70ef763e3aa373f966fe6d8cf753f9", "question": "Texture Synthesis Using Convolutional Neural Networks", "context": "Texture synthesis is important for many applications in computer graphics, vision, and image processing. However, it remains difficult to design an algorithm that is both efficient and capable of generating high quality results. In this paper, we present an efficient algorithm for realistic texture synthesis. The algorithm is easy to use and requires only a sample texture as input. It generates textures with perceived quality equal to or better than those produced by previous techniques, but runs two orders of magnitude faster. This permits us to apply texture synthesis to problems where it has traditionally been considered impractical. In particular, we have applied it to constrained synthesis for image editing and temporal texture generation. Our algorithm is derived from Markov Random Field texture models and generates textures through a deterministic searching process. We accelerate this synthesis process using tree-structured vector quantization."}
{"metadata": {"dataset": "scidocs", "query_id": "063c6ae786c34d3722c6d9060df6339e246bbc3b", "doc_id": "3e4bd583795875c6550026fc02fb111daee763b4"}, "id": "scidocs-063c6ae786c34d3722c6d9060df6339e246bbc3b-3e4bd583795875c6550026fc02fb111daee763b4", "question": "Texture Synthesis Using Convolutional Neural Networks", "context": "In this paper, we use deep neural networks for inverting face sketches to synthesize photorealistic face images. We first construct a semi-simulated dataset containing a very large number of computergenerated face sketches with different styles and corresponding face images by expanding existing unconstrained face data sets. We then train models achieving state-of-the-art results on both computer-generated sketches and hand-drawn sketches by leveraging recent advances in deep learning such as batch normalization, deep residual learning, perceptual losses and stochastic optimization in combination with our new dataset. We finally demonstrate potential applications of our models in fine arts and forensic arts. In contrast to existing patch-based approaches, our deep-neuralnetwork-based approach can be used for synthesizing photorealistic face images by inverting face sketches in the wild."}
{"metadata": {"dataset": "scidocs", "query_id": "063c6ae786c34d3722c6d9060df6339e246bbc3b", "doc_id": "17e9d3ba861db8a6d323e1410fe5ca0986d5ad6a"}, "id": "scidocs-063c6ae786c34d3722c6d9060df6339e246bbc3b-17e9d3ba861db8a6d323e1410fe5ca0986d5ad6a", "question": "Texture Synthesis Using Convolutional Neural Networks", "context": "A non-parametric method for texture synthesis is proposed. The texture synthesis process grows a new image outward from an initial seed, one pixel at a time. A Markov random field model is assumed, and the conditional distribution of a pixel given all its neighbors synthesized so far is estimated by querying the sample image and finding all similar neighborhoods. The degree of randomness is controlled by a single perceptually intuitive parameter. The method aims at preserving as much local structure as possible and produces good results for a wide variety of synthetic and real-world textures."}
{"metadata": {"dataset": "scidocs", "query_id": "063c6ae786c34d3722c6d9060df6339e246bbc3b", "doc_id": "b3dbbae9257f7a1b0b5c2138aee7701899c219a9"}, "id": "scidocs-063c6ae786c34d3722c6d9060df6339e246bbc3b-b3dbbae9257f7a1b0b5c2138aee7701899c219a9", "question": "Texture Synthesis Using Convolutional Neural Networks", "context": "Detection of tumor nuclei in cancer histology images requires sophisticated techniques due to the irregular shape, size and chromatin texture of the tumor nuclei. Some very recently proposed methods employ deep convolutional neural networks (CNNs) to detect cells in H&E stained images. However, all such methods use some form of raw pixel intensities as input and rely on the CNN to learn the deep features. In this work, we extend a recently proposed spatially constrained CNN (SC-CNN) by proposing features that capture texture characteristics and show that although CNN produces good results on automatically learned features, it can perform better if the input consists of a combination of handcrafted features and the raw data. The handcrafted features are computed through the scattering transform which gives non-linear invariant texture features. The combination of handcrafted features with raw data produces sharp proximity maps and better detection results than the results of raw intensities with a similar kind of CNN architecture."}
{"metadata": {"dataset": "scidocs", "query_id": "063c6ae786c34d3722c6d9060df6339e246bbc3b", "doc_id": "94f039e2e825d0ad343fdcf30ac974be318fa9c7"}, "id": "scidocs-063c6ae786c34d3722c6d9060df6339e246bbc3b-94f039e2e825d0ad343fdcf30ac974be318fa9c7", "question": "Texture Synthesis Using Convolutional Neural Networks", "context": "We propose multi-view and volumetric convolutional neural networks (ConvNets) for 3D shape recognition, which combines surface normal and height fields to capture local geometry and physical size of an object. This strategy helps distinguishing between objects with similar geometries but different sizes. This is especially useful for enhancing volumetric ConvNets and classifying 3D scans with insufficient surface details. Experimental results on CAD and real-world scan datasets showed that our technique outperforms previous approaches."}
{"metadata": {"dataset": "scidocs", "query_id": "063c6ae786c34d3722c6d9060df6339e246bbc3b", "doc_id": "7571e852a75a1a5117f5f36b581c2f46a8e068cf"}, "id": "scidocs-063c6ae786c34d3722c6d9060df6339e246bbc3b-7571e852a75a1a5117f5f36b581c2f46a8e068cf", "question": "Texture Synthesis Using Convolutional Neural Networks", "context": "We present a new, fast and flexible pipeline for indoor scene synthesis that is based on deep convolutional generative models. Our method operates on a top-down image-based representation, and inserts objects iteratively into the scene by predicting their category, location, orientation and size with separate neural network modules. Our pipeline naturally supports automatic completion of partial scenes, as well as synthesis of complete scenes. Our method is significantly faster than the previous image-based method and generates result that outperforms state-of-the-art generative scene models in terms of faithfulness to training data and perceived visual quality."}
{"metadata": {"dataset": "scidocs", "query_id": "063c6ae786c34d3722c6d9060df6339e246bbc3b", "doc_id": "dbde4f47efed72cbb99f412a9a4c17fe39fa04fc"}, "id": "scidocs-063c6ae786c34d3722c6d9060df6339e246bbc3b-dbde4f47efed72cbb99f412a9a4c17fe39fa04fc", "question": "Texture Synthesis Using Convolutional Neural Networks", "context": "Natural image generation is currently one of the most actively explored fields in Deep Learning. Many approaches, e.g. for state-of-the-art artistic style transfer or natural texture synthesis, rely on the statistics of hierarchical representations in supervisedly trained deep neural networks. It is, however, unclear what aspects of this feature representation are crucial for natural image generation: is it the depth, the pooling or the training of the features on natural images? We here address this question for the task of natural texture synthesis and show that none of the above aspects are indispensable. Instead, we demonstrate that natural textures of high perceptual quality can be generated from networks with only a single layer, no pooling and random filters."}
{"metadata": {"dataset": "scidocs", "query_id": "063c6ae786c34d3722c6d9060df6339e246bbc3b", "doc_id": "94e21eb765285ec9ed431006613d320586477e2c"}, "id": "scidocs-063c6ae786c34d3722c6d9060df6339e246bbc3b-94e21eb765285ec9ed431006613d320586477e2c", "question": "Texture Synthesis Using Convolutional Neural Networks", "context": "Shape deformation requires expert user manipulation even when the object under consideration is in a high fidelity format such as a 3D mesh. It becomes even more complicated if the data is represented as a point set or a depth scan with significant self occlusions. We introduce an end-to-end solution to this tedious process using a volumetric Convolutional Neural Network (CNN) that learns deformation flows in 3D. Our network architectures take the voxelized representation of the shape and a semantic deformation intention (e.g., make more sporty) as input and generate a deformation flow at the output. We show that such deformation flows can be trivially applied to the input shape, resulting in a novel deformed version of the input without losing detail information. Our experiments show that the CNN approach achieves comparable results with state of the art methods when applied to CAD models. When applied to single frame depth scans, and partial/noisy CAD models we achieve ∼60% less error compared to the state-of-the-art."}
{"metadata": {"dataset": "scidocs", "query_id": "063c6ae786c34d3722c6d9060df6339e246bbc3b", "doc_id": "493765198c1d2c4fc3307e20d3e0df6313d0b88c"}, "id": "scidocs-063c6ae786c34d3722c6d9060df6339e246bbc3b-493765198c1d2c4fc3307e20d3e0df6313d0b88c", "question": "Texture Synthesis Using Convolutional Neural Networks", "context": "There has been a growing interest in using different approaches to improve the coding efficiency of modern video codec in recent years as demand for web-based video consumption increases. In this paper, we propose a model-based approach that uses texture analysis/synthesis to reconstruct blocks in texture regions of a video to achieve potential coding gains using the AV1 codec developed by the Alliance for Open Media (AOM). The proposed method uses convolutional neural networks to extract texture regions in a frame, which are then reconstructed using a global motion model. Our preliminary results show an increase in coding efficiency while maintaining satisfactory visual quality. Introduction With the increasing amount of videos being created and consumed, better video compression tools are needed to provide fast transmission and high visual quality. Modern video coding standards utilize spatial and temporal redundancy in the videos to achieve high coding efficiency and high visual quality with motion compensation techniques and 2-D orthogonal transforms. However, efficient exploitation of statistical dependencies measured by a mean squared error (MSE) does not always produce the best psychovisual result, and may require higher data rate to preserve detail information in the video. Recent advancement in GPU computing has enabled the analysis of large scale data using deep learning method. Deep learning techniques have shown promising performance in many applications such as object detection, natural language process, and synthetic images generation [1, 2, 3, 4]. Several methods have been developed for video applications to improve coding efficiency using deep learning. In [5], sample adaptive offset (SAO) is replaced by a CNN-based in-loop filter (IFCNN) to improve the coding efficiency in HEVC. By learning the predicted residue between the quantized reconstructed frames obtained after deblocking filter (DF) and the original frames, IFCNN is able to reconstruct video frames with higher quality without requiring any bit transmission during coding process. Similar to [5], [6] proposes a Variable-filter-size Residue-learning CNN (VRCNN) to improving coding efficiency by replacing DF and SAO in HEVC. VRCNN is based on the concept of ARCNN [7] which is originally designed for JPEG applications. Instead of only using spatial information to train a CNN to reduce the coding artifacts in HEVC, [8] proposed a spatial temporal residue network (STResNet) as an additional in-loop filter after SAO. A rate-distortion optimization strategy is used to control the on/off switch of the proposed in-loop filter. There are also some works that have been done in the decoder of HEVC to improve the coding efficiency. In [9], a deep CNN-based auto decoder (DCAD) is implemented in the decoder of HEVC to improve the video quality of decoded video. DCAD is trained to learn the predict residue between decoded video frames and original video frames. By adding the predicted residual generated from DCAD to the compressed video frames, this method enhances the compressed video frames to higher quality. In summary, the above methods improve the coding efficiency by enhancing the quality of reconstructed video frames. However, they require different trained models for video reconstruction at different quantization levels. We are interested in developing deep learning approaches to only encode visually relevant information and use a different coding method for “perceptually insignificant” regions in a frame, which can lead to substantial data rate reductions while maintaining visual quality. In particular, we have developed a model based approach that can be used to improve the coding efficiency by identifying texture areas in a video frame that contain detail irrelevant information, which the viewer does not perceive specific details and can be skipped or encoded at a much lower data rate. The task is then to divide a frame into “perceptually insignificant” texture region and then use a texture model for the pixels in that region. In 1959, Schreiber and colleagues proposed a coding method that divides an image into textures and edges and used it in image coding [10]. This work was later extended by using the human visual system and statistical model to determine the texture region [11, 12, 13]. More recently, several groups have focused on adapting perceptual based approaches to the video coding framework [14]. In our previous work [15], we introduced a texture analyzer before encoding the input sequences to identify detail irrelevant regions in the frame which are classified into different texture classes. At the encoder, no inter-frame prediction is performed for these regions. Instead, displacement of the entire texture region is modeled by just one set of motion parameters. Therefore, only the model parameters are transmitted to the decoder for reconstructing the texture regions using a texture synthesizer. Non-texture regions in the frame are coded conventionally. Since this method uses feature extraction based on texture segmentation technique, a proper set of parameters are required to achieve accurate texture segmentation for different videos. Deep learning methods usually do not require such parameter tuning for inference. As a result, deep learning techniques can be developed to perform texture segmentation and classification for the proposed model-based video coding. A Fisher vector convolution neural networks (FVCNN) that can produce segmentation labels for different texture classes was proposed in [16]. One of the advantage of FV-CNN is that the image input size is flexible and is not limited by the network architecture. Instead of doing pixel-wise classification on texture regions, a texture classification CNN network was described in [17]. To reduce computational expenses, [17] uses a small classification network to classify image patches with size of 227 × 227. A smaller network is needed to classify smaller image patches in our case. In this paper, we propose a block-based texture segmentation method to extract texture region in a video frame using convolutional neural networks. The block-based segmentation network classifies each 16 × 16 block in a frame as texture or non-texture. The identified texture region is then synthesized using the temporal correlations among the frames. Our method was implemented using the AOM/AV1 codec. Preliminary results show significant bitrate savings while maintaining a satisfactory visual quality."}
{"metadata": {"dataset": "scidocs", "query_id": "065985d4d0854c51f52ad7a7507b267d9b88ab1c", "doc_id": "no"}, "id": "scidocs-065985d4d0854c51f52ad7a7507b267d9b88ab1c-no", "question": "Multi-class active learning for image classification", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "065985d4d0854c51f52ad7a7507b267d9b88ab1c", "doc_id": "51e95da85a91844ee939147c6f647f749437f42c"}, "id": "scidocs-065985d4d0854c51f52ad7a7507b267d9b88ab1c-51e95da85a91844ee939147c6f647f749437f42c", "question": "Multi-class active learning for image classification", "context": "Image classification is an important task in computer vision. However, how to assign suitable labels to images is a subjective matter, especially when some images can be categorized into multiple classes simultaneously. Multilabel image classification focuses on the problem that each image can have one or multiple labels. It is known that manually labelling images is time-consuming and expensive. In order to reduce the human effort of labelling images, especially multilabel images, we proposed a multilabel SVM active learning method. We also proposed two selection strategies: Max Loss strategy and Mean Max Loss strategy. Experimental results on both artificial data and real-world images demonstrated the advantage of proposed method."}
{"metadata": {"dataset": "scidocs", "query_id": "065985d4d0854c51f52ad7a7507b267d9b88ab1c", "doc_id": "f09ba8ac4fe0ff51757660208ed55a89fd9282b3"}, "id": "scidocs-065985d4d0854c51f52ad7a7507b267d9b88ab1c-f09ba8ac4fe0ff51757660208ed55a89fd9282b3", "question": "Multi-class active learning for image classification", "context": "Training robust deep learning (DL) systems for medical image classification or segmentation is challenging due to limited images covering different disease types and severity. We propose an active learning (AL) framework to select most informative samples and add to the training data. We use conditional generative adversarial networks (cGANs) to generate realistic chest xray images with different disease characteristics by conditioning its generation on a real image sample. Informative samples to add to the training set are identified using a Bayesian neural network. Experiments show our proposed AL framework is able to achieve state of the art performance by using about 35% of the full dataset, thus saving significant time and effort over conventional methods."}
{"metadata": {"dataset": "scidocs", "query_id": "065985d4d0854c51f52ad7a7507b267d9b88ab1c", "doc_id": "f24df001ba37746d1d28919f38a7704db5d178c3"}, "id": "scidocs-065985d4d0854c51f52ad7a7507b267d9b88ab1c-f24df001ba37746d1d28919f38a7704db5d178c3", "question": "Multi-class active learning for image classification", "context": "The iterative supervised learning setting, in which learning algorithms can actively query an oracle for labels, e.g. a human annotator that understands the nature of the problem, is called active learning. As the learner is allowed to interactively choose the data from which it learns, it is expected that the learner would perform better with less training. The active learning approach is appropriate to machine learning applications where training labels are costly to obtain but unlabeled data is abundant. Although active learning has been widely considered for single-label learning, this is not the case for multi-label learning, in which objects can have more than one class label and a multi-label learner is trained to assign multiple labels simultaneously to an object. There are different scenarios to query the annotator. This work focuses on the scenario in which the evaluation of unlabeled data is taken into account to select the object to be labeled. In this scenario, several multilabel active learning algorithms were identified in the literature. These algorithms were implemented in a common framework and experimentally evaluated in two multi-label datasets which have different properties. The influence of the properties of the datasets in the results obtained by the multi-label active learning algoritm is highlighted."}
{"metadata": {"dataset": "scidocs", "query_id": "065985d4d0854c51f52ad7a7507b267d9b88ab1c", "doc_id": "f4702d55b49ba075edc34309423880d091eac2b3"}, "id": "scidocs-065985d4d0854c51f52ad7a7507b267d9b88ab1c-f4702d55b49ba075edc34309423880d091eac2b3", "question": "Multi-class active learning for image classification", "context": "Screening mammography is an important front-line tool for the early detection of breast cancer, and some 39 million exams are conducted each year in the United States alone. Here, we describe a multi-scale convolutional neural network (CNN) trained with a curriculum learning strategy that achieves high levels of accuracy in classifying mammograms. Specifically, we first train CNN-based patch classifiers on segmentation masks of lesions in mammograms, and then use the learned features to initialize a scanning-based model that renders a decision on the whole image, trained end-to-end on outcome data. We demonstrate that our approach effectively handles the “needle in a haystack” nature of full-image mammogram classification, achieving 0.92 AUROC on the DDSM dataset."}
{"metadata": {"dataset": "scidocs", "query_id": "065985d4d0854c51f52ad7a7507b267d9b88ab1c", "doc_id": "49d4cb2e1788552a04c7f8fec33fbfabb3882995"}, "id": "scidocs-065985d4d0854c51f52ad7a7507b267d9b88ab1c-49d4cb2e1788552a04c7f8fec33fbfabb3882995", "question": "Multi-class active learning for image classification", "context": "This paper investigates recent research on active learning for (geo) text and image classification, with an emphasis on methods that combine visual analytics and/or deep learning. Deep learning has attracted substantial attention across many domains of science and practice, because it can find intricate patterns in big data; but successful application of the methods requires a big set of labeled data. Active learning, which has the potential to address the data labeling challenge, has already had success in geospatial applications such as trajectory classification from movement data and (geo) text and image classification. This review is intended to be particularly relevant for extension of these methods to GISience, to support work in domains such as geographic information retrieval from text and image repositories, interpretation of spatial language, and related geo-semantics challenges. Specifically, to provide a structure for leveraging recent advances, we group the relevant work into five categories: active learning, visual analytics, active learning with visual analytics, active deep learning, plus GIScience and Remote Sensing (RS) using active learning and active deep learning. Each category is exemplified by recent influential work. Based on this framing and our systematic review of key research, we then discuss some of the main challenges of integrating active learning with visual analytics and deep learning, and point out research opportunities from technical and application perspectives—for application-based opportunities, with emphasis on those that address big data with geospatial components."}
{"metadata": {"dataset": "scidocs", "query_id": "065985d4d0854c51f52ad7a7507b267d9b88ab1c", "doc_id": "1e58d7e5277288176456c66f6b1433c41ca77415"}, "id": "scidocs-065985d4d0854c51f52ad7a7507b267d9b88ab1c-1e58d7e5277288176456c66f6b1433c41ca77415", "question": "Multi-class active learning for image classification", "context": "We propose an iterative crowd-enabled active learning algorithm for building high-precision visual classifiers from unlabeled images. Our method employs domain experts to identify a small number of examples of a specific visual event. These expert-labeled examples seed a classifier, which is then iteratively trained by active querying of a non-expert crowd. These non-experts actively refine the classifiers at every iteration by answering simple binary questions about the classifiers’ detections. The advantage of this approach is that experts efficiently shepherd an unsophisticated crowd into training a classifier capable of fine-grained distinctions. This obviates the need to label an entire dataset to obtain high-precision classifiers. We find these classifiers are advantageous for creating a large vocabulary of visual attributes for specialized taxonomies. We demonstrate our crowd active learning pipeline by creating classifiers for attributes related to North American birds and fashion."}
{"metadata": {"dataset": "scidocs", "query_id": "065985d4d0854c51f52ad7a7507b267d9b88ab1c", "doc_id": "ce4126d64dc547f8101af4f1dead36f8df474fc8"}, "id": "scidocs-065985d4d0854c51f52ad7a7507b267d9b88ab1c-ce4126d64dc547f8101af4f1dead36f8df474fc8", "question": "Multi-class active learning for image classification", "context": "Outsourcing an image classification task raises privacy concerns, both from the image provider’s perspective, who wishes to keep their images confidential, and from the classification algorithm provider’s perspective, who wishes to protect the intellectual property of their classifier. We propose EPIC, an efficient private image classification system based on support vector machine (SVM) learning, secure against malicious adversaries. EPIC builds upon transfer learning techniques known from the Machine Learning (ML) literature and minimizes the load on the privacy-preserving part. Our solution is based on Multiparty Computation (MPC), it is 34 times faster than Gazelle (USENIX’18) –the state-of-the-art in private image classification– and it improves the communication cost by 50 times, with a 7% higher accuracy on CIFAR-10 dataset. For the same accuracy as Gazelle achieves on CIFAR-10, EPIC is 700 times faster and the communication cost is reduced by 500 times."}
{"metadata": {"dataset": "scidocs", "query_id": "065985d4d0854c51f52ad7a7507b267d9b88ab1c", "doc_id": "6ee2bf4f9647e110fa7788c932a9592cf8eaeee0"}, "id": "scidocs-065985d4d0854c51f52ad7a7507b267d9b88ab1c-6ee2bf4f9647e110fa7788c932a9592cf8eaeee0", "question": "Multi-class active learning for image classification", "context": "In this paper, we propose a group-sensitive multiple kernel learning (GS-MKL) method to accommodate the intra-class diversity and the inter-class correlation for object categorization. By introducing an intermediate representation “group” between images and object categories, GS-MKL attempts to find appropriate kernel combination for each group to get a finer depiction of object categories. For each category, images within a group share a set of kernel weights while images from different groups may employ distinct sets of kernel weights. In GS-MKL, such group-sensitive kernel combinations together with the multi-kernels based classifier are optimized in a joint manner to seek a trade-off between capturing the diversity and keeping the invariance for each category. Extensive experiments show that our proposed GS-MKL method has achieved encouraging performance over three challenging datasets."}
{"metadata": {"dataset": "scidocs", "query_id": "065985d4d0854c51f52ad7a7507b267d9b88ab1c", "doc_id": "330008af4074ef0e2b21787677783827d6a15056"}, "id": "scidocs-065985d4d0854c51f52ad7a7507b267d9b88ab1c-330008af4074ef0e2b21787677783827d6a15056", "question": "Multi-class active learning for image classification", "context": "In open set recognition, a classifier must label instances of known classes while detecting instances of unknown classes not encountered during training. To detect unknown classes while still generalizing to new instances of existing classes, we introduce a dataset augmentation technique that we call counterfactual image generation. Our approach, based on generative adversarial networks, generates examples that are close to training set examples yet do not belong to any training category. By augmenting training with examples generated by this optimization, we can reformulate open set recognition as classification with one additional class, which includes the set of novel and unknown examples. Our approach outperforms existing open set recognition algorithms on a selection of image classification tasks."}
{"metadata": {"dataset": "scidocs", "query_id": "065985d4d0854c51f52ad7a7507b267d9b88ab1c", "doc_id": "40f94c7e8dbce2c0fd0edc29d4ba94f52266acdd"}, "id": "scidocs-065985d4d0854c51f52ad7a7507b267d9b88ab1c-40f94c7e8dbce2c0fd0edc29d4ba94f52266acdd", "question": "Multi-class active learning for image classification", "context": "The advance of deep learning has made huge changes in computer vision and produced various off-the-shelf trained models. Particularly, Convolutional Neural Network (CNN) has been widely used to build image classification model which allow researchers transfer the pre-trained learning model for other classifications. We propose a transfer learning method to detect breast cancer using histopathology images based on Google's Inception v3 model which were initially trained for the classification of non-medical images. The pilot study shows the feasibility of transfer learning in the detection of breast cancer with AUC of 0.93."}
{"metadata": {"dataset": "scidocs", "query_id": "0674058618d04def58c79a0b28174301ef591433", "doc_id": "no"}, "id": "scidocs-0674058618d04def58c79a0b28174301ef591433-no", "question": "RainForest—A Framework for Fast Decision Tree Construction of Large Datasets", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0674058618d04def58c79a0b28174301ef591433", "doc_id": "dd41010faa2c848729bc79614f1846a2267f1904"}, "id": "scidocs-0674058618d04def58c79a0b28174301ef591433-dd41010faa2c848729bc79614f1846a2267f1904", "question": "RainForest—A Framework for Fast Decision Tree Construction of Large Datasets", "context": "A Random Forest consists of several independent decision trees arranged in a forest. A majority vote over all trees leads to the final decision. In this paper we propose a Random Forest framework which incorporates a cascade structure consisting of several stages together with a bootstrap approach. By introducing the cascade, 99% of the test images can be rejected by the first and second stage with minimal computational effort leading to a massively speeded-up detection framework. Three different cascade voting strategies are implemented and evaluated. Additionally, the training and classification speed-up is analyzed. Several experiments on public available datasets for pedestrian detection, lateral car detection and unconstrained face detection demonstrate the benefit of our contribution."}
{"metadata": {"dataset": "scidocs", "query_id": "0674058618d04def58c79a0b28174301ef591433", "doc_id": "5e9fafb0e70130417e04cf942a0a6f4b43ad583e"}, "id": "scidocs-0674058618d04def58c79a0b28174301ef591433-5e9fafb0e70130417e04cf942a0a6f4b43ad583e", "question": "RainForest—A Framework for Fast Decision Tree Construction of Large Datasets", "context": "Decision trees and ensembles of decision trees are very popular in machine learning and often achieve state-of-the-art performance on black-box prediction tasks. However, popular variants such as C4.5, CART, boosted trees and random forests lack a probabilistic interpretation since they usually just specify an algorithm for training a model. We take a probabilistic approach where we cast the decision tree structures and the parameters associated with the nodes of a decision tree as a probabilistic model; given labeled examples, we can train the probabilistic model using a variety of approaches (Bayesian learning, maximum likelihood, etc). The probabilistic approach allows us to encode prior assumptions about tree structures and share statistical strength between node parameters; furthermore, it offers a principled mechanism to obtain probabilistic predictions which is crucial for applications where uncertainty quantification is important. Existing work on Bayesian decision trees relies on Markov chain Monte Carlo which can be computationally slow and suffer from poor mixing. We propose a novel sequential Monte Carlo algorithm that computes a particle approximation to the posterior over trees in a top-down fashion. We also propose a novel sampler for Bayesian additive regression trees by combining the above top-down particle filtering algorithm with the Particle Gibbs (Andrieu et al., 2010) framework. Finally, we propose Mondrian forests (MFs), a computationally efficient hybrid solution that is competitive with non-probabilistic counterparts in terms of speed and accuracy, but additionally produces well-calibrated uncertainty estimates. MFs use the Mondrian process (Roy and Teh, 2009) as the randomization mechanism and hierarchically smooth the node parameters within each tree (using a hierarchical probabilistic model and approximate Bayesian updates), but combine the trees in a non-Bayesian fashion. MFs can be grown in an incremental/online fashion and remarkably, the distribution of online MFs is the same as that of batch MFs."}
{"metadata": {"dataset": "scidocs", "query_id": "0674058618d04def58c79a0b28174301ef591433", "doc_id": "e0e731805c073c4589375c8b8f65769834201114"}, "id": "scidocs-0674058618d04def58c79a0b28174301ef591433-e0e731805c073c4589375c8b8f65769834201114", "question": "RainForest—A Framework for Fast Decision Tree Construction of Large Datasets", "context": "Classification for very large datasets has many practical applications in data mining. Techniques such as discretization and dataset sampling can be used to scale up decision tree classifiers to large datasets. Unfortunately, both of these techniques can cause a significant loss in accuracy. We present a novel decision tree classifier called CLOUDS, which samples the splitting points for numeric attributes followed by an estimation step to narrow the search space of the best split. CLOUDS reduces computation and I/O complexity substantially compared to state of the art classifters, while maintaining the quality of the generated trees in terms of accuracy and tree size. We provide experimental results with a number of real and synthetic data~ets."}
{"metadata": {"dataset": "scidocs", "query_id": "0674058618d04def58c79a0b28174301ef591433", "doc_id": "b75f57b742cbfc12fe15790ce27e75ed5f4a9349"}, "id": "scidocs-0674058618d04def58c79a0b28174301ef591433-b75f57b742cbfc12fe15790ce27e75ed5f4a9349", "question": "RainForest—A Framework for Fast Decision Tree Construction of Large Datasets", "context": "Nowadays with a growing number of online controlling systems in the organization and also a high demand of monitoring and stats facilities that uses data streams to log and control their subsystems, data stream mining becomes more and more vital. Hoeffding Trees (also called Very Fast Decision Trees a.k.a. VFDT) as a Big Data approach in dealing with the data stream for classification and regression problems showed good performance in handling facing challenges and making the possibility of any-time prediction. Although these methods outperform other methods e.g. Artificial Neural Networks (ANN) and Support Vector Regression (SVR), they suffer from high latency in adapting with new concepts when the statistical distribution of incoming data changes. In this article, we introduced a new algorithm that can detect and handle concept drift phenomenon properly. This algorithms also benefits from fast startup ability which helps systems to be able to predict faster than other algorithms at the beginning of data stream arrival. We also have shown that our approach will overperform other controversial approaches for classification and regression tasks."}
{"metadata": {"dataset": "scidocs", "query_id": "0674058618d04def58c79a0b28174301ef591433", "doc_id": "04dc5e68bc6f81e7d71d9bd3add36e6f6f077539"}, "id": "scidocs-0674058618d04def58c79a0b28174301ef591433-04dc5e68bc6f81e7d71d9bd3add36e6f6f077539", "question": "RainForest—A Framework for Fast Decision Tree Construction of Large Datasets", "context": "This paper investigates the connections between two state of the art classifiers: decision forests (DFs, including decision jungles) and convolutional neural networks (CNNs). Decision forests are computationally efficient thanks to their conditional computation property (computation is confined to only a small region of the tree, the nodes along a single branch). CNNs achieve state of the art accuracy, thanks to their representation learning capabilities. We present a systematic analysis of how to fuse conditional computation with representation learning and achieve a continuum of hybrid models with different ratios of accuracy vs. efficiency. We call this new family of hybrid models conditional networks. Conditional networks can be thought of as: i) decision trees augmented with data transformation operators, or ii) CNNs, with block-diagonal sparse weight matrices, and explicit data routing functions. Experimental validation is performed on the common task of image classification on both the CIFAR and Imagenet datasets. Compared to state of the art CNNs, our hybrid models yield the same accuracy with a fraction of the compute cost and much smaller number of parameters."}
{"metadata": {"dataset": "scidocs", "query_id": "0674058618d04def58c79a0b28174301ef591433", "doc_id": "5092375789732afbfbfe2f5ede0792af6c562813"}, "id": "scidocs-0674058618d04def58c79a0b28174301ef591433-5092375789732afbfbfe2f5ede0792af6c562813", "question": "RainForest—A Framework for Fast Decision Tree Construction of Large Datasets", "context": "Boosted decision trees are among the most popular learning techniques in use today. While exhibiting fast speeds at test time, relatively slow training renders them impractical for applications with real-time learning requirements. We propose a principled approach to overcome this drawback. We prove a bound on the error of a decision stump given its preliminary error on a subset of the training data; the bound may be used to prune unpromising features early in the training process. We propose a fast training algorithm that exploits this bound, yielding speedups of an order of magnitude at no cost in the final performance of the classifier. Our method is not a new variant of Boosting; rather, it is used in conjunction with existing Boosting algorithms and other sampling methods to achieve even greater speedups."}
{"metadata": {"dataset": "scidocs", "query_id": "0674058618d04def58c79a0b28174301ef591433", "doc_id": "38b0877fa6ac3ebfbb29d74f761fea394ee190f3"}, "id": "scidocs-0674058618d04def58c79a0b28174301ef591433-38b0877fa6ac3ebfbb29d74f761fea394ee190f3", "question": "RainForest—A Framework for Fast Decision Tree Construction of Large Datasets", "context": "Lazy learning algorithms, exemplified by nearestneighbor algorithms, do not induce a concise hypothesis from a given training set; the inductive process is delayed until a test instance is given. Algorithms for constructing decision trees, such as C4.5, ID3, and CART create a single “best” decision tree during the training phase, and this tree is then used to classify test instances. The tests at the nodes of the constructed tree are good on average, but there may be better tests for classifying a specific instance. We propose a lazy decision tree algorithm-LAzuDT-that conceptually constructs the “best” decision tree for each test instance. In practice, only a path needs to be constructed, and a caching scheme makes the algorithm fast. The algorithm is robust with respect to missing values without resorting to the complicated methods usually seen in induction of decision trees. Experiments on real and artificial problems are presented."}
{"metadata": {"dataset": "scidocs", "query_id": "0674058618d04def58c79a0b28174301ef591433", "doc_id": "7a63dd6b58db9e6557ed7a2c47637d95d6dd7863"}, "id": "scidocs-0674058618d04def58c79a0b28174301ef591433-7a63dd6b58db9e6557ed7a2c47637d95d6dd7863", "question": "RainForest—A Framework for Fast Decision Tree Construction of Large Datasets", "context": "Decision tree induction is one of the most employed methods to extract knowledge from data, since the representation of knowledge is very intuitive and easily understandable by humans. The most successful strategy for inducing decision trees, the greedy top-down approach, has been continuously improved by researchers over the years. This work, following recent breakthroughs in the automatic design of machine learning algorithms, proposes a hyper-heuristic evolutionary algorithm for automatically generating decision-tree induction algorithms, named HEAD-DT. We perform extensive experiments in 20 public data sets to assess the performance of HEAD-DT, and we compare it to traditional decision-tree algorithms such as C4.5 and CART. Results show that HEAD-DT can generate algorithms that significantly outperform C4.5 and CART regarding predictive accuracy and F-Measure."}
{"metadata": {"dataset": "scidocs", "query_id": "0674058618d04def58c79a0b28174301ef591433", "doc_id": "6e5e76268f292929ccba794ea4dcbb4c68899df7"}, "id": "scidocs-0674058618d04def58c79a0b28174301ef591433-6e5e76268f292929ccba794ea4dcbb4c68899df7", "question": "RainForest—A Framework for Fast Decision Tree Construction of Large Datasets", "context": "Since the Hoeffding tree algorithm was proposed in the literature, decision trees became one of the most popular tools for mining data streams. The key point of constructing the decision tree is to determine the best attribute to split the considered node. Several methods to solve this problem were presented so far. However, they are either wrongly mathematically justified (e.g., in the Hoeffding tree algorithm) or time-consuming (e.g., in the McDiarmid tree algorithm). In this paper, we propose a new method which significantly outperforms the McDiarmid tree algorithm and has a solid mathematical basis. Our method ensures, with a high probability set by the user, that the best attribute chosen in the considered node using a finite data sample is the same as it would be in the case of the whole data stream."}
{"metadata": {"dataset": "scidocs", "query_id": "0674058618d04def58c79a0b28174301ef591433", "doc_id": "e928564981b35eccc1035df3badf74de7611d9cc"}, "id": "scidocs-0674058618d04def58c79a0b28174301ef591433-e928564981b35eccc1035df3badf74de7611d9cc", "question": "RainForest—A Framework for Fast Decision Tree Construction of Large Datasets", "context": "This article presents an algorithm for inducing multiclass decision trees with multivariate tests at internal decision nodes. Each test is constructed by training a linear machine and eliminating variables in a controlled manner. Empirical results demonstrate that the algorithm builds small accurate trees across a variety of tasks."}
{"metadata": {"dataset": "scidocs", "query_id": "068a88330c93a41058d6e04e576d7e1a21dc6ee7", "doc_id": "no"}, "id": "scidocs-068a88330c93a41058d6e04e576d7e1a21dc6ee7-no", "question": "Convex Color Image Segmentation with Optimal Transport Distances", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "068a88330c93a41058d6e04e576d7e1a21dc6ee7", "doc_id": "0080118b0eb02af581ff32b85a1bb6aed7081f45"}, "id": "scidocs-068a88330c93a41058d6e04e576d7e1a21dc6ee7-0080118b0eb02af581ff32b85a1bb6aed7081f45", "question": "Convex Color Image Segmentation with Optimal Transport Distances", "context": "Optimal transport distances are a fundamental family of dis tances for probability measures and histograms of features. Despite their appeali ng theoretical properties, excellent performance in retrieval tasks and intuiti ve formulation, their computation involves the resolution of a linear program whose c ost an quickly become prohibitive whenever the size of the support of these me asur s or the histograms’ dimension exceeds a few hundred. We propose in this work a new family of optimal transport distances that look at transport probl ems from a maximumentropy perspective. We smooth the classic optimal transpo rt pr blem with an entropic regularization term, and show that the resulting o ptimum is also a distance which can be computed through Sinkhorn’s matrix scali ng algorithm at a speed that is several orders of magnitude faster than that of transport solvers. We also show that this regularized distance improves upon clas si optimal transport distances on the MNIST classification problem."}
{"metadata": {"dataset": "scidocs", "query_id": "068a88330c93a41058d6e04e576d7e1a21dc6ee7", "doc_id": "80f8d31b80dc3195442e667dbc5309578520c2e3"}, "id": "scidocs-068a88330c93a41058d6e04e576d7e1a21dc6ee7-80f8d31b80dc3195442e667dbc5309578520c2e3", "question": "Convex Color Image Segmentation with Optimal Transport Distances", "context": "The state-of-the-art interactive image segmentation algorithms are sensitive to the user inputs and often unable to produce an accurate boundary with a small amount of user interaction. They frequently rely on laborious user editing to refine the segmentation boundary. In this paper, we propose a robust and accurate interactive method based on the recently developed continuous-domain convex active contour model. The proposed method exhibits many desirable properties of an effective interactive image segmentation algorithm, including robustness to user inputs and different initializations, the ability to produce a smooth and accurate boundary contour, and the ability to handle topology changes. Experimental results on a benchmark data set show that the proposed tool is highly effective and outperforms the state-of-the-art interactive image segmentation algorithms."}
{"metadata": {"dataset": "scidocs", "query_id": "068a88330c93a41058d6e04e576d7e1a21dc6ee7", "doc_id": "c1f3e92cfec36fd70f9fe37366d0899877ad227e"}, "id": "scidocs-068a88330c93a41058d6e04e576d7e1a21dc6ee7-c1f3e92cfec36fd70f9fe37366d0899877ad227e", "question": "Convex Color Image Segmentation with Optimal Transport Distances", "context": "The individualization of an object from a digital image is a common problem in the field of image processing. We propose a modified version of the watershed algorithm for image segmentation. We have presented an adaptive masking and a threshoding mechanism over each color channel to overcome over segmentation problem, before combining the segmentation from each channel into the final one. Our proposed method ensures accuracy and quality of the 10 different kinds of color images. The experimental results are obtained using image quality assessment (IQA) metrics such as PSNR, MSE, PSNRRGB and Color Image Quality Measure (CQM) based on reversible YUV color transformation. Consequently, the proposed modified watershed approach can enhance the image segmentation performance. Similarly, it is worth noticing that our proposed MWS approach is faster than many other segmentation algorithms, which makes it appropriate for real-time application. According to the visual and quantitative verification, the proposed algorithm is performing better than three other algorithms."}
{"metadata": {"dataset": "scidocs", "query_id": "068a88330c93a41058d6e04e576d7e1a21dc6ee7", "doc_id": "ac9748ea3945eb970cc32a37db7cfdfd0f22e74c"}, "id": "scidocs-068a88330c93a41058d6e04e576d7e1a21dc6ee7-ac9748ea3945eb970cc32a37db7cfdfd0f22e74c", "question": "Convex Color Image Segmentation with Optimal Transport Distances", "context": "A method is presented for automated segmentation of vessels in two-dimensional color images of the retina. This method can be used in computer analyses of retinal images, e.g., in automated screening for diabetic retinopathy. The system is based on extraction of image ridges, which coincide approximately with vessel centerlines. The ridges are used to compose primitives in the form of line elements. With the line elements an image is partitioned into patches by assigning each image pixel to the closest line element. Every line element constitutes a local coordinate frame for its corresponding patch. For every pixel, feature vectors are computed that make use of properties of the patches and the line elements. The feature vectors are classified using a kNN-classifier and sequential forward feature selection. The algorithm was tested on a database consisting of 40 manually labeled images. The method achieves an area under the receiver operating characteristic curve of 0.952. The method is compared with two recently published rule-based methods of Hoover et al. and Jiang et al. . The results show that our method is significantly better than the two rule-based methods (p<0.01). The accuracy of our method is 0.944 versus 0.947 for a second observer."}
{"metadata": {"dataset": "scidocs", "query_id": "068a88330c93a41058d6e04e576d7e1a21dc6ee7", "doc_id": "2ab78d140d0bde65803057afa751ee6609f47ba7"}, "id": "scidocs-068a88330c93a41058d6e04e576d7e1a21dc6ee7-2ab78d140d0bde65803057afa751ee6609f47ba7", "question": "Convex Color Image Segmentation with Optimal Transport Distances", "context": "An unsupervised video object segmentation algorithm, which discovers a primary object in a video sequence automatically, is proposed in this work. We introduce three energies in terms of foreground and background probability distributions: Markov, spatiotemporal, and antagonistic energies. Then, we minimize a hybrid of the three energies to separate a primary object from its background. However, the hybrid energy is nonconvex. Therefore, we develop the alternate convex optimization (ACO) scheme, which decomposes the nonconvex optimization into two quadratic programs. Moreover, we propose the forward-backward strategy, which performs the segmentation sequentially from the first to the last frames and then vice versa, to exploit temporal correlations. Experimental results on extensive datasets demonstrate that the proposed ACO algorithm outperforms the state-of-the-art techniques significantly."}
{"metadata": {"dataset": "scidocs", "query_id": "068a88330c93a41058d6e04e576d7e1a21dc6ee7", "doc_id": "00a4eef18b0236875dc3785eb6996d374e78714a"}, "id": "scidocs-068a88330c93a41058d6e04e576d7e1a21dc6ee7-00a4eef18b0236875dc3785eb6996d374e78714a", "question": "Convex Color Image Segmentation with Optimal Transport Distances", "context": "Proceedings of the 1998 IEEE International Conference on Computer Vision, Bombay, India We introduce a new distance between two distributions that we call the Earth Mover’s Distance (EMD), which reflects the minimal amount of work that must be performed to transform one distribution into the other by moving “distribution mass” around. This is a special case of the transportation problem from linear optimization, for which efficient algorithms are available. The EMD also allows for partial matching. When used to compare distributions that have the same overall mass, the EMD is a true metric, and has easy-to-compute lower bounds. In this paper we focus on applications to image databases, especially color and texture. We use the EMD to exhibit the structure of color-distribution and texture spaces by means of MultiDimensional Scaling displays. We also propose a novel approach to the problem of navigating through a collection of color images, which leads to a new paradigm for image database search."}
{"metadata": {"dataset": "scidocs", "query_id": "068a88330c93a41058d6e04e576d7e1a21dc6ee7", "doc_id": "dffe3ef1fb5671c3d7d7e0042215503eb96442b9"}, "id": "scidocs-068a88330c93a41058d6e04e576d7e1a21dc6ee7-dffe3ef1fb5671c3d7d7e0042215503eb96442b9", "question": "Convex Color Image Segmentation with Optimal Transport Distances", "context": "Vectorized total variation (VTV) is very successful convex regularizer to solve various color image recovery problems. Despite the fact that color channels of natural color images are closely related, existing variants of VTV can not utilize this prior efficiently. We proposed L∞-VTV as a convex regularizer can penalize the violation of such inter-channel dependency by employing weighted L∞ (L-infty) norm. We also introduce an effective algorithm for an image denoising problem using L∞-VTV. Experimental results shows that our proposed method can outperform the conventional methods."}
{"metadata": {"dataset": "scidocs", "query_id": "068a88330c93a41058d6e04e576d7e1a21dc6ee7", "doc_id": "9bd6bff7c3eae7ec6da8ed7aeb70491bcd40177e"}, "id": "scidocs-068a88330c93a41058d6e04e576d7e1a21dc6ee7-9bd6bff7c3eae7ec6da8ed7aeb70491bcd40177e", "question": "Convex Color Image Segmentation with Optimal Transport Distances", "context": "Image registration is the process of establishing a common geometric reference frame between two or more image data sets possibly taken at different times. In this paper we present a method for computing elastic registration and warping maps based on the Monge–Kantorovich theory of optimal mass transport. This mass transport method has a number of important characteristics. First, it is parameter free. Moreover, it utilizes all of the grayscale data in both images, places the two images on equal footing and is symmetrical: the optimal mapping from image A to image B being the inverse of the optimal mapping from B to A. The method does not require that landmarks be specified, and the minimizer of the distance functional involved is unique; there are no other local minimizers. Finally, optimal transport naturally takes into account changes in density that result from changes in area or volume. Although the optimal transport method is certainly not appropriate for all registration and warping problems, this mass preservation property makes the Monge–Kantorovich approach quite useful for an interesting class of warping problems, as we show in this paper. Our method for finding the registration mapping is based on a partial differential equation approach to the minimization of the L 2 Kantorovich–Wasserstein or “Earth Mover's Distance” under a mass preservation constraint. We show how this approach leads to practical algorithms, and demonstrate our method with a number of examples, including those from the medical field. We also extend this method to take into account changes in intensity, and show that it is well suited for applications such as image morphing."}
{"metadata": {"dataset": "scidocs", "query_id": "068a88330c93a41058d6e04e576d7e1a21dc6ee7", "doc_id": "c504c88dbea0c1fe9383710646c8180ef44b9bc9"}, "id": "scidocs-068a88330c93a41058d6e04e576d7e1a21dc6ee7-c504c88dbea0c1fe9383710646c8180ef44b9bc9", "question": "Convex Color Image Segmentation with Optimal Transport Distances", "context": "Image segmentation remains one of the major challenges in image analysis. In medical applications, skilled operators are usually employed to extract the desired regions that may be anatomically separate but statistically indistinguishable. Such manual processing is subject to operator errors and biases, is extremely time consuming, and has poor reproducibility. We propose a robust algorithm for the segmentation of three-dimensional (3-D) image data based on a novel combination of adaptive K-mean clustering and knowledge-based morphological operations. The proposed adaptive K-mean clustering algorithm is capable of segmenting the regions of smoothly varying intensity distributions. Spatial constraints are incorporated in the clustering algorithm through the modeling of the regions by Gibbs random fields. Knowledge-based morphological operations are then applied to the segmented regions to identify the desired regions according to the a priori anatomical knowledge of the region-of-interest. This proposed technique has been successfully applied to a sequence of cardiac CT volumetric images to generate the volumes of left ventricle chambers at 16 consecutive temporal frames. Our final segmentation results compare favorably with the results obtained using manual outlining. Extensions of this approach to other applications can be readily made when a priori knowledge of a given object is available."}
{"metadata": {"dataset": "scidocs", "query_id": "068a88330c93a41058d6e04e576d7e1a21dc6ee7", "doc_id": "c014c1bc895a097f0eeea09f224e0f31284f20c5"}, "id": "scidocs-068a88330c93a41058d6e04e576d7e1a21dc6ee7-c014c1bc895a097f0eeea09f224e0f31284f20c5", "question": "Convex Color Image Segmentation with Optimal Transport Distances", "context": "We present a preprocessing-based acceleration technique for computing bi-criteria Pareto-optimal journeys in public transit networks, based on the well-known RAPTOR algorithm [16]. Our key idea is to first partition a hypergraph into cells, in which vertices correspond to routes (e.g., bus lines) and hyperedges to stops, and to then mark routes sufficient for optimal travel across cells. The query can then be restricted to marked routes and those in the source and target cells. This results in a practical approach, suitable for networks that are too large to be efficiently handled by the basic RAPTOR algorithm. 1998 ACM Subject Classification G.2.2 Graph Theory"}
{"metadata": {"dataset": "scidocs", "query_id": "06e0780f589f04edd1e55f5a0d9872696280b40e", "doc_id": "no"}, "id": "scidocs-06e0780f589f04edd1e55f5a0d9872696280b40e-no", "question": "Divide and correct: using clusters to grade short answers at scale", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "06e0780f589f04edd1e55f5a0d9872696280b40e", "doc_id": "7545f90299a10dae1968681f6bd268b9b5ab2c37"}, "id": "scidocs-06e0780f589f04edd1e55f5a0d9872696280b40e-7545f90299a10dae1968681f6bd268b9b5ab2c37", "question": "Divide and correct: using clusters to grade short answers at scale", "context": "We introduce a new approach to the machine-assisted grading of short answer questions. We follow past work in automated grading by first training a similarity metric between student responses, but then go on to use this metric to group responses into clusters and subclusters. The resulting groupings allow teachers to grade multiple responses with a single action, provide rich feedback to groups of similar answers, and discover modalities of misunderstanding among students; we refer to this amplification of grader effort as “powergrading.” We develop the means to further reduce teacher effort by automatically performing actions when an answer key is available. We show results in terms of grading progress with a small “budget” of human actions, both from our method and an LDA-based approach, on a test corpus of 10 questions answered by 698 respondents."}
{"metadata": {"dataset": "scidocs", "query_id": "06e0780f589f04edd1e55f5a0d9872696280b40e", "doc_id": "070096ce36bba240b39b5ddb7bc6071311478843"}, "id": "scidocs-06e0780f589f04edd1e55f5a0d9872696280b40e-070096ce36bba240b39b5ddb7bc6071311478843", "question": "Divide and correct: using clusters to grade short answers at scale", "context": "In this work we address the task of computerassisted assessment of short student answers. We combine several graph alignment features with lexical semantic similarity measures using machine learning techniques and show that the student answers can be more accurately graded than if the semantic measures were used in isolation. We also present a first attempt to align the dependency graphs of the student and the instructor answers in order to make use of a structural component in the automatic grading of student answers."}
{"metadata": {"dataset": "scidocs", "query_id": "06e0780f589f04edd1e55f5a0d9872696280b40e", "doc_id": "1e1c3c3f1ad33e1424584ba7b2f5b6681b842dce"}, "id": "scidocs-06e0780f589f04edd1e55f5a0d9872696280b40e-1e1c3c3f1ad33e1424584ba7b2f5b6681b842dce", "question": "Divide and correct: using clusters to grade short answers at scale", "context": "To verify cluster separation in high-dimensional data, analysts often reduce the data with a dimension reduction (DR) technique, and then visualize it with 2D Scatterplots, interactive 3D Scatterplots, or Scatterplot Matrices (SPLOMs). With the goal of providing guidance between these visual encoding choices, we conducted an empirical data study in which two human coders manually inspected a broad set of 816 scatterplots derived from 75 datasets, 4 DR techniques, and the 3 previously mentioned scatterplot techniques. Each coder scored all color-coded classes in each scatterplot in terms of their separability from other classes. We analyze the resulting quantitative data with a heatmap approach, and qualitatively discuss interesting scatterplot examples. Our findings reveal that 2D scatterplots are often 'good enough', that is, neither SPLOM nor interactive 3D adds notably more cluster separability with the chosen DR technique. If 2D is not good enough, the most promising approach is to use an alternative DR technique in 2D. Beyond that, SPLOM occasionally adds additional value, and interactive 3D rarely helps but often hurts in terms of poorer class separation and usability. We summarize these results as a workflow model and implications for design. Our results offer guidance to analysts during the DR exploration process."}
{"metadata": {"dataset": "scidocs", "query_id": "06e0780f589f04edd1e55f5a0d9872696280b40e", "doc_id": "813308251c76640f0f9f98c54339ae73752793aa"}, "id": "scidocs-06e0780f589f04edd1e55f5a0d9872696280b40e-813308251c76640f0f9f98c54339ae73752793aa", "question": "Divide and correct: using clusters to grade short answers at scale", "context": "Short documents are typically represented by very sparse vectors, in the space of terms. In this case, traditional techniques for calculating text similarity results in measures which are very close to zero, since documents even the very similar ones have a very few or mostly no terms in common. In order to alleviate this limitation, the representation of short-text segments should be enriched by incorporating information about correlation between terms. In other words, if two short segments do not have any common words, but terms from the first segment appear frequently with terms from the second segment in other documents, this means that these segments are semantically related, and their similarity measure should be high. Towards achieving this goal, we employ a method for enhancing document clustering using statistical semantics. However, the problem of high computation time arises when calculating correlation between all terms. In this work, we propose the selection of a few terms, and using these terms with the Nystr\\\"om method to approximate the term-term correlation matrix. The selection of the terms for the Nystr\\\"om method is performed by randomly sampling terms with probabilities proportional to the lengths of their vectors in the document space. This allows more important terms to have more influence on the approximation of the term-term correlation matrix and accordingly achieves better accuracy."}
{"metadata": {"dataset": "scidocs", "query_id": "06e0780f589f04edd1e55f5a0d9872696280b40e", "doc_id": "2be92d9137f4af64a059da33a58b148d153fc446"}, "id": "scidocs-06e0780f589f04edd1e55f5a0d9872696280b40e-2be92d9137f4af64a059da33a58b148d153fc446", "question": "Divide and correct: using clusters to grade short answers at scale", "context": "Data Clustering is used to extract meaningful information and to develop significant relationships among variables stored in large data set/data warehouse. In this paper data clustering technique named k-means clustering is applied to analyze student’s learning behavior. The student’s evaluation factor like class quizzes, mid and final exam assignment are studied. It is recommended that all these correlated information should be conveyed to the class advisor before the conduction of final exam. This study will help the teachers to reduce the drop out ratio to a significant level and improve the performance of students."}
{"metadata": {"dataset": "scidocs", "query_id": "06e0780f589f04edd1e55f5a0d9872696280b40e", "doc_id": "0c383e6fb2ef73538e76acfbc1c6132b76b47537"}, "id": "scidocs-06e0780f589f04edd1e55f5a0d9872696280b40e-0c383e6fb2ef73538e76acfbc1c6132b76b47537", "question": "Divide and correct: using clusters to grade short answers at scale", "context": "SimpleQuestions is a commonly used benchmark for single-factoid question answering (QA) over Knowledge Graphs (KG). Existing QA systems rely on various components to solve different sub-tasks of the problem (such as entity detection, entity linking, relation prediction and evidence integration). In this work, we propose a different approach to the problem and present an information retrieval style solution for it. We adopt a two-phase approach: candidate generation and candidate re-ranking to answer questions. We propose a Triplet-Siamese-Hybrid CNN (TSHCNN) to re-rank candidate answers. Our approach achieves an accuracy of 80% which sets a new state-of-the-art on the SimpleQuestions dataset."}
{"metadata": {"dataset": "scidocs", "query_id": "06e0780f589f04edd1e55f5a0d9872696280b40e", "doc_id": "03fbf51605f68c730a6b37d40d805875a078e307"}, "id": "scidocs-06e0780f589f04edd1e55f5a0d9872696280b40e-03fbf51605f68c730a6b37d40d805875a078e307", "question": "Divide and correct: using clusters to grade short answers at scale", "context": "68 Computer C lustering is a discovery process in data mining. 1 It groups a set of data in a way that maximizes the similarity within clusters and minimizes the similarity between two different clusters. 1,2 These discovered clusters can help explain the characteristics of the underlying data distribution and serve as the foundation for other data mining and analysis techniques. Clustering is useful in characterizing customer groups based on purchasing patterns, categorizing Web documents, 3 grouping genes and proteins that have similar func-tionality, 4 grouping spatial locations prone to earthquakes based on seismological data, and so on. Most existing clustering algorithms find clusters that fit some static model. Although effective in some cases, these algorithms can break down—that is, cluster the data incorrectly—if the user doesn't select appropriate static-model parameters. Or sometimes the model cannot adequately capture the clusters' characteristics. Most of these algorithms break down when the data contains clusters of diverse shapes, densities , and sizes. Existing algorithms use a static model of the clusters and do not use information about the nature of individual clusters as they are merged. Furthermore, one set of schemes (the CURE algorithm and related schemes) ignores the information about the aggregate interconnectivity of items in two clusters. The other set of schemes (the Rock algorithm, group averaging method, and related schemes) ignores information about the closeness of two clusters as defined by the similarity of the closest items across two clusters. (For more information, see the \" Limitations of Traditional Clustering Algorithms \" sidebar.) By only considering either interconnectivity or close-ness, these algorithms can easily select and merge the wrong pair of clusters. For instance, an algorithm that focuses only on the closeness of two clusters will incorrectly merge the clusters in Figure 1a over those in Figure 1b. Similarly, an algorithm that focuses only on interconnectivity will, in Figure 2, incorrectly merge the dark-blue with the red cluster rather than the green one. Here, we assume that the aggregate interconnec-tivity between the items in the dark-blue and red clusters is greater than that of the dark-blue and green clusters. However, the border points of the dark-blue cluster are much closer to those of the green cluster than those of the red cluster. Chameleon is a new agglomerative hierarchical clustering algorithm that overcomes the limitations of existing clustering algorithms. Figure 3 (on page 70) provides an overview of the overall approach …"}
{"metadata": {"dataset": "scidocs", "query_id": "06e0780f589f04edd1e55f5a0d9872696280b40e", "doc_id": "c8bd02c043c6712f62bd4a0f926719c7e94c64e2"}, "id": "scidocs-06e0780f589f04edd1e55f5a0d9872696280b40e-c8bd02c043c6712f62bd4a0f926719c7e94c64e2", "question": "Divide and correct: using clusters to grade short answers at scale", "context": "Different software systems for automatic question answering have been developed in recent years. Some systems perform well on specific domains, but may not be appropriate for other domains. As the complexity and scaling of such information systems become ever greater, it is much more challenging to effectively and efficiently determine which toolkits, algorithms, knowledge bases or other resources should be integrated into a system so that one can achieve a desired or optimal level of performance on a given task. In this working notepaper, we present a generic framework that can be used for any machinereading task and automatically find the best configuration of algorithmic components as well as values of their corresponding parameters. Although we have designed the framework for all QA4MRE-2013 tasks (i.e. Main task, Biomedical about Alzheimer’s and Entrance Exam), our analysis will mostly focus on Biomedical about Alzheimer’s task. We introduce the Configuration Space Exploration (CSE) framework, an extension to the Unstructured Information Management Architecture (UIMA) which provides a general distributed solution for building and exploring possible configurations for any intelligent information system. For the Biomedial about Alzheimer’s task, CSE was used to generate more than 1000 different configurations from existing components; we selected the 3 best runs for submission. We achieved an average c@1 of 0.27; our highest score was 0.60 for reading-test-1, and our lowest was 0.0 for reading-test3. We further enhanced the system by introducing point-wise mutual information (PMI) scoring for answer ranking, which produced an average c@1 of 0.4025, with a highest score of 0.77 for reading test-1 and a lowest score of 0.2 for reading test-2."}
{"metadata": {"dataset": "scidocs", "query_id": "06e0780f589f04edd1e55f5a0d9872696280b40e", "doc_id": "60e752707e3adb38939e43d8579392edda26c198"}, "id": "scidocs-06e0780f589f04edd1e55f5a0d9872696280b40e-60e752707e3adb38939e43d8579392edda26c198", "question": "Divide and correct: using clusters to grade short answers at scale", "context": "Non-negative matrix factorization (NMF) has been successfully applied in document clustering. However, experiments on short texts, such as microblogs, Q&A documents and news titles, suggest unsatisfactory performance of NMF. An major reason is that the traditional term weighting schemes, like binary weight and tfidf, cannot well capture the terms' discriminative power and importance in short texts, due to the sparsity of data. To tackle this problem, we proposed a novel term weighting scheme for NMF, derived from the Normalized Cut (Ncut) problem on the term affinity graph. Different from idf, which emphasizes discriminability on document level, the Ncut weighting measures terms' discriminability on term level. Experiments on two data sets show our weighting scheme significantly boosts NMF's performance on short text clustering."}
{"metadata": {"dataset": "scidocs", "query_id": "06e0780f589f04edd1e55f5a0d9872696280b40e", "doc_id": "0c53bdd975f351ebe88e5cc130df1e9097f69e95"}, "id": "scidocs-06e0780f589f04edd1e55f5a0d9872696280b40e-0c53bdd975f351ebe88e5cc130df1e9097f69e95", "question": "Divide and correct: using clusters to grade short answers at scale", "context": "The recently introducedInformation Bottleneck method [21] provides an information theoretic framework, for extr acting features of one variable, that are relevant for the values of an ther variable. Several previous works already suggeste d applying this method for document clustering, gene expression data a nalysis, spectral analysis and more. In this work we present a novel implementation of this method for supervised text cla ssification. Specifically, we apply the information bottlen eck method to find word-clusters that preserve the information a bout document categories and use these clusters as features for classification. Previous work [1] used a similar clustering procedure to show that word-clusters can significantly redu c the feature space dimensionality, with only a minor change in cl assification accuracy. In this work we present similar resul ts and go further to show that when the training sample is small word clusters can yield significant improvement in classificatio n accuracy (up to18%) over the performance using the words directly."}
{"metadata": {"dataset": "scidocs", "query_id": "071961fc3d61b893c12f07abfa2906859152e3a9", "doc_id": "no"}, "id": "scidocs-071961fc3d61b893c12f07abfa2906859152e3a9-no", "question": "Learning to Rank Non-Factoid Answers: Comment Selection in Web Forums", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "071961fc3d61b893c12f07abfa2906859152e3a9", "doc_id": "b345fd87cb98fc8b2320530609c63b1b347b14d9"}, "id": "scidocs-071961fc3d61b893c12f07abfa2906859152e3a9-b345fd87cb98fc8b2320530609c63b1b347b14d9", "question": "Learning to Rank Non-Factoid Answers: Comment Selection in Web Forums", "context": "We present an end-to-end system for open-domain non-factoid question answering. We leverage the information on the ever-growing World Wide Web, and the capabilities of modern search engines to find the relevant information. Our QA system is composed of three components: (i) query formulation module (QFM) (ii) candidate answer generation module (CAGM) and (iii) answer selection module (ASM). A thorough empirical evaluation using two datasets demonstrates that the proposed approach is highly competitive."}
{"metadata": {"dataset": "scidocs", "query_id": "071961fc3d61b893c12f07abfa2906859152e3a9", "doc_id": "1c78809a7fd22c95f4d60cd707c32280a019940f"}, "id": "scidocs-071961fc3d61b893c12f07abfa2906859152e3a9-1c78809a7fd22c95f4d60cd707c32280a019940f", "question": "Learning to Rank Non-Factoid Answers: Comment Selection in Web Forums", "context": "In this paper we describe and evaluate a Question Answering (QA) system that goes beyond answering factoid questions. Our approach to QA assumes no restrictions on the type of questions that are handled, and no assumption that the answers to be provided are factoids. We present an unsupervised approach for collecting question and answer pairs from FAQ pages, which we use to collect a corpus of 1 million question/answer pairs from FAQ pages available on the Web. This corpus is used to train various statistical models employed by our QA system: a statistical chunker used to transform a natural language-posed question into a phrase-based query to be submitted for exact match to an off-the-shelf search engine; an answer/question translation model, used to assess the likelihood that a proposed answer is indeed an answer to the posed question; and an answer language model, used to assess the likelihood that a proposed answer is a well-formed answer. We evaluate our QA system in a modular fashion, by comparing the performance of baseline algorithms against our proposed algorithms for various modules in our QA system. The evaluation shows that our system achieves reasonable performance in terms of answer accuracy for a large variety of complex, non-factoid questions."}
{"metadata": {"dataset": "scidocs", "query_id": "071961fc3d61b893c12f07abfa2906859152e3a9", "doc_id": "17ba9019ec7eb1f03d64f4d80e47376a4f6f8583"}, "id": "scidocs-071961fc3d61b893c12f07abfa2906859152e3a9-17ba9019ec7eb1f03d64f4d80e47376a4f6f8583", "question": "Learning to Rank Non-Factoid Answers: Comment Selection in Web Forums", "context": "Community-based Question Answering sites, such as Yahoo! Answers or Baidu Zhidao, allow users to get answers to complex, detailed and personal questions from other users. However, since answering a question depends on the ability and willingness of users to address the asker's needs, a significant fraction of the questions remain unanswered. We measured that in Yahoo! Answers, this fraction represents 15% of all incoming English questions. At the same time, we discovered that around 25% of questions in certain categories are recurrent, at least at the question-title level, over a period of one year.\n We attempt to reduce the rate of unanswered questions in Yahoo! Answers by reusing the large repository of past resolved questions, openly available on the site. More specifically, we estimate the probability whether certain new questions can be satisfactorily answered by a best answer from the past, using a statistical model specifically trained for this task. We leverage concepts and methods from query-performance prediction and natural language processing in order to extract a wide range of features for our model. The key challenge here is to achieve a level of quality similar to the one provided by the best human answerers.\n We evaluated our algorithm on offline data extracted from Yahoo! Answers, but more interestingly, also on online data by using three \"live\" answering robots that automatically provide past answers to new questions when a certain degree of confidence is reached. We report the success rate of these robots in three active Yahoo! Answers categories in terms of both accuracy, coverage and askers' satisfaction. This work presents a first attempt, to the best of our knowledge, of automatic question answering to questions of social nature, by reusing past answers of high quality."}
{"metadata": {"dataset": "scidocs", "query_id": "071961fc3d61b893c12f07abfa2906859152e3a9", "doc_id": "9222c69ca851b26e8338b0082dfafbc663d1be50"}, "id": "scidocs-071961fc3d61b893c12f07abfa2906859152e3a9-9222c69ca851b26e8338b0082dfafbc663d1be50", "question": "Learning to Rank Non-Factoid Answers: Comment Selection in Web Forums", "context": "We present a question answering system over DBpedia, filling the gap between user information needs expressed in natural language and a structured query interface expressed in SPARQL over the underlying knowledge base (KB). Given the KB, our goal is to comprehend a natural language query and provide corresponding accurate answers. Focusing on solving the non-aggregation questions, in this paper, we construct a subgraph of the knowledge base from the detected entities and propose a graph traversal method to solve both the semantic item mapping problem and the disambiguation problem in a joint way. Compared with existing work, we simplify the process of query intention understanding and pay more attention to the answer path ranking. We evaluate our method on a non-aggregation question dataset and further on a complete dataset. Experimental results show that our method achieves best performance compared with several state-of-the-art systems."}
{"metadata": {"dataset": "scidocs", "query_id": "071961fc3d61b893c12f07abfa2906859152e3a9", "doc_id": "33dc678ba56819b2fd05c1b538b0398125f4ccd3"}, "id": "scidocs-071961fc3d61b893c12f07abfa2906859152e3a9-33dc678ba56819b2fd05c1b538b0398125f4ccd3", "question": "Learning to Rank Non-Factoid Answers: Comment Selection in Web Forums", "context": "Purpose – This study investigates the ways in which effectiveness of answers in Yahoo! Answers, one of the largest community question answering sites (CQAs), is related to question types and answerer reputation. Effective answers are defined as those that are detailed, readable, superior in quality, and contributed promptly. Five question types that were studied include factoid, list, definition, complex interactive, and opinion. Answerer reputation refers to the past track record of answerers in the community. Design/Methodology/Approach – The dataset comprises 1,459 answers posted in Yahoo! Answers in response to 464 questions that were distributed across the five question types. The analysis was done using factorial analysis of variance. Findings – The results indicate that factoid, definition and opinion questions were comparable in attracting high quality as well as readable answers. Although reputed answerers generally fared better in offering detailed and high quality answers, novices were found to submit more readable responses. Moreover, novices were more prompt in answering factoid, list and definition questions. Originality/value – By analyzing variations in answer effectiveness with a twin-focus on question types and answerer reputation, this study explores a strand of CQA research that has hitherto received limited attention. The findings offer insights to users and designers of CQAs."}
{"metadata": {"dataset": "scidocs", "query_id": "071961fc3d61b893c12f07abfa2906859152e3a9", "doc_id": "b38ea4cdf9012fa7a76d7e700260aa99855947c3"}, "id": "scidocs-071961fc3d61b893c12f07abfa2906859152e3a9-b38ea4cdf9012fa7a76d7e700260aa99855947c3", "question": "Learning to Rank Non-Factoid Answers: Comment Selection in Web Forums", "context": "Users participate in online discussion forums to learn from others and share their knowledge with the community. They often start a thread with a question or sharing their new findings on a certain topic. We find that, unlike Community Question Answering, where questions are mostly factoid based, the threads in a forum are often open-ended (e.g., asking for recommendations from others) without a single correct answer. In this paper, we address the task of identifying helpful posts in a forum thread, to aid users comprehend long running discussion threads, which often contain repetitive or even irrelevant posts. We propose a recurrent neural network based architecture to model (i) the relevance of a post regarding the original post starting the thread and (ii) the novelty it brings to the discussion, compared to the previous posts in the thread. Experimental results on five different types of online forum datasets show that our model significantly outperforms the state-of-the-art neural network models for text classification."}
{"metadata": {"dataset": "scidocs", "query_id": "071961fc3d61b893c12f07abfa2906859152e3a9", "doc_id": "0025b963134b1c0b64c1389af19610d038ab7072"}, "id": "scidocs-071961fc3d61b893c12f07abfa2906859152e3a9-0025b963134b1c0b64c1389af19610d038ab7072", "question": "Learning to Rank Non-Factoid Answers: Comment Selection in Web Forums", "context": "There are many applications in which it is desirable to order rather than classify instances. Here we consider the problem of learning how to order, given feedback in the form of preference judgments, i.e., statements to the effect that one instance should be ranked ahead of another. We outline a two-stage approach in which one first learns by conventional means a preference function, of the form PREF , which indicates whether it is advisable to rank before . New instances are then ordered so as to maximize agreements with the learned preference function. We show that the problem of finding the ordering that agrees best with a preference function is NP-complete, even under very restrictive assumptions. Nevertheless, we describe a simple greedy algorithm that is guaranteed to find a good approximation. We then discuss an on-line learning algorithm, based on the “Hedge” algorithm, for finding a good linear combination of ranking “experts.” We use the ordering algorithm combined with the on-line learning algorithm to find a combination of “search experts,” each of which is a domain-specific query expansion strategy for a WWW search engine, and present experimental results that demonstrate the merits of our approach."}
{"metadata": {"dataset": "scidocs", "query_id": "071961fc3d61b893c12f07abfa2906859152e3a9", "doc_id": "8f50078de9d41bd756cb0dcb346a916cc9d50ce6"}, "id": "scidocs-071961fc3d61b893c12f07abfa2906859152e3a9-8f50078de9d41bd756cb0dcb346a916cc9d50ce6", "question": "Learning to Rank Non-Factoid Answers: Comment Selection in Web Forums", "context": "Observed in many real applications, a top-k query often consists of two components to reflect a user's preference: a selection condition and a ranking function. A user may not only propose ad hoc ranking functions, but also use different interesting subsets of the data. In many cases, a user may want to have a thorough study of the data by initiating a multi-dimensional analysis of the top-k query results. Previous work on top-k query processing mainly focuses on optimizing data access according to the ranking function only. The problem of efficient answering top-k queries with multi-dimensional selections has not been well addressed yet.This paper proposes a new computational model, called ranking cube, for efficient answering top-k queries with multi-dimensional selections. We define a rank-aware measure for the cube, capturing our goal of responding to multi-dimensional ranking analysis. Based on the ranking cube, an efficient query algorithm is developed which progressively retrieves data blocks until the top-k results are found. The curse of dimensionality is a well-known challenge for the data cube and we cope with this difficulty by introducing a new technique of ranking fragments. Our experiments on Microsoft's SQL Server 2005 show that our proposed approaches have significant improvement over the previous methods."}
{"metadata": {"dataset": "scidocs", "query_id": "071961fc3d61b893c12f07abfa2906859152e3a9", "doc_id": "c5d2dbd4454fe9610cb2a7b19772fba8fa229567"}, "id": "scidocs-071961fc3d61b893c12f07abfa2906859152e3a9-c5d2dbd4454fe9610cb2a7b19772fba8fa229567", "question": "Learning to Rank Non-Factoid Answers: Comment Selection in Web Forums", "context": "We analyze the question queries submitted to a large commercial web search engine to get insights about what people ask, and to better tailor the search results to the users' needs. Based on a dataset of about one billion question queries submitted during the year 2012, we investigate askers' querying behavior with the support of automatic query categorization. While the importance of question queries is likely to increase, at present they only make up 3-4% of the total search traffic.\n Since questions are such a small part of the query stream, and are more likely to be unique than shorter queries, click-through information is typically rather sparse. Thus, query categorization methods based on the categories of clicked web documents do not work well for questions. As an alternative, we propose a robust question query classification method that uses the labeled questions from a large community question answering platform (CQA) as a training set. The resulting classifier is then transferred to the web search questions. Even though questions on CQA platforms tend to be different to web search questions, our categorization method proves competitive with strong baselines with respect to classification accuracy.\n To show the scalability of our proposed method we apply the classifiers to about one billion question queries and discuss the trade-offs between performance and accuracy that different classification models offer."}
{"metadata": {"dataset": "scidocs", "query_id": "071961fc3d61b893c12f07abfa2906859152e3a9", "doc_id": "3fc109c976bf1c3f06aba9310f86dcd0c124ab41"}, "id": "scidocs-071961fc3d61b893c12f07abfa2906859152e3a9-3fc109c976bf1c3f06aba9310f86dcd0c124ab41", "question": "Learning to Rank Non-Factoid Answers: Comment Selection in Web Forums", "context": "An analysis of the social video sharing platform YouTube reveals a high amount of community feedback through comments for published videos as well as through meta ratings for these comments. In this paper, we present an in-depth study of commenting and comment rating behavior on a sample of more than 6 million comments on 67,000 YouTube videos for which we analyzed dependencies between comments, views, comment ratings and topic categories. In addition, we studied the influence of sentiment expressed in comments on the ratings for these comments using the SentiWordNet thesaurus, a lexical WordNet-based resource containing sentiment annotations. Finally, to predict community acceptance for comments not yet rated, we built different classifiers for the estimation of ratings for these comments. The results of our large-scale evaluations are promising and indicate that community feedback on already rated comments can help to filter new unrated comments or suggest particularly useful but still unrated comments."}
{"metadata": {"dataset": "scidocs", "query_id": "071f47b7bc5830643e31dbed82e0375bf9b26559", "doc_id": "no"}, "id": "scidocs-071f47b7bc5830643e31dbed82e0375bf9b26559-no", "question": "Ad Hoc Retrieval Experiments Using WordNet and Automatically Constructed Thesauri", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "071f47b7bc5830643e31dbed82e0375bf9b26559", "doc_id": "37b0f219c1f2fbc4b432b24a5fe91dd733f19b7f"}, "id": "scidocs-071f47b7bc5830643e31dbed82e0375bf9b26559-37b0f219c1f2fbc4b432b24a5fe91dd733f19b7f", "question": "Ad Hoc Retrieval Experiments Using WordNet and Automatically Constructed Thesauri", "context": "Although commonly used in both commercial and experimental information retrieval systems, thesauri have not demonstrated consistent beneets for retrieval performance, and it is diicult to construct a thesaurus automatically for large text databases. In this paper, an approach, called PhraseFinder, is proposed to construct collection-dependent association thesauri automatically using large full-text document collections. The association thesaurus can be accessed through natural language queries in INQUERY, an information retrieval system based on the probabilistic inference network. Experiments are conducted in IN-QUERY to evaluate diierent types of association thesauri, and thesauri constructed for a variety of collections."}
{"metadata": {"dataset": "scidocs", "query_id": "071f47b7bc5830643e31dbed82e0375bf9b26559", "doc_id": "ca56018ed7042d8528b5a7cd8f332c5737b53b1f"}, "id": "scidocs-071f47b7bc5830643e31dbed82e0375bf9b26559-ca56018ed7042d8528b5a7cd8f332c5737b53b1f", "question": "Ad Hoc Retrieval Experiments Using WordNet and Automatically Constructed Thesauri", "context": "A well constructed thesaurus has long been recognized as a valuable tool in the effective operation of an information retrieval system. This paper reports the results of experiments designed to determine the validity of an approach to the automatic construction of global thesauri (described originally by Crouch in [1] and [2] based on a clustering of the document collection. The authors validate the approach by showing that the use of thesauri generated by this method results in substantial improvements in retrieval effectiveness in four test collections. The term discrimination value theory, used in the thesaurus generation algorithm to determine a term's membership in a particular thesaurus class, is found not to be useful in distinguishing a “good” from an “indifferent” or “poor” thesaurus class). In conclusion, the authors suggest an alternate approach to automatic thesaurus construction which greatly simplifies the work of producing viable thesaurus classes. Experimental results show that the alternate approach described herein in some cases produces thesauri which are comparable in retrieval effectiveness to those produced by the first method at much lower cost."}
{"metadata": {"dataset": "scidocs", "query_id": "071f47b7bc5830643e31dbed82e0375bf9b26559", "doc_id": "d2071c1e4a6030dc0005dbfeefdd196a8b293e84"}, "id": "scidocs-071f47b7bc5830643e31dbed82e0375bf9b26559-d2071c1e4a6030dc0005dbfeefdd196a8b293e84", "question": "Ad Hoc Retrieval Experiments Using WordNet and Automatically Constructed Thesauri", "context": "During the course of TREC{1 the low-level search functions were split o into a separate Basic Search System (BSS) [2], but retrieval and ranking of documents was still done using the \\classical\" probabilistic model of Robertson and Sparck Jones[7] with no account taken of document length or term frequency within document or query. Four runs were submitted to NIST for evaluation: automatic ad hoc, automatic routing, manual ad hoc and manual ad hoc with feedback. The results were undistinguished, although not among the worst. Of the ad hoc runs, the manual was better than the automatic (in which only the CONCEPTS elds of the topics were used), and feedback appeared bene cial."}
{"metadata": {"dataset": "scidocs", "query_id": "071f47b7bc5830643e31dbed82e0375bf9b26559", "doc_id": "e2ec6bb2bcd4985817b4af9196a53d36cc3f9dde"}, "id": "scidocs-071f47b7bc5830643e31dbed82e0375bf9b26559-e2ec6bb2bcd4985817b4af9196a53d36cc3f9dde", "question": "Ad Hoc Retrieval Experiments Using WordNet and Automatically Constructed Thesauri", "context": "This paper presents a framework for knowledge discovery and concept exploration. In order to enhance the concept exploration capability of knowledge-based systems and to alleviate the limitations of the manual browsing approach, we have developed two spreading activation-based algorithms for concept exploration in large, heterogeneous networks of concepts (e.g., multiple thesauri). One algorithm, which is based on the symbolic Al paradigm, performs a conventional branch-and-bound search on a semantic net representation to identify other highly relevant concepts (a serial, optimal search process). The second algorithm, which is based on the neural network approach, executes the Hopfield net parallel relaxation and convergence process to identify “convergent” concepts for some initial queries (a parallel, heuristic search process). Both algorithms can be adopted for automatic, multiple-thesauri consultation. We tested these two algorithms on a large text-based knowledge network of about 13,000 nodes (terms) and 80,000 directed links in the area of computing technologies. This knowledge network was created from two external thesauri and one automatically generated thesaurus. We conducted experiments to compare the behaviors and performances of the two algorithms with the hypertext-like browsing process. Our experiment revealed that manual browsing achieved higher-term recall but lower-term precision in comparison to the algorithmic systems. However, it was also a much more laborious and cognitively demanding process. In document retrieval, there were no statistically significant differences in document recall and precision between the algorithms and the manual browsing process. In light of the effort required by the manual browsing process, our proposed algorithmic approach presents a viable option for efficiently traversing largescale, multiple thesauri (knowledge network)."}
{"metadata": {"dataset": "scidocs", "query_id": "071f47b7bc5830643e31dbed82e0375bf9b26559", "doc_id": "076945bc9fe2000208d6809af1699ac04e6ccee6"}, "id": "scidocs-071f47b7bc5830643e31dbed82e0375bf9b26559-076945bc9fe2000208d6809af1699ac04e6ccee6", "question": "Ad Hoc Retrieval Experiments Using WordNet and Automatically Constructed Thesauri", "context": "Information retrieval systems traditionally rely on textual keywords to index and retrieve documents. Keyword-based retrieval may return inaccurate and incomplete results when different keywords are used to describe the same concept in the documents and in the queries. Furthermore, the relationship between these related keywords may be semantic rather than syntactic, and capturing it thus requires access to comprehensive human world knowledge. Concept-based retrieval methods have attempted to tackle these difficulties by using manually built thesauri, by relying on term cooccurrence data, or by extracting latent word relationships and concepts from a corpus. In this article we introduce a new concept-based retrieval approach based on Explicit Semantic Analysis (ESA), a recently proposed method that augments keyword-based text representation with concept-based features, automatically extracted from massive human knowledge repositories such as Wikipedia. Our approach generates new text features automatically, and we have found that high-quality feature selection becomes crucial in this setting to make the retrieval more focused. However, due to the lack of labeled data, traditional feature selection methods cannot be used, hence we propose new methods that use self-generated labeled training data. The resulting system is evaluated on several TREC datasets, showing superior performance over previous state-of-the-art results."}
{"metadata": {"dataset": "scidocs", "query_id": "071f47b7bc5830643e31dbed82e0375bf9b26559", "doc_id": "0ef311acf523d4d0e2cc5f747a6508af2c89c5f7"}, "id": "scidocs-071f47b7bc5830643e31dbed82e0375bf9b26559-0ef311acf523d4d0e2cc5f747a6508af2c89c5f7", "question": "Ad Hoc Retrieval Experiments Using WordNet and Automatically Constructed Thesauri", "context": "Search algorithms incorporating some form of topic model have a long history in information retrieval. For example, cluster-based retrieval has been studied since the 60s and has recently produced good results in the language model framework. An approach to building topic models based on a formal generative model of documents, Latent Dirichlet Allocation (LDA), is heavily cited in the machine learning literature, but its feasibility and effectiveness in information retrieval is mostly unknown. In this paper, we study how to efficiently use LDA to improve ad-hoc retrieval. We propose an LDA-based document model within the language modeling framework, and evaluate it on several TREC collections. Gibbs sampling is employed to conduct approximate inference in LDA and the computational complexity is analyzed. We show that improvements over retrieval using cluster-based models can be obtained with reasonable efficiency."}
{"metadata": {"dataset": "scidocs", "query_id": "071f47b7bc5830643e31dbed82e0375bf9b26559", "doc_id": "59407446503d49a8cf5f5643b17502835b62f139"}, "id": "scidocs-071f47b7bc5830643e31dbed82e0375bf9b26559-59407446503d49a8cf5f5643b17502835b62f139", "question": "Ad Hoc Retrieval Experiments Using WordNet and Automatically Constructed Thesauri", "context": "This paper describes an automatic indexing procedure that uses the “IS-A” relations contained within WordNet and the set of nouns contained in a text to select a sense for each plysemous noun in the text. The result of the indexing procedure is a vector in which some of the terms represent word senses instead of word stems. Retrieval experiments comparing the effectivenss of these sense-based vectors vs. stem-based vectors show the stem-based vectors to be superior overall, although the sense-based vectors do improve the performance of some queries. The overall degradation is due in large part to the difficulty of disambiguating senses in short query statements. An analysis of these results suggests two conclusions: the IS-A links define a generalization/specialization hierarchy that is not sufficient to reliably select the correct sense of a noun from the set of fine sense distinctions in WordNet; and missing correct matches because of incorrect sense resolution has a much more deleterious effect on retrieval performance than does making spurious matches."}
{"metadata": {"dataset": "scidocs", "query_id": "071f47b7bc5830643e31dbed82e0375bf9b26559", "doc_id": "cc06ad0ed7b91c93b64d10ee4ed700c33f0d9f13"}, "id": "scidocs-071f47b7bc5830643e31dbed82e0375bf9b26559-cc06ad0ed7b91c93b64d10ee4ed700c33f0d9f13", "question": "Ad Hoc Retrieval Experiments Using WordNet and Automatically Constructed Thesauri", "context": "Using both the lesion method and functional imaging (positron emission tomography) in large cohorts of subjects investigated with the same experimental tasks, we tested the following hypotheses: (A) that the retrieval of words which denote concrete entities belonging to distinct conceptual categories depends upon partially segregated regions in higher-order cortices of the left temporal lobe; and (B) that the retrieval of conceptual knowledge pertaining to the same concrete entities also depends on partially segregated regions; however, those regions will be different from those postulated in hypothesis A, and located predominantly in the right hemisphere (the second hypothesis tested only with the lesion method). The analyses provide support for hypothesis A in that several regions outside the classical Broca and Wernicke language areas are involved in name retrieval of concrete entities, and that there is a partial segregation in the temporal lobe with respect to the conceptual category to which the entities belong, and partial support for hypothesis B in that retrieval of conceptual knowledge is partially segregated from name retrieval in the lesion study. Those regions identified here are seen as parts of flexible, multi-component systems serving concept and word retrieval for concrete entities belonging to different conceptual categories. By comparing different approaches the article also addresses a number of method issues that have surfaced in recent studies in this field."}
{"metadata": {"dataset": "scidocs", "query_id": "071f47b7bc5830643e31dbed82e0375bf9b26559", "doc_id": "123e98412253f2b8a602a0d699a5f493e17c62ef"}, "id": "scidocs-071f47b7bc5830643e31dbed82e0375bf9b26559-123e98412253f2b8a602a0d699a5f493e17c62ef", "question": "Ad Hoc Retrieval Experiments Using WordNet and Automatically Constructed Thesauri", "context": "We report on a system for automatically generating and displaying crosswords from a system manager supplied database of potential clues and corresponding words that index those clues. The system relies on the lexical relations encoded in WordNet to enhance the aesthetics of the resulting crossword by making it easier to automatically identify a grid that may be populated with words and clues that have a thematic focus. The system architecture is provided in overview, as is empirical evaluation."}
{"metadata": {"dataset": "scidocs", "query_id": "071f47b7bc5830643e31dbed82e0375bf9b26559", "doc_id": "e50a316f97c9a405aa000d883a633bd5707f1a34"}, "id": "scidocs-071f47b7bc5830643e31dbed82e0375bf9b26559-e50a316f97c9a405aa000d883a633bd5707f1a34", "question": "Ad Hoc Retrieval Experiments Using WordNet and Automatically Constructed Thesauri", "context": "The experimental evidence accumulated over the past 20 years indicates that text indexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective termweighting systems. This article summarizes the insights gained in automatic term weighting, and provides baseline single-term-indexing models with which other more elaborate content analysis procedures can be compared. 1. AUTOMATIC TEXT ANALYSIS In the late 195Os, Luhn [l] first suggested that automatic text retrieval systems could be designed based on a comparison of content identifiers attached both to the stored texts and to the users’ information queries. Typically, certain words extracted from the texts of documents and queries would be used for content identification; alternatively, the content representations could be chosen manually by trained indexers familiar with the subject areas under consideration and with the contents of the document collections. In either case, the documents would be represented by term vectors of the form D= (ti,tj,...ytp) (1) where each tk identifies a content term assigned to some sample document D. Analogously, the information requests, or queries, would be represented either in vector form, or in the form of Boolean statements. Thus, a typical query Q might be formulated as Q = (qa,qbr.. . ,4r) (2)"}
{"metadata": {"dataset": "scidocs", "query_id": "07a5c4ba84268708146aa4bf5cad9491b3e35051", "doc_id": "no"}, "id": "scidocs-07a5c4ba84268708146aa4bf5cad9491b3e35051-no", "question": "Deep Reinforcement Learning for Dialogue Generation", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "07a5c4ba84268708146aa4bf5cad9491b3e35051", "doc_id": "22ca497e24466737981f9ca1690d6c712b7e1276"}, "id": "scidocs-07a5c4ba84268708146aa4bf5cad9491b3e35051-22ca497e24466737981f9ca1690d6c712b7e1276", "question": "Deep Reinforcement Learning for Dialogue Generation", "context": "Recently, a number of authors have proposed treating dialog ue systems as Markov decision processes (MDPs). However, the practical applica tion of MDP algorithms to dialogue systems faces a number of severe technical chall enges. We have built a general software tool (RLDS, for Reinforcement Learning fo r Dialogue Systems) based on the MDP framework, and have applied it to dialogue co rpora gathered from two dialogue systems built at AT&T Labs. Our experiment s demonstrate that RLDS holds promise as a tool for “browsing” and understandin g correlations in complex, temporally dependent dialogue corpora."}
{"metadata": {"dataset": "scidocs", "query_id": "07a5c4ba84268708146aa4bf5cad9491b3e35051", "doc_id": "265a32d3e5a55140389df0a0b666ac5c2dfaa0bd"}, "id": "scidocs-07a5c4ba84268708146aa4bf5cad9491b3e35051-265a32d3e5a55140389df0a0b666ac5c2dfaa0bd", "question": "Deep Reinforcement Learning for Dialogue Generation", "context": "Learning from sparse and delayed reward is a central issue in reinforcement learning. In this paper, to tackle reward sparseness problem of task oriented dialogue management, we propose a curriculum based approach on the number of slots of user goals. This curriculum makes it possible to learn dialogue management for sets of user goals with large number of slots. We also propose a dialogue policy based on progressive neural networks whose modules with parameters are appended with previous parameters fixed as the curriculum proceeds, and this policy improves performances over the one with single set of parameters."}
{"metadata": {"dataset": "scidocs", "query_id": "07a5c4ba84268708146aa4bf5cad9491b3e35051", "doc_id": "3d0e07fc681428f230d1be6a79c5b40450e2e332"}, "id": "scidocs-07a5c4ba84268708146aa4bf5cad9491b3e35051-3d0e07fc681428f230d1be6a79c5b40450e2e332", "question": "Deep Reinforcement Learning for Dialogue Generation", "context": "End-to-end neural models show great promise towards building conversational agents that are trained from data and on-line experience using supervised and reinforcement learning. However, these models require a large corpus of dialogues to learn effectively. For goal-oriented dialogues, such datasets are expensive to collect and annotate, since each task involves a separate schema and database of entities. Further, the Wizard-of-Oz approach commonly used for dialogue collection does not provide sufficient coverage of salient dialogue flows, which is critical for guaranteeing an acceptable task completion rate in consumer-facing conversational agents. In this paper, we study a recently proposed approach for building an agent for arbitrary tasks by combining dialogue self-play and crowd-sourcing to generate fully-annotated dialogues with diverse and natural utterances. We discuss the advantages of this approach for industry applications of conversational agents, wherein an agent can be rapidly bootstrapped to deploy in front of users and further optimized via interactive learning from actual users of the system."}
{"metadata": {"dataset": "scidocs", "query_id": "07a5c4ba84268708146aa4bf5cad9491b3e35051", "doc_id": "754cd9d2853ead8610ef6949cf3e6e6a48e69168"}, "id": "scidocs-07a5c4ba84268708146aa4bf5cad9491b3e35051-754cd9d2853ead8610ef6949cf3e6e6a48e69168", "question": "Deep Reinforcement Learning for Dialogue Generation", "context": "Researchers have recently started investigating deep neural networks for dialogue applications. In particular, generative sequence-to-sequence (Seq2Seq) models have shown promising results for unstructured tasks, such as word-level dialogue response generation. The hope is that such models will be able to leverage massive amounts of data to learn meaningful natural language representations and response generation strategies, while requiring a minimum amount of domain knowledge and hand-crafting. An important challenge is to develop models that can effectively incorporate dialogue context and generate meaningful and diverse responses. In support of this goal, we review recently proposed models based on generative encoder-decoder neural network architectures, and show that these models have better ability to incorporate long-term dialogue history, to model uncertainty and ambiguity in dialogue, and to generate responses with high-level compositional structure."}
{"metadata": {"dataset": "scidocs", "query_id": "07a5c4ba84268708146aa4bf5cad9491b3e35051", "doc_id": "0c9ba3329d6ec82ae581cde268614abd0313fdeb"}, "id": "scidocs-07a5c4ba84268708146aa4bf5cad9491b3e35051-0c9ba3329d6ec82ae581cde268614abd0313fdeb", "question": "Deep Reinforcement Learning for Dialogue Generation", "context": "Designing the dialogue policy of a spoken dialogue system involves many nontrivial choices. This paper presents a reinforcement learning approach for automatically optimizing a dialogue policy, which addresses the technical challenges in applying reinforcement learning to a working dialogue system with human users. We report on the design, construction and empirical evaluation of NJFun, an experimental spoken dialogue system that provides users with access to information about fun things to do in New Jersey. Our results show that by optimizing its performance via reinforcement learning, NJFun measurably improves system performance."}
{"metadata": {"dataset": "scidocs", "query_id": "07a5c4ba84268708146aa4bf5cad9491b3e35051", "doc_id": "4761f0fd96284103ecd3603349c7f8078ad28676"}, "id": "scidocs-07a5c4ba84268708146aa4bf5cad9491b3e35051-4761f0fd96284103ecd3603349c7f8078ad28676", "question": "Deep Reinforcement Learning for Dialogue Generation", "context": "In this paper, drawing intuition from the Turing test, we propose using adversarial training for open-domain dialogue generation: the system is trained to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning (RL) problem where we jointly train two systems, a generative model to produce response sequences, and a discriminator—analagous to the human evaluator in the Turing test— to distinguish between the human-generated dialogues and the machine-generated ones. The outputs from the discriminator are then used as rewards for the generative model, pushing the system to generate dialogues that mostly resemble human dialogues. In addition to adversarial training we describe a model for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines."}
{"metadata": {"dataset": "scidocs", "query_id": "07a5c4ba84268708146aa4bf5cad9491b3e35051", "doc_id": "5293778b5139599368dab36f6fa4e49e4c25df45"}, "id": "scidocs-07a5c4ba84268708146aa4bf5cad9491b3e35051-5293778b5139599368dab36f6fa4e49e4c25df45", "question": "Deep Reinforcement Learning for Dialogue Generation", "context": "Natural language generation lies at the core of generative dialogue systems and conversational agents. We describe an ensemble neural language generator, and present several novel methods for data representation and augmentation that yield improved results in our model. We test the model on three datasets in the restaurant, TV and laptop domains, and report both objective and subjective evaluations of our best model. Using a range of automatic metrics, as well as human evaluators, we show that our approach achieves better results than state-of-the-art models on the same datasets."}
{"metadata": {"dataset": "scidocs", "query_id": "07a5c4ba84268708146aa4bf5cad9491b3e35051", "doc_id": "2402066417256a70d7bf36ee163af5eba0aed211"}, "id": "scidocs-07a5c4ba84268708146aa4bf5cad9491b3e35051-2402066417256a70d7bf36ee163af5eba0aed211", "question": "Deep Reinforcement Learning for Dialogue Generation", "context": "The natural language generation (NLG) component of a spoken dialogue system (SDS) usually needs a substantial amount of handcrafting or a well-labeled dataset to be trained on. These limitations add significantly to development costs and make cross-domain, multi-lingual dialogue systems intractable. Moreover, human languages are context-aware. The most natural response should be directly learned from data rather than depending on predefined syntaxes or rules. This paper presents a statistical language generator based on a joint recurrent and convolutional neural network structure which can be trained on dialogue act-utterance pairs without any semantic alignments or predefined grammar trees. Objective metrics suggest that this new model outperforms previous methods under the same experimental conditions. Results of an evaluation by human judges indicate that it produces not only high quality but linguistically varied utterances which are preferred compared to n-gram and rule-based systems."}
{"metadata": {"dataset": "scidocs", "query_id": "07a5c4ba84268708146aa4bf5cad9491b3e35051", "doc_id": "66c54b8ba52a6eae6727354dedaacbab1dd5a8ea"}, "id": "scidocs-07a5c4ba84268708146aa4bf5cad9491b3e35051-66c54b8ba52a6eae6727354dedaacbab1dd5a8ea", "question": "Deep Reinforcement Learning for Dialogue Generation", "context": "Until recently, the goal of developing opendomain dialogue systems that not only emulate human conversation but fulfill complex tasks, such as travel planning, seemed elusive. However, we start to observe promising results in the last few years as the large amount of conversation data is available for training and the breakthroughs in deep learning and reinforcement learning are applied to dialogue. In this tutorial, we start with a brief introduction to the history of dialogue research. Then, we describe in detail the deep learning and reinforcement learning technologies that have been developed for two types of dialogue systems. First is a task-oriented dialogue system that can help users accomplish tasks, ranging from meeting scheduling to vacation planning. Second is a social bot that can converse seamlessly and appropriately with humans. In the final part of the tutorial, we review attempts to developing opendomain neural dialogue systems by combining the strengths of task-oriented dialogue systems and social bots. The tutorial material is available at http://opendialogue.miulab.tw."}
{"metadata": {"dataset": "scidocs", "query_id": "07a5c4ba84268708146aa4bf5cad9491b3e35051", "doc_id": "4637f6625e7f40d3984c8a2a7a76bbd64f47bc04"}, "id": "scidocs-07a5c4ba84268708146aa4bf5cad9491b3e35051-4637f6625e7f40d3984c8a2a7a76bbd64f47bc04", "question": "Deep Reinforcement Learning for Dialogue Generation", "context": "The ability to compute an accurate reward function is essential for optimising a dialogue policy via reinforcement learning. In real-world applications, using explicit user feedback as the reward signal is often unreliable and costly to collect. This problem can be mitigated if the user’s intent is known in advance or data is available to pre-train a task success predictor off-line. In practice neither of these apply for most real world applications. Here we propose an on-line learning framework whereby the dialogue policy is jointly trained alongside the reward model via active learning with a Gaussian process model. This Gaussian process operates on a continuous space dialogue representation generated in an unsupervised fashion using a recurrent neural network encoder-decoder. The experimental results demonstrate that the proposed framework is able to significantly reduce data annotation costs and mitigate noisy user feedback in dialogue policy learning."}
{"metadata": {"dataset": "scidocs", "query_id": "08c970df2d62d4bc27e65e8c389a0227a41109a5", "doc_id": "no"}, "id": "scidocs-08c970df2d62d4bc27e65e8c389a0227a41109a5-no", "question": "Regression testing of GUIs", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "08c970df2d62d4bc27e65e8c389a0227a41109a5", "doc_id": "1a5214cdb88ca0c4f276d8c4e5797d19c662b8a4"}, "id": "scidocs-08c970df2d62d4bc27e65e8c389a0227a41109a5-1a5214cdb88ca0c4f276d8c4e5797d19c662b8a4", "question": "Regression testing of GUIs", "context": "The widespread recognition of the usefulness of graphical user interfaces GUIs has established their importance as critical components of today's software. Although the use of GUIs continues to grow, GUI testing has remained a neglected research area. Since GUIs have c haracteristics that are diierent from those of conventional software, such as user events for input and graphical output, techniques developed to test conventional software cannot bedirectly applied to test GUIs. This thesis develops a uniied solution to the GUI testing problem with the particular goals of automation and integration of tools and techniques used in various phases of GUI testing. These goals are accomplished by developing a GUI testing framework with a GUI model as its central component. For eeciency and scalability, a GUI is represented as a hierarchy of components, each used as a basic unit of testing. The framework also includes a test coverage evaluator, test case generator, test oracle, test executor, and regression tester. The test coverage evaluator employs hierarchical, event-based coverage criteria to automatically specify what to test in a GUI and to determine whether the test suite has adequately tested the GUI. The test case generator employs plan generation techniques from artiicial intelligence to automatically generate a test suite. A test executor automatically executes all the test cases on the GUI. As test cases are being executed, a test oracle automatically determines the correctness of the GUI. The test oracle employs a model of the expected state of the GUI in terms of its constituent objects and their properties. After changes are made to a GUI, a regression tester partitions the original GUI test suite into valid test cases that represent correct inputtoutput for the modiied GUI and invalid test cases that no longer represent correct inputtoutput. The regression tester employs a new technique to reuse some of the invalid test cases by repairing them. A cursory exploration of extending the framework to handle the new testing requirements of web-user interfaces WUIs is also done. The framework iv has been implemented and experiments have demonstrated that the developed techniques are both practical and useful. v Acknowledgements I w ould like to thank my parents whose constant eeorts, encouragement and hard work made achieving the goal of obtaining a Ph.D. possible. I thank all my teachers in schools, colleges, and universities whose dedication and hard work helped lay the foundation for this work. Special thanks …"}
{"metadata": {"dataset": "scidocs", "query_id": "08c970df2d62d4bc27e65e8c389a0227a41109a5", "doc_id": "6f098bda64fbd59215a3e9686306b4dfb7ed3ac7"}, "id": "scidocs-08c970df2d62d4bc27e65e8c389a0227a41109a5-6f098bda64fbd59215a3e9686306b4dfb7ed3ac7", "question": "Regression testing of GUIs", "context": "A widespread recognition of the usefulness of graphical user interfaces (GUIs) has established their importance as critical components of today's software. GUIs have characteristics different from traditional software, and conventional testing techniques do not directly apply to GUIs. This paper's focus is on coverage critieria for GUIs, important rules that provide an objective measure of test quality. We present new coverage criteria to help determine whether a GUI has been adequately tested. These coverage criteria use events and event sequences to specify a measure of test adequacy. Since the total number of permutations of event sequences in any non-trivial GUI is extremely large, the GUI's hierarchical structure is exploited to identify the important event sequences to be tested. A GUI is decomposed into GUI components, each of which is used as a basic unit of testing. A representation of a GUI component, called an event-flow graph, identifies the interaction of events within a component and intra-component criteria are used to evaluate the adequacy of tests on these events. The hierarchical relationship among components is represented by an integration tree, and inter-component coverage criteria are used to evaluate the adequacy of test sequences that cross components. Algorithms are given to construct event-flow graphs and an integration tree for a given GUI, and to evaluate the coverage of a given test suite with respect to the new coverage criteria. A case study illustrates the usefulness of the coverage report to guide further testing and an important correlation between event-based coverage of a GUI and statement coverage of its software's underlying code."}
{"metadata": {"dataset": "scidocs", "query_id": "08c970df2d62d4bc27e65e8c389a0227a41109a5", "doc_id": "d2ae393cf723228cf6f96d61ee068c681203e943"}, "id": "scidocs-08c970df2d62d4bc27e65e8c389a0227a41109a5-d2ae393cf723228cf6f96d61ee068c681203e943", "question": "Regression testing of GUIs", "context": "Graphical User Interfaces (GUIs) can be found in almost all modern desktop, tablet and smartphone applications. Since they are the glue between an application’s components, they lend themselves to system level testing. Unfortunately, although many tools promise automation, GUI testing is still an inherently difficult task and involves great manual labor. However, tests that aim at critical faults, like crashes and excessive response times, are completely automatable and can be very effective. These robustness tests often apply random algorithms to select the actions to be executed on the GUI. This paper proposes a new approach to fully automated robustness testing of complex GUI applications with the goal to enhance the fault finding capabilities. The approach uses a well-known machine learning algorithm called Q-Learning in order to combine the advantages of random and coverage-based testing. We will explain how it operates, how we plan to implement it and provide arguments for its usefulness."}
{"metadata": {"dataset": "scidocs", "query_id": "08c970df2d62d4bc27e65e8c389a0227a41109a5", "doc_id": "caf43effcd2f115aad2a57b1c0fe93700fa9e60a"}, "id": "scidocs-08c970df2d62d4bc27e65e8c389a0227a41109a5-caf43effcd2f115aad2a57b1c0fe93700fa9e60a", "question": "Regression testing of GUIs", "context": "Effective system testing of applications with a Graphical User Interface (GUI) front-end demands careful generation of event sequences as well as providing relevant test data for parameterized widgets, i.e., widgets that accept input values such as textboxes and textareas. Current GUI testing techniques either manipulate the source code of the application under test (AUT) to generate the test data, or blindly use a set of random string values. In this paper, we propose a third novel way to generate relevant test data for GUI testing. We exploit the information provided in the GUI structure to extract a set of key identifiers for each parameterized widget. These identifiers are used to compose appropriate search phrases and collect relevant test data from the Internet. The results of an empirical study on five GUI-based applications show that the proposed approach is applicable and can get some hard-to-cover branches in the subject programs to execute. The proposed technique works from the black-box perspective and is entirely independent from GUI modeling and event sequence generation, thus it does not need access to the source code of AUT and provides an opportunity to be integrated with the existing GUI testing frameworks."}
{"metadata": {"dataset": "scidocs", "query_id": "08c970df2d62d4bc27e65e8c389a0227a41109a5", "doc_id": "56b9100895e9855c2d9e72f81bb5933ac8a5c17c"}, "id": "scidocs-08c970df2d62d4bc27e65e8c389a0227a41109a5-56b9100895e9855c2d9e72f81bb5933ac8a5c17c", "question": "Regression testing of GUIs", "context": "When software is modified, during development and maintenance, it is <i>regression tested</i> to provide confidence that the changes did not introduce unexpected errors and that new features behave as expected. One important problem in regression testing is how to select a subset of test cases, from the test suite used for the original version of the software, when testing a modified version of the software. Regression-test-selection techniques address this problem. Safe regression-test-selection techniques select every test case in the test suite that may behave differently in the original and modified versions of the software. Among existing safe regression testing techniques, efficient techniques are often too imprecise and achieve little savings in testing effort, whereas precise techniques are too expensive when used on large systems. This paper presents a new regression-test-selection technique for Java programs that is safe, precise, and yet scales to large systems. It also presents a tool that implements the technique and studies performed on a set of subjects ranging from 70 to over 500 KLOC. The studies show that our technique can efficiently reduce the regression testing effort and, thus, achieve considerable savings."}
{"metadata": {"dataset": "scidocs", "query_id": "08c970df2d62d4bc27e65e8c389a0227a41109a5", "doc_id": "833723cbc2d1930d7e002acd882fda73152b213c"}, "id": "scidocs-08c970df2d62d4bc27e65e8c389a0227a41109a5-833723cbc2d1930d7e002acd882fda73152b213c", "question": "Regression testing of GUIs", "context": "Regression testing is a necessary but costly maintenance activity aimed at demonstrating that code has not been adversely aaected by changes. A selective approach to regression testing selects tests for a modi-ed program from an existing test suite. We present a new technique for selective regression testing. Our algorithm constructs control dependence graphs for program versions, and uses these graphs to determine which tests from the existing test suite may exhibit changed behavior on the new version. Unlike most previous techniques for selective retest, our algorithm selects every test from the original test suite that might expose errors in the modiied program, and does this without prior knowledge of program modiications. Our algorithm handles all language constructs and program modiications, and is easily automated."}
{"metadata": {"dataset": "scidocs", "query_id": "08c970df2d62d4bc27e65e8c389a0227a41109a5", "doc_id": "54f52815972d02c0a67ea0d049f4922e05ba465d"}, "id": "scidocs-08c970df2d62d4bc27e65e8c389a0227a41109a5-54f52815972d02c0a67ea0d049f4922e05ba465d", "question": "Regression testing of GUIs", "context": "Regression testing is an important activity performed to validate modified software, and one of its key tasks is regression test selection (RTS) -- selecting a subset of existing test cases to run on the modified software. Most existing RTS techniques focus on changes made to code components and completely ignore non-code elements, such as configuration files and databases, which can also change and affect the system behavior. To address this issue, we present a new RTS technique that performs accurate test selection in the presence of changes to non-code components. To do this, our technique computes traceability between test cases and the external data accessed by an application, and uses this information to perform RTS in the presence of changes to non-code elements. We present our technique, a prototype implementation of our technique, and a set of preliminary empirical results that illustrate the feasibility, effectiveness, and potential usefulness of our approach."}
{"metadata": {"dataset": "scidocs", "query_id": "08c970df2d62d4bc27e65e8c389a0227a41109a5", "doc_id": "2d4abf7523cda78e39029c46b19cbae74e7ee31b"}, "id": "scidocs-08c970df2d62d4bc27e65e8c389a0227a41109a5-2d4abf7523cda78e39029c46b19cbae74e7ee31b", "question": "Regression testing of GUIs", "context": "Regression testing is an expensive but necessary maintenance activity performed on modified software to provide confidence that changes are correct and do not adversely affect other portions of the softwore. A regression test selection technique choses, from an existing test set, thests that are deemed necessary to validate modified software. We present a new technique for regression test selection. Our algorithms construct control flow graphs for a precedure or program and its modified version and use these graphs to select tests that execute changed code from the original test suite. We prove that, under certain conditions, the set of tests our technique selects includes every test from the original test suite that con expose faults in the modified procedfdure or program. Under these conditions our algorithms are safe. Moreover, although our algorithms may select some tests that cannot expose faults, they are at lease as precise as other safe regression test selection algorithms. Unlike many other regression test selection algorithms, our algorithms handle all language constructs and all types of program modifications. We have implemented our algorithms; initial empirical studies indicate that our technique can significantly reduce the cost of regression testing modified software."}
{"metadata": {"dataset": "scidocs", "query_id": "08c970df2d62d4bc27e65e8c389a0227a41109a5", "doc_id": "2e0305d97f2936ee2a87b87ea901d500a8fbcd16"}, "id": "scidocs-08c970df2d62d4bc27e65e8c389a0227a41109a5-2e0305d97f2936ee2a87b87ea901d500a8fbcd16", "question": "Regression testing of GUIs", "context": "Abstract Gaussian Process (GP) regression models typically assume that residuals are Gaussian and have the same variance for all observations. However, applications with input-dependent noise (heteroscedastic residuals) frequently arise in practice, as do applications in which the residuals do not have a Gaussian distribution. In this paper, we propose a GP Regression model with a latent variable that serves as an additional unobserved covariate for the regression. This model (which we call GPLC) allows for heteroscedasticity since it allows the function to have a changing partial derivative with respect to this unobserved covariate. With a suitable covariance function, our GPLC model can handle (a) Gaussian residuals with input-dependent variance, or (b) nonGaussian residuals with input-dependent variance, or (c) Gaussian residuals with constant variance. We compare our model, using synthetic datasets, with a model proposed by Goldberg, Williams and Bishop (1998), which we refer to as GPLV, which only deals with case (a), as well as a standard GP model which can handle only case (c). Markov Chain Monte Carlo methods are developed for both modelsl. Experiments show that when the data is heteroscedastic, both GPLC and GPLV give better results (smaller mean squared error and negative log-probability density) than standard GP regression. In addition, when the residual are Gaussian, our GPLC model is generally nearly as good as GPLV, while when the residuals are non-Gaussian, our GPLC model is better than GPLV."}
{"metadata": {"dataset": "scidocs", "query_id": "08c970df2d62d4bc27e65e8c389a0227a41109a5", "doc_id": "5fd16142d0c3e0ae69da838dccad42aaea1a1745"}, "id": "scidocs-08c970df2d62d4bc27e65e8c389a0227a41109a5-5fd16142d0c3e0ae69da838dccad42aaea1a1745", "question": "Regression testing of GUIs", "context": "Context. Software testing is the process of finding faults in software while executing it. The results of the testing are used to find and correct faults. Software defect prediction estimates where faults are likely to occur in source code. The results from the defect prediction can be used to optimize testing and ultimately improve software quality. Machine learning, that concerns computer programs learning from data, is used to build prediction models which then can be used to classify data. Objectives. In this study we, in collaboration with Ericsson, investigated whether software metrics from source code files combined with metrics from their respective tests predicts faults with better prediction performance compared to using only metrics from the source code files. Methods. A literature review was conducted to identify inputs for an experiment. The experiment was applied on one repository from Ericsson to identify the best performing set of metrics. Results. The prediction performance results of three metric sets are presented and compared with each other. Wilcoxon’s signed rank tests are performed on four different performance measures for each metric set and each machine learning algorithm to demonstrate significant differences of the results. Conclusions. We conclude that metrics from tests can be used to predict faults. However, the combination of source code metrics and test metrics do not outperform using only source code metrics. Moreover, we conclude that models built with metrics from the test metric set with minimal information of the source code can in fact predict faults in the source code."}
{"metadata": {"dataset": "scidocs", "query_id": "08eeaae7108e35a9639ef750a75132d0c71b2dd1", "doc_id": "no"}, "id": "scidocs-08eeaae7108e35a9639ef750a75132d0c71b2dd1-no", "question": "Link Prediction using Supervised Learning ∗", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "08eeaae7108e35a9639ef750a75132d0c71b2dd1", "doc_id": "00d23e5c06f90bed0c9d4aec22babb2f7488817f"}, "id": "scidocs-08eeaae7108e35a9639ef750a75132d0c71b2dd1-00d23e5c06f90bed0c9d4aec22babb2f7488817f", "question": "Link Prediction using Supervised Learning ∗", "context": "We propose to solve the link prediction problem in graphs using a supervised matrix factorization approach. The model learns latent features from the topological structure of a (possibly directed) graph, and is shown to make better predictions than popular unsupervised scores. We show how these latent features may be combined with optional explicit features for nodes or edges, which yields better performance than using either type of feature exclusively. Finally, we propose a novel approach to address the class imbalance problem which is common in link prediction by directly optimizing for a ranking loss. Our model is optimized with stochastic gradient descent and scales to large graphs. Results on several datasets show the efficacy of our approach."}
{"metadata": {"dataset": "scidocs", "query_id": "08eeaae7108e35a9639ef750a75132d0c71b2dd1", "doc_id": "2e4c06dd00c4c09ad5ac6be883cc66c19d88ea79"}, "id": "scidocs-08eeaae7108e35a9639ef750a75132d0c71b2dd1-2e4c06dd00c4c09ad5ac6be883cc66c19d88ea79", "question": "Link Prediction using Supervised Learning ∗", "context": "We examine two fundamental tasks associated with graph representation learning: link prediction and semi-supervised node classification. We present a novel autoencoder architecture capable of learning a joint representation of both local graph structure and available node features for the multi-task learning of link prediction and node classification. Our autoencoder architecture is efficiently trained end-to-end in a single learning stage to simultaneously perform link prediction and node classification, whereas previous related methods require multiple training steps that are difficult to optimize. We provide a comprehensive empirical evaluation of our models on nine benchmark graph-structured datasets and demonstrate significant improvement over related methods for graph representation learning. Reference code and data are available at https://github.com/vuptran/graph-representation-learning."}
{"metadata": {"dataset": "scidocs", "query_id": "08eeaae7108e35a9639ef750a75132d0c71b2dd1", "doc_id": "7d3cf8315cfb586dae984b0aa986ea0857c18cf0"}, "id": "scidocs-08eeaae7108e35a9639ef750a75132d0c71b2dd1-7d3cf8315cfb586dae984b0aa986ea0857c18cf0", "question": "Link Prediction using Supervised Learning ∗", "context": "One of the core tasks in social network analysis is to predict the formation of links (i.e. various types of relationships) over time. Previous research has generally represented the social network in the form of a graph and has leveraged topological and semantic measures of similarity between two nodes to evaluate the probability of link formation. Here we introduce a novel local probabilistic graphical model method that can scale to large graphs to estimate the joint co-occurrence probability of two nodes. Such a probability measure captures information that is not captured by either topological measures or measures of semantic similarity, which are the dominant measures used for link prediction. We demonstrate the effectiveness of the co-occurrence probability feature by using it both in isolation and in combination with other topological and semantic features for predicting co-authorship collaborations on real datasets."}
{"metadata": {"dataset": "scidocs", "query_id": "08eeaae7108e35a9639ef750a75132d0c71b2dd1", "doc_id": "a190742917469c60e96e401140a73de5548be360"}, "id": "scidocs-08eeaae7108e35a9639ef750a75132d0c71b2dd1-a190742917469c60e96e401140a73de5548be360", "question": "Link Prediction using Supervised Learning ∗", "context": "Many real-world complex networks, like client-product or file-provider relations, have a bipartite nature and evolve during time. Predicting links that will appear in them is one of the main approach to understand their dynamics. Only few works address the bipartite case, though, despite its high practical interest and the specific challenges it raises. We define in this paper the notion of internal links in bipartite graphs and propose a link prediction method based on them. We describe the method and experimentally compare it to a basic collaborative filtering approach. We present results obtained for two typical practical cases. We reach the conclusion that our method performs very well, and that internal links play an important role in bipartite graphs and their dynamics."}
{"metadata": {"dataset": "scidocs", "query_id": "08eeaae7108e35a9639ef750a75132d0c71b2dd1", "doc_id": "0a6d7e8e61c54c796f53120fdb86a25177e00998"}, "id": "scidocs-08eeaae7108e35a9639ef750a75132d0c71b2dd1-0a6d7e8e61c54c796f53120fdb86a25177e00998", "question": "Link Prediction using Supervised Learning ∗", "context": "In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.1"}
{"metadata": {"dataset": "scidocs", "query_id": "08eeaae7108e35a9639ef750a75132d0c71b2dd1", "doc_id": "45cb1ff9e1221b52ba3c26e33e98550f6117ae5a"}, "id": "scidocs-08eeaae7108e35a9639ef750a75132d0c71b2dd1-45cb1ff9e1221b52ba3c26e33e98550f6117ae5a", "question": "Link Prediction using Supervised Learning ∗", "context": "In some online social network services (SNSs), the members are allowed to label their relationships with others, and such relationships can be represented as the links with signed values (positive or negative). The networks containing such relations are named signed social networks (SSNs), and some real-world complex systems can be also modeled with SSNs. Given the information of the observed structure of an SSN, the link prediction aims to estimate the values of the unobserved links. Noticing that most of the previous approaches for link prediction are based on the members’ similarity and the supervised learning method, however, research work on the investigation of the hidden principles that drive the behaviors of social members are rarely conducted. In this paper, the deep belief network (DBN)-based approaches for link prediction are proposed. Including an unsupervised link prediction model, a feature representation method and a DBN-based link prediction method are introduced. The experiments are done on the datasets from three SNSs (social networking services) in different domains, and the results show that our methods can predict the values of the links with high performance and have a good generalization ability across these datasets."}
{"metadata": {"dataset": "scidocs", "query_id": "08eeaae7108e35a9639ef750a75132d0c71b2dd1", "doc_id": "ee742cdcec6fb80fda256c7202ffc3e7e2b34f4f"}, "id": "scidocs-08eeaae7108e35a9639ef750a75132d0c71b2dd1-ee742cdcec6fb80fda256c7202ffc3e7e2b34f4f", "question": "Link Prediction using Supervised Learning ∗", "context": ""}
{"metadata": {"dataset": "scidocs", "query_id": "08eeaae7108e35a9639ef750a75132d0c71b2dd1", "doc_id": "2c424f21607ff6c92e640bfe3da9ff105c08fac4"}, "id": "scidocs-08eeaae7108e35a9639ef750a75132d0c71b2dd1-2c424f21607ff6c92e640bfe3da9ff105c08fac4", "question": "Link Prediction using Supervised Learning ∗", "context": "Supervised deep learning has been successfully applied to many recognition problems. Although it can approximate a complex many-to-one function well when a large amount of training data is provided, it is still challenging to model complex structured output representations that effectively perform probabilistic inference and make diverse predictions. In this work, we develop a deep conditional generative model for structured output prediction using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows for fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build robust structured prediction algorithms, such as input noise-injection and multi-scale prediction objective at training. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic structured output predictions using stochastic inference. Furthermore, the proposed training methods are complimentary, which leads to strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset."}
{"metadata": {"dataset": "scidocs", "query_id": "08eeaae7108e35a9639ef750a75132d0c71b2dd1", "doc_id": "77f8a60da1af42112e49d3b1b32243123066e09e"}, "id": "scidocs-08eeaae7108e35a9639ef750a75132d0c71b2dd1-77f8a60da1af42112e49d3b1b32243123066e09e", "question": "Link Prediction using Supervised Learning ∗", "context": "Many criteria can be used to evaluate the performance of supervised learning. Different criteria are appropriate in different settings, and it is not always clear which criteria to use. A further complication is that learning methods that perform well on one criterion may not perform well on other criteria. For example, SVMs and boosting are designed to optimize accuracy, whereas neural nets typically optimize squared error or cross entropy. We conducted an empirical study using a variety of learning methods (SVMs, neural nets, k-nearest neighbor, bagged and boosted trees, and boosted stumps) to compare nine boolean classification performance metrics: Accuracy, Lift, F-Score, Area under the ROC Curve, Average Precision, Precision/Recall Break-Even Point, Squared Error, Cross Entropy, and Probability Calibration. Multidimensional scaling (MDS) shows that these metrics span a low dimensional manifold. The three metrics that are appropriate when predictions are interpreted as probabilities: squared error, cross entropy, and calibration, lay in one part of metric space far away from metrics that depend on the relative order of the predicted values: ROC area, average precision, break-even point, and lift. In between them fall two metrics that depend on comparing predictions to a threshold: accuracy and F-score. As expected, maximum margin methods such as SVMs and boosted trees have excellent performance on metrics like accuracy, but perform poorly on probability metrics such as squared error. What was not expected was that the margin methods have excellent performance on ordering metrics such as ROC area and average precision. We introduce a new metric, SAR, that combines squared error, accuracy, and ROC area into one metric. MDS and correlation analysis shows that SAR is centrally located and correlates well with other metrics, suggesting that it is a good general purpose metric to use when more specific criteria are not known."}
{"metadata": {"dataset": "scidocs", "query_id": "08eeaae7108e35a9639ef750a75132d0c71b2dd1", "doc_id": "109f6c2ea18058cca7b9cc734b30dea6dd64cac0"}, "id": "scidocs-08eeaae7108e35a9639ef750a75132d0c71b2dd1-109f6c2ea18058cca7b9cc734b30dea6dd64cac0", "question": "Link Prediction using Supervised Learning ∗", "context": "Prediction of short-term traffic flow has become one of the major research fields in intelligent transportation systems. Accurately estimated traffic flow forecasts are important for operating effective and proactive traffic management systems in the context of dynamic traffic assignment. For predicting short-term traffic flows, recent traffic information is clearly a more significant indicator of the near-future traffic flow. In other words, the relative significance depending on the time difference between traffic flow data should be considered. Although there have been several research works for short-term traffic flow predictions, they are offline methods. This paper presents a novel prediction model, called online learning weighted support-vector regression (OLWSVR), for short-term traffic flow predictions. The OLWSVR model is compared with several well-known prediction models, including artificial neural network models, locally weighted regression, conventional support-vector regression, and online learning support-vector regression. The results show that the performance of the proposed model is superior to that of existing models."}
{"metadata": {"dataset": "scidocs", "query_id": "09109a5375d8e2ca752bedde17ba5acad1df61cb", "doc_id": "no"}, "id": "scidocs-09109a5375d8e2ca752bedde17ba5acad1df61cb-no", "question": "Sensor fusion for semantic segmentation of urban scenes", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "09109a5375d8e2ca752bedde17ba5acad1df61cb", "doc_id": "3ecc4821d55c0e528690777be3588fc9cf023882"}, "id": "scidocs-09109a5375d8e2ca752bedde17ba5acad1df61cb-3ecc4821d55c0e528690777be3588fc9cf023882", "question": "Sensor fusion for semantic segmentation of urban scenes", "context": "For applications such as augmented reality, autonomous driving, self-localization/camera pose estimation and scene parsing are crucial technologies. In this paper, we propose a unified framework to tackle these two problems simultaneously. The uniqueness of our design is a sensor fusion scheme which integrates camera videos, motion sensors (GPS/IMU), and a 3D semantic map in order to achieve robustness and efficiency of the system. Specifically, we first have an initial coarse camera pose obtained from consumer-grade GPS/IMU, based on which a label map can be rendered from the 3D semantic map. Then, the rendered label map and the RGB image are jointly fed into a pose CNN, yielding a corrected camera pose. In addition, to incorporate temporal information, a multi-layer recurrent neural network (RNN) is further deployed improve the pose accuracy. Finally, based on the pose from RNN, we render a new label map, which is fed together with the RGB image into a segment CNN which produces perpixel semantic label. In order to validate our approach, we build a dataset with registered 3D point clouds and video camera images. Both the point clouds and the images are semantically-labeled. Each video frame has ground truth pose from highly accurate motion sensors. We show that practically, pose estimation solely relying on images like PoseNet [25] may fail due to street view confusion, and it is important to fuse multiple sensors. Finally, various ablation studies are performed, which demonstrate the effectiveness of the proposed system. In particular, we show that scene parsing and pose estimation are mutually beneficial to achieve a more robust and accurate system."}
{"metadata": {"dataset": "scidocs", "query_id": "09109a5375d8e2ca752bedde17ba5acad1df61cb", "doc_id": "000f90380d768a85e2316225854fc377c079b5c4"}, "id": "scidocs-09109a5375d8e2ca752bedde17ba5acad1df61cb-000f90380d768a85e2316225854fc377c079b5c4", "question": "Sensor fusion for semantic segmentation of urban scenes", "context": "Semantic image segmentation is an essential component of modern autonomous driving systems, as an accurate understanding of the surrounding scene is crucial to navigation and action planning. Current state-of-the-art approaches in semantic image segmentation rely on pre-trained networks that were initially developed for classifying images as a whole. While these networks exhibit outstanding recognition performance (i.e., what is visible?), they lack localization accuracy (i.e., where precisely is something located?). Therefore, additional processing steps have to be performed in order to obtain pixel-accurate segmentation masks at the full image resolution. To alleviate this problem we propose a novel ResNet-like architecture that exhibits strong localization and recognition performance. We combine multi-scale context with pixel-level accuracy by using two processing streams within our network: One stream carries information at the full image resolution, enabling precise adherence to segment boundaries. The other stream undergoes a sequence of pooling operations to obtain robust features for recognition. The two streams are coupled at the full image resolution using residuals. Without additional processing steps and without pre-training, our approach achieves an intersection-over-union score of 71.8% on the Cityscapes dataset."}
{"metadata": {"dataset": "scidocs", "query_id": "09109a5375d8e2ca752bedde17ba5acad1df61cb", "doc_id": "f3dab33e5f00e80ed4ce97d3443e96eb0ee96301"}, "id": "scidocs-09109a5375d8e2ca752bedde17ba5acad1df61cb-f3dab33e5f00e80ed4ce97d3443e96eb0ee96301", "question": "Sensor fusion for semantic segmentation of urban scenes", "context": "Robust scene understanding of outdoor environments using passive optical sensors is a onerous and essential task for autonomous navigation. The problem is heavily characterized by changing environmental conditions throughout the day and across seasons. Robots should be equipped with models that are impervious to these factors in order to be operable and more importantly to ensure safety in the real-world. In this paper, we propose a novel semantic segmentation architecture and the convoluted mixture of deep experts (CMoDE) fusion technique that enables a multi-stream deep neural network to learn features from complementary modalities and spectra, each of which are specialized in a subset of the input space. Our model adaptively weighs class-specific features of expert networks based on the scene condition and further learns fused representations to yield robust segmentation. We present results from experimentation on three publicly available datasets that contain diverse conditions including rain, summer, winter, dusk, fall, night and sunset, and show that our approach exceeds the state-of-the-art. In addition, we evaluate the performance of autonomously traversing several kilometres of a forested environment using only the segmentation for perception."}
{"metadata": {"dataset": "scidocs", "query_id": "09109a5375d8e2ca752bedde17ba5acad1df61cb", "doc_id": "9360ce51ec055c05fd0384343792c58363383952"}, "id": "scidocs-09109a5375d8e2ca752bedde17ba5acad1df61cb-9360ce51ec055c05fd0384343792c58363383952", "question": "Sensor fusion for semantic segmentation of urban scenes", "context": "In this paper we address the problem of semantic labeling of indoor scenes on RGB-D data. With the availability of RGB-D cameras, it is expected that additional depth measurement will improve the accuracy. Here we investigate a solution how to incorporate complementary depth information into a semantic segmentation framework by making use of convolutional neural networks (CNNs). Recently encoder-decoder type fully convolutional CNN architectures have achieved a great success in the field of semantic segmentation. Motivated by this observation we propose an encoder-decoder type network, where the encoder part is composed of two branches of networks that simultaneously extract features from RGB and depth images and fuse depth features into the RGB feature maps as the network goes deeper. Comprehensive experimental evaluations demonstrate that the proposed fusion-based architecture achieves competitive results with the state-of-the-art methods on the challenging SUN RGB-D benchmark obtaining 76.27% global accuracy, 48.30% average class accuracy and 37.29% average intersectionover-union score."}
{"metadata": {"dataset": "scidocs", "query_id": "09109a5375d8e2ca752bedde17ba5acad1df61cb", "doc_id": "9eacc0de031c0b7c94927bb112ab939dc81a73a5"}, "id": "scidocs-09109a5375d8e2ca752bedde17ba5acad1df61cb-9eacc0de031c0b7c94927bb112ab939dc81a73a5", "question": "Sensor fusion for semantic segmentation of urban scenes", "context": "Applications that provide location related services need to understand the environment in which humans live such that verbal references and human interaction are possible. We formulate this semantic labelling task as the problem of learning the semantic labels from the perceived 3D structure. In this contribution we propose a batch approach and a novel multi-view frame fusion technique to exploit multiple views for improving the semantic labelling results. The batch approach works offline and is the direct application of an existing single-view method to scene reconstructions with multiple views. The multi-view frame fusion works in an incremental fashion accumulating the single-view results, hence allowing the online multi-view semantic segmentation of single frames and the offline reconstruction of semantic maps. Our experiments show the superiority of the approaches based on our fusion scheme, which leads to a more accurate semantic labelling."}
{"metadata": {"dataset": "scidocs", "query_id": "09109a5375d8e2ca752bedde17ba5acad1df61cb", "doc_id": "fd7b38076f69785352d654a0665f165933d049e8"}, "id": "scidocs-09109a5375d8e2ca752bedde17ba5acad1df61cb-fd7b38076f69785352d654a0665f165933d049e8", "question": "Sensor fusion for semantic segmentation of urban scenes", "context": "Semantic segmentation, as dense pixel-wise classification task, played an important tache in scene understanding. There are two main challenges in many state-of-the-art works: 1) most backbone of segmentation models that often were extracted from pretrained classification models generated poor performance in small categories because they were lacking in spatial information and 2) the gap of combination between high-level and low-level features in segmentation models has led to inaccurate predictions. To handle these challenges, in this paper, we proposed a new tailored backbone and attention select module for segmentation tasks. Specifically, our new backbone was modified from the original Resnet, which can yield better segmentation performance. Attention select module employed spatial and channel self-attention mechanism to reinforce the propagation of contextual features, which can aggregate semantic and spatial information simultaneously. In addition, based on our new backbone and attention select module, we further proposed our segmentation model for street scenes understanding. We conducted a series of ablation studies on two public benchmarks, including Cityscapes and CamVid dataset to demonstrate the effectiveness of our proposals. Our model achieved a mIoU score of 71.5% on the Cityscapes test set with only fine annotation data and 60.1% on the CamVid test set."}
{"metadata": {"dataset": "scidocs", "query_id": "09109a5375d8e2ca752bedde17ba5acad1df61cb", "doc_id": "4e348a6bb29f7ac5514ba52d503417424153223c"}, "id": "scidocs-09109a5375d8e2ca752bedde17ba5acad1df61cb-4e348a6bb29f7ac5514ba52d503417424153223c", "question": "Sensor fusion for semantic segmentation of urban scenes", "context": "Semantic segmentation has made encouraging progress due to the success of deep convolutional networks in recent years. Meanwhile, depth sensors become prevalent nowadays, so depth maps can be acquired more easily. However, there are few studies that focus on the RGB-D semantic segmentation task. Exploiting the depth information effectiveness to improve performance is a challenge. In this paper, we propose a novel solution named LDFNet, which incorporates Luminance, Depth and Color information by a fusion-based network. It includes a sub-network to process depth maps and employs luminance images to assist the depth information in processes. LDFNet outperforms the other state-of-art systems on the Cityscapes dataset, and its inference speed is faster than most of the existing networks. The experimental results show the effectiveness of the proposed multi-modal fusion network and its potential for practical applications."}
{"metadata": {"dataset": "scidocs", "query_id": "09109a5375d8e2ca752bedde17ba5acad1df61cb", "doc_id": "0d4dbd59e42e615ccf6cd4f71203be97afac48fb"}, "id": "scidocs-09109a5375d8e2ca752bedde17ba5acad1df61cb-0d4dbd59e42e615ccf6cd4f71203be97afac48fb", "question": "Sensor fusion for semantic segmentation of urban scenes", "context": "Traditional video understanding tasks include human action recognition and actor/object semantic segmentation. However, the combined task of providing semantic segmentation for different actor classes simultaneously with their action class remains a challenging but necessary task for many applications. In this work, we propose a new end-to-end architecture for tackling this task in videos. Our model effectively leverages multiple input modalities, contextual information, and multitask learning in the video to directly output semantic segmentations in a single unified framework. We train and benchmark our model on the Actor-Action Dataset (A2D) for joint actor-action semantic segmentation, and demonstrate state-of-the-art performance for both segmentation and detection. We also perform experiments verifying our approach improves performance for zero-shot recognition, indicating generalizability of our jointly learned feature space."}
{"metadata": {"dataset": "scidocs", "query_id": "09109a5375d8e2ca752bedde17ba5acad1df61cb", "doc_id": "649d0aa1be51cc545de52fc584640501efdcf68b"}, "id": "scidocs-09109a5375d8e2ca752bedde17ba5acad1df61cb-649d0aa1be51cc545de52fc584640501efdcf68b", "question": "Sensor fusion for semantic segmentation of urban scenes", "context": "Semantic segmentation provides a practical way to segment remotely sensed images into multiple ground objects simultaneously, which can be potentially applied to multiple remote sensed related aspects. Current classification algorithms in remotely sensed images are mostly limited by different imaging conditions, the multiple ground objects are difficult to be separated from each other due to high intraclass spectral variances and interclass spectral similarities. In this study, we propose an end-to-end framework to semantically segment high-resolution aerial images without postprocessing to refine the segmentation results. The framework provides a pixel-wise segmentation result, comprising convolutional neural network structure and pyramid pooling module, which aims to extract feature maps at multiple scales. The proposed model is applied to the ISPRS Vaihingen benchmark dataset from the ISPRS 2D Semantic Labeling Challenge. Its segmentation results are compared with previous state-of-the-art method UZ_1, UPB and three other methods that segment images into objects of all the classes (including clutter/background) based on true orthophoto tiles, and achieve the highest overall accuracy of 87.8% over the published performances, to the best of our knowledge. The results validate the efficiency of the proposed model in segmenting multiple ground objects from remotely sensed images simultaneously."}
{"metadata": {"dataset": "scidocs", "query_id": "09109a5375d8e2ca752bedde17ba5acad1df61cb", "doc_id": "adb705bd2b16d1f839d803ae5a94dc555f4e61ff"}, "id": "scidocs-09109a5375d8e2ca752bedde17ba5acad1df61cb-adb705bd2b16d1f839d803ae5a94dc555f4e61ff", "question": "Sensor fusion for semantic segmentation of urban scenes", "context": "Dempster-Shafer theory provides a sensor fusion framework that autonomously accounts for obstacle occlusion in dynamic, urban environments. However, to discern static and moving obstacles, the Dempster-Shafer approach requires manual tuning of parameters dependent on the situation and sensor types. The proposed methodology utilizes a deep fully convolutional neural network to improve the robust performance of the information fusion algorithm in distinguishing static and moving obstacles from navigable space. The image-like spatial structure of probabilistic occupancy allows a semantic segmentation framework to discern classes for individual grid cells. A subset of the KITTI LIDAR tracking dataset in combination with semantic map data was used for the information fusion task. The probabilistic occupancy grid output of the Dempster-Shafer information fusion algorithm was provided as input to the neural network. The network then learned an offset from the original DST result to improve semantic labeling performance. The proposed framework outperformed the baseline approach in the mean intersection over union metric reaching 0.546 and 0.531 in the validation and test sets respectively. However, little improvement was achieved in discerning moving and static cells due to the limited dataset size. To improve model performance in future work, the dataset will be expanded to facilitate more effective learning, and temporal data will be fed through individual convolutional networks prior to being merged in channels as input to the main network."}
{"metadata": {"dataset": "scidocs", "query_id": "095923857403ebb1578ce82b085c97c75b522fa2", "doc_id": "no"}, "id": "scidocs-095923857403ebb1578ce82b085c97c75b522fa2-no", "question": "Training Faster by Separating Modes of Variation in Batch-normalized Models", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "095923857403ebb1578ce82b085c97c75b522fa2", "doc_id": "1827de6fa9c9c1b3d647a9d707042e89cf94abf0"}, "id": "scidocs-095923857403ebb1578ce82b085c97c75b522fa2-1827de6fa9c9c1b3d647a9d707042e89cf94abf0", "question": "Training Faster by Separating Modes of Variation in Batch-normalized Models", "context": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."}
{"metadata": {"dataset": "scidocs", "query_id": "095923857403ebb1578ce82b085c97c75b522fa2", "doc_id": "1be81623e9cd9434a7bc088fdaa9d858fcfb3da5"}, "id": "scidocs-095923857403ebb1578ce82b085c97c75b522fa2-1be81623e9cd9434a7bc088fdaa9d858fcfb3da5", "question": "Training Faster by Separating Modes of Variation in Batch-normalized Models", "context": "Batch Normalization is quite effective at accelerating and improving the training of deep models. However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples. We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference. We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-i.i.d. minibatches. At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency."}
{"metadata": {"dataset": "scidocs", "query_id": "095923857403ebb1578ce82b085c97c75b522fa2", "doc_id": "324bcd46fcb737a3ebb1fb2ab8ea93a0fed624da"}, "id": "scidocs-095923857403ebb1578ce82b085c97c75b522fa2-324bcd46fcb737a3ebb1fb2ab8ea93a0fed624da", "question": "Training Faster by Separating Modes of Variation in Batch-normalized Models", "context": "In distributed training of deep neural networks, parallel minibatch SGD is widely used to speed up the training process by using multiple workers. It uses multiple workers to sample local stochastic gradient in parallel, aggregates all gradients in a single server to obtain the average, and update each worker’s local model using a SGD update with the averaged gradient. Ideally, parallel mini-batch SGD can achieve a linear speed-up of the training time (with respect to the number of workers) compared with SGD over a single worker. However, such linear scalability in practice is significantly limited by the growing demand for gradient communication as more workers are involved. Model averaging, which periodically averages individual models trained over parallel workers, is another common practice used for distributed training of deep neural networks since (Zinkevich et al. 2010) (McDonald, Hall, and Mann 2010). Compared with parallel mini-batch SGD, the communication overhead of model averaging is significantly reduced. Impressively, tremendous experimental works have verified that model averaging can still achieve a good speed-up of the training time as long as the averaging interval is carefully controlled. However, it remains a mystery in theory why such a simple heuristic works so well. This paper provides a thorough and rigorous theoretical study on why model averaging can work as well as parallel mini-batch SGD with significantly less communication overhead. Introduction Consider the distributed training of deep neural networks over multiple workers (Dean et al. 2012), where all workers can access all or partial training data and aim to find a common model that yields the minimum training loss. Such a scenario can be modeled as the following distributed parallel non-convex optimization min x∈Rm f(x) ∆ = 1 N N ∑"}
{"metadata": {"dataset": "scidocs", "query_id": "095923857403ebb1578ce82b085c97c75b522fa2", "doc_id": "0426408774fea8d724609769d6954dd75454a97e"}, "id": "scidocs-095923857403ebb1578ce82b085c97c75b522fa2-0426408774fea8d724609769d6954dd75454a97e", "question": "Training Faster by Separating Modes of Variation in Batch-normalized Models", "context": "Variational autoencoders are a powerful framework for unsupervised learning. However, previous work has been restricted to shallow models with one or two layers of fully factorized stochastic latent variables, limiting the flexibility of the latent representation. We propose three advances in training algorithms of variational autoencoders, for the first time allowing to train deep models of up to five stochastic layers, (1) using a structure similar to the Ladder network as the inference model, (2) warm-up period to support stochastic units staying active in early training, and (3) use of batch normalization. Using these improvements we show state-of-the-art log-likelihood results for generative modeling on several benchmark datasets."}
{"metadata": {"dataset": "scidocs", "query_id": "095923857403ebb1578ce82b085c97c75b522fa2", "doc_id": "226966243877f186d346be01047cf71cee1b5ec4"}, "id": "scidocs-095923857403ebb1578ce82b085c97c75b522fa2-226966243877f186d346be01047cf71cee1b5ec4", "question": "Training Faster by Separating Modes of Variation in Batch-normalized Models", "context": "The (batch) EM algorithm plays an important role in unsupervised induction, but it sometimes suffers from slow convergence. In this paper, we show that online variants (1) provide significant speedups and (2) can even find better solutions than those found by batch EM. We support these findings on four unsupervised tasks: part-of-speech tagging, document classification, word segmentation, and word alignment."}
{"metadata": {"dataset": "scidocs", "query_id": "095923857403ebb1578ce82b085c97c75b522fa2", "doc_id": "63396a1a658863faeb9e3c8ee1f1934fd00a7512"}, "id": "scidocs-095923857403ebb1578ce82b085c97c75b522fa2-63396a1a658863faeb9e3c8ee1f1934fd00a7512", "question": "Training Faster by Separating Modes of Variation in Batch-normalized Models", "context": "Many studies have shown that Deep Convolutional Neural Networks (DCNNs) exhibit great accuracies given large training datasets in image recognition tasks. Optimization technique known as asynchronous mini-batch Stochastic Gradient Descent (SGD) is widely used for deep learning because it gives fast training speed and good recognition accuracies, while it may increases generalization error if training parameters are in inappropriate ranges. We propose a performance model of a distributed DCNN training system called SPRINT that uses asynchronous GPU processing based on mini-batch SGD. The model considers the probability distribution of mini-batch size and gradient staleness that are the core parameters of asynchronous SGD training. Our performance model takes DCNN architecture and machine specifications as input parameters, and predicts time to sweep entire dataset, mini-batch size and staleness with 5%, 9% and 19% error in average respectively on several supercomputers with up to thousands of GPUs. Experimental results on two different supercomputers show that our model can steadily choose the fastest machine configuration that nearly meets a target mini-batch size."}
{"metadata": {"dataset": "scidocs", "query_id": "095923857403ebb1578ce82b085c97c75b522fa2", "doc_id": "260b95930133787c008d01fb870f909171c7e677"}, "id": "scidocs-095923857403ebb1578ce82b085c97c75b522fa2-260b95930133787c008d01fb870f909171c7e677", "question": "Training Faster by Separating Modes of Variation in Batch-normalized Models", "context": "Very deep CNNs with small 3 × 3 kernels have recently been shown to achieve very strong performance as acoustic models in hybrid NN-HMM speech recognition systems. In this paper we investigate how to efficiently scale these models to larger datasets. Specifically, we address the design choice of pooling and padding along the time dimension which renders convolutional evaluation of sequences highly inefficient. We propose a new CNN design without timepadding and without timepooling, which is slightly suboptimal for accuracy, but has two significant advantages: it enables sequence training and deployment by allowing efficient convolutional evaluation of full utterances, and, it allows for batch normalization to be straightforwardly adopted to CNNs on sequence data. Through batch normalization, we recover the lost peformance from removing the time-pooling, while keeping the benefit of efficient convolutional evaluation. We demonstrate the performance of our models both on larger scale data than before, and after sequence training. Our very deep CNN model sequence trained on the 2000h switchboard dataset obtains 9.4 word error rate on the Hub5 test-set, matching with a single model the performance of the 2015 IBM system combination, which was the previous best published result."}
{"metadata": {"dataset": "scidocs", "query_id": "095923857403ebb1578ce82b085c97c75b522fa2", "doc_id": "0b4b6932d5df74b366d9235b40334bc40d719c72"}, "id": "scidocs-095923857403ebb1578ce82b085c97c75b522fa2-0b4b6932d5df74b366d9235b40334bc40d719c72", "question": "Training Faster by Separating Modes of Variation in Batch-normalized Models", "context": "In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44% to 7.05% in SVHN with 500 labels and from 18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels."}
{"metadata": {"dataset": "scidocs", "query_id": "095923857403ebb1578ce82b085c97c75b522fa2", "doc_id": "21940188396e1ab4f47db30cf6fd289eee2a930a"}, "id": "scidocs-095923857403ebb1578ce82b085c97c75b522fa2-21940188396e1ab4f47db30cf6fd289eee2a930a", "question": "Training Faster by Separating Modes of Variation in Batch-normalized Models", "context": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study (Tommasi et al. 2015) shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without finetuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives stateof-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance."}
{"metadata": {"dataset": "scidocs", "query_id": "095923857403ebb1578ce82b085c97c75b522fa2", "doc_id": "3653be0cf0eafea9a43388e1be93e5520f0ad898"}, "id": "scidocs-095923857403ebb1578ce82b085c97c75b522fa2-3653be0cf0eafea9a43388e1be93e5520f0ad898", "question": "Training Faster by Separating Modes of Variation in Batch-normalized Models", "context": "While active learning (AL) has been widely studied for classification problems, limited efforts have been done on AL for regression. In this paper, we introduce a new AL framework for regression, expected model change maximization (EMCM), which aims at choosing the unlabeled data instances that result in the maximum change of the current model once labeled. The model change is quantified as the difference between the current model parameters and the updated parameters after the inclusion of the newly selected examples. In light of the stochastic gradient descent learning rule, we approximate the change as the gradient of the loss function with respect to each single candidate instance. Under the EMCM framework, we propose novel AL algorithms for the linear and nonlinear regression models. In addition, by simulating the behavior of the sequential AL policy when applied for <inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula> iterations, we further extend the algorithms to batch mode AL to simultaneously choose a set of <inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula> most informative instances at each query time. Extensive experimental results on both UCI and StatLib benchmark data sets have demonstrated that the proposed algorithms are highly effective and efficient."}
{"metadata": {"dataset": "scidocs", "query_id": "0963302a589b5476df76040ab22a3315e0f84bb1", "doc_id": "no"}, "id": "scidocs-0963302a589b5476df76040ab22a3315e0f84bb1-no", "question": "Lire: lucene image retrieval: an extensible java CBIR library", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0963302a589b5476df76040ab22a3315e0f84bb1", "doc_id": "bcdd2458633e6f9955a2b18846e5c85c7b047e08"}, "id": "scidocs-0963302a589b5476df76040ab22a3315e0f84bb1-bcdd2458633e6f9955a2b18846e5c85c7b047e08", "question": "Lire: lucene image retrieval: an extensible java CBIR library", "context": "Algorithms of Content-Based Image Retrieval (CBIR) have been well developed along with the explosion of information. These algorithms are mainly distinguished based on feature used to describe the image content. In this paper, the algorithms that are based on color feature and texture feature for image retrieval will be presented. Color Coherence Vector based image retrieval algorithm is also attempted during the implementation process, but the best result is generated from the algorithms that weights color and texture. 80% satisfying rate is achieved."}
{"metadata": {"dataset": "scidocs", "query_id": "0963302a589b5476df76040ab22a3315e0f84bb1", "doc_id": "9201fe9244071f6c4d1bac1b612f2b6aa12ca18f"}, "id": "scidocs-0963302a589b5476df76040ab22a3315e0f84bb1-9201fe9244071f6c4d1bac1b612f2b6aa12ca18f", "question": "Lire: lucene image retrieval: an extensible java CBIR library", "context": "Content-based image retrieval (CBIR) has been an active research topic in the last decade. Feature extraction and representation is one of the most important issues in the CBIR. In this paper, we propose a content-based image retrieval method based on an efficient integration of color and texture features. As its color features, pseudo-Zernike chromaticity distribution moments in opponent chromaticity space are used. As its texture features, rotation-invariant and scale-invariant image descriptor in steerable pyramid domain are adopted, which offers an efficient and flexible approximation of early processing in the human visual system. The integration of color and texture information provides a robust feature set for color image retrieval. Experimental results show that the proposed method yields higher retrieval accuracy than some conventional methods even though its feature vector dimension is not higher than those of the latter for different test DBs."}
{"metadata": {"dataset": "scidocs", "query_id": "0963302a589b5476df76040ab22a3315e0f84bb1", "doc_id": "af9d52af8eeb60edd3da3f03196d072ed57f3ec2"}, "id": "scidocs-0963302a589b5476df76040ab22a3315e0f84bb1-af9d52af8eeb60edd3da3f03196d072ed57f3ec2", "question": "Lire: lucene image retrieval: an extensible java CBIR library", "context": "The aim of this paper is to review the present state of the art in content-based image retrieval (CBIR), a technique for retrieving images on the basis of automatically-derived features like color, texture and shape. Our findings are based both on a review of the relevant literature and on discussions with researchers in the field. There is need to find a desired image from a collection is shared by many professional groups, including journalists, design engineers and art historians. During the requirements of image users can vary considerably, it can be useful to illustrate image queries into three levels of abstraction first is primitive features such as color or shape, second is logical features such as the identity of objects shown and last is abstract attributes such as the significance of the scenes depicted. While CBIR systems currently operate well only at the lowest of these levels, most users demand higher levels of retrieval."}
{"metadata": {"dataset": "scidocs", "query_id": "0963302a589b5476df76040ab22a3315e0f84bb1", "doc_id": "13ae009718f0edf8f6c3d8dc55dd620f3f953ec9"}, "id": "scidocs-0963302a589b5476df76040ab22a3315e0f84bb1-13ae009718f0edf8f6c3d8dc55dd620f3f953ec9", "question": "Lire: lucene image retrieval: an extensible java CBIR library", "context": ": In this paper the proposed system uses content based image retrieval (CBIR) technique for identification of seed e.g. wheat, rice, gram etc. on the basis of their features. CBIR is a technique to identify or recognize the image on the basis of features present in image. Basically features are classified in to four categories 1.color 2.Shape 3. texture 4. size .In this system we are extracting color, shape feature extraction. After that classifying images in to categories using neural network according to the weights and image displayed from the category for which neural network shows maximum weight. category1 belongs to wheat and category2 belongs to gram. Experiment was conducted on 200 images of wheat and gram by using Euclidean distance(ED) and artificial neural network techniques. From 200 images 150 are used for training purpose and 50 images are used for testing purpose. The precision rate of the system by using ED is 84.4 percent By using Artificial neural network precision rate is 95 percent."}
{"metadata": {"dataset": "scidocs", "query_id": "0963302a589b5476df76040ab22a3315e0f84bb1", "doc_id": "51354699bfd423bb99d34c06d0061540d8ed178e"}, "id": "scidocs-0963302a589b5476df76040ab22a3315e0f84bb1-51354699bfd423bb99d34c06d0061540d8ed178e", "question": "Lire: lucene image retrieval: an extensible java CBIR library", "context": "Image retrieval is an important problem for researchers in computer vision and content-based image retrieval (CBIR) fields. Over the last decades, many image retrieval systems were based on image representation as a set of extracted low-level features such as color, texture and shape. Then, systems calculate similarity metrics between features in order to find similar images to a query image. The disadvantage of this approach is that images visually and semantically different may be similar in the low level feature space. So, it is necessary to develop tools to optimize retrieval of information. Integration of vector space models is one solution to improve the performance of image retrieval. In this paper, we present an efficient and effective retrieval framework which includes a vectorization technique combined with a pseudo relevance model. The idea is to transform any similarity matching model (between images) to a vector space model providing a score. A study on several methodologies to obtain the vectorization is presented. Some experiments have been undertaken on Wang, Oxford5k and Inria Holidays datasets to show the performance of our proposed framework."}
{"metadata": {"dataset": "scidocs", "query_id": "0963302a589b5476df76040ab22a3315e0f84bb1", "doc_id": "50e983fd06143cad9d4ac75bffc2ef67024584f2"}, "id": "scidocs-0963302a589b5476df76040ab22a3315e0f84bb1-50e983fd06143cad9d4ac75bffc2ef67024584f2", "question": "Lire: lucene image retrieval: an extensible java CBIR library", "context": "LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets."}
{"metadata": {"dataset": "scidocs", "query_id": "0963302a589b5476df76040ab22a3315e0f84bb1", "doc_id": "cccd96c81fb365bb5eab587c4adb1e40d9235b9b"}, "id": "scidocs-0963302a589b5476df76040ab22a3315e0f84bb1-cccd96c81fb365bb5eab587c4adb1e40d9235b9b", "question": "Lire: lucene image retrieval: an extensible java CBIR library", "context": "Categorization of medical images means selecting the appropriate class for a given image out of a set of pre-defined categories. This is an important step for data mining and content-based image retrieval (CBIR). So far, published approaches are capable to distinguish up to 10 categories. In this paper, we evaluate automatic categorization into more than 80 categories describing the imaging modality and direction as well as the body part and biological system examined. Based on 6231 reference images from hospital routine, 85.5% correctness is obtained combining global texture features with scaled images. With a frequency of 97.7%, the correct class is within the best ten matches, which is sufficient for medical CBIR applications."}
{"metadata": {"dataset": "scidocs", "query_id": "0963302a589b5476df76040ab22a3315e0f84bb1", "doc_id": "17a4b08d28ca23e6a077abd427aa88f70fec8c3e"}, "id": "scidocs-0963302a589b5476df76040ab22a3315e0f84bb1-17a4b08d28ca23e6a077abd427aa88f70fec8c3e", "question": "Lire: lucene image retrieval: an extensible java CBIR library", "context": "Although software reuse presents clear advantages for programmer productivit.y and code reliability, it is not practiced enough. One of the reasons for the only moderate success of reuse is the lack of software libr'lries that facilitate the actual locating and understanding of reusable components. This paper dpscrihes II technology for automatically I\\Ssembling large softwar~ libraries that promote softwarp rpuse by helping the user locate the components closest to hfO'r/his neeels Softwarp libraries are aut.omatically assemblfO'd from a set. of Ilnorganizpd component.s by using informat.ion rptripval techniques. The constrllction of the library is done in !.wo steps. First, attribut.es ;up alJtornal.ically pxtracted from nat.urlll language docllmpntat.ion by IIsing a npIV inJexing scheme ba~fO'd an t.he nntinns of lexical affinities anci quantity of inforrnllt.ion. Then, a hierarchy for browsing is alJtomat.iclllly generaterl using a clustering techniq1le that draws only nn the informlltion provided by the attribulps. Thanks to the frfO'p-!.pxt inrlexing se-heme, tools following this approach can accept free-style nat'Jral language 'luprips. This tpChllOlogy h1\\.S bfO'en impll\"ment .. d in the GURU ~ystpm, whirh h<l5 bpen applied 1.0 construct an organized library of Alx utilities. An exppriment. was rondllrtpd in orrlpr to pvalullte the n·t.rievll\\ effectivl\"nfO'ss of GURU 1\\.5 compl\\red to INFOExPLORER a hypprt.pxt lihrary syslplll for Alx 3 on the IBM RISe System/6000 series. We followpr\\ Ihe usual evaluation prOf\"pr\\lJr!' us .. .! in informat.ion rptrifO'vl\\l, hased upon recall and precision ml\"a5IJrPS, and d\"t.erminpd t 1111 I. nllr syst.f'Tll pr>rforms l!i% better on a random test set, while being milch le~s pxpensive to build than INFOEXl'LORF.R."}
{"metadata": {"dataset": "scidocs", "query_id": "0963302a589b5476df76040ab22a3315e0f84bb1", "doc_id": "626a38a32e2255e5bef98880ebbddf6994840e9e"}, "id": "scidocs-0963302a589b5476df76040ab22a3315e0f84bb1-626a38a32e2255e5bef98880ebbddf6994840e9e", "question": "Lire: lucene image retrieval: an extensible java CBIR library", "context": "Local binary pattern (LBP) is widely adopted for efficient image feature description and simplicity. To describe the color images, it is required to combine the LBPs from each channel of the image. The traditional way of binary combination is to simply concatenate the LBPs from each channel, but it increases the dimensionality of the pattern. In order to cope with this problem, this paper proposes a novel method for image description with multichannel decoded LBPs. We introduce adder- and decoder-based two schemas for the combination of the LBPs from more than one channel. Image retrieval experiments are performed to observe the effectiveness of the proposed approaches and compared with the existing ways of multichannel techniques. The experiments are performed over 12 benchmark natural scene and color texture image databases, such as Corel-1k, MIT-VisTex, USPTex, Colored Brodatz, and so on. It is observed that the introduced multichannel adder- and decoder-based LBPs significantly improve the retrieval performance over each database and outperform the other multichannel-based approaches in terms of the average retrieval precision and average retrieval rate."}
{"metadata": {"dataset": "scidocs", "query_id": "0963302a589b5476df76040ab22a3315e0f84bb1", "doc_id": "dbb35fc25270c183a10cf5d463487531529a599b"}, "id": "scidocs-0963302a589b5476df76040ab22a3315e0f84bb1-dbb35fc25270c183a10cf5d463487531529a599b", "question": "Lire: lucene image retrieval: an extensible java CBIR library", "context": "In this paper, we address the problem of ranking clinical documents using centrality based approach. We model the documents to be ranked as nodes in a graph and place edges between documents based on their similarity. Given a query, we compute similarity of the query with respect to every document in the graph. Based on these similarity values, documents are ranked for a given query. Initially, Lucene is used to retrieve top fifty documents that are relevant to the query and then our proposed approach is applied on these retrieved documents to rerank them. Experimental results show that our approach did not perform well as the documents retrieved by Lucene are not among the top 50 documents in the Gold Standard."}
{"metadata": {"dataset": "scidocs", "query_id": "09b349399b8b696d365185ee3896dfae77af8ac5", "doc_id": "no"}, "id": "scidocs-09b349399b8b696d365185ee3896dfae77af8ac5-no", "question": "Ontology-Based Integration of Cross-Linked Datasets", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "09b349399b8b696d365185ee3896dfae77af8ac5", "doc_id": "1a2c6843b9e781f2f77e875f3d073ab686f6fae3"}, "id": "scidocs-09b349399b8b696d365185ee3896dfae77af8ac5-1a2c6843b9e781f2f77e875f3d073ab686f6fae3", "question": "Ontology-Based Integration of Cross-Linked Datasets", "context": "In distributed geospatial applications with heterogeneous databases, an ontology-driven approach to data integration relies on the alignment of the concepts of a global ontology that describe the domain, with the concepts of the ontologies that describe the data in the distributed databases. Once the alignment between the global ontology and each distributed ontology is established, agreements that encode a variety of mappings between concepts are derived. In this way, users can potentially query hundreds of geospatial databases using a single query. Using our approach, querying can be easily extended to new data sources and, therefore, to new regions. In this paper, we describe the AgreementMaker, a tool that displays the ontologies, supports several mapping layers visually, presents automatically generated mappings, and finally produces the agreements. r 2007 Elsevier Ltd. All rights reserved."}
{"metadata": {"dataset": "scidocs", "query_id": "09b349399b8b696d365185ee3896dfae77af8ac5", "doc_id": "4dee208efe5878bce18f60ce8268c4bdb7470775"}, "id": "scidocs-09b349399b8b696d365185ee3896dfae77af8ac5-4dee208efe5878bce18f60ce8268c4bdb7470775", "question": "Ontology-Based Integration of Cross-Linked Datasets", "context": "Current data integration approaches are mostly limited to few data sources, partly due to the use of binary match approaches between pairs of sources. We thus advocate for the development of more holistic, clustering-based data integration approaches that scale to many data sources. We outline different use cases and provide an overview of initial approaches for holistic schema/ontology integration and entity clustering. The discussion also considers open data repositories and socalled knowledge graphs."}
{"metadata": {"dataset": "scidocs", "query_id": "09b349399b8b696d365185ee3896dfae77af8ac5", "doc_id": "1a8e864093212caf1ec98c207c55cc21d4dc5775"}, "id": "scidocs-09b349399b8b696d365185ee3896dfae77af8ac5-1a8e864093212caf1ec98c207c55cc21d4dc5775", "question": "Ontology-Based Integration of Cross-Linked Datasets", "context": "The evolution of the Web of documents into a Web of services and data has resulted in an increased availability of data from almost any domain. For example, general domain knowledge bases such as DBpedia or Wikidata, or domain specific Web sources like the Oxford Art archive, allow for accessing knowledge about a wide variety of entities including people, organizations, or art paintings. However, these data sources publish data in different ways, and they may be equipped with different search capabilities, e.g., SPARQL endpoints or REST services, thus requiring data integration techniques that provide a unified view of the published data. We devise a semantic data integration approach named FuhSen that exploits keyword and structured search capabilities of Web data sources and generates on-demand knowledge graphs merging data collected from available Web sources. Resulting knowledge graphs model semantics or meaning of merged data in terms of entities that satisfy keyword queries, and relationships among those entities. FuhSen relies on both RDF to semantically describe the collected entities, and on semantic similarity measures to decide on relatedness among entities that should be merged. We empirically evaluate the results of FuhSen data integration techniques on data from the DBpedia knowledge base. The experimental results suggest that FuhSen data integration techniques accurately integrate similar entities semantically into knowledge graphs."}
{"metadata": {"dataset": "scidocs", "query_id": "09b349399b8b696d365185ee3896dfae77af8ac5", "doc_id": "40f2479545c8e3f44a09b3cc7bb2bcf60f0a37d1"}, "id": "scidocs-09b349399b8b696d365185ee3896dfae77af8ac5-40f2479545c8e3f44a09b3cc7bb2bcf60f0a37d1", "question": "Ontology-Based Integration of Cross-Linked Datasets", "context": "As more and more ontologies are defined with different terms, ontology matching plays a crucial role in addressing the semantic heterogeneity problem in many different disciplines. Many efforts have been made to discover correspondences among terms in different ontologies. Most studies directly match two ontologies by utilizing terminological and structural methods that rely on ontologies themselves only. However, the decentralized characteristic of ontologies raises the uncertainty in ontology matching. To address this problem, we propose a four-stage ontology matching framework (FOMF) to enhance ontology matching performance. It is built upon the commonly accepted claim that an external comprehensive knowledge base can be used as a semantic bridge between domain ontologies for ontology matching. First, FOMF semantically maps domain ontologies to a knowledge base and then produces different types of alignments, including equivalence, subclass, sameas, and instance alignments. Similarities between two domain ontologies are next employed to enhance the equivalence and sameas alignments discovery. Finally, based on acquired alignments, inferred alignments are deduced to guarantee the completeness of matching results. Our experimental results show the superiority of the proposed method over the existing ones."}
{"metadata": {"dataset": "scidocs", "query_id": "09b349399b8b696d365185ee3896dfae77af8ac5", "doc_id": "9439871c45158dd4cd42109342780de5226fe4dd"}, "id": "scidocs-09b349399b8b696d365185ee3896dfae77af8ac5-9439871c45158dd4cd42109342780de5226fe4dd", "question": "Ontology-Based Integration of Cross-Linked Datasets", "context": "Methods to design of formal ontologies have been in focus of research since the early nineties when their importance and conceivable practical application in engineering sciences had been understood. However, often significant customization of generic methodologies is required when they are applied in tangible scenarios. In this paper, we present a methodology for ontology design developed in the context of data integration. In this scenario, a targeting ontology is applied as a mediator for distinct schemas of individual data sources and, furthermore, as a reference schema for federated data queries. The methodology has been used and evaluated in a case study aiming at integration of buildings' energy and carbon emission related data. We claim that we have made the design process much more efficient and that there is a high potential to reuse the methodology."}
{"metadata": {"dataset": "scidocs", "query_id": "09b349399b8b696d365185ee3896dfae77af8ac5", "doc_id": "406cebc2b0195cda4e2d175bf9e1871bb8237b69"}, "id": "scidocs-09b349399b8b696d365185ee3896dfae77af8ac5-406cebc2b0195cda4e2d175bf9e1871bb8237b69", "question": "Ontology-Based Integration of Cross-Linked Datasets", "context": "The integration of information of different kinds, such as spatial and alphanumeric at different levels of detail, is a challenge. While a solution is not reached, it is widely recognized that the need to integrate information is so pressing that it does not matter if detail is lost, as long as integration is achieved. This paper shows the potential for information retrieval at different levels of granularity inside the framework of information systems based on ontologies. Ontologies are theories that use a specific vocabulary to describe entities, classes, properties and functions related to a certain view of the world. The use of an ontology, translated into an active information system component, leads to ontology-driven information systems and, in the specific case of GIS, leads to what we call ontology-driven geographic information systems."}
{"metadata": {"dataset": "scidocs", "query_id": "09b349399b8b696d365185ee3896dfae77af8ac5", "doc_id": "407748e97d8d3878535f6371ad324708915bf6d9"}, "id": "scidocs-09b349399b8b696d365185ee3896dfae77af8ac5-407748e97d8d3878535f6371ad324708915bf6d9", "question": "Ontology-Based Integration of Cross-Linked Datasets", "context": "Many organizations nowadays face the problem of accessing existing data sources by means of flexible mechanisms that are both powerful and efficient. Ontologies are widely considered as a suitable formal tool for sophisticated data access. The ontology expresses the domain of interest of the information system at a high level of abstraction, and the relationship between data at the sources and instances of concepts and roles in the ontology is expressed by means of mappings. In this paper we present a solution to the problem of designing effective systems for ontology-based data access. Our solution is based on three main ingredients. First, we present a new ontology language, based on Description Logics, that is particularly suited to reason with large amounts of instances. The second ingredient is a novel mapping language that is able to deal with the so-called impedance mismatch problem, i.e., the problem arising from the difference between the basic elements managed by the sources, namely data, and the elements managed by the ontology, namely objects. The third ingredient is the query answering method, that combines reasoning at the level of the ontology with specific mechanisms for both taking into account the mappings and efficiently accessing the data at the sources."}
{"metadata": {"dataset": "scidocs", "query_id": "09b349399b8b696d365185ee3896dfae77af8ac5", "doc_id": "47165ad181b3da3a615e6e8bc8509fcfc53f49c4"}, "id": "scidocs-09b349399b8b696d365185ee3896dfae77af8ac5-47165ad181b3da3a615e6e8bc8509fcfc53f49c4", "question": "Ontology-Based Integration of Cross-Linked Datasets", "context": "With the explosive growth of web data, effective and efficient technologies are in urgent need for retrieving semantically relevant contents of heterogeneous modalities. Previous studies devote efforts to modeling simple cross-modal statistical dependencies, and globally projecting the heterogeneous modalities into a measurable subspace. However, global projections cannot appropriately adapt to diverse contents, and the naturally existing multilevel semantic relation in web data is ignored. We study the problem of semantic coherent retrieval, where documents from different modalities should be ranked by the semantic relevance to the query. Accordingly, we propose TINA, a correlation learning method by adaptive hierarchical semantic aggregation. First, by joint modeling of content and ontology similarities, we build a semantic hierarchy to measure multilevel semantic relevance. Second, with a set of local linear projections and probabilistic membership functions, we propose two paradigms for local expert aggregation, i.e., local projection aggregation and local distance aggregation. To learn the cross-modal projections, we optimize the structure risk objective function that involves semantic coherence measurement, local projection consistency, and the complexity penalty of local projections. Compared to existing approaches, a better bias-variance tradeoff is achieved by TINA in real-world cross-modal correlation learning tasks. Extensive experiments on widely used NUS-WIDE and ICML-Challenge for image-text retrieval demonstrate that TINA better adapts to the multilevel semantic relation and content divergence, and, thus, outperforms state of the art with better semantic coherence."}
{"metadata": {"dataset": "scidocs", "query_id": "09b349399b8b696d365185ee3896dfae77af8ac5", "doc_id": "1dc776a1f94f5a4e56ceb088827e34964a57c79c"}, "id": "scidocs-09b349399b8b696d365185ee3896dfae77af8ac5-1dc776a1f94f5a4e56ceb088827e34964a57c79c", "question": "Ontology-Based Integration of Cross-Linked Datasets", "context": "There exists a large body of scienti c literature devoted to ontology matching, aligning, mapping translating and merging. With it, comes a long list (90+) of tools that support various aspects of these operations. We have approached such tools from the perspective of the INTER-IoT project, in which one of the goals is to facilitate semantic interoperability of Internet of Things platforms. Thus, we had to answer a question: what is actually available when one needs to align/merge ontologies. Here, we summarize our ndings."}
{"metadata": {"dataset": "scidocs", "query_id": "09b349399b8b696d365185ee3896dfae77af8ac5", "doc_id": "4658a728225d2408a93c8706c2418dc4379d6be7"}, "id": "scidocs-09b349399b8b696d365185ee3896dfae77af8ac5-4658a728225d2408a93c8706c2418dc4379d6be7", "question": "Ontology-Based Integration of Cross-Linked Datasets", "context": "Ontologies are at the heart of knowledge management and make use of information that is not only written in English but also in many other natural languages. In order to enable knowledge discovery, sharing and reuse of these multilingual ontologies, it is necessary to support ontology mapping despite natural language barriers. This paper examines the soundness of a generic approach that involves machine translation tools and monolingual ontology matching techniques in cross-lingual ontology mapping scenarios. In particular, experimental results collected from case studies which engage mappings of independent ontologies that are labeled in English and Chinese are presented. Based on findings derived from these studies, limitations of this generic approach are discussed. It is shown with evidence that appropriate translations of conceptual labels in ontologies are of crucial importance when applying monolingual matching techniques in cross-lingual ontology mapping. Finally, to address the identified challenges, a semantic-oriented cross-lingual ontology mapping (SOCOM) framework is proposed and discussed."}
{"metadata": {"dataset": "scidocs", "query_id": "09d275d72bdd404df1270ca0d23574c10c27e4ac", "doc_id": "no"}, "id": "scidocs-09d275d72bdd404df1270ca0d23574c10c27e4ac-no", "question": "Control Flow Analysis for Reverse Engineering of Sequence Diagrams", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "09d275d72bdd404df1270ca0d23574c10c27e4ac", "doc_id": "6ef99de74e6da9be3b2ece569c7be2a15c1db5db"}, "id": "scidocs-09d275d72bdd404df1270ca0d23574c10c27e4ac-6ef99de74e6da9be3b2ece569c7be2a15c1db5db", "question": "Control Flow Analysis for Reverse Engineering of Sequence Diagrams", "context": "When a secure component executes sensitive operations, the information carried by the power consumption can be used to recover secret information. Many different techniques have been developped to recover this secret, but only few of them focus on the recovering of the executed code itself. Indeed, the code knowledge acquired through this step of Simple Power Analysis (SPA) can help to identify implementation weaknesses and to improve further kinds of attacks. In this paper we present a new approach improving the SPA based on a pattern recognition methodology, that can be used to automatically identify the processed instructions that leak through power consumption. We firstly process a geometrical classification with chosen instructions to enable the automatic identification of any sequence of instructions. Such an analysis is used to reverse general purpose code executions of a recent secure component."}
{"metadata": {"dataset": "scidocs", "query_id": "09d275d72bdd404df1270ca0d23574c10c27e4ac", "doc_id": "bf9069992134977bf93384620153dfe39d94b7cf"}, "id": "scidocs-09d275d72bdd404df1270ca0d23574c10c27e4ac-bf9069992134977bf93384620153dfe39d94b7cf", "question": "Control Flow Analysis for Reverse Engineering of Sequence Diagrams", "context": "Reverse Engineering (RE) has been historically considered as a powerful approach to understand electronic hardware in order to gain competitive intelligence or accomplish piracy. In recent years, it has also been looked at as a way to authenticate hardware intellectual properties in the court of law. In this paper, we propose a beneficial role of RE in post-silicon validation of integrated circuits (IC) with respect to IC functionality, reliability and integrity. Unlike traditional destructive RE approaches, we propose a fast non-destructive side-channel analysis approach that can hierarchically extract structural information from an IC through its transient current signature. Such a top-down side-channel analysis approach is capable of reliably identifying pipeline stages and functional blocks. It is also suitable to distinguish sequential elements from combinational gates. For extraction of random logic structures (e.g. control blocks and finite state machines) we combine side-channel analysis with logic testing based Boolean function extraction. The proposed approach is amenable to automation, scalable, and can be applied as part of post-silicon validation process to verify that each IC implements exclusively the functionality described in the specification and is free from malicious modification or Trojan attacks. Simulation results on a pipelined DLX processor demonstrate the effectiveness of the proposed approach."}
{"metadata": {"dataset": "scidocs", "query_id": "09d275d72bdd404df1270ca0d23574c10c27e4ac", "doc_id": "75b8c0abfd45fd77d7a61da7d12bdf516e3139c7"}, "id": "scidocs-09d275d72bdd404df1270ca0d23574c10c27e4ac-75b8c0abfd45fd77d7a61da7d12bdf516e3139c7", "question": "Control Flow Analysis for Reverse Engineering of Sequence Diagrams", "context": "Reverse engineering is an important tool in mitigating vulnerabilities in binaries. As a lot of software is developed in object-oriented languages, reverse engineering of object-oriented code is of critical importance. One of the major hurdles in reverse engineering binaries compiled from object-oriented code is the use of dynamic dispatch. In the absence of debug information, any dynamic dispatch may seem to jump to many possible targets, posing a significant challenge to a reverse engineer trying to track the program flow. We present a novel technique that allows us to statically determine the likely targets of virtual function calls. Our technique uses object tracelets – statically constructed sequences of operations performed on an object – to capture potential runtime behaviors of the object. Our analysis automatically pre-labels some of the object tracelets by relying on instances where the type of an object is known. The resulting type-labeled tracelets are then used to train a statistical language model (SLM) for each type.We then use the resulting ensemble of SLMs over unlabeled tracelets to generate a ranking of their most likely types, from which we deduce the likely targets of dynamic dispatches.We have implemented our technique and evaluated it over real-world C++ binaries. Our evaluation shows that when there are multiple alternative targets, our approach can drastically reduce the number of targets that have to be considered by a reverse engineer."}
{"metadata": {"dataset": "scidocs", "query_id": "09d275d72bdd404df1270ca0d23574c10c27e4ac", "doc_id": "ae155256be15eab9ccc14089fde367f65b482a58"}, "id": "scidocs-09d275d72bdd404df1270ca0d23574c10c27e4ac-ae155256be15eab9ccc14089fde367f65b482a58", "question": "Control Flow Analysis for Reverse Engineering of Sequence Diagrams", "context": "Three novel control computation (control flow) obfuscation methods are described for protecting Java class files. They are basic block fission obfuscation, intersecting loop obfuscation and replacing goto obfuscation. The basic block fission obfuscation splits some chosen basic block(s) into more basic blocks, in which opaque predicates and goto instructions are inserted to make decompiling unsuccessful. The intersecting loop obfuscation intersects two loops and then adds this pattern into programs. The intersecting loop structure should not appear in a Java source program or a class file. The replacing goto obfuscation replaces goto instructions with conditional branch instructions. The newmethods were tested against 16 decompilers. The study also implemented multi-level exit obfuscation and single-level exit obfuscation for comparison. Both the intersecting loop obfuscation and the replacing goto obfuscation successfully defeated all the decompilers."}
{"metadata": {"dataset": "scidocs", "query_id": "09d275d72bdd404df1270ca0d23574c10c27e4ac", "doc_id": "53732eff5c29585bc84d0ec280b0923639bf740e"}, "id": "scidocs-09d275d72bdd404df1270ca0d23574c10c27e4ac-53732eff5c29585bc84d0ec280b0923639bf740e", "question": "Control Flow Analysis for Reverse Engineering of Sequence Diagrams", "context": "In object oriented programming, the functionalities of a system result from the interactions (message exchanges) among the objects allocated by the system. While designing object interactions is far more complex than designing the object structure in forward engineering, the problem of understanding object interactions during code evolution is even harder, because the related information is spread across the code. In this paper, a technique for the automatic extraction of UML interaction diagrams from C++ code is proposed. The algorithm is based on a static, conservative flow analysis, that approximates the behavior of the system in any execution and for any possible input. Applicability of the approach to large software is achieved by means of two mechanisms: partial analysis and focusing. Usage of our method on a real world, large C++ system confirmed its viability."}
{"metadata": {"dataset": "scidocs", "query_id": "09d275d72bdd404df1270ca0d23574c10c27e4ac", "doc_id": "15dd88a84e2581e9d408b2be142b0dba6b9c4c3e"}, "id": "scidocs-09d275d72bdd404df1270ca0d23574c10c27e4ac-15dd88a84e2581e9d408b2be142b0dba6b9c4c3e", "question": "Control Flow Analysis for Reverse Engineering of Sequence Diagrams", "context": "Recent trends of hardware intellectual property (IP) piracy and reverse engineering pose major business and security concerns to an IP-based system-on-chip (SoC) design flow. In this paper, we propose a Register Transfer Level (RTL) hardware IP protection technique based on low-overhead key-based obfuscation of control and data flow. The basic idea is to transform the RTL core into control and data flow graph (CDFG) and then integrate a well-obfuscated finite state machine (FSM) of special structure, referred as “Mode-Control FSM”, into the CDFG in a manner that normal functional behavior is enabled only after application of a specific input sequence. We provide formal analysis of the effectiveness of the proposed approach and present a simple metric to quantify the level of obfuscation. We also present an integrated design flow that implements the proposed obfuscation at low computational overhead. Simulation results for two open-source IP cores show that high levels of security is achievable at nominal area and power overheads under delay constraint."}
{"metadata": {"dataset": "scidocs", "query_id": "09d275d72bdd404df1270ca0d23574c10c27e4ac", "doc_id": "59c5eecef1f35bd331f7e9ebe1a1a784bcdb76a5"}, "id": "scidocs-09d275d72bdd404df1270ca0d23574c10c27e4ac-59c5eecef1f35bd331f7e9ebe1a1a784bcdb76a5", "question": "Control Flow Analysis for Reverse Engineering of Sequence Diagrams", "context": "In this paper, a new decoupling control scheme, in terms of an internal model control (IMC) structure, is proposed for two-input -two-output (TITO) processes with time delays. Noteworthy improvement of the decoupling regulation can be achieved for the nominal system output responses, and moreover, either of the system output responses can be quantitatively regulated by virtue of the analytical relationship between the adjustable parameters of the decoupling controller matrix and the nominal system transfer matrix. The ideally optimal controller matrix is analytically derived by proposing the practically desired diagonal system transfer matrix, in terms of the robust H 2 optimal performance objective. Because of the difficulty of physical implementation, its practical form is carefully configured according to whether there exist any complex righthalf-plane (RHP) zeros in the process transfer matrix determinant. At the same time, tuning constraints for the proposed decoupling controller matrix to hold the control system robust stability are analyzed in the presence of the process additive and multiplicative uncertainties, and, accordingly, the on-line tuning rule is provided to cope with the process unmodeled dynamics in practice. Finally, illustrative simulation examples are included to demonstrate the remarkable superiority of the proposed method."}
{"metadata": {"dataset": "scidocs", "query_id": "09d275d72bdd404df1270ca0d23574c10c27e4ac", "doc_id": "23e8236644775fd5d8ff5536ba06b960e19f904b"}, "id": "scidocs-09d275d72bdd404df1270ca0d23574c10c27e4ac-23e8236644775fd5d8ff5536ba06b960e19f904b", "question": "Control Flow Analysis for Reverse Engineering of Sequence Diagrams", "context": "Control-Flow Integrity (CFI) has been recognized as an important low-level security property. Its enforcement can defeat most injected and existing code attacks, including those based on Return-Oriented Programming (ROP). Previous implementations of CFI have required compiler support or the presence of relocation or debug information in the binary. In contrast, we present a technique for applying CFI to stripped binaries on x86/Linux. Ours is the first work to apply CFI to complex shared libraries such as glibc. Through experimental evaluation, we demonstrate that our CFI implementation is effective against control-flow hijack attacks, and eliminates the vast majority of ROP gadgets. To achieve this result, we have developed robust techniques for disassembly, static analysis, and transformation of large binaries. Our techniques have been tested on over 300MB of binaries (executables and shared libraries)."}
{"metadata": {"dataset": "scidocs", "query_id": "09d275d72bdd404df1270ca0d23574c10c27e4ac", "doc_id": "9cf8464e03a2c78e2816b0478bd323bf40ab802b"}, "id": "scidocs-09d275d72bdd404df1270ca0d23574c10c27e4ac-9cf8464e03a2c78e2816b0478bd323bf40ab802b", "question": "Control Flow Analysis for Reverse Engineering of Sequence Diagrams", "context": "Over the last decade many metaheuristics have been applied t o the flowshop scheduling problem, ranging from Simulated Annealing or Tabu Search to complex hybrid techniques. Some of these methods provide excellent effectiveness and e fficiency at the expense of being utterly complicated. In fact, several published meth ods require substantial implementation efforts, exploit problem specific speed-up techniqu es that cannot be applied to slight variations of the original problem, and often re-implement ations of these methods by other researchers produce results that are quite different from t he original ones. In this work we present a new iterated greedy algorithm that applies two pha ses iteratively, namedestruction, were some jobs are eliminated from the incumbent solution, andconstruction, where the eliminated jobs are reinserted into the sequence using t he well known NEH construction Corresponding author"}
{"metadata": {"dataset": "scidocs", "query_id": "09d275d72bdd404df1270ca0d23574c10c27e4ac", "doc_id": "27f989a4994a6b76acae6fe63f992a0146ed1168"}, "id": "scidocs-09d275d72bdd404df1270ca0d23574c10c27e4ac-27f989a4994a6b76acae6fe63f992a0146ed1168", "question": "Control Flow Analysis for Reverse Engineering of Sequence Diagrams", "context": "Integrated circuits (ICs) are now designed and fabricated in a globalized multi-vendor environment making them vulnerable to malicious design changes, the insertion of hardware trojans/malware and intellectual property (IP) theft. Algorithmic reverse engineering of digital circuits can mitigate these concerns by enabling analysts to detect malicious hardware, verify the integrity of ICs and detect IP violations.\n In this paper, we present a set of algorithms for the reverse engineering of digital circuits starting from an unstructured netlist and resulting in a high-level netlist with components such as register files, counters, adders and subtracters. Our techniques require no manual intervention and experiments show that they determine the functionality of more than 51% and up to 93% of the gates in each of the practical test circuits that we examine."}
{"metadata": {"dataset": "scidocs", "query_id": "0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a", "doc_id": "no"}, "id": "scidocs-0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a-no", "question": "DialPort: Connecting the spoken dialog research community to real user data", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a", "doc_id": "1d1861764141b0255389fecfc309ef74151033fc"}, "id": "scidocs-0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a-1d1861764141b0255389fecfc309ef74151033fc", "question": "DialPort: Connecting the spoken dialog research community to real user data", "context": "In this paper we discuss the recent evolution of spoken dialog systems in commercial deployments. Yet based on a simple finite state machine design paradigm, dialog systems reached today a higher level of complexity. The availability of massive amounts of data during deployment led to the development of continuous optimization strategy pushing the design and development of spoken dialog applications from an art to science. At the same time new methods for evaluating the subjective caller experience are available. Finally we describe the inevitable evolution for spoken dialog applications from speech only to multimodal interaction."}
{"metadata": {"dataset": "scidocs", "query_id": "0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a", "doc_id": "20690b7465e6fef5337f0c9be0a302d33b3c9b3a"}, "id": "scidocs-0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a-20690b7465e6fef5337f0c9be0a302d33b3c9b3a", "question": "DialPort: Connecting the spoken dialog research community to real user data", "context": "In a spoken dialog system, dialog state tracking deduces information about the user’s goal as the dialog progresses, synthesizing evidence such as dialog acts over multiple turns with external data sources. Recent approaches have been shown to overcome ASR and SLU errors in some applications. However, there are currently no common testbeds or evaluation measures for this task, hampering progress. The dialog state tracking challenge seeks to address this by providing a heterogeneous corpus of 15K human-computer dialogs in a standard format, along with a suite of 11 evaluation metrics. The challenge received a total of 27 entries from 9 research groups. The results show that the suite of performance metrics cluster into 4 natural groups. Moreover, the dialog systems that benefit most from dialog state tracking are those with less discriminative speech recognition confidence scores. Finally, generalization is a key problem: in 2 of the 4 test sets, fewer than half of the entries out-performed simple baselines. 1 Overview and motivation Spoken dialog systems interact with users via natural language to help them achieve a goal. As the interaction progresses, the dialog manager maintains a representation of the state of the dialog in a process called dialog state tracking (DST). For example, in a bus schedule information system, the dialog state might indicate the user’s desired bus route, origin, and destination. Dialog state tracking is difficult because automatic speech ∗Most of the work for the challenge was performed when the second and third authors were with Honda Research Institute, Mountain View, CA, USA recognition (ASR) and spoken language understanding (SLU) errors are common, and can cause the system to misunderstand the user’s needs. At the same time, state tracking is crucial because the system relies on the estimated dialog state to choose actions – for example, which bus schedule information to present to the user. Most commercial systems use hand-crafted heuristics for state tracking, selecting the SLU result with the highest confidence score, and discarding alternatives. In contrast, statistical approaches compute scores for many hypotheses for the dialog state (Figure 1). By exploiting correlations between turns and information from external data sources – such as maps, bus timetables, or models of past dialogs – statistical approaches can overcome some SLU errors. Numerous techniques for dialog state tracking have been proposed, including heuristic scores (Higashinaka et al., 2003), Bayesian networks (Paek and Horvitz, 2000; Williams and Young, 2007), kernel density estimators (Ma et al., 2012), and discriminative models (Bohus and Rudnicky, 2006). Techniques have been fielded which scale to realistically sized dialog problems and operate in real time (Young et al., 2010; Thomson and Young, 2010; Williams, 2010; Mehta et al., 2010). In end-to-end dialog systems, dialog state tracking has been shown to improve overall system performance (Young et al., 2010; Thomson and Young, 2010). Despite this progress, direct comparisons between methods have not been possible because past studies use different domains and system components, for speech recognition, spoken language understanding, dialog control, etc. Moreover, there is little agreement on how to evaluate dialog state tracking. Together these issues limit progress in this research area. The Dialog State Tracking Challenge (DSTC) provides a first common testbed and evaluation"}
{"metadata": {"dataset": "scidocs", "query_id": "0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a", "doc_id": "436c3119d16ce2e3c243ffe7a4a1a5dc40b128aa"}, "id": "scidocs-0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a-436c3119d16ce2e3c243ffe7a4a1a5dc40b128aa", "question": "DialPort: Connecting the spoken dialog research community to real user data", "context": "We describe an integrated approach for statistical modeling of discourse structure for natural conversational speech. Our model is based on 42`dialog acts' which were hand-labeled in 1155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech. We developed several models and algorithms to automatically detect dialog acts from transcribed or automatically recognized words and from prosodic properties of the speech signal, and by using a statistical discourse grammar. All of these components were probabilistic in nature and estimated from data, employing a variety of techniques (hidden Markov models, N-gram language models, maximum entropy estimation, decision tree classiiers, and neural networks). In preliminary studies, we achieved a dialog act labeling accuracy of 65% based on recognized words and prosody, and an accuracy of 72% based on word transcripts. Since humans achieve 84% on this task (with chance performance at 35%) we nd these results encouraging."}
{"metadata": {"dataset": "scidocs", "query_id": "0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a", "doc_id": "2a94fa0de804b5efaae1a66f50c3ea96539c46b8"}, "id": "scidocs-0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a-2a94fa0de804b5efaae1a66f50c3ea96539c46b8", "question": "DialPort: Connecting the spoken dialog research community to real user data", "context": "This paper presents a design and experiments of developing a non-goal dialog system by utilizing human-to-human conversation examples from drama television. The aim is to build a conversational agent that can interact with users in as natural a fashion as possible, while reducing the time requirement for database design and collection. A number of the challenging design issues we faced are described, including (1) filtering and constructing a dialog example database from the drama conversations, and (2) retrieving a proper system response by finding the best dialog example based on the current user query. Subjective evaluation from a small user study is also discussed."}
{"metadata": {"dataset": "scidocs", "query_id": "0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a", "doc_id": "078b55e2f4899cf95a4c8d65613c340fa190acf8"}, "id": "scidocs-0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a-078b55e2f4899cf95a4c8d65613c340fa190acf8", "question": "DialPort: Connecting the spoken dialog research community to real user data", "context": "A spoken dialog system, while communicating with a user, must keep track of what the user wants from the system at each step. This process, termed dialog state tracking, is essential for a successful dialog system as it directly informs the system’s actions. The first Dialog State Tracking Challenge allowed for evaluation of different dialog state tracking techniques, providing common testbeds and evaluation suites. This paper presents a second challenge, which continues this tradition and introduces some additional features – a new domain, changing user goals and a richer dialog state. The challenge received 31 entries from 9 research groups. The results suggest that while large improvements on a competitive baseline are possible, trackers are still prone to degradation in mismatched conditions. An investigation into ensemble learning demonstrates the most accurate tracking can be achieved by combining multiple trackers."}
{"metadata": {"dataset": "scidocs", "query_id": "0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a", "doc_id": "7671f4a3ad961c9f5c00d91dd46dc6e2d3bd5b39"}, "id": "scidocs-0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a-7671f4a3ad961c9f5c00d91dd46dc6e2d3bd5b39", "question": "DialPort: Connecting the spoken dialog research community to real user data", "context": "Spoken dialog systems typically rely on recognition confidence scores to guard against potential misundersta ndings. While confidence scores can provide an initial assessment for the reliability of the information obtained fro m the user, ideally systems should leverage information that is available in subsequent user responses to update and improve the accuracy of their beliefs. We present a machine-learning based solution for this problem. We use a compresse d representation of beliefs that tracks up to k hypotheses for each concept at any given time. We train a generalized l inear model to perform the updates. Experimental results show that the proposed approach significantly outperform s heuristic rules used for this task in current systems. Fu rthermore, a user study with a mixed-initiative spoken dialog sy stem shows that the approach leads to significant gains in task success and in the efficiency of the interaction, a cross a wide range of recognition error-rates."}
{"metadata": {"dataset": "scidocs", "query_id": "0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a", "doc_id": "dde4b9ccbbcfd589c963aa8e5aec3f5fa236be65"}, "id": "scidocs-0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a-dde4b9ccbbcfd589c963aa8e5aec3f5fa236be65", "question": "DialPort: Connecting the spoken dialog research community to real user data", "context": "In a conventional telephone conversation between two speakers of the same language, the interaction is real-time and the speakers process the information stream incrementally. In this work, we address the problem of incremental speech-to-speech translation (S2S) that enables cross-lingual communication between two remote participants over a telephone. We investigate the problem in a novel real-time Session Initiation Protocol (SIP) based S2S framework. The speech translation is performed incrementally based on generation of partial hypotheses from speech recognition. We describe the statistical models comprising the S2S system and the SIP architecture for enabling real-time two-way cross-lingual dialog. We present dialog experiments performed in this framework and study the tradeoff in accuracy versus latency in incremental speech translation. Experimental results demonstrate that high quality translations can be generated with the incremental approach with approximately half the latency associated with nonincremental approach."}
{"metadata": {"dataset": "scidocs", "query_id": "0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a", "doc_id": "cfdef0cd7ec53868c600005ec74a4a34f063a004"}, "id": "scidocs-0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a-cfdef0cd7ec53868c600005ec74a4a34f063a004", "question": "DialPort: Connecting the spoken dialog research community to real user data", "context": "In a spoken dialog system, determining which action a machine should take in a given situation is a difficult problem because automatic speech recognition is unreliable and hence the state of the conversation can never be known with certainty. Much of the research in spoken dialog systems centres on mitigating this uncertainty and recent work has focussed on three largely disparate techniques: parallel dialog state hypotheses, local use of confidence scores, and automated planning. While in isolation each of these approaches can improve action selection, taken together they currently lack a unified statistical framework that admits global optimization. In this paper we cast a spoken dialog system as a partially observable Markov decision process (POMDP). We show how this formulation unifies and extends existing techniques to form a single principled framework. A number of illustrations are used to show qualitatively the potential benefits of POMDPs compared to existing techniques, and empirical results from dialog simulations are presented which demonstrate significant quantitative gains. Finally, some of the key challenges to advancing this method – in particular scalability – are briefly outlined. 2006 Elsevier Ltd. All rights reserved."}
{"metadata": {"dataset": "scidocs", "query_id": "0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a", "doc_id": "8efbb07f1ed1879a32574e7d3940063e9dafbadb"}, "id": "scidocs-0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a-8efbb07f1ed1879a32574e7d3940063e9dafbadb", "question": "DialPort: Connecting the spoken dialog research community to real user data", "context": "Recent spoken dialog systems have been able to recognize freely spoken user input in restricted domains thanks to statistical methods in the automatic speech recognition. These methods require a high number of natural language utterances to train the speech recognition engine and to assess the quality of the system. Since human speech offers many variants associated with a single intent, a high number of user utterances have to be elicited. Developers are therefore turning to crowdsourcing to collect this data. This paper compares three different methods to elicit multiple utterances for given semantics via crowd sourcing, namely with pictures, with text and with semantic entities. Specifically, we compare the methods with regard to the number of valid data and linguistic variance, whereby a quantitative and qualitative approach is proposed. In our study, the method with text led to a high variance in the utterances and a relatively low rate of invalid data."}
{"metadata": {"dataset": "scidocs", "query_id": "0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a", "doc_id": "d82d9cb8411cc8d39a26b821f98cda80b08124d7"}, "id": "scidocs-0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a-d82d9cb8411cc8d39a26b821f98cda80b08124d7", "question": "DialPort: Connecting the spoken dialog research community to real user data", "context": "In this paper, we extend the state-of-the-art NLIDB system and present a dialog interface to relational databases. Dialog interface enables users to automatically exploit the semantic context of the conversation while asking natural language queries over RDBMS, thereby making it simpler to express complex questions in a natural, piece-wise manner. We propose novel ontology-driven techniques for addressing each of the dialog-specific challenges such as co-reference resolution, ellipsis resolution, and query disambiguation, and use them in determining the overall intent of the user query. We demonstrate the applicability and usefulness of dialog interface over two different domains viz. finance and healthcare."}
{"metadata": {"dataset": "scidocs", "query_id": "0a40663fdcf7c5fb7cfc459693116c41309e7eca", "doc_id": "no"}, "id": "scidocs-0a40663fdcf7c5fb7cfc459693116c41309e7eca-no", "question": "Algorithmic Nuggets in Content Delivery", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0a40663fdcf7c5fb7cfc459693116c41309e7eca", "doc_id": "9da9786e6aba77c72516abb5cfa124402cfdb9f1"}, "id": "scidocs-0a40663fdcf7c5fb7cfc459693116c41309e7eca-9da9786e6aba77c72516abb5cfa124402cfdb9f1", "question": "Algorithmic Nuggets in Content Delivery", "context": "Today, content delivery is a heterogeneous ecosystem composed by various independent infrastructures. The ever increasing growth of Internet traffic has encouraged the proliferation of different architectures to serve content provider needs and user demand. Despite the differences among the technology, their low level implementation can be characterized in a few fundamental building blocks: network storage, request routing, and data transfer. Existing solutions are inefficient because they try to build an information centric service model over a network infrastructure which was designed to support host-to-host communications. Information-Centric Networking (ICN) paradigm has been proposed as a possible solution to this mismatch. ICN integrates content delivery as a native network feature. The rationale is to architect a network that automatically interprets, processes, and delivers content (information) independently of its location. This paper makes the following contributions: 1) it identifies a set of building blocks for content delivery, 2) it surveys the most popular approaches to realize the above building blocks, 3) it compares content delivery solutions relying on the current Internet infrastructure with novel ICN approaches."}
{"metadata": {"dataset": "scidocs", "query_id": "0a40663fdcf7c5fb7cfc459693116c41309e7eca", "doc_id": "d4884e0c1059046b42c0770f051e485275b93724"}, "id": "scidocs-0a40663fdcf7c5fb7cfc459693116c41309e7eca-d4884e0c1059046b42c0770f051e485275b93724", "question": "Algorithmic Nuggets in Content Delivery", "context": "We propose to take a novel approach to robot system design where each building block of a larger system is represented as a differentiable program, i.e. a deep neural network. This representation allows for integrating algorithmic planning and deep learning in a principled manner, and thus combine the benefits of model-free and model-based methods. We apply the proposed approach to a challenging partially observable robot navigation task. The robot must navigate to a goal in a previously unseen 3-D environment without knowing its initial location, and instead relying on a 2-D floor map and visual observations from an onboard camera. We introduce the Navigation Networks (NavNets) that encode state estimation, planning and acting in a single, end-to-end trainable recurrent neural network. In preliminary simulation experiments we successfully trained navigation networks to solve the challenging partially observable navigation task."}
{"metadata": {"dataset": "scidocs", "query_id": "0a40663fdcf7c5fb7cfc459693116c41309e7eca", "doc_id": "5fe18d35bad4238b80a99ec8c4b98aca99a7e389"}, "id": "scidocs-0a40663fdcf7c5fb7cfc459693116c41309e7eca-5fe18d35bad4238b80a99ec8c4b98aca99a7e389", "question": "Algorithmic Nuggets in Content Delivery", "context": "Advertisers develop algorithms to select the most relevant advertisements for users. However, the opacity of these algorithms, along with their potential for violating user privacy, has decreased user trust and preference in behavioral advertising. To mitigate this, advertisers have started to communicate algorithmic processes in behavioral advertising. However, how revealing parts of the algorithmic process affects users' perceptions towards ads and platforms is still an open question. To investigate this, we exposed 32 users to why an ad is shown to them, what advertising algorithms infer about them, and how advertisers use this information. Users preferred interpretable, non-creepy explanations about why an ad is presented, along with a recognizable link to their identity. We further found that exposing users to their algorithmically-derived attributes led to algorithm disillusionment---users found that advertising algorithms they thought were perfect were far from it. We propose design implications to effectively communicate information about advertising algorithms."}
{"metadata": {"dataset": "scidocs", "query_id": "0a40663fdcf7c5fb7cfc459693116c41309e7eca", "doc_id": "97bac7ce5d830ae475bd4f5485add51f6914f2ae"}, "id": "scidocs-0a40663fdcf7c5fb7cfc459693116c41309e7eca-97bac7ce5d830ae475bd4f5485add51f6914f2ae", "question": "Algorithmic Nuggets in Content Delivery", "context": "Content delivery networks play a crucial role in today’s Internet. They serve a large portion of the multimedia on the Internet and solve problems of scalability and indirectly network congestion (at a price). However, most content delivery networks rely on a statically deployed configuration of nodes and network topology that makes it hard to grow and scale dynamically. We present ActiveCDN, a novel CDN architecture that allows a content publisher to dynamically scale their content delivery services using network virtualization and cloud computing techniques."}
{"metadata": {"dataset": "scidocs", "query_id": "0a40663fdcf7c5fb7cfc459693116c41309e7eca", "doc_id": "2d54e36c216de770f849125a62b4a2e74284792a"}, "id": "scidocs-0a40663fdcf7c5fb7cfc459693116c41309e7eca-2d54e36c216de770f849125a62b4a2e74284792a", "question": "Algorithmic Nuggets in Content Delivery", "context": "Automatic multimedia learning resources recommendation has become an increasingly relevant problem: it allows students to discover new learning resources that match their tastes, and enables the e-learning system to target the learning resources to the right students. In this paper, we propose a content-based recommendation algorithm based on convolutional neural network (CNN). The CNN can be used to predict the latent factors from the text information of the multimedia resources. To train the CNN, its input and output should first be solved. For its input, the language model is used. For its output, we propose the latent factor model, which is regularized by L 1-norm. Furthermore, the split Bregman iteration method is introduced to solve the model. The major novelty of the proposed recommendation algorithm is that the text information is used directly to make the content-based recommendation without tagging. Experimental results on public databases in terms of quantitative assessment show significant improvements over conventional methods. In addition, the split Bregman iteration method which is introduced to solve the model can greatly improve the training efficiency."}
{"metadata": {"dataset": "scidocs", "query_id": "0a40663fdcf7c5fb7cfc459693116c41309e7eca", "doc_id": "996c790e05d113550cc1f6bb1143b92c6d07233f"}, "id": "scidocs-0a40663fdcf7c5fb7cfc459693116c41309e7eca-996c790e05d113550cc1f6bb1143b92c6d07233f", "question": "Algorithmic Nuggets in Content Delivery", "context": "Nowadays, a prosperity of electronic commerce (E-commerce) not only gives more convenience to consumers but brings more new opportunities in online advertising and marketing. Online advertisers can understand more about consumer preferences based on their daily online shopping and browsing behaviors. The development of big data and cloud computing techniques further empower advertisers and marketers to have a data-driven and consumer-specific preference recommendation based on the online browsing histories. In this research, a decision support system is proposed to predict a consumer purchase intention during browsing sessions. The proposed decision support system categorizes online browsing activities into purchase-oriented and general sessions using extreme boosting machines. With the browsing content entropy features, the proposed method achieves 41.81% recall and 34.35% F score. It further shows its strong predictive capability compared to other benchmark algorithms including logistic regression and traditional ensemble models. The proposed method can be implemented in real-time bidding algorithms for online advertising strategies. Ad deliveries on browsing session with potential purchase intention not only improve the effectiveness of advertisements but significantly increase last-touch attributions for campaign performance."}
{"metadata": {"dataset": "scidocs", "query_id": "0a40663fdcf7c5fb7cfc459693116c41309e7eca", "doc_id": "a6df9a75a7a946cad8c32ee2a8c88d826a21430c"}, "id": "scidocs-0a40663fdcf7c5fb7cfc459693116c41309e7eca-a6df9a75a7a946cad8c32ee2a8c88d826a21430c", "question": "Algorithmic Nuggets in Content Delivery", "context": "This is the first time New York University (NYU) participates in the event nugget (EN) evaluation of the Text Analysis Conference (TAC). We developed EN systems for both subtasks of event nugget, i.e, EN Task 1: Event Nugget Detection and EN Task 2: Event Nugget Detection and Coreference. The systems are mainly based on our recent research on deep learning for event detection (Nguyen and Grishman, 2015a; Nguyen and Grishman, 2016a). Due to the limited time we could devote to system development this year, we only ran the systems on the English evaluation data. However, we expect that the adaptation of the current systems to new languages can be done quickly. The development experiments show that although our current systems do not rely on complicated feature engineering, they significantly outperform the reported systems last year for the EN subtasks on the 2015 evaluation data."}
{"metadata": {"dataset": "scidocs", "query_id": "0a40663fdcf7c5fb7cfc459693116c41309e7eca", "doc_id": "1d1ebcb8de65575d870ceacf4af613961caa2257"}, "id": "scidocs-0a40663fdcf7c5fb7cfc459693116c41309e7eca-1d1ebcb8de65575d870ceacf4af613961caa2257", "question": "Algorithmic Nuggets in Content Delivery", "context": "Text production is a key component of many NLP applications. In data-driven approaches, it is used for instance, to generate dialogue turns from dialogue moves [Wen et al., 2015, Wen et al., 2016, Novikova et al., 2017], to verbalise the content of Knowledge bases [Gardent et al., 2017a, Gardent et al., 2017b] or to generate natural English sentences from rich linguistic representations, such as dependency trees [Belz et al., 2011, Mille et al., 2018] or Abstract Meaning Representations [May and Priyadarshi, 2017, Konstas et al., 2017, Song et al., ]. In text-driven methods on the other hand, text production is at work in sentence compression [Knight and Marcu, 2000, Cohn and Lapata, 2008, Filippova and Strube, 2008, Pitler, 2010, Filippova et al., 2015, Toutanova et al., 2016]; sentence fusion [McKeown et al., 2010, Filippova, 2010, Thadani and McKeown, 2013]; paraphrasing [Dras, 1999, Barzilay and McKeown, 2001, Bannard and Callison-Burch, 2005, Wubben et al., 2010, Narayan et al., 2016, Dong et al., 2017, Mallinson et al., 2017]; sentence (or text) simplification [Siddharthan et al., 2004, Zhu et al., 2010, Woodsend and Lapata, 2011, Wubben et al., 2012, Narayan and Gardent, 2014, Xu et al., 2015, Narayan and Gardent, 2016, Zhang and Lapata, 2017, Narayan et al., 2017], text summarisation [Wan, 2010, Nenkova and McKeown, 2011, Woodsend and Lapata, 2010, Rush et al., 2015, Cheng and Lapata, 2016, Nallapati et al., 2016, Chen et al., 2016, Tan and Wan, 2017, See et al., 2017, Nallapati et al., 2017, Paulus et al., 2017, Yasunaga et al., 2017, Narayan et al., 2018a, Narayan et al., 2018b, Pasunuru and Bansal, 2018, Celikyilmaz et al., 2018] and end-to-end dialogue systems [Li et al., 2017]. Following the success of encoder-decoder models in modeling sequence-rewriting tasks such as machine translation [Sutskever et al., 2011, Bahdanau et al., 2014], deep learning models have successfully been applied to the various text production tasks. For instance, [Rush et al., 2015] utilize a local attention-based model for abstractive summarisation, [Shang et al., 2015] propose an encoder-decoder model for response generation, [See et al., 2017] uses a hybrid of encoder-decoder model and pointer network [Vinyals et al., 2015] for story highlight generation, [Dong et al., 2017] exploits an encoder-decoder model for question rephrasing and [Konstas et al., 2017] for AMR generation. In this tutorial, we will cover the fundamentals and the state-of-the-art research on neural models for text production. Each text production task raises a slightly different communication goal (e.g, how to take the dialogue context into account when producing a dialogue turn; how to detect and merge relevant information when summarising a text; or how to produce a well-formed text that correctly capture the information contained in some input data in the case of data-to-text generation). We will outline the constraints specific to each subtasks and examine how the existing neural models account for them."}
{"metadata": {"dataset": "scidocs", "query_id": "0a40663fdcf7c5fb7cfc459693116c41309e7eca", "doc_id": "3d16326c34fdbf397876fcc173702846ae9f5fc0"}, "id": "scidocs-0a40663fdcf7c5fb7cfc459693116c41309e7eca-3d16326c34fdbf397876fcc173702846ae9f5fc0", "question": "Algorithmic Nuggets in Content Delivery", "context": "This paper presents an algorithm for content-based forwarding, an essential function in content-based networking. Unlike in traditional address-based unicast or multicast networks, where messages are given explicit destination addresses, the movement of messages through a content-based network is driven by predicates applied to the content of the messages. Forwarding in such a network amounts to evaluating the predicates stored in a router's forwarding table in order to decide to which neighbor routers the message should be sent. We are interested in finding a forwarding algorithm that can make this decision as quickly as possible in situations where there are numerous, complex predicates and high volumes of messages. We present such an algorithm and give the results of studies evaluating its performance."}
{"metadata": {"dataset": "scidocs", "query_id": "0a40663fdcf7c5fb7cfc459693116c41309e7eca", "doc_id": "9b2c2e87b90bf1cf7f41d796b2298204a10050a8"}, "id": "scidocs-0a40663fdcf7c5fb7cfc459693116c41309e7eca-9b2c2e87b90bf1cf7f41d796b2298204a10050a8", "question": "Algorithmic Nuggets in Content Delivery", "context": "Publish/subscribe systems based on gossip protocols are elastically to scale in and out and provides suitable consistency guarantees for data safety and high availability but, does not deal with end-to-end message delay and message order-based consistency. Especially in real-time collaborative applications, it is possible for the messages to take each a different time to arrive at end users. So, these applications should be based on P/S infrastructure including dealing with message deadlines and message ordering consistencies. Gossip communication is becoming one of the promising solutions for addressing P/S scalability problems in providing information propagation functionality by exploiting a mixture of diverse consistency options. In this paper, we present a new causal order protocol based on scalable P/S architecture for real-time collaborative applications in social web platforms to guarantee causally ordered message delivery, respecting deadline-constraints from brokers to subscribers. In the proposed protocol, every broker manages a 2-dimensional vector, representing its knowledge of the last message sent by each broker at a certain time. But, every broker disseminates a multicast message with a 1-dimensional vector, the time-stamped information that represents the maximum number of gossip rounds to subscribers because all messages disseminated by brokers have the same deadline as the maximum number of gossip rounds. Therefore, the proposed protocol for P/S based on gossiping results in very low communication overhead from brokers to subscribers in the context of respecting deadline-constraints."}
{"metadata": {"dataset": "scidocs", "query_id": "0ae51a9ac89e363097bcd675a56901b9444fd739", "doc_id": "no"}, "id": "scidocs-0ae51a9ac89e363097bcd675a56901b9444fd739-no", "question": "Construction and optimal search of interpolated motion graphs", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0ae51a9ac89e363097bcd675a56901b9444fd739", "doc_id": "c1f8a3a1b4df9b7856d4fbcfa91ef2752bcc7070"}, "id": "scidocs-0ae51a9ac89e363097bcd675a56901b9444fd739-c1f8a3a1b4df9b7856d4fbcfa91ef2752bcc7070", "question": "Construction and optimal search of interpolated motion graphs", "context": "The Iterative Closest Point (ICP) algorithm is widely used to register two roughly aligned surfaces. Its most expensive step is the search for the closest point. Many efficient variants have been proposed to speed up this step, however they do not guarantee that the closest point on a triangulated surface is found. Instead they might result in an approximation of the closest point. In this paper we present a method for the closest point search that is fast and exact. The method was implemented and used to evaluate the accuracy of head motion estimation using dense 3D data obtained from a stereo camera system. The results show that the accuracy of the estimated head motion is better than 1 mm for translational movements and better than 1 degree for rotations. Key-Words: Iterative Closest Point, 3D Registration, Stereo, Head Motion Estimation"}
{"metadata": {"dataset": "scidocs", "query_id": "0ae51a9ac89e363097bcd675a56901b9444fd739", "doc_id": "44e6fdffac4e670bd2a96203628fcfb6c6816ad4"}, "id": "scidocs-0ae51a9ac89e363097bcd675a56901b9444fd739-44e6fdffac4e670bd2a96203628fcfb6c6816ad4", "question": "Construction and optimal search of interpolated motion graphs", "context": "We present an energy-based approach to visual odometry from RGB-D images of a Microsoft Kinect camera. To this end we propose an energy function which aims at finding the best rigid body motion to map one RGB-D image into another one, assuming a static scene filmed by a moving camera. We then propose a linearization of the energy function which leads to a 6×6 normal equation for the twist coordinates representing the rigid body motion. To allow for larger motions, we solve this equation in a coarse-to-fine scheme. Extensive quantitative analysis on recently proposed benchmark datasets shows that the proposed solution is faster than a state-of-the-art implementation of the iterative closest point (ICP) algorithm by two orders of magnitude. While ICP is more robust to large camera motion, the proposed method gives better results in the regime of small displacements which are often the case in camera tracking applications."}
{"metadata": {"dataset": "scidocs", "query_id": "0ae51a9ac89e363097bcd675a56901b9444fd739", "doc_id": "1fc8209f86ec99e3275a2cb78153dc2ccc7e1be9"}, "id": "scidocs-0ae51a9ac89e363097bcd675a56901b9444fd739-1fc8209f86ec99e3275a2cb78153dc2ccc7e1be9", "question": "Construction and optimal search of interpolated motion graphs", "context": "Recent approaches on visual scene understanding attempt to build a scene graph – a computational representation of objects and their pairwise relationships. Such rich semantic representation is very appealing, yet difficult to obtain from a single image, especially when considering complex spatial arrangements in the scene. Differently, an image sequence conveys useful information using the multi-view geometric relations arising from camera motions. Indeed, object relationships are naturally related to the 3D scene structure. To this end, this paper proposes a system that first computes the geometrical location of objects in a generic scene and then efficiently constructs scene graphs from video by embedding such geometrical reasoning. Such compelling representation is obtained using a new model where geometric and visual features are merged using an RNN framework. We report results on a dataset we created for the task of 3D scene graph generation in multiple views."}
{"metadata": {"dataset": "scidocs", "query_id": "0ae51a9ac89e363097bcd675a56901b9444fd739", "doc_id": "14d079a4f3655034083fc749ba2f8f370f28a81a"}, "id": "scidocs-0ae51a9ac89e363097bcd675a56901b9444fd739-14d079a4f3655034083fc749ba2f8f370f28a81a", "question": "Construction and optimal search of interpolated motion graphs", "context": "This paper presents a volumetric formulation for the multiview stereo problem which is amenable to a computationally tractable global optimization using Graph-cuts. Our approach is to seek the optimal partitioning of 3D space into two regions labeled as \"object\" and \"empty\" under a cost functional consisting of the following two terms: 1) A term that forces the boundary between the two regions to pass through photo-consistent locations; and 2) a ballooning term that inflates the \"object\" region. To take account of the effect of occlusion on the first term, we use an occlusion robust photo-consistency metric based on normalized cross correlation, which does not assume any geometric knowledge about the reconstructed object. The globally optimal 3D partitioning can be obtained as the minimum cut solution of a weighted graph."}
{"metadata": {"dataset": "scidocs", "query_id": "0ae51a9ac89e363097bcd675a56901b9444fd739", "doc_id": "1f48eaf7c34a0faa9f533ed457b5e8e86cf1a15a"}, "id": "scidocs-0ae51a9ac89e363097bcd675a56901b9444fd739-1f48eaf7c34a0faa9f533ed457b5e8e86cf1a15a", "question": "Construction and optimal search of interpolated motion graphs", "context": "Multi-view structure from motion (SfM) estimates the position and orientation of pictures in a common 3D coordinate frame. When views are treated incrementally, this external calibration can be subject to drift, contrary to global methods that distribute residual errors evenly. We propose a new global calibration approach based on the fusion of relative motions between image pairs. We improve an existing method for robustly computing global rotations. We present an efficient a contrario trifocal tensor estimation method, from which stable and precise translation directions can be extracted. We also define an efficient translation registration method that recovers accurate camera positions. These components are combined into an original SfM pipeline. Our experiments show that, on most datasets, it outperforms in accuracy other existing incremental and global pipelines. It also achieves strikingly good running times: it is about 20 times faster than the other global method we could compare to, and as fast as the best incremental method. More importantly, it features better scalability properties."}
{"metadata": {"dataset": "scidocs", "query_id": "0ae51a9ac89e363097bcd675a56901b9444fd739", "doc_id": "f78f63a2a4093ec549ff2eaa683b247c9612a9ce"}, "id": "scidocs-0ae51a9ac89e363097bcd675a56901b9444fd739-f78f63a2a4093ec549ff2eaa683b247c9612a9ce", "question": "Construction and optimal search of interpolated motion graphs", "context": "This paper presents a high-accuracy method for fine registration of two partially overlapping point clouds that have been coarsely registered. The proposed algorithm, which is named dual interpolating point-tosurface method, is principally a modified variant of point-to-surface Iterative Closest Point (ICP) algorithm. The original correspondences are established by adopting a dual surface fitting approach using B-spline interpolation. A novel auxiliary pair constraint based on the surface fitting approach, together with surface curvature information, is employed to remove unreliable point matches. The combined constraint directly utilizes global rigid motion consistency in conjunction with local geometric invariant to reject false correspondences precisely and efficiently. The experimental results involving a number of realistic point clouds demonstrate that the new method can obtain accurate and robust fine registration for pairwise 3D point clouds. This method addresses highest accuracy alignment with less focus on recovery from poor coarse registrations. 2009 Elsevier B.V. All rights reserved."}
{"metadata": {"dataset": "scidocs", "query_id": "0ae51a9ac89e363097bcd675a56901b9444fd739", "doc_id": "3c5c3e264e238fe1b76cfe528a0ef26b97f7309b"}, "id": "scidocs-0ae51a9ac89e363097bcd675a56901b9444fd739-3c5c3e264e238fe1b76cfe528a0ef26b97f7309b", "question": "Construction and optimal search of interpolated motion graphs", "context": "This paper proposes an edge-directed interpolation algorithm for natural images. The basic idea is to first estimate local covariance coefficients from a low-resolution image and then use these covariance estimates to adapt the interpolation at a higher resolution based on the geometric duality between the low-resolution covariance and the high-resolution covariance. The edge-directed property of covariance-based adaptation attributes to its capability of tuning the interpolation coefficients to match an arbitrarily oriented step edge. A hybrid approach of switching between bilinear interpolation and covariance-based adaptive interpolation is proposed to reduce the overall computational complexity. Two important applications of the new interpolation algorithm are studied: resolution enhancement of grayscale images and reconstruction of color images from CCD samples. Simulation results demonstrate that our new interpolation algorithm substantially improves the subjective quality of the interpolated images over conventional linear interpolation."}
{"metadata": {"dataset": "scidocs", "query_id": "0ae51a9ac89e363097bcd675a56901b9444fd739", "doc_id": "5f8112731bd0a2bfa91158753242bae2f540d72d"}, "id": "scidocs-0ae51a9ac89e363097bcd675a56901b9444fd739-5f8112731bd0a2bfa91158753242bae2f540d72d", "question": "Construction and optimal search of interpolated motion graphs", "context": "This paper presents a color structured light technique for recovering object shape from one or more images. The technique works by projecting a pattern of stripes of alternating colors and matching the projected color transitions with observed edges in the image. The correspondence problem is solved using a novel, multi-pass dynamic programming algorithm that eliminates global smoothness assumptions and strict ordering constraints present in previous formulations. The resulting approach is suitable for generating both highspeed scans of moving objects when projecting a single stripe pattern and high-resolution scans of static scenes using a short sequence of time-shifted stripe patterns. In the latter case, spacetime analysis is used at each sensor pixel to obtain inter-frame depth localization. Results are demonstrated for a variety of complex scenes."}
{"metadata": {"dataset": "scidocs", "query_id": "0ae51a9ac89e363097bcd675a56901b9444fd739", "doc_id": "d764fd52625933692843941825845fcd5ee27f0d"}, "id": "scidocs-0ae51a9ac89e363097bcd675a56901b9444fd739-d764fd52625933692843941825845fcd5ee27f0d", "question": "Construction and optimal search of interpolated motion graphs", "context": "In this work, we study the unsupervised video object segmentation problem where moving objects are segmented without prior knowledge of these objects. First, we propose a motion-based bilateral network to estimate the background based on the motion pattern of non-object regions. The bilateral network reduces false positive regions by accurately identifying background objects. Then, we integrate the background estimate from the bilateral network with instance embeddings into a graph, which allows multiple frame reasoning with graph edges linking pixels from different frames. We classify graph nodes by defining and minimizing a cost function, and segment the video frames based on the node labels. The proposed method outperforms previous state-of-the-art unsupervised video object segmentation methods against the DAVIS 2016 and the FBMS-59 datasets."}
{"metadata": {"dataset": "scidocs", "query_id": "0ae51a9ac89e363097bcd675a56901b9444fd739", "doc_id": "532b273366aefbfd3ac598f11d4331603c4716b5"}, "id": "scidocs-0ae51a9ac89e363097bcd675a56901b9444fd739-532b273366aefbfd3ac598f11d4331603c4716b5", "question": "Construction and optimal search of interpolated motion graphs", "context": "In this paper, we present a fast new fully dynamic algorithm for the st-mincut/max-flow problem. We show how this algorithm can be used to efficiently compute MAP estimates for dynamically changing MRF models of labeling problems in computer vision, such as image segmentation. Specifically, given the solution of the max-flow problem on a graph, we show how to efficiently compute the maximum flow in a modified version of the graph. Our experiments showed that the time taken by our algorithm is roughly proportional to the number of edges whose weights were different in the two graphs. We test the performance of our algorithm on one particular problem: the object-background segmentation problem for video and compare it with the best known st-mincut algorithm. The results show that the dynamic graph cut algorithm is much faster than its static counterpart and enables real time image segmentation. It should be noted that our method is generic and can be used to yield similar improvements in many other cases that involve dynamic change in the graph"}
{"metadata": {"dataset": "scidocs", "query_id": "0b40af1ad2b9781fa14e999db2d7d3270b6d2862", "doc_id": "no"}, "id": "scidocs-0b40af1ad2b9781fa14e999db2d7d3270b6d2862-no", "question": "Data Clustering: 50 Years Beyond K-means", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0b40af1ad2b9781fa14e999db2d7d3270b6d2862", "doc_id": "d43f0d39a09691ac35d7a9592c06d2a698a349d7"}, "id": "scidocs-0b40af1ad2b9781fa14e999db2d7d3270b6d2862-d43f0d39a09691ac35d7a9592c06d2a698a349d7", "question": "Data Clustering: 50 Years Beyond K-means", "context": "In the big data era, k-means clustering has been widely adopted as a basic processing tool in various contexts. However, its computational cost could be prohibitively high when the data size and the cluster number are large. The processing bottleneck of k-means lies in the operation of seeking the closest centroid in each iteration. In this paper, a novel solution towards the scalability issue of k-means is presented. In the proposal, k-means is supported by an approximate k-nearest neighbors graph. In the k-means iteration, each data sample is only compared to clusters that its nearest neighbors reside. Since the number of nearest neighbors we consider is much less than k, the processing cost in this step becomes minor and irrelevant to k. The processing bottleneck is therefore broken. The most interesting thing is that k-nearest neighbor graph is constructed by calling the fast k-means itself. Compared with existing fast k-means variants, the proposed algorithm achieves hundreds to thousands times speed-up while maintaining high clustering quality. As it is tested on 10 million 512-dimensional data, it takes only 5.2 hours to produce 1 million clusters. In contrast, it would take 3 years for traditional k-means to fulfill the same scale of clustering."}
{"metadata": {"dataset": "scidocs", "query_id": "0b40af1ad2b9781fa14e999db2d7d3270b6d2862", "doc_id": "2be92d9137f4af64a059da33a58b148d153fc446"}, "id": "scidocs-0b40af1ad2b9781fa14e999db2d7d3270b6d2862-2be92d9137f4af64a059da33a58b148d153fc446", "question": "Data Clustering: 50 Years Beyond K-means", "context": "Data Clustering is used to extract meaningful information and to develop significant relationships among variables stored in large data set/data warehouse. In this paper data clustering technique named k-means clustering is applied to analyze student’s learning behavior. The student’s evaluation factor like class quizzes, mid and final exam assignment are studied. It is recommended that all these correlated information should be conveyed to the class advisor before the conduction of final exam. This study will help the teachers to reduce the drop out ratio to a significant level and improve the performance of students."}
{"metadata": {"dataset": "scidocs", "query_id": "0b40af1ad2b9781fa14e999db2d7d3270b6d2862", "doc_id": "8deafc34941a79b9cfc348ab63ec51752c7b1cde"}, "id": "scidocs-0b40af1ad2b9781fa14e999db2d7d3270b6d2862-8deafc34941a79b9cfc348ab63ec51752c7b1cde", "question": "Data Clustering: 50 Years Beyond K-means", "context": "The exponential growth in the amount of data gathered from various sources has resulted in the need for more efficient algorithms to quickly analyze large datasets. Clustering techniques, like K-Means are useful in analyzing data in a parallel fashion. K-Means largely depends upon a proper initialization to produce optimal results. K-means++ initialization algorithm provides solution based on providing an initial set of centres to the K-Means algorithm. However, its inherent sequential nature makes it suffer from various limitations when applied to large datasets. For instance, it makes k iterations to find k centres. In this paper, we present an algorithm that attempts to overcome the drawbacks of previous algorithms. Our work provides a method to select a good initial seeding in less time, facilitating fast and accurate cluster analysis over large datasets."}
{"metadata": {"dataset": "scidocs", "query_id": "0b40af1ad2b9781fa14e999db2d7d3270b6d2862", "doc_id": "c0b54e53b2702f403369ceb9924769bfc4fcebb3"}, "id": "scidocs-0b40af1ad2b9781fa14e999db2d7d3270b6d2862-c0b54e53b2702f403369ceb9924769bfc4fcebb3", "question": "Data Clustering: 50 Years Beyond K-means", "context": "Clustering is a process that aims to group the similar records in one cluster and dissimilar records in different clusters. K-means is one of the most popular and well-known clustering technique for its simplicity and light weight. However, the main drawback of K-means clustering technique is that it requires a user (data miner) to estimate the number of clusters in advance. Another limitation of K-means is that it has a tendency to get stuck at local optima. In order to overcome these limitations many evolutionary algorithm based clustering techniques have been proposed since the 1990s and applied to various fields. In this paper, we present an up-to-date review of some major evolutionary algorithm based clustering techniques for the last twenty (20) years (1995-2015). A total of 63 ranked (i.e. based on citation reports and JCR/CORE rank) evolutionary algorithm based clustering approaches are reviewed. Maximum of the techniques do not require any user to define the number of clusters in advance. We present the limitations and advantages of some evolutionary algorithm based clustering techniques. We also present a thorough discussion and future research directions of evolutionary algorithm based clustering techniques."}
{"metadata": {"dataset": "scidocs", "query_id": "0b40af1ad2b9781fa14e999db2d7d3270b6d2862", "doc_id": "0bacca0993a3f51649a6bb8dbb093fc8d8481ad4"}, "id": "scidocs-0b40af1ad2b9781fa14e999db2d7d3270b6d2862-0bacca0993a3f51649a6bb8dbb093fc8d8481ad4", "question": "Data Clustering: 50 Years Beyond K-means", "context": "Clustering is traditionally viewed as an unsupervised method for data analysis. However, in some cases information about the problem domain is available in addition to the data instances themselves. In this paper, we demonstrate how the popular k-means clustering algorithm can be profitably modified to make use of this information. In experiments with artificial constraints on six data sets, we observe improvements in clustering accuracy. We also apply this method to the real-world problem of automatically detecting road lanes from GPS data and observe dramatic increases in performance."}
{"metadata": {"dataset": "scidocs", "query_id": "0b40af1ad2b9781fa14e999db2d7d3270b6d2862", "doc_id": "41809d7fc7c41cf4d0afd5823034b5c0ac2949aa"}, "id": "scidocs-0b40af1ad2b9781fa14e999db2d7d3270b6d2862-41809d7fc7c41cf4d0afd5823034b5c0ac2949aa", "question": "Data Clustering: 50 Years Beyond K-means", "context": "Principal component analysis (PCA) is a widely used statistical technique for unsupervised dimension reduction. K-means clustering is a commonly used data clustering for performing unsupervised learning tasks. Here we prove that principal components are the continuous solutions to the discrete cluster membership indicators for K-means clustering. New lower bounds for K-means objective function are derived, which is the total variance minus the eigenvalues of the data covariance matrix. These results indicate that unsupervised dimension reduction is closely related to unsupervised learning. Several implications are discussed. On dimension reduction, the result provides new insights to the observed effectiveness of PCA-based data reductions, beyond the conventional noise-reduction explanation that PCA, via singular value decomposition, provides the best low-dimensional linear approximation of the data. On learning, the result suggests effective techniques for K-means data clustering. DNA gene expression and Internet newsgroups are analyzed to illustrate our results. Experiments indicate that the new bounds are within 0.5-1.5% of the optimal values."}
{"metadata": {"dataset": "scidocs", "query_id": "0b40af1ad2b9781fa14e999db2d7d3270b6d2862", "doc_id": "a4188db982a6e979d838c955a1688c1a10db5d48"}, "id": "scidocs-0b40af1ad2b9781fa14e999db2d7d3270b6d2862-a4188db982a6e979d838c955a1688c1a10db5d48", "question": "Data Clustering: 50 Years Beyond K-means", "context": "The clustering techniques are the most important part of the data analysis and k-means is the oldest and popular clustering technique used. The paper discusses the traditional K-means algorithm with advantages and disadvantages of it. It also includes researched on enhanced k-means proposed by various authors and it also includes the techniques to improve traditional K-means for better accuracy and efficiency. There are two area of concern for improving K-means; 1) is to select initial centroids and 2) by assigning data points to nearest cluster by using equations for calculating mean and distance between two data points. The time complexity of the proposed K-means technique will be lesser that then the traditional one with increase in accuracy and efficiency. The main purpose of the article is to proposed techniques to enhance the techniques for deriving initial centroids and the assigning of the data points to its nearest clusters. The clustering technique proposed in this paper is enhancing the accuracy and time complexity but it still needs some further improvements and in future it is also viable to include efficient techniques for selecting value for initial clusters(k). Experimental results show that the improved method can effectively improve the speed of clustering and accuracy, reducing the computational complexity of the k-means. Introduction Clustering is a process of grouping data objects into disjointed clusters so that the data in the same cluster are similar, but data belonging to different cluster differ. A cluster is a collection of data object that are similar to one another are in same cluster and dissimilar to the objects are in other clusters [k-4]. At present the applications of computer technology in increasing rapidly which Unnati R. Raval et al, International Journal of Computer Science and Mobile Computing, Vol.5 Issue.5, May2016, pg. 191-203 © 2016, IJCSMC All Rights Reserved 192 created high volume and high dimensional data sets [10]. These data is stored digitally in electronic media, thus providing potential for the development of automatic data analysis, classification and data retrieval [10]. The clustering is important part of the data analysis which partitioned given dataset in to subset of similar data points in each subset and dissimilar to data from other clusters [1]. The clustering analysis is very useful with increasing in digital data to draw meaningful information or drawing interesting patters from the data sets hence it finds applications in many fields like bioinformatics, pattern recognition, image processing, data mining, marketing and economics etc [4]. There have been many clustering techniques proposed but K-means is one of the oldest and most popular clustering techniques. In this method the number of cluster (k) is predefined prior to analysis and then the selection of the initial centroids will be made randomly and it followed by iterative process of assigning each data point to its nearest centroid. This process will keep repeating until convergence criteria met. However, there are shortcomings of K-means, it is important to proposed techniques that enhance the final result of analysis. This article includes researched on papers [1,2,3,4,5,6] which made some very important improvements towards the accuracy and efficiency of the clustering technique. Basic K-means Algorithm : A centroid-based Clustering technique According to the basic K-mean clustering algorithm, clusters are fully dependent on the selection of the initial clusters centroids. K data elements are selected as initial centers; then distances of all data elements are calculated by Euclidean distance formula. Data elements having less distance to centroids are moved to the appropriate cluster. The process is continued until no more changes occur in clusters[k-1]. This partitioning clustering is most popular and fundamental technique [1]. It is vastly used clustering technique which requires user specified parameters like number of clusters k, cluster initialisation and cluster metric [2]. First it needs to define initial clusters which makes subsets (or groups) of nearest points (from centroid) inside the data set and these subsets (or groups) called clusters [1]. Secondly, it finds means value for each cluster and define new centroid to allocate data points to this new centroid and this iterative process will goes on until centroid [3] does not changes. The simplest algorithm for the traditional K-means [2] is as follows; Input: D = {d1, d2, d3,.......dn} // set of n numbers of data points K // The number of desire Clusters Output: A set of k clusters 1. Select k points as initial centroids. 2. Repeat 3. From K clusters by assigning each data point to its nearest centroid. 4. Recompute the centroid for each cluster until centroid does not change [2]. Unnati R. Raval et al, International Journal of Computer Science and Mobile Computing, Vol.5 Issue.5, May2016, pg. 191-203 © 2016, IJCSMC All Rights Reserved 193 Start"}
{"metadata": {"dataset": "scidocs", "query_id": "0b40af1ad2b9781fa14e999db2d7d3270b6d2862", "doc_id": "10a7b139435977094d230414372a82cdfec6d8db"}, "id": "scidocs-0b40af1ad2b9781fa14e999db2d7d3270b6d2862-10a7b139435977094d230414372a82cdfec6d8db", "question": "Data Clustering: 50 Years Beyond K-means", "context": "To cluster increasingly massive data sets that are common today in data and text mining, we propose a parallel implementation of the k-means clustering algorithm based on the message passing model. The proposed algorithm exploits the inherent data-parallelism in the kmeans algorithm. We analytically show that the speedup and the scaleup of our algorithm approach the optimal as the number of data points increases. We implemented our algorithm on an IBM POWERparallel SP2 with a maximum of 16 nodes. On typical test data sets, we observe nearly linear relative speedups, for example, 15.62 on 16 nodes, and essentially linear scaleup in the size of the data set and in the number of clusters desired. For a 2 gigabyte test data set, our implementation drives the 16 node SP2 at more than 1.8 gigaflops."}
{"metadata": {"dataset": "scidocs", "query_id": "0b40af1ad2b9781fa14e999db2d7d3270b6d2862", "doc_id": "91a073617e082582cac90aea197535e9ab206349"}, "id": "scidocs-0b40af1ad2b9781fa14e999db2d7d3270b6d2862-91a073617e082582cac90aea197535e9ab206349", "question": "Data Clustering: 50 Years Beyond K-means", "context": "The proliferation and ubiquity of temporal data across many disciplines has generated substantial interest in the analysis and mining of time series. Clustering is one of the most popular data mining methods, not only due to its exploratory power, but also as a preprocessing step or subroutine for other techniques. In this paper, we describe k-Shape, a novel algorithm for time-series clustering. k-Shape relies on a scalable iterative refinement procedure, which creates homogeneous and well-separated clusters. As its distance measure, k-Shape uses a normalized version of the cross-correlation measure in order to consider the shapes of time series while comparing them. Based on the properties of that distance measure, we develop a method to compute cluster centroids, which are used in every iteration to update the assignment of time series to clusters. An extensive experimental evaluation against partitional, hierarchical, and spectral clustering methods, with the most competitive distance measures, showed the robustness of k-Shape. Overall, k-Shape emerges as a domain-independent, highly accurate, and efficient clustering approach for time series with broad applications."}
{"metadata": {"dataset": "scidocs", "query_id": "0b40af1ad2b9781fa14e999db2d7d3270b6d2862", "doc_id": "71ad31bd506ea571f6c04a293ff298f42fa7b47c"}, "id": "scidocs-0b40af1ad2b9781fa14e999db2d7d3270b6d2862-71ad31bd506ea571f6c04a293ff298f42fa7b47c", "question": "Data Clustering: 50 Years Beyond K-means", "context": "In this paper, we characterize Google applications, based on a one-month Google trace with over 650k jobs running across over 12000 heterogeneous hosts from a Google data center. On one hand, we carefully compute the valuable statistics about task events and resource utilization for Google applications, based on various types of resources (such as CPU, memory) and execution types (e.g., whether they can run batch tasks or not). Resource utilization per application is observed with an extremely typical Pareto principle. On the other hand, we classify applications via a K-means clustering algorithm with optimized number of sets, based on task events and resource usage. The number of applications in the K-means clustering sets follows a Pareto-similar distribution. We believe our work is very interesting and valuable for the further investigation of Cloud environment."}
{"metadata": {"dataset": "scidocs", "query_id": "0bb9e12f068657407cde9f76e35bd540184edb3e", "doc_id": "no"}, "id": "scidocs-0bb9e12f068657407cde9f76e35bd540184edb3e-no", "question": "Verification based ECG biometrics with cardiac irregular conditions using heartbeat level and segment level information fusion", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0bb9e12f068657407cde9f76e35bd540184edb3e", "doc_id": "36e4bad3400ce11b64db43ed47d2844f47678c5a"}, "id": "scidocs-0bb9e12f068657407cde9f76e35bd540184edb3e-36e4bad3400ce11b64db43ed47d2844f47678c5a", "question": "Verification based ECG biometrics with cardiac irregular conditions using heartbeat level and segment level information fusion", "context": "Security concerns increase as the technology for falsification advances. There are strong evidences that a difficult to falsify biometric trait, the human heartbeat, can be used for identity recognition. Existing solutions for biometric recognition from electrocardiogram (ECG) signals are based on temporal and amplitude distances between detected fiducial points. Such methods rely heavily on the accuracy of fiducial detection, which is still an open problem due to the difficulty in exact localization of wave boundaries. This paper presents a systematic analysis for human identification from ECG data. A fiducial-detection-based framework that incorporates analytic and appearance attributes is first introduced. The appearance-based approach needs detection of one fiducial point only. Further, to completely relax the detection of fiducial points, a new approach based on autocorrelation (AC) in conjunction with discrete cosine transform (DCT) is proposed. Experimentation demonstrates that the AC/DCT method produces comparable recognition accuracy with the fiducial-detection-based approach."}
{"metadata": {"dataset": "scidocs", "query_id": "0bb9e12f068657407cde9f76e35bd540184edb3e", "doc_id": "b77dc5d09e39caf87df306d6f1b5758907bbbe6a"}, "id": "scidocs-0bb9e12f068657407cde9f76e35bd540184edb3e-b77dc5d09e39caf87df306d6f1b5758907bbbe6a", "question": "Verification based ECG biometrics with cardiac irregular conditions using heartbeat level and segment level information fusion", "context": "An automatic configuration that can detect the position of R-waves, classify the normal sinus rhythm (NSR) and other four arrhythmic types from the continuous ECG signals obtained from the MIT-BIH arrhythmia database is proposed. In this configuration, a support vector machine (SVM) was used to detect and mark the ECG heartbeats with raw signals and differential signals of a lead ECG. An algorithm based on the extracted markers segments waveforms of Lead II and V1 of the ECG as the pattern classification features. A self-constructing neural fuzzy inference network (SoNFIN) was used to classify NSR and four arrhythmia types, including premature ventricular contraction (PVC), premature atrium contraction (PAC), left bundle branch block (LBBB), and right bundle branch block (RBBB). In a real scenario, the classification results show the accuracy achieved is 96.4%. This performance is suitable for a portable ECG monitor system for home care purposes."}
{"metadata": {"dataset": "scidocs", "query_id": "0bb9e12f068657407cde9f76e35bd540184edb3e", "doc_id": "61566dcd32e8e724353b95a469bf5849904a1058"}, "id": "scidocs-0bb9e12f068657407cde9f76e35bd540184edb3e-61566dcd32e8e724353b95a469bf5849904a1058", "question": "Verification based ECG biometrics with cardiac irregular conditions using heartbeat level and segment level information fusion", "context": "The electrocardiogram (ECG) is an emerging biometric modality that has seen about 13 years of development in peer-reviewed literature, and as such deserves a systematic review and discussion of the associated methods and findings. In this paper, we review most of the techniques that have been applied to the use of the electrocardiogram for biometric recognition. In particular, we categorize the methodologies based on the features and the classification schemes. Finally, a comparative analysis of the authentication performance of a few of the ECG biometric systems is presented, using our inhouse database. The comparative study includes the cases where training and testing data come from the same and different sessions (days). The authentication results show that most of the algorithms that have been proposed for ECG-based biometrics perform well when the training and testing data come from the same session. However, when training and testing data come from different sessions, a performance degradation occurs. Multiple training sessions were incorporated to diminish the loss in performance. That notwithstanding, only a few of the proposed ECG recognition algorithms appear to be able to support performance improvement due to multiple training sessions. Only three of these algorithms produced equal error rates (EERs) in the single digits, including an EER of 5.5% using a method proposed by us."}
{"metadata": {"dataset": "scidocs", "query_id": "0bb9e12f068657407cde9f76e35bd540184edb3e", "doc_id": "04e8648b268ffe16b0f0eab68402bdb433708b83"}, "id": "scidocs-0bb9e12f068657407cde9f76e35bd540184edb3e-04e8648b268ffe16b0f0eab68402bdb433708b83", "question": "Verification based ECG biometrics with cardiac irregular conditions using heartbeat level and segment level information fusion", "context": "The electrocardiogram (ECG also called EKG) trace expresses cardiac features that are unique to an individual. The ECG processing followed a logical series of experiments with quantifiable metrics. Data filters were designed based upon the observed noise sources. Fiducial points were identified on the filtered data and extracted digitally for each heartbeat. From the fiducial points, stable features were computed that characterize the uniqueness of an individual. The tests show that the extracted features are independent of sensor location, invariant to the individual’s state of anxiety, and unique to an individual. 2004 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved."}
{"metadata": {"dataset": "scidocs", "query_id": "0bb9e12f068657407cde9f76e35bd540184edb3e", "doc_id": "ff0f6c4c5913a909dc2f319fa68d064f4d93db5f"}, "id": "scidocs-0bb9e12f068657407cde9f76e35bd540184edb3e-ff0f6c4c5913a909dc2f319fa68d064f4d93db5f", "question": "Verification based ECG biometrics with cardiac irregular conditions using heartbeat level and segment level information fusion", "context": "Irrevocability is one major issue in existing bio-cryptosystems. In this paper, we proposed a cancellable bio-cryptosystem by taking the full advantage of cancellable and non-invertible properties of bio-hashing biometrics. Specifically, two transformed templates are generated by using the bio-hashing algorithm and applied into two different secure sketches, fuzzy commitment sketch and fuzzy vault sketch, respectively. These two secure sketches can be fused in two different ways: AND fusion and OR fusion, so as to emphasis either on the recognition accuracy or the security level of the system. Experimental results and security analysis show the validity of the proposed scheme."}
{"metadata": {"dataset": "scidocs", "query_id": "0bb9e12f068657407cde9f76e35bd540184edb3e", "doc_id": "5c11bab81f973d4d4ba2d9c4ca9e105e2b89cc0b"}, "id": "scidocs-0bb9e12f068657407cde9f76e35bd540184edb3e-5c11bab81f973d4d4ba2d9c4ca9e105e2b89cc0b", "question": "Verification based ECG biometrics with cardiac irregular conditions using heartbeat level and segment level information fusion", "context": "Objectives: Atrial fibrillation (AF) is a common heart rhythm disorder associated with deadly and debilitating consequences including heart failure, stroke, poor mental health, reduced quality of life and death. Having an automatic system that diagnoses various types of cardiac arrhythmias would assist cardiologists to initiate appropriate preventive measures and to improve the analysis of cardiac disease. To this end, this paper introduces a new approach to detect and classify automatically cardiac arrhythmias in electrocardiograms (ECG) recordings. Methods: The proposed approach used a combination of Convolution Neural Networks (CNNs) and a sequence of Long Short-Term Memory (LSTM) units, with pooling, dropout and normalization techniques to improve their accuracy. The network predicted a classification at every 18th input sample and we selected the final prediction for classification. Results were cross-validated on the Physionet Challenge 2017 training dataset, which contains 8,528 single lead ECG recordings lasting from 9s to just over 60s. Results: Using the proposed structure and no explicit feature selection, 10-fold stratified cross-validation gave an overall F-measure of 0.83.10±0.015 on the held-out test data (mean ± standard deviation over all folds) and 0.80 on the hidden dataset of the Challenge entry server."}
{"metadata": {"dataset": "scidocs", "query_id": "0bb9e12f068657407cde9f76e35bd540184edb3e", "doc_id": "39655a2a25dae954bdbb6b679a86d6dfa39d47ad"}, "id": "scidocs-0bb9e12f068657407cde9f76e35bd540184edb3e-39655a2a25dae954bdbb6b679a86d6dfa39d47ad", "question": "Verification based ECG biometrics with cardiac irregular conditions using heartbeat level and segment level information fusion", "context": "In this work, a deep convolutional neural network (CNN) is proposed to detect atrial fibrillation (AF) among the normal, noisy and other categories of cardiac arrhythmias electrocardiogram (ECG) recordings. The proposed CNN is trained by stochastic gradient descent with the categorical cross-entropy loss function. The network performance is evaluated on training (75%) and validation (25%) data sets that are obtained from 2017 Physionet/CinC challenge database. The proposed CNN model respectively achieves the average accuracy and F1 score of 87% and 0.84 on validation data set. One of the main advantages of this work besides high accuracy and reliability, is to simplify the feature extraction process and to remove the need for detecting ECG signal fiducial points and extracting hand-crafted features unlike conventional methods available in the literature. Moreover, it provides an opportunity for ECG screening in a large population, especially for atrial fibrillation screening, using wearable devices such as OM apparel that records high-quality single channel ECG signal."}
{"metadata": {"dataset": "scidocs", "query_id": "0bb9e12f068657407cde9f76e35bd540184edb3e", "doc_id": "ea4a2dfa0eab0260a184e5aad0046d62c0dd52bc"}, "id": "scidocs-0bb9e12f068657407cde9f76e35bd540184edb3e-ea4a2dfa0eab0260a184e5aad0046d62c0dd52bc", "question": "Verification based ECG biometrics with cardiac irregular conditions using heartbeat level and segment level information fusion", "context": "Accurate measurement of heart sound and murmur parameters is of great importance in the automated analysis of phonocardiogram (PCG) signals. In this paper, we propose a novel unified PCG signal delineation and murmur classification method without the use of reference signal for automatic detection and classification of heart sounds and murmurs. The major components of the proposed method are the empirical wavelet transform-based PCG signal decomposition for discriminating heart sounds from heart murmurs and suppressing background noises, the Shannon entropy envelope extraction, the instantaneous phase-based boundary determination, heart sound and murmur parameter extraction, the systole/diastole discrimination and the decision rules-based murmur classification. The accuracy and robustness of the proposed method is evaluated using a wide variety of normal and abnormal PCG signals taken from the standard PCG databases, including PASCAL heart sounds challenge database, PhysioNet/CinC challenge heart sound database, and real-time PCG signals. Evaluation results show that the proposed method achieves an average sensitivity (Se) of 94.38%, positive predictivity (Pp) of 97.25%, and overall accuracy (OA) of 91.92% for heart sound segmentation and Se of 97.58%, Pp of 96.46%, and OA of 94.21% in detecting the presence of heart murmurs for SNR of 10 dB. The method yields an average classification accuracy of 95.5% for the PCG signals with SNR of 20 dB. Results show that the proposed method outperforms other existing heart sound segmentation and murmur classification methods."}
{"metadata": {"dataset": "scidocs", "query_id": "0bb9e12f068657407cde9f76e35bd540184edb3e", "doc_id": "0502017650165c742ad7eaf14fe72b5925c90707"}, "id": "scidocs-0bb9e12f068657407cde9f76e35bd540184edb3e-0502017650165c742ad7eaf14fe72b5925c90707", "question": "Verification based ECG biometrics with cardiac irregular conditions using heartbeat level and segment level information fusion", "context": "Over the past few years, the evaluation of Electrocardio-graphic (ECG) signals as a prospective biometric modality has revealed promising results. Given the vital and continuous nature of this information source, ECG signals offer several advantages to the field of biometrics; yet, several challenges currently prevent the ECG from being adopted as a biometric modality in operational settings. These arise partially due to ECG signal's clinical tradition and intru-siveness, but also from the lack of evidence on the permanence of the ECG templates over time. The problem of in-trusiveness has been recently overcome with the “off-the-person” approach for capturing ECG signals. In this paper we provide an evaluation of the permanence of ECG signals collected at the fingers, with respect to the biometric authentication performance. Our experimental results on a small dataset suggest that further research is necessary to account for and understand sources of variability found in some subjects. Despite these limitations, “off-the-person” ECG appears to be a viable trait for multi-biometric or standalone biometrics, low user throughput, real-world scenarios."}
{"metadata": {"dataset": "scidocs", "query_id": "0bb9e12f068657407cde9f76e35bd540184edb3e", "doc_id": "f62b9c6ef565e820d21dfa64e4aed00323a50417"}, "id": "scidocs-0bb9e12f068657407cde9f76e35bd540184edb3e-f62b9c6ef565e820d21dfa64e4aed00323a50417", "question": "Verification based ECG biometrics with cardiac irregular conditions using heartbeat level and segment level information fusion", "context": "BACKGROUND\nDiagnosing atrial fibrillation (AF) before ischemic stroke occurs is a priority for stroke prevention in AF. Smartphone camera-based photoplethysmographic (PPG) pulse waveform measurement discriminates between different heart rhythms, but its ability to diagnose AF in real-world situations has not been adequately investigated. We sought to assess the diagnostic performance of a standalone smartphone PPG application, Cardiio Rhythm, for AF screening in primary care setting.\n\n\nMETHODS AND RESULTS\nPatients with hypertension, with diabetes mellitus, and/or aged ≥65 years were recruited. A single-lead ECG was recorded by using the AliveCor heart monitor with tracings reviewed subsequently by 2 cardiologists to provide the reference standard. PPG measurements were performed by using the Cardiio Rhythm smartphone application. AF was diagnosed in 28 (2.76%) of 1013 participants. The diagnostic sensitivity of the Cardiio Rhythm for AF detection was 92.9% (95% CI] 77-99%) and was higher than that of the AliveCor automated algorithm (71.4% [95% CI 51-87%]). The specificities of Cardiio Rhythm and the AliveCor automated algorithm were comparable (97.7% [95% CI: 97-99%] versus 99.4% [95% CI 99-100%]). The positive predictive value of the Cardiio Rhythm was lower than that of the AliveCor automated algorithm (53.1% [95% CI 38-67%] versus 76.9% [95% CI 56-91%]); both had a very high negative predictive value (99.8% [95% CI 99-100%] versus 99.2% [95% CI 98-100%]).\n\n\nCONCLUSIONS\nThe Cardiio Rhythm smartphone PPG application provides an accurate and reliable means to detect AF in patients at risk of developing AF and has the potential to enable population-based screening for AF."}
{"metadata": {"dataset": "scidocs", "query_id": "0bbdd4905f23994e0b5a0d91fc332b2af336f1e8", "doc_id": "no"}, "id": "scidocs-0bbdd4905f23994e0b5a0d91fc332b2af336f1e8-no", "question": "Recipient Revocable Identity-Based Broadcast Encryption: How to Revoke Some Recipients in IBBE without Knowledge of the Plaintext", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0bbdd4905f23994e0b5a0d91fc332b2af336f1e8", "doc_id": "6ba7a9e2bc2c8f3fe7623fc3006c150eb5bf1717"}, "id": "scidocs-0bbdd4905f23994e0b5a0d91fc332b2af336f1e8-6ba7a9e2bc2c8f3fe7623fc3006c150eb5bf1717", "question": "Recipient Revocable Identity-Based Broadcast Encryption: How to Revoke Some Recipients in IBBE without Knowledge of the Plaintext", "context": "This paper describes the first identity-based broadcast encryption scheme (IBBE) with constant size ciphertexts and private keys. In our scheme, the public key is of size linear in the maximal size m of the set of receivers, which is smaller than the number of possible users (identities) in the system. Compared with a recent broadcast encryption system introduced by Boneh, Gentry and Waters (BGW), our system has comparable properties, but with a better efficiency: the public key is shorter than in BGW. Moreover, the total number of possible users in the system does not have to be fixed in the setup."}
{"metadata": {"dataset": "scidocs", "query_id": "0bbdd4905f23994e0b5a0d91fc332b2af336f1e8", "doc_id": "8d4ee7bb188474523fa9d36a68acf1672e6abf3a"}, "id": "scidocs-0bbdd4905f23994e0b5a0d91fc332b2af336f1e8-8d4ee7bb188474523fa9d36a68acf1672e6abf3a", "question": "Recipient Revocable Identity-Based Broadcast Encryption: How to Revoke Some Recipients in IBBE without Knowledge of the Plaintext", "context": "We propose a novel scheme for selective distribution of content, encoded as documents, that preserves the privacy of the users to whom the documents are delivered and is based on an efficient and novel group key management scheme. Our document broadcasting approach is based on access control policies specifying which users can access which documents, or subdocuments. Based on such policies, a broadcast document is segmented into multiple subdocuments, each encrypted with a different key. In line with modern attribute-based access control, policies are specified against identity attributes of users. However our broadcasting approach is privacy-preserving in that users are granted access to a specific document, or subdocument, according to the policies without the need of providing in clear information about their identity attributes to the document publisher. Under our approach, not only does the document publisher not learn the values of the identity attributes of users, but it also does not learn which policy conditions are verified by which users, thus inferences about the values of identity attributes are prevented. Moreover, our key management scheme on which the proposed broadcasting approach is based is efficient in that it does not require to send the decryption keys to the users along with the encrypted document. Users are able to reconstruct the keys to decrypt the authorized portions of a document based on subscription information they have received from the document publisher. The scheme also efficiently handles new subscription of users and revocation of subscriptions."}
{"metadata": {"dataset": "scidocs", "query_id": "0bbdd4905f23994e0b5a0d91fc332b2af336f1e8", "doc_id": "076be17f97325fda82d1537aaa48798eb66ba91f"}, "id": "scidocs-0bbdd4905f23994e0b5a0d91fc332b2af336f1e8-076be17f97325fda82d1537aaa48798eb66ba91f", "question": "Recipient Revocable Identity-Based Broadcast Encryption: How to Revoke Some Recipients in IBBE without Knowledge of the Plaintext", "context": "Identity-based encryption (IBE) is an exciting alternative to public-key encryption, as IBE eliminates the need for a Public Key Infrastructure (PKI). The senders using an IBE do not need to look up the public keys and the corresponding certificates of the receivers, the identities (e.g. emails or IP addresses) of the latter are sufficient to encrypt. Any setting, PKI- or identity-based, must provide a means to revoke users from the system. Efficient revocation is a well-studied problem in the traditional PKI setting. However in the setting of IBE, there has been little work on studying the revocation mechanisms. The most practical solution requires the senders to also use time periods when encrypting, and all the receivers (regardless of whether their keys have been compromised or not) to update their private keys regularly by contacting the trusted authority. We note that this solution does not scale well -- as the number of users increases, the work on key updates becomes a bottleneck. We propose an IBE scheme that significantly improves key-update efficiency on the side of the trusted party (from linear to logarithmic in the number of users), while staying efficient for the users. Our scheme builds on the ideas of the Fuzzy IBE primitive and binary tree data structure, and is provably secure."}
{"metadata": {"dataset": "scidocs", "query_id": "0bbdd4905f23994e0b5a0d91fc332b2af336f1e8", "doc_id": "4987aaf293f1715aeda9387f832e3630a79fe74b"}, "id": "scidocs-0bbdd4905f23994e0b5a0d91fc332b2af336f1e8-4987aaf293f1715aeda9387f832e3630a79fe74b", "question": "Recipient Revocable Identity-Based Broadcast Encryption: How to Revoke Some Recipients in IBBE without Knowledge of the Plaintext", "context": "We present an Identity Based Encryption (IBE) system that is fully secure in the standard model and has several advantages over previous such systems – namely, computational efficiency, shorter public parameters, and a “tight” security reduction, albeit to a stronger assumption that depends on the number of private key generation queries made by the adversary. Our assumption is a variant of Boneh et al.’s decisional Bilinear Diffie-Hellman Exponent assumption, which has been used to construct efficient hierarchical IBE and broadcast encryption systems. The construction is remarkably simple. It also provides recipient anonymity automatically, providing a second (and more efficient) solution to the problem of achieving anonymous IBE without random oracles. Finally, our proof of CCA2 security, which has more in common with the security proof for the Cramer-Shoup encryption scheme than with security proofs for other IBE systems, may be of independent interest."}
{"metadata": {"dataset": "scidocs", "query_id": "0bbdd4905f23994e0b5a0d91fc332b2af336f1e8", "doc_id": "c4df7d84eb47e416af043dc20d053d3cb45e9571"}, "id": "scidocs-0bbdd4905f23994e0b5a0d91fc332b2af336f1e8-c4df7d84eb47e416af043dc20d053d3cb45e9571", "question": "Recipient Revocable Identity-Based Broadcast Encryption: How to Revoke Some Recipients in IBBE without Knowledge of the Plaintext", "context": "We introduce new theoretical measures for the qualitative and quantitative assessment of encryption schemes designed for broadcast transmissions. The goal is to allow a central broadcast site to broadcast secure transmissions to an arbitrary set of recipients while minimizing key management related transmissions. We present several schemes that allow a center to broadcast a secret to any subset of privileged users out of a universe of size so that coalitions of users not in the privileged set cannot learn the secret. The most interesting scheme requires every user to store keys and the center to broadcast messages regardless of the size of the privileged set. This scheme is resilient to any coalition of users. We also present a scheme that is resilient with probability against a random subset of users. This scheme requires every user to store keys and the center to broadcast messages. Preliminary version appeared in Advances in Cryptology CRYPTO ’93 Proceedings, Lecture Notes in Computer Science, Vol. 773, 1994, pp. 480–491. Dept. of Computer Science, School of Mathematics, Tel Aviv University, Tel Aviv, Israel, and Algorithmic Research Ltd. E-mail fiat@math.tau.ac.il. Incumbent of the Morris and Rose Goldman Career Development Chair, Dept. of Applied Mathematics and Computer Science, Weizmann Institute of Science, Rehovot 76100, Israel. Research supported by an Alon Fellowship and a grant from the Israel Science Foundation administered by the Israeli Academy of Sciences. E-mail: naor@wisdom.weizmann.ac.il."}
{"metadata": {"dataset": "scidocs", "query_id": "0bbdd4905f23994e0b5a0d91fc332b2af336f1e8", "doc_id": "6ac3171269609e4c2ca1c3326d1433a9c5b6c121"}, "id": "scidocs-0bbdd4905f23994e0b5a0d91fc332b2af336f1e8-6ac3171269609e4c2ca1c3326d1433a9c5b6c121", "question": "Recipient Revocable Identity-Based Broadcast Encryption: How to Revoke Some Recipients in IBBE without Knowledge of the Plaintext", "context": "Abst rac t . A broadcast encryption system allows a center to communicate securely over a broadcast channel with selected sets of users. Each time the set of privileged users changes, the center enacts a protocol to establish a new broadcast key that only the privileged users can obtain, and subsequent transmissions by the center are encrypted using the new broadcast key. We study the inherent trade-off between the number of establishment keys held by each user and the number of transmissions needed to establish a new broadcast key. For every given upper bound on the number of establishment keys held by each user, we prove a lower bound on the number of transmissions needed to establish a new broad~ cast key. We show that these bounds are essentially tight, by describing broadcast encryption systems that come close to these bounds."}
{"metadata": {"dataset": "scidocs", "query_id": "0bbdd4905f23994e0b5a0d91fc332b2af336f1e8", "doc_id": "da09bc42bbf5421b119abea92716186a1ca3f02f"}, "id": "scidocs-0bbdd4905f23994e0b5a0d91fc332b2af336f1e8-da09bc42bbf5421b119abea92716186a1ca3f02f", "question": "Recipient Revocable Identity-Based Broadcast Encryption: How to Revoke Some Recipients in IBBE without Knowledge of the Plaintext", "context": "We introduce a new type of Identity-Based Encryption (IBE) scheme that we call Fuzzy Identity-Based Encryption. In Fuzzy IBE we view an identity as set of descriptive attributes. A Fuzzy IBE scheme allows for a private key for an identity, ω, to decrypt a ciphertext encrypted with an identity, ω′, if and only if the identities ω and ω′ are close to each other as measured by the “set overlap” distance metric. A Fuzzy IBE scheme can be applied to enable encryption using biometric inputs as identities; the error-tolerance property of a Fuzzy IBE scheme is precisely what allows for the use of biometric identities, which inherently will have some noise each time they are sampled. Additionally, we show that Fuzzy-IBE can be used for a type of application that we term “attribute-based encryption”. In this paper we present two constructions of Fuzzy IBE schemes. Our constructions can be viewed as an Identity-Based Encryption of a message under several attributes that compose a (fuzzy) identity. Our IBE schemes are both error-tolerant and secure against collusion attacks. Additionally, our basic construction does not use random oracles. We prove the security of our schemes under the Selective-ID security model."}
{"metadata": {"dataset": "scidocs", "query_id": "0bbdd4905f23994e0b5a0d91fc332b2af336f1e8", "doc_id": "2a61ff2716b68672cbfeddb3ecd166dc856afe96"}, "id": "scidocs-0bbdd4905f23994e0b5a0d91fc332b2af336f1e8-2a61ff2716b68672cbfeddb3ecd166dc856afe96", "question": "Recipient Revocable Identity-Based Broadcast Encryption: How to Revoke Some Recipients in IBBE without Knowledge of the Plaintext", "context": "Multicast stream authentication and signing is an important and challenging problem. Applications include the continuous authentication of radio and TV Internet broadcasts, and authenticated data distribution by satellite. The main challenges are fourfold. First, authenticity must be guaranteed even when only the sender of the data is trusted. Second, the scheme needs to scale to potentially millions of receivers. Third, streamed media distribution can have high packet loss. Finally, the system needs to be efficient to support fast packet rates. We propose two efficient schemes, TESLA and EMSS, for secure lossy multicast streams. TESLA, short for Timed Efficient Stream Loss-tolerant Authentication, offers sender authentication, strong loss robustness, high scalability, and minimal overhead, at the cost of loose initial time synchronization and slightly delayed authentication. EMSS, short for Efficient Multi-chained Stream Signature, provides nonrepudiation of origin, high loss resistance, and low overhead, at the cost of slightly delayed verification. This work began in Summer 1999 when Adrian Perrig and Dawn Song were visiting the IBM T. J. Watson research lab. Initial research on stream authentication was done during Summer 1999 by Ran Canetti, Adrian Perrig, and Dawn Song at IBM. Additional improvements were suggested by J. D. Tygar in Fall 1999 at UC Berkeley. Implementation was done in Fall 1999 by Adrian Perrig at UC Berkeley. The work on stream signatures was done by J. D. Tygar, Adrian Perrig, and Dawn Song at UC Berkeley. Additional work was performed by Ran Canetti in Spring 2000. Ran Canetti is at IBM T. J. Watson Research Center, and Adrian Perrig, Dawn Song, and J. D. Tygar are at the Computer Science Division, UC Berkeley. This research was suported in part by the Defense Advanced Research Projects Agency under DARPA contract N6601-99-28913 (under supervision of the Space and Naval Warfare Systems Center San Diego), by the National Science foundation under grant FD99-79852, and by the United States Postal Service under grant USPS 1025 90-98-C-3513. Views and conclusions contained in this document are those of the authors and do not necessarily represent the official opinion or policies, either expressed or implied of the US government or any of its agencies, DARPA, NSF, USPS, or IBM."}
{"metadata": {"dataset": "scidocs", "query_id": "0bbdd4905f23994e0b5a0d91fc332b2af336f1e8", "doc_id": "22ab75ccad4fe337b330004111e255ee89e2ed31"}, "id": "scidocs-0bbdd4905f23994e0b5a0d91fc332b2af336f1e8-22ab75ccad4fe337b330004111e255ee89e2ed31", "question": "Recipient Revocable Identity-Based Broadcast Encryption: How to Revoke Some Recipients in IBBE without Knowledge of the Plaintext", "context": "Traditional user authentication involves entering a username and password into a system. Strong authentication security demands, among other requirements, long, frequently hard-to-remember passwords. Two-factor authentication aids in the security, even though, as a side effect, might worsen user experience. We depict a mobile sign-on scheme that benefits from the dynamic relationship between a user's attributes, the service the user wishes to utilize, and location (where the user is, and what services are available there) as an authentication factor. We demonstrate our scheme employing Bluetooth Low Energy beacons for location awareness and the expressiveness of Attribute-Based Encryption to capture and leverage the described relationship. Bluetooth Low Energy beacons broadcast encrypted messages with encoded access policies. Within range of the beacons, a user with appropriate attributes is able to decrypt the broadcast message and obtain parameters that allow the user to perform a short or simplified login."}
{"metadata": {"dataset": "scidocs", "query_id": "0bbdd4905f23994e0b5a0d91fc332b2af336f1e8", "doc_id": "0b277244b78a172394d3cbb68cc068fb1ebbd745"}, "id": "scidocs-0bbdd4905f23994e0b5a0d91fc332b2af336f1e8-0b277244b78a172394d3cbb68cc068fb1ebbd745", "question": "Recipient Revocable Identity-Based Broadcast Encryption: How to Revoke Some Recipients in IBBE without Knowledge of the Plaintext", "context": "As more sensitive data is shared and stored by third-party sites on the Internet, there will be a need to encrypt data stored at these sites. One drawback of encrypting data, is that it can be selectively shared only at a coarse-grained level (i.e., giving another party your private key). We develop a new cryptosystem for fine-grained sharing of encrypted data that we call Key-Policy Attribute-Based Encryption (KP-ABE). In our cryptosystem, ciphertexts are labeled with sets of attributes and private keys are associated with access structures that control which ciphertexts a user is able to decrypt. We demonstrate the applicability of our construction to sharing of audit-log information and broadcast encryption. Our construction supports delegation of private keys which subsumesHierarchical Identity-Based Encryption (HIBE)."}
{"metadata": {"dataset": "scidocs", "query_id": "0bdb616e15d3d6f17b90e2e5c588bfecac13768a", "doc_id": "no"}, "id": "scidocs-0bdb616e15d3d6f17b90e2e5c588bfecac13768a-no", "question": "Odin's Runes: A Rule Language for Information Extraction", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0bdb616e15d3d6f17b90e2e5c588bfecac13768a", "doc_id": "7891d3a91b86190df1756b08e69f879108579f40"}, "id": "scidocs-0bdb616e15d3d6f17b90e2e5c588bfecac13768a-7891d3a91b86190df1756b08e69f879108579f40", "question": "Odin's Runes: A Rule Language for Information Extraction", "context": "Rule-based information extraction is an important approach for processing the increasingly available amount of unstructured data. The manual creation of rule-based applications is a time-consuming and tedious task, which requires qualified knowledge engineers. The costs of this process can be reduced by providing a suitable rule language and extensive tooling support. This paper presents UIMA Ruta, a tool for rule-based information extraction and text processing applications. The system was designed with focus on rapid development. The rule language and its matching paradigm facilitate the quick specification of comprehensible extraction knowledge. They support a compact representation while still providing a high level of expressiveness. These advantages are supplemented by the development environment UIMA Ruta Workbench. It provides, in addition to extensive editing support, essential assistance for explanation of rule execution, introspection, automatic validation, and rule induction. UIMA Ruta is a useful tool for academia and industry due to its open source license. We compare UIMA Ruta to related rule-based systems especially concerning the compactness of the rule representation, the expressiveness, and the provided tooling support. The competitiveness of the runtime performance is shown in relation to a popular and freelyavailable system. A selection of case studies implemented with UIMA Ruta illustrates the usefulness of the system in real-world scenarios."}
{"metadata": {"dataset": "scidocs", "query_id": "0bdb616e15d3d6f17b90e2e5c588bfecac13768a", "doc_id": "16bd1fbe3694173eda4ad4338a85f8288d19bf02"}, "id": "scidocs-0bdb616e15d3d6f17b90e2e5c588bfecac13768a-16bd1fbe3694173eda4ad4338a85f8288d19bf02", "question": "Odin's Runes: A Rule Language for Information Extraction", "context": "Information extraction is a form of shallow text processing which locates a specified set of relevant items in natural language documents. Such systems can be useful, but require domain-specific knowledge and rules, and are time-consuming and difficult to build by hand, making infomation extraction a good testbed for the application of machine learning techniques to natural language processing. This paper presents a system, RAPIER, that takes pairs of documents and filled templates and induces pattern-match rules that directly extract fillers for the slots in the template. The learning algorithm incorporates techniques from several inductive logic programming systems and learns unbounded patterns that include constraints on the words and part-of-speech tags surrounding the filler. Encouraging results are presented on learning to extract information from computer job postings from the newsgroup misc. jobs. offered."}
{"metadata": {"dataset": "scidocs", "query_id": "0bdb616e15d3d6f17b90e2e5c588bfecac13768a", "doc_id": "016d34269a505a74d1f481314b30c13049d993bb"}, "id": "scidocs-0bdb616e15d3d6f17b90e2e5c588bfecac13768a-016d34269a505a74d1f481314b30c13049d993bb", "question": "Odin's Runes: A Rule Language for Information Extraction", "context": "Information extraction is a form of shallow text processing that locates a specified set of relevant items in a natural-language document. Systems for this task require significant domain-specific knowledge and are time-consuming and difficult to build by ha nd, making them a good application for machine learning. We present an algorithm, R APIER, that uses pairs of sample documents and filled templates to induce pattern-match rules that dire ctly extract fillers for the slots in the template. RAPIER is a bottom-up learning algorithm that incorporates techni ques from several inductive logic programming systems. We have implemented t h algorithm in a system that allows patterns to have constraints on the words, part-of-speech t ags, and semantic classes present in the filler and the surrounding text. We present encouraging expe rimental results on two domains."}
{"metadata": {"dataset": "scidocs", "query_id": "0bdb616e15d3d6f17b90e2e5c588bfecac13768a", "doc_id": "e98d0ac651dd00ba690647ad62e2c8832e5b3434"}, "id": "scidocs-0bdb616e15d3d6f17b90e2e5c588bfecac13768a-e98d0ac651dd00ba690647ad62e2c8832e5b3434", "question": "Odin's Runes: A Rule Language for Information Extraction", "context": "Information Extraction (IE) is the key technology enabling analytics over unstructured and semi-structured data. Not surprisingly, it is becoming a critical building block for a wide range of emerging applications. To satisfy the rising demands for information extraction in real-world applications, it is crucial to lower the barrier to entry for IE development and enable users with general computer science background to develop higher quality extractors. In this demonstration, we present VINERY, an intuitive yet expressive visual IDE for information extraction. We show how it supports the full cycle of IE development without requiring a single line of code and enables a wide range of users to develop high quality IE extractors with minimal efforts. The extractors visually built in VINERY are automatically translated into semantically equivalent extractors in a state-of-the-art declarative language for IE. We also demonstrate how the auto-generated extractors can then be imported into a conventional Eclipse-based IDE for further enhancement. The results of our user studies indicate that VINERY is a significant step forward in facilitating extractor development for both expert and novice IE developers."}
{"metadata": {"dataset": "scidocs", "query_id": "0bdb616e15d3d6f17b90e2e5c588bfecac13768a", "doc_id": "916ac71109a4c59640ba2de167acab7276d33429"}, "id": "scidocs-0bdb616e15d3d6f17b90e2e5c588bfecac13768a-916ac71109a4c59640ba2de167acab7276d33429", "question": "Odin's Runes: A Rule Language for Information Extraction", "context": "Building shallow semantic representations from text corpora is the first step to perform more complex tasks such as text entailment, enrichment of knowledge bases, or question answering. Open Information Extraction (OIE) is a recent unsupervised strategy to extract billions of basic assertions from massive corpora, which can be considered as being a shallow semantic representation of those corpora. In this paper, we propose a new multilingual OIE system based on robust and fast rule-based dependency parsing. It permits to extract more precise assertions (verb-based triples) from text than state of the art OIE systems, keeping a crucial property of those systems: scaling to Web-size document collections."}
{"metadata": {"dataset": "scidocs", "query_id": "0bdb616e15d3d6f17b90e2e5c588bfecac13768a", "doc_id": "12dbbc1f31d302b528e7d260b6a51fb280112ab3"}, "id": "scidocs-0bdb616e15d3d6f17b90e2e5c588bfecac13768a-12dbbc1f31d302b528e7d260b6a51fb280112ab3", "question": "Odin's Runes: A Rule Language for Information Extraction", "context": "Research on information extraction (IE) seeks to distill relational tuples from natural language text, such as the contents of the WWW. Most IE work has focussed on identifying static facts, encoding them as binary relations. This is unfortunate, because the vast majority of facts are fluents, only holding true during an interval of time. It is less helpful to extract PresidentOf(Bill-Clinton, USA) without the temporal scope 1/20/93 1/20/01. This paper presents TIE, a novel, information-extraction system, which distills facts from text while inducing as much temporal information as possible. In addition to recognizing temporal relations between times and events, TIE performs global inference, enforcing transitivity to bound the start and ending times for each event. We introduce the notion of temporal entropy as a way to evaluate the performance of temporal IE systems and present experiments showing that TIE outperforms three alternative approaches."}
{"metadata": {"dataset": "scidocs", "query_id": "0bdb616e15d3d6f17b90e2e5c588bfecac13768a", "doc_id": "34ba24f51f8e1f15b9345382c3c3917a08b20325"}, "id": "scidocs-0bdb616e15d3d6f17b90e2e5c588bfecac13768a-34ba24f51f8e1f15b9345382c3c3917a08b20325", "question": "Odin's Runes: A Rule Language for Information Extraction", "context": "We propose ClausIE, a novel, clause-based approach to open information extraction, which extracts relations and their arguments from natural language text. ClausIE fundamentally differs from previous approaches in that it separates the detection of ``useful'' pieces of information expressed in a sentence from their representation in terms of extractions. In more detail, ClausIE exploits linguistic knowledge about the grammar of the English language to first detect clauses in an input sentence and to subsequently identify the type of each clause according to the grammatical function of its constituents. Based on this information, ClausIE is able to generate high-precision extractions; the representation of these extractions can be flexibly customized to the underlying application. ClausIE is based on dependency parsing and a small set of domain-independent lexica, operates sentence by sentence without any post-processing, and requires no training data (whether labeled or unlabeled). Our experimental study on various real-world datasets suggests that ClausIE obtains higher recall and higher precision than existing approaches, both on high-quality text as well as on noisy text as found in the web."}
{"metadata": {"dataset": "scidocs", "query_id": "0bdb616e15d3d6f17b90e2e5c588bfecac13768a", "doc_id": "202b3b3bb4a5190ce53b77564f9ae1dc65f3489b"}, "id": "scidocs-0bdb616e15d3d6f17b90e2e5c588bfecac13768a-202b3b3bb4a5190ce53b77564f9ae1dc65f3489b", "question": "Odin's Runes: A Rule Language for Information Extraction", "context": ""}
{"metadata": {"dataset": "scidocs", "query_id": "0bdb616e15d3d6f17b90e2e5c588bfecac13768a", "doc_id": "6f441e3db17dfb6e576890eeef3da29fb3e02c4d"}, "id": "scidocs-0bdb616e15d3d6f17b90e2e5c588bfecac13768a-6f441e3db17dfb6e576890eeef3da29fb3e02c4d", "question": "Odin's Runes: A Rule Language for Information Extraction", "context": "Although the OWL Web Ontology Language adds considerable expressive power to the Semantic Web it does have expressive limitations, particularly with respect to what can be said about properties. We present SWRL (the Semantic Web Rules Language), a Horn clause rules extension to OWL that overcomes many of these limitations. SWRL extends OWL in a syntactically and semantically coherent manner: the basic syntax for SWRL rules is an extension of the abstract syntax for OWL DL and OWL Lite; SWRL rules are given formal meaning via an extension of the OWL DL model-theoretic semantics; SWRL rules are given an XML syntax based on the OWL XML presentation syntax; and a mapping from SWRL rules to RDF graphs is given based on the OWL RDF/XML exchange syntax. We discuss the expressive power of SWRL, showing that the ontology consistency problem is undecidable, provide several examples of SWRL usage, and discuss a prototype implementation of reasoning support for SWRL."}
{"metadata": {"dataset": "scidocs", "query_id": "0bdb616e15d3d6f17b90e2e5c588bfecac13768a", "doc_id": "fc10f1ccd2396c1adb4652c807a0c4f6e7534624"}, "id": "scidocs-0bdb616e15d3d6f17b90e2e5c588bfecac13768a-fc10f1ccd2396c1adb4652c807a0c4f6e7534624", "question": "Odin's Runes: A Rule Language for Information Extraction", "context": "A new algorithm, called Hamming Clustering (HC), is proposed to extract a set of rules underlying a given classification problem. It is able to reconstruct the and-or expression associated with any Boolean function from a training set of"}
{"metadata": {"dataset": "scidocs", "query_id": "0bf046038a555bc848030a28530f9836e5611b96", "doc_id": "no"}, "id": "scidocs-0bf046038a555bc848030a28530f9836e5611b96-no", "question": "ModDrop: Adaptive Multi-Modal Gesture Recognition", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0bf046038a555bc848030a28530f9836e5611b96", "doc_id": "50ca90bc847694a7a2d9a291f0d903a15e408481"}, "id": "scidocs-0bf046038a555bc848030a28530f9836e5611b96-50ca90bc847694a7a2d9a291f0d903a15e408481", "question": "ModDrop: Adaptive Multi-Modal Gesture Recognition", "context": "We propose a generalized approach to human gesture recognition based on multiple data modalities such as depth video, articulated pose and speech. In our system, each gesture is decomposed into large-scale body motion and local subtle movements such as hand articulation. The idea of learning at multiple scales is also applied to the temporal dimension, such that a gesture is considered as a set of characteristic motion impulses, or dynamic poses. Each modality is first processed separately in short spatio-temporal blocks, where discriminative data-specific features are either manually extracted or learned. Finally, we employ a Recurrent Neural Network for modeling large-scale temporal dependencies, data fusion and ultimately gesture classification. Our experiments on the 2013 Challenge on Multimodal Gesture Recognition dataset have demonstrated that using multiple modalities at several spatial and temporal scales leads to a significant increase in performance allowing the model to compensate for errors of individual classifiers as well as noise in the separate channels."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf046038a555bc848030a28530f9836e5611b96", "doc_id": "84f7b7be76bc9f34e6ed9ee15defafaeb85ec419"}, "id": "scidocs-0bf046038a555bc848030a28530f9836e5611b96-84f7b7be76bc9f34e6ed9ee15defafaeb85ec419", "question": "ModDrop: Adaptive Multi-Modal Gesture Recognition", "context": "Gesture recognition aims to recognize meaningful movements of human bodies, and is of utmost importance in intelligent human–computer/robot interactions. In this paper, we present a multimodal gesture recognition method based on 3-D convolution and convolutional long-short-term-memory (LSTM) networks. The proposed method first learns short-term spatiotemporal features of gestures through the 3-D convolutional neural network, and then learns long-term spatiotemporal features by convolutional LSTM networks based on the extracted short-term spatiotemporal features. In addition, fine-tuning among multimodal data is evaluated, and we find that it can be considered as an optional skill to prevent overfitting when no pre-trained models exist. The proposed method is verified on the ChaLearn LAP large-scale isolated gesture data set (IsoGD) and the Sheffield Kinect gesture (SKIG) data set. The results show that our proposed method can obtain the state-of-the-art recognition accuracy (51.02% on the validation set of IsoGD and 98.89% on SKIG)."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf046038a555bc848030a28530f9836e5611b96", "doc_id": "180208256d32be9d25dbe79092e4c49ec400780f"}, "id": "scidocs-0bf046038a555bc848030a28530f9836e5611b96-180208256d32be9d25dbe79092e4c49ec400780f", "question": "ModDrop: Adaptive Multi-Modal Gesture Recognition", "context": "Microsoft Kinect's output is a multi-modal signal which gives RGB videos, depth sequences and skeleton information simultaneously. Various action recognition techniques focused on different single modalities of the signals and built their classifiers over the features extracted from one of these channels. For better recognition performance, it's desirable to fuse these multi-modal information into an integrated set of discriminative features. Most of current fusion methods merged heterogeneous features in a holistic manner and ignored the complementary properties of these modalities in finer levels. In this paper, we proposed a new hierarchical bag-of-words feature fusion technique based on multi-view structured spar-sity learning to fuse atomic features from RGB and skeletons for the task of action recognition."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf046038a555bc848030a28530f9836e5611b96", "doc_id": "7e49a6f11a8843b2ff5bdbf7cf95617c6219f757"}, "id": "scidocs-0bf046038a555bc848030a28530f9836e5611b96-7e49a6f11a8843b2ff5bdbf7cf95617c6219f757", "question": "ModDrop: Adaptive Multi-Modal Gesture Recognition", "context": "Action recognition in videos remains a challenging problem in the machine learning community. Particularly challenging is the differing degree of intra-class variation between actions: While background information is enough to distinguish certain classes, many others are abstract and require fine-grained knowledge for discrimination. To approach this problem, in this work we evaluate different modalities on the recently published Moments in Time dataset, a collection of one million videos of short length."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf046038a555bc848030a28530f9836e5611b96", "doc_id": "b38958628b36c53d02f66124e09dbd0be0bd6672"}, "id": "scidocs-0bf046038a555bc848030a28530f9836e5611b96-b38958628b36c53d02f66124e09dbd0be0bd6672", "question": "ModDrop: Adaptive Multi-Modal Gesture Recognition", "context": "Vision-based vehicle detection is an important issue for advanced driver assistance systems. In this paper, we presented an improved multi-vehicle detection and tracking method using cascade Adaboost and Adaptive Kalman filter(AKF) with target identity awareness. A cascade Adaboost classifier using Haar-like features was built for vehicle detection, followed by a more comprehensive verification process which could refine the vehicle hypothesis in terms of both location and dimension. In vehicle tracking, each vehicle was tracked with independent identity by an Adaptive Kalman filter in collaboration with a data association approach. The AKF adaptively adjusted the measurement and process noise covariance through on-line stochastic modelling to compensate the dynamics changes. The data association correctly assigned different detections with tracks using global nearest neighbour(GNN) algorithm while considering the local validation. During tracking, a temporal context based track management was proposed to decide whether to initiate, maintain or terminate the tracks of different objects, thus suppressing the sparse false alarms and compensating the temporary detection failures. Finally, the proposed method was tested on various challenging real roads, and the experimental results showed that the vehicle detection performance was greatly improved with higher accuracy and robustness."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf046038a555bc848030a28530f9836e5611b96", "doc_id": "473cbc5ec2609175041e1410bc6602b187d03b23"}, "id": "scidocs-0bf046038a555bc848030a28530f9836e5611b96-473cbc5ec2609175041e1410bc6602b187d03b23", "question": "ModDrop: Adaptive Multi-Modal Gesture Recognition", "context": "The paper describes a novel technique for the recognition of emotions from multimodal data. We focus on the recognition of the six prototypic emotions. The results from the facial expression recognition and from the emotion recognition from speech are combined using a bi-modal multimodal semantic data fusion model that determines the most probable emotion of the subject. Two types of models based on geometric face features for facial expression recognition are being used, depending on the presence or absence of speech. In our approach we define an algorithm that is robust to changes of face shape that occur during regular speech. The influence of phoneme generation on the face shape during speech is removed by using features that are only related to the eyes and the eyebrows. The paper includes results from testing the presented models."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf046038a555bc848030a28530f9836e5611b96", "doc_id": "3ee02dff33c03d98fb5a2cecf298b77171f0d0dc"}, "id": "scidocs-0bf046038a555bc848030a28530f9836e5611b96-3ee02dff33c03d98fb5a2cecf298b77171f0d0dc", "question": "ModDrop: Adaptive Multi-Modal Gesture Recognition", "context": "We constructed an acoustic, gesture-based recognition system called Multiwave, which leverages the Doppler Effect to translate multidimensional movements into user interface commands. Our system only requires the use of two speakers and a microphone to be operational. Since these components are already built in to most end user systems, our design makes gesture-based input more accessible to a wider range of end users. By generating a known high frequency tone from multiple speakers and detecting movement using changes in the sound waves, we are able to calculate a Euclidean representation of hand velocity that is then used for more natural gesture recognition and thus, more meaningful interaction mappings. We present the results of a user study of Multiwave to evaluate recognition rates for different gestures and report accuracy rates comparable to or better than the current state of the art. We also report subjective user feedback and some lessons learned from our system that provide additional insight for future applications of multidimensional gesture recognition."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf046038a555bc848030a28530f9836e5611b96", "doc_id": "edf5861705647e0670f841cc6abcd2371823b13e"}, "id": "scidocs-0bf046038a555bc848030a28530f9836e5611b96-edf5861705647e0670f841cc6abcd2371823b13e", "question": "ModDrop: Adaptive Multi-Modal Gesture Recognition", "context": "A real time static isolated gesture recognition application using a Hidden Markov Model approach with features extracted from gestures' silhouettes is presented. Nine different hand poses with various degrees of rotation are considered. The system, both simple and effective, uses color images of the hands to be recognized directly from the camera and is capable of processing 23 frames per second on a Quad Core Intel Processor."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf046038a555bc848030a28530f9836e5611b96", "doc_id": "4aa5191088edafc2d3ae6232d9db4145d0099529"}, "id": "scidocs-0bf046038a555bc848030a28530f9836e5611b96-4aa5191088edafc2d3ae6232d9db4145d0099529", "question": "ModDrop: Adaptive Multi-Modal Gesture Recognition", "context": "Multimodal emotion recognition is the task of detecting emotions present in user-generated multimedia content. Such resources contain complementary information in multiple modalities. A stiff challenge often faced is the complexity associated with feature-level fusion of these heterogeneous modes. In this paper, we propose a new feature-level fusion method based on self-attention mechanism. We also compare it with traditional fusion methods such as concatenation, outer-product, etc. Analyzed using textual and speech (audio) modalities, our results suggest that the proposed fusion method outperforms others in the context of utterance-level emotion recognition in videos."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf046038a555bc848030a28530f9836e5611b96", "doc_id": "926b99cbea04fb213c9984b10acf2235a3949ebb"}, "id": "scidocs-0bf046038a555bc848030a28530f9836e5611b96-926b99cbea04fb213c9984b10acf2235a3949ebb", "question": "ModDrop: Adaptive Multi-Modal Gesture Recognition", "context": "In this paper we propose a novel method for human action recognition based on boosted key-frame selection and correlated pyramidal motion feature representations. Instead of using an unsupervised method to detect interest points, a Pyramidal Motion Feature (PMF), which combines optical flow with a biologically inspired feature, is extracted from each frame of a video sequence. The AdaBoost learning algorithm is then applied to select the most discriminative frames from a large feature pool. In this way, we obtain the top-ranked boosted frames of each video sequence as the key frames which carry the most representative motion information. Furthermore, we utilise the correlogram which focuses not only on probabilistic distributions within one frame but also on the temporal relationships of the action sequence. In the classification phase, a Support-Vector Machine (SVM) is adopted as the final classifier for human action recognition. To demonstrate generalizability, our method has been systematically tested on a variety of datasets and shown to be more effective and accurate for action recognition compared to the previous work. We obtain overall accuracies of: 95.5%, 93.7%, and 36.5% with our proposed method on the KTH, the multiview IXMAS and the challenging HMDB51 datasets, respectively. & 2012 Elsevier Ltd. All rights reserved."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf8527d093600c50208faca0b32eef2372ec0d4", "doc_id": "no"}, "id": "scidocs-0bf8527d093600c50208faca0b32eef2372ec0d4-no", "question": "The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0bf8527d093600c50208faca0b32eef2372ec0d4", "doc_id": "f5d9c0182d8578f7c0a99ad9bdd4ff62e5f7c68d"}, "id": "scidocs-0bf8527d093600c50208faca0b32eef2372ec0d4-f5d9c0182d8578f7c0a99ad9bdd4ff62e5f7c68d", "question": "The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank", "context": "We tackle the problem of entity linking for large collections of online pages; Our system, ZenCrowd, identifies entities from natural language text using state of the art techniques and automatically connects them to the Linked Open Data cloud. We show how one can take advantage of human intelligence to improve the quality of the links by dynamically generating micro-tasks on an online crowdsourcing platform. We develop a probabilistic framework to make sensible decisions about candidate links and to identify unreliable human workers. We evaluate ZenCrowd in a real deployment and show how a combination of both probabilistic reasoning and crowdsourcing techniques can significantly improve the quality of the links, while limiting the amount of work performed by the crowd."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf8527d093600c50208faca0b32eef2372ec0d4", "doc_id": "4ba18b2f35515f7f3ad3bc38100730c5808a52af"}, "id": "scidocs-0bf8527d093600c50208faca0b32eef2372ec0d4-4ba18b2f35515f7f3ad3bc38100730c5808a52af", "question": "The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank", "context": "We describe a joint probabilistic model for modeling the contents and inter-connectivity of document collections such as sets of web pages or research paper archives. The model is based on a probabilistic factor decomposition and allows identifying principal topics of the collection as well as authoritative documents within those topics. Furthermore, the relationships between topics is mapped out in order to build a predictive model of link content. Among the many applications of this approach are information retrieval and search, topic identification, query disambiguation, focused web crawling, web authoring, and bibliometric analysis."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf8527d093600c50208faca0b32eef2372ec0d4", "doc_id": "0bee052af002eb197277cd222d62154c7de4ac8a"}, "id": "scidocs-0bf8527d093600c50208faca0b32eef2372ec0d4-0bee052af002eb197277cd222d62154c7de4ac8a", "question": "The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank", "context": "Web spam pages use various techniques to achieve higher-than-deserved rankings in a search engine’s results. While human experts can identify spam, it is too expensive to manually evaluate a large number of pages. Instead, we propose techniques to semiautomatically separate reputable, good pages from spam. We first select a small set of seed pages to be evaluated by an expert. Once we manually identify the reputable seed pages, we use the link structure of the web to discover other pages that are likely to be good. In this paper we discuss possible ways to implement the seed selection and the discovery of good pages. We present results of experiments run on the World Wide Web indexed by AltaVista and evaluate the performance of our techniques. Our results show that we can effectively filter out spam from a significant fraction of the web, based on a good seed set of less than 200 sites."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf8527d093600c50208faca0b32eef2372ec0d4", "doc_id": "38fcc670f7369b3ee7f402cc5b214262829c139f"}, "id": "scidocs-0bf8527d093600c50208faca0b32eef2372ec0d4-38fcc670f7369b3ee7f402cc5b214262829c139f", "question": "The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank", "context": "Factorizing low-rank matrices has many applications in machine learning and statistics. For probabilistic models in the Bayes optimal setting, a general expression for the mutual information has been proposed using heuristic statistical physics computations, and proven in few specific cases. Here, we show how to rigorously prove the conjectured formula for the symmetric rank-one case. This allows to express the minimal mean-square-error and to characterize the detectability phase transitions in a large set of estimation problems ranging from community detection to sparse PCA. We also show that for a large set of parameters, an iterative algorithm called approximate message-passing is Bayes optimal. There exists, however, a gap between what currently known polynomial algorithms can do and what is expected information theoretically. Additionally, the proof technique has an interest of its own and exploits three essential ingredients: the interpolation method introduced in statistical physics by Guerra, the analysis of the approximate message-passing algorithm and the theory of spatial coupling and threshold saturation in coding. Our approach is generic and applicable to other open problems in statistical estimation where heuristic statistical physics predictions are available. Consider the following probabilistic rank-one matrix estimation problem: one has access to noisy observations w=(wij)i,j=1 of the pair-wise product of the components of a vector s=(s1, . . . , sn)∈R with i.i.d components distributed as Si∼P0, i=1, . . . , n. The entries of w are observed through a noisy element-wise (possibly non-linear) output probabilistic channel Pout(wij |sisj/ √ n). The goal is to estimate the vector s from w assuming that both P0 and Pout are known and independent of n (noise is symmetric so that wij =wji). Many important problems in statistics and machine learning can be expressed in this way, such as sparse PCA [Zou et al. (2006)], the Wigner spike model [Johnstone and Lu (2012); Deshpande and Montanari (2014)], community detection [Deshpande et al. (2015)] or matrix completion [Candès and Recht (2009)]. Proving a result initially derived by a heuristic method from statistical physics, we give an explicit expression for the mutual information and the information theoretic minimal mean-square-error (MMSE) in the asymptotic n→+∞ limit. Our results imply that for 1 ar X iv :1 60 6. 04 14 2v 1 [ cs .I T ] 1 3 Ju n 20 16 a large region of parameters, the posterior marginal expectations of the underlying signal components (often assumed intractable to compute) can be obtained in the leading order in n using a polynomial-time algorithm called approximate message-passing (AMP) [Rangan and Fletcher (2012); Deshpande and Montanari (2014); Deshpande et al. (2015); Lesieur et al. (2015b)]. We also demonstrate the existence of a region where both AMP and spectral methods [Baik et al. (2005)] fail to provide a good answer to the estimation problem, while it is nevertheless information theoretically possible to do so. We illustrate our theorems with examples and also briefly discuss the implications in terms of computational complexity. 1. Setting and main results 1.1 The additive white Gaussian noise setting A standard and natural setting is the case of additive white Gaussian noise (AWGN) of known variance ∆, wij = sisj √ n + zij √ ∆, (1) where z=(zij)i,j=1 is a symmetric matrix with i.i.d entries Zij∼N (0, 1), 1≤ i≤j≤n. Perhaps surprisingly, it turns out that this Gaussian setting is sufficient to completely characterize all the problems discussed in the introduction, even if these have more complicated output channels. This is made possible by a theorem of channel universality [Krzakala et al. (2016)] (already proven for community detection in [Deshpande et al. (2015)] and conjectured in [Lesieur et al. (2015a)]). This theorem states that given an output channel Pout(w|y), such that logPout(w|y = 0) is three times differentiable with bounded second and third derivatives, then the mutual information satisfies I(S;W)=I(S;SST/ √ n+Z √ ∆)+O( √ n), where ∆ is the inverse Fisher information (evaluated at y = 0) of the output channel: ∆−1 := EPout(w|0)[(∂y logPout(W |y)|y=0)]. Informally, this means that we only have to compute the mutual information for an AWGN channel to take care of a wide range of problems, which can be expressed in terms of their Fisher information. In this paper we derive rigorously, for a large class of signal distributions P0, an explicit one-letter formula for the mutual information per variable I(S;W)/n in the asymptotic limit n→+∞."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf8527d093600c50208faca0b32eef2372ec0d4", "doc_id": "7d3cf8315cfb586dae984b0aa986ea0857c18cf0"}, "id": "scidocs-0bf8527d093600c50208faca0b32eef2372ec0d4-7d3cf8315cfb586dae984b0aa986ea0857c18cf0", "question": "The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank", "context": "One of the core tasks in social network analysis is to predict the formation of links (i.e. various types of relationships) over time. Previous research has generally represented the social network in the form of a graph and has leveraged topological and semantic measures of similarity between two nodes to evaluate the probability of link formation. Here we introduce a novel local probabilistic graphical model method that can scale to large graphs to estimate the joint co-occurrence probability of two nodes. Such a probability measure captures information that is not captured by either topological measures or measures of semantic similarity, which are the dominant measures used for link prediction. We demonstrate the effectiveness of the co-occurrence probability feature by using it both in isolation and in combination with other topological and semantic features for predicting co-authorship collaborations on real datasets."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf8527d093600c50208faca0b32eef2372ec0d4", "doc_id": "fad4cf87ca2f3948b7a71b306e321454af7b346b"}, "id": "scidocs-0bf8527d093600c50208faca0b32eef2372ec0d4-fad4cf87ca2f3948b7a71b306e321454af7b346b", "question": "The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank", "context": "The open nature of the World Wide Web makes evaluating webpage credibility challenging for users. In this paper, we aim to automatically assess web credibility by investigating various characteristics of webpages. Specifically, we first identify features from textual content, link structure, webpages design, as well as their social popularity learned from popular social media sites (e.g., Facebook, Twitter). A set of statistical analyses methods are applied to select the most informative features, which are then used to infer webpages credibility by employing supervised learning algorithms. Real dataset-based experiments under two application settings show that we attain an accuracy of 75% for classification, and an improvement of 53% for the mean absolute error (MAE), with respect to the random baseline approach, for regression."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf8527d093600c50208faca0b32eef2372ec0d4", "doc_id": "3b1953ff2c0c9dd045afe6766afb91599522052b"}, "id": "scidocs-0bf8527d093600c50208faca0b32eef2372ec0d4-3b1953ff2c0c9dd045afe6766afb91599522052b", "question": "The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank", "context": "This paper discusses efficient techniques for computing PageRank, a ranking metric for hypertext documents. We show that PageRank can be computed for very large subgraphs of the web (up to hundreds of millions of nodes) on machines with limited main memory. Running-time measurements on various memory configurations are presented for PageRank computation over the 24-million-page Stanford WebBase archive. We discuss several methods for analyzing the convergence of PageRank based on the induced ordering of the pages. We present convergence results helpful for determining the number of iterations necessary to achieve a useful PageRank assignment, both in the absence and presence of search queries."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf8527d093600c50208faca0b32eef2372ec0d4", "doc_id": "f0041b836d507b8d22367a6ef7faab583769de82"}, "id": "scidocs-0bf8527d093600c50208faca0b32eef2372ec0d4-f0041b836d507b8d22367a6ef7faab583769de82", "question": "The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank", "context": "Probabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data. Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm, the utilized model is able to deal with domain{specific synonymy as well as with polysemous words. In contrast to standard Latent Semantic Indexing (LSI) by Singular Value Decomposition, the probabilistic variant has a solid statistical foundation and defines a proper generative data model. Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching methods as well as over LSI. In particular, the combination of models with different dimensionalities has proven to be advantageous."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf8527d093600c50208faca0b32eef2372ec0d4", "doc_id": "65227ddbbd12015ba8a45a81122b1fa540e79890"}, "id": "scidocs-0bf8527d093600c50208faca0b32eef2372ec0d4-65227ddbbd12015ba8a45a81122b1fa540e79890", "question": "The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank", "context": "The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages objectively and mechanically, e ectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to e ciently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation."}
{"metadata": {"dataset": "scidocs", "query_id": "0bf8527d093600c50208faca0b32eef2372ec0d4", "doc_id": "3c37515e7037925c3f0a475b03be72dc853b8533"}, "id": "scidocs-0bf8527d093600c50208faca0b32eef2372ec0d4-3c37515e7037925c3f0a475b03be72dc853b8533", "question": "The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank", "context": "The presence (and, sometimes, prominence) of incorrect and misleading content on the Web can have serious consequences for people who increasingly rely on the internet as their information source for topics such as health, politics, and financial advice. In this paper, we identify and collect several page features (such as popularity among specialized user groups) that are currently difficult or impossible for end users to assess, yet provide valuable signals regarding credibility. We then present visualizations designed to augment search results and Web pages with the most promising of these features. Our lab evaluation finds that our augmented search results are particularly effective at increasing the accuracy of users'\" credibility assessments, highlighting the potential of data aggregation and simple interventions to help people make more informed decisions as they search for information online."}
{"metadata": {"dataset": "scidocs", "query_id": "0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512", "doc_id": "no"}, "id": "scidocs-0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512-no", "question": "Let's go public! taking a spoken dialog system to the real world", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512", "doc_id": "1d1861764141b0255389fecfc309ef74151033fc"}, "id": "scidocs-0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512-1d1861764141b0255389fecfc309ef74151033fc", "question": "Let's go public! taking a spoken dialog system to the real world", "context": "In this paper we discuss the recent evolution of spoken dialog systems in commercial deployments. Yet based on a simple finite state machine design paradigm, dialog systems reached today a higher level of complexity. The availability of massive amounts of data during deployment led to the development of continuous optimization strategy pushing the design and development of spoken dialog applications from an art to science. At the same time new methods for evaluating the subjective caller experience are available. Finally we describe the inevitable evolution for spoken dialog applications from speech only to multimodal interaction."}
{"metadata": {"dataset": "scidocs", "query_id": "0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512", "doc_id": "436c3119d16ce2e3c243ffe7a4a1a5dc40b128aa"}, "id": "scidocs-0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512-436c3119d16ce2e3c243ffe7a4a1a5dc40b128aa", "question": "Let's go public! taking a spoken dialog system to the real world", "context": "We describe an integrated approach for statistical modeling of discourse structure for natural conversational speech. Our model is based on 42`dialog acts' which were hand-labeled in 1155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech. We developed several models and algorithms to automatically detect dialog acts from transcribed or automatically recognized words and from prosodic properties of the speech signal, and by using a statistical discourse grammar. All of these components were probabilistic in nature and estimated from data, employing a variety of techniques (hidden Markov models, N-gram language models, maximum entropy estimation, decision tree classiiers, and neural networks). In preliminary studies, we achieved a dialog act labeling accuracy of 65% based on recognized words and prosody, and an accuracy of 72% based on word transcripts. Since humans achieve 84% on this task (with chance performance at 35%) we nd these results encouraging."}
{"metadata": {"dataset": "scidocs", "query_id": "0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512", "doc_id": "cfdef0cd7ec53868c600005ec74a4a34f063a004"}, "id": "scidocs-0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512-cfdef0cd7ec53868c600005ec74a4a34f063a004", "question": "Let's go public! taking a spoken dialog system to the real world", "context": "In a spoken dialog system, determining which action a machine should take in a given situation is a difficult problem because automatic speech recognition is unreliable and hence the state of the conversation can never be known with certainty. Much of the research in spoken dialog systems centres on mitigating this uncertainty and recent work has focussed on three largely disparate techniques: parallel dialog state hypotheses, local use of confidence scores, and automated planning. While in isolation each of these approaches can improve action selection, taken together they currently lack a unified statistical framework that admits global optimization. In this paper we cast a spoken dialog system as a partially observable Markov decision process (POMDP). We show how this formulation unifies and extends existing techniques to form a single principled framework. A number of illustrations are used to show qualitatively the potential benefits of POMDPs compared to existing techniques, and empirical results from dialog simulations are presented which demonstrate significant quantitative gains. Finally, some of the key challenges to advancing this method – in particular scalability – are briefly outlined. 2006 Elsevier Ltd. All rights reserved."}
{"metadata": {"dataset": "scidocs", "query_id": "0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512", "doc_id": "20690b7465e6fef5337f0c9be0a302d33b3c9b3a"}, "id": "scidocs-0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512-20690b7465e6fef5337f0c9be0a302d33b3c9b3a", "question": "Let's go public! taking a spoken dialog system to the real world", "context": "In a spoken dialog system, dialog state tracking deduces information about the user’s goal as the dialog progresses, synthesizing evidence such as dialog acts over multiple turns with external data sources. Recent approaches have been shown to overcome ASR and SLU errors in some applications. However, there are currently no common testbeds or evaluation measures for this task, hampering progress. The dialog state tracking challenge seeks to address this by providing a heterogeneous corpus of 15K human-computer dialogs in a standard format, along with a suite of 11 evaluation metrics. The challenge received a total of 27 entries from 9 research groups. The results show that the suite of performance metrics cluster into 4 natural groups. Moreover, the dialog systems that benefit most from dialog state tracking are those with less discriminative speech recognition confidence scores. Finally, generalization is a key problem: in 2 of the 4 test sets, fewer than half of the entries out-performed simple baselines. 1 Overview and motivation Spoken dialog systems interact with users via natural language to help them achieve a goal. As the interaction progresses, the dialog manager maintains a representation of the state of the dialog in a process called dialog state tracking (DST). For example, in a bus schedule information system, the dialog state might indicate the user’s desired bus route, origin, and destination. Dialog state tracking is difficult because automatic speech ∗Most of the work for the challenge was performed when the second and third authors were with Honda Research Institute, Mountain View, CA, USA recognition (ASR) and spoken language understanding (SLU) errors are common, and can cause the system to misunderstand the user’s needs. At the same time, state tracking is crucial because the system relies on the estimated dialog state to choose actions – for example, which bus schedule information to present to the user. Most commercial systems use hand-crafted heuristics for state tracking, selecting the SLU result with the highest confidence score, and discarding alternatives. In contrast, statistical approaches compute scores for many hypotheses for the dialog state (Figure 1). By exploiting correlations between turns and information from external data sources – such as maps, bus timetables, or models of past dialogs – statistical approaches can overcome some SLU errors. Numerous techniques for dialog state tracking have been proposed, including heuristic scores (Higashinaka et al., 2003), Bayesian networks (Paek and Horvitz, 2000; Williams and Young, 2007), kernel density estimators (Ma et al., 2012), and discriminative models (Bohus and Rudnicky, 2006). Techniques have been fielded which scale to realistically sized dialog problems and operate in real time (Young et al., 2010; Thomson and Young, 2010; Williams, 2010; Mehta et al., 2010). In end-to-end dialog systems, dialog state tracking has been shown to improve overall system performance (Young et al., 2010; Thomson and Young, 2010). Despite this progress, direct comparisons between methods have not been possible because past studies use different domains and system components, for speech recognition, spoken language understanding, dialog control, etc. Moreover, there is little agreement on how to evaluate dialog state tracking. Together these issues limit progress in this research area. The Dialog State Tracking Challenge (DSTC) provides a first common testbed and evaluation"}
{"metadata": {"dataset": "scidocs", "query_id": "0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512", "doc_id": "780b05a35f2c7dd4b4d6e2a844ef5e145f1972ae"}, "id": "scidocs-0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512-780b05a35f2c7dd4b4d6e2a844ef5e145f1972ae", "question": "Let's go public! taking a spoken dialog system to the real world", "context": "In multi-turn dialogs, natural language understanding models can introduce obvious errors by being blind to contextual information. To incorporate dialog history, we present a neural architecture with Speaker-Sensitive Dual Memory Networks which encode utterances differently depending on the speaker. This addresses the different extents of information available to the system — the system knows only the surface form of user utterances while it has the exact semantics of system output. We performed experiments on real user data from Microsoft Cortana, a commercial personal assistant. The result showed a significant performance improvement over the state-of-the-art slot tagging models using contextual information."}
{"metadata": {"dataset": "scidocs", "query_id": "0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512", "doc_id": "43d15ec7a3f7c26830541ea57f4af56b61983ca4"}, "id": "scidocs-0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512-43d15ec7a3f7c26830541ea57f4af56b61983ca4", "question": "Let's go public! taking a spoken dialog system to the real world", "context": "The Carnegie Mellon Communicator system helps users create complex travel itineraries through a conversational interface. Itineraries consist of (multi-leg) flights, hotel and car reservations and are built from actual travel information for North America, obtained from the Web. The system manages dialog using a schema-based approach. Schemas correspond to major units of task information (such as a flight leg) and define conversational topics, or foci of interaction, meaningful to the user."}
{"metadata": {"dataset": "scidocs", "query_id": "0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512", "doc_id": "2a94fa0de804b5efaae1a66f50c3ea96539c46b8"}, "id": "scidocs-0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512-2a94fa0de804b5efaae1a66f50c3ea96539c46b8", "question": "Let's go public! taking a spoken dialog system to the real world", "context": "This paper presents a design and experiments of developing a non-goal dialog system by utilizing human-to-human conversation examples from drama television. The aim is to build a conversational agent that can interact with users in as natural a fashion as possible, while reducing the time requirement for database design and collection. A number of the challenging design issues we faced are described, including (1) filtering and constructing a dialog example database from the drama conversations, and (2) retrieving a proper system response by finding the best dialog example based on the current user query. Subjective evaluation from a small user study is also discussed."}
{"metadata": {"dataset": "scidocs", "query_id": "0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512", "doc_id": "8efbb07f1ed1879a32574e7d3940063e9dafbadb"}, "id": "scidocs-0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512-8efbb07f1ed1879a32574e7d3940063e9dafbadb", "question": "Let's go public! taking a spoken dialog system to the real world", "context": "Recent spoken dialog systems have been able to recognize freely spoken user input in restricted domains thanks to statistical methods in the automatic speech recognition. These methods require a high number of natural language utterances to train the speech recognition engine and to assess the quality of the system. Since human speech offers many variants associated with a single intent, a high number of user utterances have to be elicited. Developers are therefore turning to crowdsourcing to collect this data. This paper compares three different methods to elicit multiple utterances for given semantics via crowd sourcing, namely with pictures, with text and with semantic entities. Specifically, we compare the methods with regard to the number of valid data and linguistic variance, whereby a quantitative and qualitative approach is proposed. In our study, the method with text led to a high variance in the utterances and a relatively low rate of invalid data."}
{"metadata": {"dataset": "scidocs", "query_id": "0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512", "doc_id": "078b55e2f4899cf95a4c8d65613c340fa190acf8"}, "id": "scidocs-0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512-078b55e2f4899cf95a4c8d65613c340fa190acf8", "question": "Let's go public! taking a spoken dialog system to the real world", "context": "A spoken dialog system, while communicating with a user, must keep track of what the user wants from the system at each step. This process, termed dialog state tracking, is essential for a successful dialog system as it directly informs the system’s actions. The first Dialog State Tracking Challenge allowed for evaluation of different dialog state tracking techniques, providing common testbeds and evaluation suites. This paper presents a second challenge, which continues this tradition and introduces some additional features – a new domain, changing user goals and a richer dialog state. The challenge received 31 entries from 9 research groups. The results suggest that while large improvements on a competitive baseline are possible, trackers are still prone to degradation in mismatched conditions. An investigation into ensemble learning demonstrates the most accurate tracking can be achieved by combining multiple trackers."}
{"metadata": {"dataset": "scidocs", "query_id": "0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512", "doc_id": "51cd2b9edd4bae42b1d55ee3c166bf6c56b460fc"}, "id": "scidocs-0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512-51cd2b9edd4bae42b1d55ee3c166bf6c56b460fc", "question": "Let's go public! taking a spoken dialog system to the real world", "context": "Spoken dialogue systems typically use predefined semantic slots to parse users' natural language inputs into unified semantic representations. To define the slots, domain experts and professional annotators are often involved, and the cost can be expensive. In this paper, we ask the following question: given a collection of unlabeled raw audios, can we use the frame semantics theory to automatically induce and fill the semantic slots in an unsupervised fashion? To do this, we propose the use of a state-of-the-art frame-semantic parser, and a spectral clustering based slot ranking model that adapts the generic output of the parser to the target semantic space. Empirical experiments on a real-world spoken dialogue dataset show that the automatically induced semantic slots are in line with the reference slots created by domain experts: we observe a mean averaged precision of 69.36% using ASR-transcribed data. Our slot filling evaluations also indicate the promising future of this proposed approach."}
{"metadata": {"dataset": "scidocs", "query_id": "0c1c94f582cfaa727a03a452ea71cab809d8f7ce", "doc_id": "no"}, "id": "scidocs-0c1c94f582cfaa727a03a452ea71cab809d8f7ce-no", "question": "Implementation of flash flood monitoring system based on wireless sensor network in Bangladesh", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0c1c94f582cfaa727a03a452ea71cab809d8f7ce", "doc_id": "4a9c1b4569289623bf9812ffe2225e4b3d7acb22"}, "id": "scidocs-0c1c94f582cfaa727a03a452ea71cab809d8f7ce-4a9c1b4569289623bf9812ffe2225e4b3d7acb22", "question": "Implementation of flash flood monitoring system based on wireless sensor network in Bangladesh", "context": "Flooding is one of the major disasters occurring in various parts of the world. The system for real-time monitoring of water conditions: water level; flow; and precipitation level, was developed to be employed in monitoring flood in Nakhon Si Thammarat, a southern province in Thailand. The two main objectives of the developed system is to serve 1) as information channel for flooding between the involved authorities and experts to enhance their responsibilities and collaboration and 2) as a web based information source for the public, responding to their need for information on water condition and flooding. The developed system is composed of three major components: sensor network, processing/transmission unit, and database/ application server. These real-time data of water condition can be monitored remotely by utilizing wireless sensors network that utilizes the mobile General Packet Radio Service (GPRS) communication in order to transmit measured data to the application server. We implemented a so-called VirtualCOM, a middleware that enables application server to communicate with the remote sensors connected to a GPRS data unit (GDU). With VirtualCOM, a GDU behaves as if it is a cable directly connected the remote sensors to the application server. The application server is a web-based system implemented using PHP and JAVA as the web application and MySQL as its relational database. Users can view real-time water condition as well as the forecasting of the water condition directly from the web via web browser or via WAP. The developed system has demonstrated the applicability of today’s sensors in wirelessly monitor real-time water conditions."}
{"metadata": {"dataset": "scidocs", "query_id": "0c1c94f582cfaa727a03a452ea71cab809d8f7ce", "doc_id": "67161d331d496ad5255ad8982759a1c853856932"}, "id": "scidocs-0c1c94f582cfaa727a03a452ea71cab809d8f7ce-67161d331d496ad5255ad8982759a1c853856932", "question": "Implementation of flash flood monitoring system based on wireless sensor network in Bangladesh", "context": "This paper proposes architecture for an early warning floods system to alert public against flood disasters. An effective early warning system must be developed with linkages between four elements, which are accurate data collection to undertake risk assessments, development of hazard monitoring services, communication on risk related information and existence of community response capabilities. This project focuses on monitoring water level remotely using wireless sensor network. The project also utilizes Global System for Mobile communication (GSM) and short message service (SMS) to relay data from sensors to computers or directly alert the respective victim's through their mobile phone. It is hope that the proposed architecture can be further develop into a functioning system, which would be beneficial to the community and act as a precautionary action to save lives in the case of flood disaster."}
{"metadata": {"dataset": "scidocs", "query_id": "0c1c94f582cfaa727a03a452ea71cab809d8f7ce", "doc_id": "63aa729924d672a37f04bd3a18e60bd0510bb3a3"}, "id": "scidocs-0c1c94f582cfaa727a03a452ea71cab809d8f7ce-63aa729924d672a37f04bd3a18e60bd0510bb3a3", "question": "Implementation of flash flood monitoring system based on wireless sensor network in Bangladesh", "context": "Recent years have shown an alarmous increase in rain fall induced landslides. This has facilitated the need for having a monitoring system to predict the landslides which could eventually reduce the loss of human life. We have developed and deployed a Wireless Sensor Network to monitor rainfall induced landslide, in Munnar, South India. A successful landslide warning was issued in June 2009 using this system. The system is being enhanced by incorporating a Wireless Geophone Network to locate the initiation of landslide. The paper discusses an algorithm that was developed to analyze the geophone data and automatically detect the landslide signal. A novel method to localize the landslide initiation point is detailed. The algorithm is based on the time delay inherent in the transmission of waves through the surface of the earth. The approach detailed here does not require additional energy since the geophones are self excitatory. The error rate of the approach is much less when compared to the other localization methods like RSSI. The proposed algorithm is being tested and validated, in the landslide laboratory set up at our university."}
{"metadata": {"dataset": "scidocs", "query_id": "0c1c94f582cfaa727a03a452ea71cab809d8f7ce", "doc_id": "f394fc319d90a6227ce87504d882245c6c342cd2"}, "id": "scidocs-0c1c94f582cfaa727a03a452ea71cab809d8f7ce-f394fc319d90a6227ce87504d882245c6c342cd2", "question": "Implementation of flash flood monitoring system based on wireless sensor network in Bangladesh", "context": "This paper presents a forecasting model designed using WSNs( Wireless Sensor Networks) to predict flood in rivers using simple and fast calculations to provide real-time results and save the lives of people who may be affected by the flood. Our prediction model uses multiple variable robust linear regression which is easy to understand and simple and cost effective in implementation, is speed efficient, but has low resource utilization and yet provides real time predictions with reliable accuracy, thus having features which are desirable in any real world algorithm. Our prediction model is independent of the number of parameters, i.e. any number of parameters may be added or removed based on the on-site requirements. When the water level rises, we represent it using a polynomial whose nature is used to determine if the water level may exceed the flood line in the near future. We compare our work with a contemporary algorithm to demonstrate our improvements over it. Then we present our simulation results for the predicted water level compared to the actual water level."}
{"metadata": {"dataset": "scidocs", "query_id": "0c1c94f582cfaa727a03a452ea71cab809d8f7ce", "doc_id": "411476007a7673c87b497e61848d0962fdb03d07"}, "id": "scidocs-0c1c94f582cfaa727a03a452ea71cab809d8f7ce-411476007a7673c87b497e61848d0962fdb03d07", "question": "Implementation of flash flood monitoring system based on wireless sensor network in Bangladesh", "context": "Intended for network-wide dissemination of commands, configurations and code binaries, flooding has been investigated extensively in wireless networks. However, little work has yet been done on low-duty-cycle wireless sensor networks in which nodes stay asleep most of time and wake up asynchronously. In this type of network, a broadcasting packet is rarely received by multiple nodes simultaneously, a unique constraining feature that makes existing solutions unsuitable. Combined with unreliable links, flooding in low-duty-cycle networks is a new challenging issue.\n In this paper, we introduce Opportunistic Flooding, a novel design tailored for low-duty-cycle networks with unreliable wireless links and predetermined working schedules. The key idea is to make probabilistic forwarding decisions at a sender based on the delay distribution of next-hop nodes. Only opportunistically early packets are forwarded using links outside the energy optimal tree to reduce the flooding delay and redundancy in transmission. To improve performance further, we propose a forwarder selection method to alleviate the hidden terminal problem and a link-quality-based backoff method to resolve simultaneous forwarding operations. We evaluate Opportunistic Flooding with extensive simulation and a test-bed implementation consisting of 30 MicaZ nodes. Evaluation shows our design is close to the optimal performance achievable by oracle flooding designs. Compared with improved traditional flooding, our design achieves significantly shorter flooding delay while consuming only 20% ~ 60% of the transmission energy in various low-duty-cycle network settings."}
{"metadata": {"dataset": "scidocs", "query_id": "0c1c94f582cfaa727a03a452ea71cab809d8f7ce", "doc_id": "59719ac7c1617878196e36db4fbce7cb2ac16b16"}, "id": "scidocs-0c1c94f582cfaa727a03a452ea71cab809d8f7ce-59719ac7c1617878196e36db4fbce7cb2ac16b16", "question": "Implementation of flash flood monitoring system based on wireless sensor network in Bangladesh", "context": "Reliable flooding in wireless sensor networks (WSNs) is desirable for a broad range of applications and network operations. However, it is a challenging problem to ensure 100% flooding coverage efficiently considering the combined effects of low-duty-cycle operation and unreliable wireless transmission. In this work, we propose a novel dynamic switching-based reliable flooding (DSRF) framework, which is designed as an enhancement layer to provide efficient and reliable flooding over a variety of existing flooding tree structures in low-duty-cycle WSNs. Through comprehensive simulations, we demonstrate that DSRF can effectively improve both flooding energy efficiency and latency."}
{"metadata": {"dataset": "scidocs", "query_id": "0c1c94f582cfaa727a03a452ea71cab809d8f7ce", "doc_id": "46c6e1f6ac7f19a659b197abcd10fa781ac2a30d"}, "id": "scidocs-0c1c94f582cfaa727a03a452ea71cab809d8f7ce-46c6e1f6ac7f19a659b197abcd10fa781ac2a30d", "question": "Implementation of flash flood monitoring system based on wireless sensor network in Bangladesh", "context": "In this paper, we present the application of wireless sensor technology and the advantages it will inherently have for neonatal care and monitoring at Neonatal Intensive Care Units (NICU). An electrocardiography (ECG) readout board and a wireless transceiver module developed by IMEC at the Holst Centre in the Netherlands are embedded in the proposed wireless sensor systems in combination with the signal processing and software interface developed at the Department of Industrial Design, Eindhoven University of Technology (TU/e). Through development of this prototype system, we opt to ensure correct data transmission, detection and display. The wireless system is designed to be suitable for integration into non-invasive monitoring platforms such as a smart neonatal jacket developed at TU/e. Experiments at Maxima Medical Centre (MMC) in Veldhoven, the Netherlands demonstrate the wireless transmission of ECG data from the smart jacket integrated with textile electrodes."}
{"metadata": {"dataset": "scidocs", "query_id": "0c1c94f582cfaa727a03a452ea71cab809d8f7ce", "doc_id": "d781b74cf002f9fffcb7f60c3c319c41797d702e"}, "id": "scidocs-0c1c94f582cfaa727a03a452ea71cab809d8f7ce-d781b74cf002f9fffcb7f60c3c319c41797d702e", "question": "Implementation of flash flood monitoring system based on wireless sensor network in Bangladesh", "context": "In Aquaculture, the yields (shrimp, fish etc.) depend on the water characteristics of the aquaculture pond. For maximizing fish yields, the parameters which are to be kept at certain optimal levels in water. The parameters can vary a lot during the period of a day and can rapidly change depending on the external environmental conditions. Hence it is necessary to monitor these parameters with high frequency . Wireless sensor networks are used to monitor aqua farms for relevant parameters this system consists of two modules which are transmitter station and receiver station. The data transmits through GSM to the Database at receiver station. The graphical user interface was designed, to convey the data in the form of a message to the farmers in their respective local languages to their Mobile Phones and alerts them in unhygienic environmental conditions, in order to take suitable actions. Keywords; aquaculture; wireless sensor networks; IAR-Kick;pH;"}
{"metadata": {"dataset": "scidocs", "query_id": "0c1c94f582cfaa727a03a452ea71cab809d8f7ce", "doc_id": "65ba5f2f034e334c1b1d932d798df8f8a684a679"}, "id": "scidocs-0c1c94f582cfaa727a03a452ea71cab809d8f7ce-65ba5f2f034e334c1b1d932d798df8f8a684a679", "question": "Implementation of flash flood monitoring system based on wireless sensor network in Bangladesh", "context": "An automatic railway track crack detector system for Bangladesh Railway has been proposed here which aims in building a robot that can detect and analyze any kind of crack on the railway line and send the coordinates of that faulty line to the concerned authority. This robot includes two ultrasonic sensors, GPS, GSM modules, and Arduino Mega based crack detection assembly which is cost effective and robust to facilitate better safety standards in railways. As soon as the robot passed through a crack that might cause the derailment of a train, the ultrasonic sensors sense that and generate a signal. Then this signal is fed into the Arduino Mega. At that point, with the assistance of GSM and GPS modules, an alert SMS consist of the geographic coordinate of that damaged track is sent to the nearby railway authority who can easily take necessary steps to resolve the problem before any major accident occurs. This will save several trains in Bangladesh from an unwanted discontinuity from the rail track."}
{"metadata": {"dataset": "scidocs", "query_id": "0c1c94f582cfaa727a03a452ea71cab809d8f7ce", "doc_id": "59f153ddd37e22af153aa0d7caf3ec44053aa8e8"}, "id": "scidocs-0c1c94f582cfaa727a03a452ea71cab809d8f7ce-59f153ddd37e22af153aa0d7caf3ec44053aa8e8", "question": "Implementation of flash flood monitoring system based on wireless sensor network in Bangladesh", "context": "At present, labor-saving and water-saving technology is a key issue in irrigation. A wireless solution for intelligent field irrigation system dedicated to Jew's-ear planting in Lishui, Zhejiang, China, based on ZigBee technology was proposed in this paper. Instead of conventional wired connection, the wireless design made the system easy installation and maintenance. The hardware architecture and software algorithm of wireless sensor/actuator node and portable controller, acting as the end device and coordinator in ZigBee wireless sensor network respectively, were elaborated in detail. The performance of the whole system was evaluated in the end. The long-time smooth and proper running of the system in the field proved its high reliability and practicability. As an explorative application of wireless sensor network in irrigation management, this paper offered a methodology to establish large-scale remote intelligent irrigation system."}
{"metadata": {"dataset": "scidocs", "query_id": "0c3ffd5f1b577e38604f361ee71feb312b5b0cab", "doc_id": "no"}, "id": "scidocs-0c3ffd5f1b577e38604f361ee71feb312b5b0cab-no", "question": "Solving Constraint Satisfaction Problems through Belief Propagation-guided decimation", "context": null}
{"metadata": {"dataset": "scidocs", "query_id": "0c3ffd5f1b577e38604f361ee71feb312b5b0cab", "doc_id": "a2ca4a76a7259da6921ab41eae8858513cbb1af1"}, "id": "scidocs-0c3ffd5f1b577e38604f361ee71feb312b5b0cab-a2ca4a76a7259da6921ab41eae8858513cbb1af1", "question": "Solving Constraint Satisfaction Problems through Belief Propagation-guided decimation", "context": "We study the satisfiability of randomly generated formulas formed by M clauses of exactly K literals over N Boolean variables. For a given value of N the problem is known to be most difficult when α = M/N is close to the experimental threshold αc separating the region where almost all formulas are SAT from the region where all formulas are UNSAT. Recent results from a statistical physics analysis suggest that the difficulty is related to the existence of a clustering phenomenon of the solutions when α is close to (but smaller than) αc. We introduce a new type of message passing algorithm which allows to find efficiently a satisfying assignment of the variables in this difficult region. This algorithm is iterative and composed of two main parts. The first is a message-passing procedure which generalizes the usual methods like Sum-Product or Belief Propagation: It passes messages that may be thought of as surveys over clusters of the ordinary messages. The second part uses the detailed probabilistic information obtained from the surveys in order to fix variables and simplify the problem. Eventually, the simplified problem that remains is solved by a conventional heuristic. © 2005 Wiley Periodicals, Inc. Random Struct. Alg., 27, 201–226, 2005"}
{"metadata": {"dataset": "scidocs", "query_id": "0c3ffd5f1b577e38604f361ee71feb312b5b0cab", "doc_id": "206b204618640917f278e72bd0e2a881d8cec7ad"}, "id": "scidocs-0c3ffd5f1b577e38604f361ee71feb312b5b0cab-206b204618640917f278e72bd0e2a881d8cec7ad", "question": "Solving Constraint Satisfaction Problems through Belief Propagation-guided decimation", "context": "One of the major obstacles to using Bayesian methods for pattern recognition has been its computational expense. This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible. This method, \"Expectation Propagation,\" unifies and generalizes two previous techniques: assumeddensity filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. The unification shows how both of these algorithms can be viewed as approximating the true posterior distribution with a simpler distribution, which is close in the sense of KL-divergence. Expectation Propagation exploits the best of both algorithms: the generality of assumed-density filtering and the accuracy of loopy belief propagation. Loopy belief propagation, because it propagates exact belief states, is useful for limited types of belief networks, such as purely discrete networks. Expectation Propagation approximates the belief states with expectations, such as means and variances, giving it much wider scope. Expectation Propagation also extends belief propagation in the opposite direction-propagating richer belief states which incorporate correlations between variables. This framework is demonstrated in a variety of statistical models using synthetic and real-world data. On Gaussian mixture problems, Expectation Propagation is found, for the same amount of computation, to be convincingly better than rival approximation techniques: Monte Carlo, Laplace's method, and variational Bayes. For pattern recognition, Expectation Propagation provides an algorithm for training Bayes Point Machine classifiers that is faster and more accurate than any previously known. The resulting classifiers outperform Support Vector Machines on several standard datasets, in addition to having a comparable training time. Expectation Propagation can also be used to choose an appropriate feature set for classification, via Bayesian model selection. Thesis Supervisor: Rosalind Picard Title: Associate Professor of Media Arts and Sciences"}
{"metadata": {"dataset": "scidocs", "query_id": "0c3ffd5f1b577e38604f361ee71feb312b5b0cab", "doc_id": "34ad05e8db9dbaedc5b89caa2987d34beb8e719f"}, "id": "scidocs-0c3ffd5f1b577e38604f361ee71feb312b5b0cab-34ad05e8db9dbaedc5b89caa2987d34beb8e719f", "question": "Solving Constraint Satisfaction Problems through Belief Propagation-guided decimation", "context": "We present decentralized rollout sampling policy iteration (DecRSPI) — a new algorithm for multi-agent decision problems formalized as DEC-POMDPs. DecRSPI is designed to improve scalability and tackle problems that lack an explicit model. The algorithm uses MonteCarlo methods to generate a sample of reachable belief states. Then it computes a joint policy for each belief state based on the rollout estimations. A new policy representation allows us to represent solutions compactly. The key benefits of the algorithm are its linear time complexity over the number of agents, its bounded memory usage and good solution quality. It can solve larger problems that are intractable for existing planning algorithms. Experimental results confirm the effectiveness and scalability of the approach."}
{"metadata": {"dataset": "scidocs", "query_id": "0c3ffd5f1b577e38604f361ee71feb312b5b0cab", "doc_id": "0a24f049590c014d5b40660449503368bcedc921"}, "id": "scidocs-0c3ffd5f1b577e38604f361ee71feb312b5b0cab-0a24f049590c014d5b40660449503368bcedc921", "question": "Solving Constraint Satisfaction Problems through Belief Propagation-guided decimation", "context": "We cast the partially observable control problem as a fully observable underactuated stochastic control problem in belief space and apply standard planning and control techniques. One of the difficulties of belief space planning is modeling the stochastic dynamics resulting from unknown future observations . The core of our proposal is to define deterministic beliefsystem dynamics based on an assumption that the maximum likelihood observation (calculated just prior to the observation) is always obtained. The stochastic effects of future observation s are modeled as Gaussian noise. Given this model of the dynamics, two planning and control methods are applied. In the first, linear quadratic regulation (LQR) is applied to generate policies in the belief space. This approach is shown to be optimal for linearGaussian systems. In the second, a planner is used to find locally optimal plans in the belief space. We propose a replanning approach that is shown to converge to the belief space goal in a finite number of replanning steps. These approaches are characterized in the context of a simple nonlinear manipulation problem where a planar robot simultaneously locates and grasps an object."}
{"metadata": {"dataset": "scidocs", "query_id": "0c3ffd5f1b577e38604f361ee71feb312b5b0cab", "doc_id": "be522b149aab5b672fd949326dcb3da817b3982b"}, "id": "scidocs-0c3ffd5f1b577e38604f361ee71feb312b5b0cab-be522b149aab5b672fd949326dcb3da817b3982b", "question": "Solving Constraint Satisfaction Problems through Belief Propagation-guided decimation", "context": "The popularity of particle filtering for inference in Markov chain models defined over random variables with very large or continuous domains makes it natural to consider sample–based versions of belief propagation (BP) for more general (tree–structured or loopy) graphs. Already, several such algorithms have been proposed in the literature. However, many questions remain open about the behavior of particle–based BP algorithms, and little theory has been developed to analyze their performance. In this paper, we describe a generic particle belief propagation (PBP) algorithm which is closely related to previously proposed methods. We prove that this algorithm is consistent, approaching the true BP messages as the number of samples grows large. We then use concentration bounds to analyze the finitesample behavior and give a convergence rate for the algorithm on tree–structured graphs. Our convergence rate is O(1/ √ n) where n is the number of samples, independent of the domain size of the variables."}
{"metadata": {"dataset": "scidocs", "query_id": "0c3ffd5f1b577e38604f361ee71feb312b5b0cab", "doc_id": "018300f5f0e679cee5241d9c69c8d88e00e8bf31"}, "id": "scidocs-0c3ffd5f1b577e38604f361ee71feb312b5b0cab-018300f5f0e679cee5241d9c69c8d88e00e8bf31", "question": "Solving Constraint Satisfaction Problems through Belief Propagation-guided decimation", "context": "•We introduce a simple, efficient, and general method for training directed latent variable models. – Can handle both discrete and continuous latent variables. – Easy to apply – requires no model-specific derivations. •Key idea: Train an auxiliary neural network to perform inference in the model of interest by optimizing the variational bound. – Was considered before for Helmholtz machines and rejected as infeasible due to high variance of inference net gradient estimates. •We make the approach practical using simple and general variance reduction techniques. •Promising document modelling results using sigmoid belief networks."}
{"metadata": {"dataset": "scidocs", "query_id": "0c3ffd5f1b577e38604f361ee71feb312b5b0cab", "doc_id": "10ff61e6c2a99d8aafcf1706f3e88c7e2dfec188"}, "id": "scidocs-0c3ffd5f1b577e38604f361ee71feb312b5b0cab-10ff61e6c2a99d8aafcf1706f3e88c7e2dfec188", "question": "Solving Constraint Satisfaction Problems through Belief Propagation-guided decimation", "context": "Continuous quantities are ubiquitous in models of real-world phenomena, but are surprisingly difficult to reason about automatically. Probabilistic graphical models such as Bayesian networks and Markov random fields, and algorithms for approximate inference such as belief propagation (BP), have proven to be powerful tools in a wide range of applications in statistics and artificial intelligence. However, applying these methods to models with continuous variables remains a challenging task. In this work we describe an extension of BP to continuous variable models, generalizing particle filtering, and Gaussian mixture filtering techniques for time series to more complex models. We illustrate the power of the resulting nonparametric BP algorithm via two applications: kinematic tracking of visual motion and distributed localization in sensor networks."}
{"metadata": {"dataset": "scidocs", "query_id": "0c3ffd5f1b577e38604f361ee71feb312b5b0cab", "doc_id": "707603240701ca0545e37eae14b5c6ec80b3624d"}, "id": "scidocs-0c3ffd5f1b577e38604f361ee71feb312b5b0cab-707603240701ca0545e37eae14b5c6ec80b3624d", "question": "Solving Constraint Satisfaction Problems through Belief Propagation-guided decimation", "context": "This paper introduces a probabilistic algorithm for multi-robot decision-making under uncertainty, which can be posed as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP). Dec-POMDPs are inherently synchronous decision-making frameworks which require significant computational resources to be solved, making them infeasible for many real-world robotics applications. The Decentralized Partially Observable Semi-Markov Decision Process (Dec-POSMDP) was recently introduced as an extension of the Dec-POMDP that uses high-level macro-actions to allow large-scale, asynchronous decision-making. However, existing Dec-POSMDP solution methods have limited scalability or perform poorly as the problem size grows. This paper proposes a cross-entropy based Dec-POSMDP algorithm motivated by the combinatorial optimization literature. The algorithm is applied to a constrained package delivery domain, where it significantly outperforms existing Dec-POSMDP solution methods."}
{"metadata": {"dataset": "scidocs", "query_id": "0c3ffd5f1b577e38604f361ee71feb312b5b0cab", "doc_id": "04f7ab061b2d77bf3bd139d38a0c28ab82b57c7f"}, "id": "scidocs-0c3ffd5f1b577e38604f361ee71feb312b5b0cab-04f7ab061b2d77bf3bd139d38a0c28ab82b57c7f", "question": "Solving Constraint Satisfaction Problems through Belief Propagation-guided decimation", "context": "An alternative derivation for the well-known approximate message passing (AMP) algorithm proposed by Donoho is presented in this letter. Compared with the original derivation, which exploits central limit theorem and Taylor expansion to simplify belief propagation (BP), our derivation resorts to expectation propagation (EP) and the neglect of high-order terms in large system limit. This alternative derivation leads to a different yet provably equivalent form of message passing, which explicitly establishes the intrinsic connection between AMP and EP, thereby offering some new insights in the understanding and improvement of AMP."}
{"metadata": {"dataset": "scidocs", "query_id": "0c3ffd5f1b577e38604f361ee71feb312b5b0cab", "doc_id": "2401cd5606c6bc5390acc352d00c1685f0c8af60"}, "id": "scidocs-0c3ffd5f1b577e38604f361ee71feb312b5b0cab-2401cd5606c6bc5390acc352d00c1685f0c8af60", "question": "Solving Constraint Satisfaction Problems through Belief Propagation-guided decimation", "context": "Face recognition has witnessed great progress in recent years, mainly attributed to the high-capacity model designed and the abundant labeled data collected. However, it becomes more and more prohibitive to scale up the current million-level identity annotations. In this work, we show that unlabeled face data can be as effective as the labeled ones. Here, we consider a setting closely mimicking the real-world scenario, where the unlabeled data are collected from unconstrained environments and their identities are exclusive from the labeled ones. Our main insight is that although the class information is not available, we can still faithfully approximate these semantic relationships by constructing a relational graph in a bottom-up manner. We propose Consensus-Driven Propagation (CDP) to tackle this challenging problem with two modules, the “committee” and the “mediator”, which select positive face pairs robustly by carefully aggregating multi-view information. Extensive experiments validate the effectiveness of both modules to discard outliers and mine hard positives. With CDP, we achieve a compelling accuracy of 78.18% on MegaFace identification challenge by using only 9% of the labels, comparing to 61.78% when no unlabeled data are used and 78.52% when all labels are employed."}
