{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb0dbc31a8c4971b3707f7cdf3f4c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  12%|█▎        | 1/8 [00:01<00:09,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: trec-covid\n",
      "output: output/tmp/merge-small-trec-covid.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f6c9876c354eeb826565ebfcb6a560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5416593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  25%|██▌       | 2/8 [00:26<01:31, 15.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: climate-fever\n",
      "output: output/tmp/merge-small-climate-fever.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86f03de397847c3b04388ed4e721d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4635922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  38%|███▊      | 3/8 [00:49<01:33, 18.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: dbpedia-entity\n",
      "output: output/tmp/merge-small-dbpedia-entity.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ae4c1168f1488db50e4d7626699b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5416568 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: fever\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  50%|█████     | 4/8 [01:16<01:28, 22.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: output/tmp/merge-small-fever.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be18d88c6f5746628b26d1d68fdf0c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5233329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  62%|██████▎   | 5/8 [01:41<01:09, 23.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: hotpotqa\n",
      "output: output/tmp/merge-small-hotpotqa.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabf7591981d42b3b1f6679b3d3815f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3633 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: nfcorpus\n",
      "output: output/tmp/merge-small-nfcorpus.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  75%|███████▌  | 6/8 [01:43<00:31, 15.88s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d1c713d41449fbadb5bd13e79ced3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2681468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  88%|████████▊ | 7/8 [01:54<00:14, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: nq\n",
      "output: output/tmp/merge-small-nq.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007e9fec04804d9abf655317128ba0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset: 100%|██████████| 8/8 [01:56<00:00, 14.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: scidocs\n",
      "output: output/tmp/merge-small-scidocs.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import jsonlines\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "from core.models.entailment import EntailmentDeberta\n",
    "from rank_eval import load_data, load_rank_results\n",
    "\n",
    "def merge_score(rank_score, entropy_score):\n",
    "    if entropy_score is None:\n",
    "        return rank_score\n",
    "    if entropy_score < 0.01:\n",
    "        return rank_score + 1.0\n",
    "    return rank_score\n",
    "\n",
    "dataset_names = [\"trec-covid\", \"climate-fever\", \"dbpedia-entity\", \"fever\", \"hotpotqa\", \"nfcorpus\", \"nq\", \"scidocs\"]\n",
    "for dataset_name in tqdm(dataset_names, desc='dataset'):\n",
    "    dataset_path = f'/home/song/dataset/beir/{dataset_name}'\n",
    "    queries1, docs1, scores = load_data(dataset_path, dataset_name)\n",
    "    queries = {str(qid): query['text'] for qid, query in queries1.items()}\n",
    "    docs = {str(docid): doc for docid, doc in docs1.items()}\n",
    "    rank_result_path = f'dataset/rank/{dataset_name}/{dataset_name}-rank10-small.tsv'\n",
    "    rank_results = load_rank_results(rank_result_path)\n",
    "    entropy_result_path = f'output/rerank/{dataset_name}/entropy-small.tsv'\n",
    "    entropy_results = load_rank_results(entropy_result_path)\n",
    "    print(f\"dataset: {dataset_name}\")\n",
    "    merge_results = [] # ['qid', 'query', 'docid', 'doc', 'gold_score', 'rank_index', 'rank_score', 'entropy_score', 'merge_score']\n",
    "    for qid in rank_results:\n",
    "        for i, docid in enumerate(rank_results[qid]):\n",
    "            merge_results.append([str(qid), \n",
    "                                  queries.get(str(qid), ''), \n",
    "                                  str(docid), \n",
    "                                  docs.get(str(docid), ''), \n",
    "                                  scores.get(str(qid), {}).get(str(docid), 0.0), \n",
    "                                  i,\n",
    "                                  rank_results.get(qid, {}).get(docid, 0.0),\n",
    "                                  entropy_results.get(qid, {}).get(docid, None),\n",
    "                                  merge_score(rank_results.get(qid, {}).get(docid, 0.0), entropy_results.get(qid, {}).get(docid, None))\n",
    "                                  ])\n",
    "    with open(f'output/tmp/merge-small-{dataset_name}.tsv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerow(['qid', 'query', 'docid', 'doc', 'gold_score', 'rank_index', 'rank_score', 'entropy_score', 'merge_score'])\n",
    "        writer.writerows(merge_results)\n",
    "    print(f\"output: output/tmp/merge-small-{dataset_name}.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'632589828c8b9fca2c3a59e97451fde8fa7d188d': 'An evolutionary recurrent network which automates the design of recurrent neural/fuzzy networks using a new evolutionary learning algorithm is proposed in this paper. This new evolutionary learning algorithm is based on a hybrid of genetic algorithm (GA) and particle swarm optimization (PSO), and is thus called HGAPSO. In HGAPSO, individuals in a new generation are created, not only by crossover and mutation operation as in GA, but also by PSO. The concept of elite strategy is adopted in HGAPSO, where the upper-half of the best-performing individuals in a population are regarded as elites. However, instead of being reproduced directly to the next generation, these elites are first enhanced. The group constituted by the elites is regarded as a swarm, and each elite corresponds to a particle within it. In this regard, the elites are enhanced by PSO, an operation which mimics the maturing phenomenon in nature. These enhanced elites constitute half of the population in the new generation, whereas the other half is generated by performing crossover and mutation operation on these enhanced elites. HGAPSO is applied to recurrent neural/fuzzy network design as follows. For recurrent neural network, a fully connected recurrent neural network is designed and applied to a temporal sequence production problem. For recurrent fuzzy network design, a Takagi-Sugeno-Kang-type recurrent fuzzy network is designed and applied to dynamic plant control. The performance of HGAPSO is compared to both GA and PSO in these recurrent networks design problems, demonstrating its superiority.',\n",
       " '86e87db2dab958f1bd5877dc7d5b8105d6e31e46': 'Dynamic economic dispatch (DED) is one of the main functions of power generation operation and control. It determines the optimal settings of generator units with predicted load demand over a certain period of time. The objective is to operate an electric power system most economically while the system is operating within its security limits. This paper proposes a new hybrid methodology for solving DED. The proposed method is developed in such a way that a simple evolutionary programming (EP) is applied as a based level search, which can give a good direction to the optimal global region, and a local search sequential quadratic programming (SQP) is used as a fine tuning to determine the optimal solution at the final. Ten units test system with nonsmooth fuel cost function is used to illustrate the effectiveness of the proposed method compared with those obtained from EP and SQP alone.',\n",
       " '2a047d8c4c2a4825e0f0305294e7da14f8de6fd3': \"It's not surprisingly when entering this site to get the book. One of the popular books now is the genetic fuzzy systems evolutionary tuning and learning of fuzzy knowledge bases. You may be confused because you can't find the book in the book store around your city. Commonly, the popular book will be sold quickly. And when you have found the store to buy the book, it will be so hurt when you run out of it. This is why, searching for this popular book in this website will give you benefit. You will not run out of this book.\",\n",
       " '506172b0e0dd4269bdcfe96dda9ea9d8602bbfb6': 'In this paper, we introduce a new parameter, called inertia weight, into the original particle swarm optimizer. Simulations have been done to illustrate the signilicant and effective impact of this new parameter on the particle swarm optimizer.',\n",
       " '51317b6082322a96b4570818b7a5ec8b2e330f2f': 'This paper proposes a recurrent fuzzy neural network (RFNN) structure for identifying and controlling nonlinear dynamic systems. The RFNN is inherently a recurrent multilayered connectionist network for realizing fuzzy inference using dynamic fuzzy rules. Temporal relations are embedded in the network by adding feedback connections in the second layer of the fuzzy neural network (FNN). The RFNN expands the basic ability of the FNN to cope with temporal problems. In addition, results for the FNNfuzzy inference engine, universal approximation, and convergence analysis are extended to the RFNN. For the control problem, we present the direct and indirect adaptive control approaches using the RFNN. Based on the Lyapunov stability approach, rigorous proofs are presented to guarantee the convergence of the RFNN by choosing appropriate learning rates. Finally, the RFNN is applied in several simulations (time series prediction, identification, and control of nonlinear systems). The results confirm the effectiveness of the RFNN.',\n",
       " '857a8c6c46b0a85ed6019f5830294872f2f1dcf5': 'Recent reports of a high response to bodies in the fusiform face area (FFA) challenge the idea that the FFA is exclusively selective for face stimuli. We examined this claim by conducting a functional magnetic resonance imaging experiment at both standard (3.125 x 3.125 x 4.0 mm) and high resolution (1.4 x 1.4 x 2.0 mm). In both experiments, regions of interest (ROIs) were defined using data from blocked localizer runs. Within each ROI, we measured the mean peak response to a variety of stimulus types in independent data from a subsequent event-related experiment. Our localizer scans identified a fusiform body area (FBA), a body-selective region reported recently by Peelen and Downing (2005) that is anatomically distinct from the extrastriate body area. The FBA overlapped with and was adjacent to the FFA in all but two participants. Selectivity of the FFA to faces and FBA to bodies was stronger for the high-resolution scans, as expected from the reduction in partial volume effects. When new ROIs were constructed for the high-resolution experiment by omitting the voxels showing overlapping selectivity for both bodies and faces in the localizer scans, the resulting FFA* ROI showed no response above control objects for body stimuli, and the FBA* ROI showed no response above control objects for face stimuli. These results demonstrate strong selectivities in distinct but adjacent regions in the fusiform gyrus for only faces in one region (the FFA*) and only bodies in the other (the FBA*).',\n",
       " '12f107016fd3d062dff88a00d6b0f5f81f00522d': 'The energy usage of computer systems is becoming more important, especially for battery operated systems. Displays, disks, and cpus, in that order, use the most energy. Reducing the energy used by displays and disks has been studied elsewhere; this paper considers a new method for reducing the energy used by the cpu. We introduce a new metric for cpu energy performance, millions-of-instructions-per-joule (MIPJ). We examine a class of methods to reduce MIPJ that are characterized by dynamic control of system clock speed by the operating system scheduler. Reducing clock speed alone does not reduce MIPJ, since to do the same work the system must run longer. However, a number of methods are available for reducing energy with reduced clock-speed, such as reducing the voltage [Chandrakasan et al 1992][Horowitz 1993] or using reversible [Younis and Knight 1993] or adiabatic logic [Athas et al 1994].\\n What are the right scheduling algorithms for taking advantage of reduced clock-speed, especially in the presence of applications demanding ever more instructions-per-second? We consider several methods for varying the clock speed dynamically under control of the operating system, and examine the performance of these methods against workstation traces. The primary result is that by adjusting the clock speed at a fine grain, substantial CPU energy can be saved with a limited impact on performance.',\n",
       " '1ae0ac5e13134df7a0d670fc08c2b404f1e3803c': 'Mobility prediction is one of the most essential issues that need to be explored for mobility management in mobile computing systems. In this paper, we propose a new algorithm for predicting the next inter-cell movement of a mobile user in a Personal Communication Systems network. In the first phase of our threephase algorithm, user mobility patterns are mined from the history of mobile user trajectories. In the second phase, mobility rules are extracted from these patterns, and in the last phase, mobility predictions are accomplished by using these rules. The performance of the proposed algorithm is evaluated through simulation as compared to two other prediction methods. The performance results obtained in terms of Precision and Recall indicate that our method can make more accurate predictions than the other methods. 2004 Elsevier B.V. All rights reserved.',\n",
       " '7d3c9c4064b588d5d8c7c0cb398118aac239c71b': 'We study the problem of structural graph clustering, a fundamental problem in managing and analyzing graph data. Given an undirected unweighted graph, structural graph clustering is to assign vertices to clusters, and to identify the sets of hub vertices and outlier vertices as well, such that vertices in the same cluster are densely connected to each other while vertices in different clusters are loosely connected. In this paper, we develop a new two-step paradigm for scalable structural graph clustering based on our three observations. Then, we present a <inline-formula> <tex-math notation=\"LaTeX\">$\\\\mathsf {pSCAN}$</tex-math><alternatives> <inline-graphic xlink:href=\"chang-ieq2-2618795.gif\"/></alternatives></inline-formula> approach, within the paradigm, aiming to reduce the number of structural similarity computations, and propose optimization techniques to speed up checking whether two vertices are structure-similar. <inline-formula><tex-math notation=\"LaTeX\">$\\\\mathsf {pSCAN}$ </tex-math><alternatives><inline-graphic xlink:href=\"chang-ieq3-2618795.gif\"/></alternatives></inline-formula> outputs exactly the same clusters as the existing approaches <inline-formula><tex-math notation=\"LaTeX\">$\\\\mathsf {SCAN}$ </tex-math><alternatives><inline-graphic xlink:href=\"chang-ieq4-2618795.gif\"/></alternatives></inline-formula> and <inline-formula><tex-math notation=\"LaTeX\">$\\\\mathsf {SCAN\\\\text{++}}$</tex-math><alternatives> <inline-graphic xlink:href=\"chang-ieq5-2618795.gif\"/></alternatives></inline-formula>, and we prove that <inline-formula><tex-math notation=\"LaTeX\">$\\\\mathsf {pSCAN}$</tex-math><alternatives> <inline-graphic xlink:href=\"chang-ieq6-2618795.gif\"/></alternatives></inline-formula> is worst-case optimal. Moreover, we propose efficient techniques for updating the clusters when the input graph dynamically changes, and we also extend our techniques to other similarity measures, e.g., Jaccard similarity. Performance studies on large real and synthetic graphs demonstrate the efficiency of our new approach and our dynamic cluster maintenance techniques. Noticeably, for the twitter graph with 1 billion edges, our approach takes 25 minutes while the state-of-the-art approach cannot finish even after 24 hours.',\n",
       " '305c45fb798afdad9e6d34505b4195fa37c2ee4f': \"Iron, the most ubiquitous of the transition metals and the fourth most plentiful element in the Earth's crust, is the structural backbone of our modern infrastructure. It is therefore ironic that as a nanoparticle, iron has been somewhat neglected in favor of its own oxides, as well as other metals such as cobalt, nickel, gold, and platinum. This is unfortunate, but understandable. Iron's reactivity is important in macroscopic applications (particularly rusting), but is a dominant concern at the nanoscale. Finely divided iron has long been known to be pyrophoric, which is a major reason that iron nanoparticles have not been more fully studied to date. This extreme reactivity has traditionally made iron nanoparticles difficult to study and inconvenient for practical applications. Iron however has a great deal to offer at the nanoscale, including very potent magnetic and catalytic properties. Recent work has begun to take advantage of iron's potential, and work in this field appears to be blossoming.\",\n",
       " '9f234867df1f335a76ea07933e4ae1bd34eeb48a': \"ADVERTIMENT. La consulta d’aquesta tesi queda condicionada a l’acceptació de les següents condicions d'ús: La difusió d’aquesta tesi per mitjà del servei TDX (www.tdx.cat) i a través del Dipòsit Digital de la UB (diposit.ub.edu) ha estat autoritzada pels titulars dels drets de propietat intel·lectual únicament per a usos privats emmarcats en activitats d’investigació i docència. No s’autoritza la seva reproducció amb finalitats de lucre ni la seva difusió i posada a disposició des d’un lloc aliè al servei TDX ni al Dipòsit Digital de la UB. No s’autoritza la presentació del seu contingut en una finestra o marc aliè a TDX o al Dipòsit Digital de la UB (framing). Aquesta reserva de drets afecta tant al resum de presentació de la tesi com als seus continguts. En la utilització o cita de parts de la tesi és obligat indicar el nom de la persona autora.\",\n",
       " '5ebfcd50c56e51aada28ccecd041db5e002f5862': 'This paper describes the genesis of Gualzru, a robot commissioned by a large Spanish technological company to provide advertisement services in open public spaces. Gualzru has to stand by at an interactive panel observing the people passing by and, at some point, select a promising candidate and approach her to initiate a conversation. After a small verbal interaction, the robot is supposed to convince the passerby to walk back to the panel, leaving the rest of the selling task to an interactive software embedded in it. The whole design and building process took less than three years of team composed of five groups at different geographical locations. We describe here the lessons learned during this period of time, from different points of view including the hardware, software, architectural decisions and team collaboration issues.',\n",
       " '73a7144e072356b5c9512bd4a87b22457d33760c': 'Treatment effects can be estimated from observational data as the difference in potential outcomes. In this paper, we address the challenge of estimating the potential outcome when treatment-dose levels can vary continuously over time. Further, the outcome variable may not be measured at a regular frequency. Our proposed solution represents the treatment response curves using linear time-invariant dynamical systems—this provides a flexible means for modeling response over time to highly variable dose curves. Moreover, for multivariate data, the proposed method: uncovers shared structure in treatment response and the baseline across multiple markers; and, flexibly models challenging correlation structure both across and within signals over time. For this, we build upon the framework of multiple-output Gaussian Processes. On simulated and a challenging clinical dataset, we show significant gains in accuracy over stateof-the-art models.',\n",
       " 'c2aa3c7fd59a43c949844e98569429261dba36e6': 'A planar helical antenna is presented for achieving wideband end-fire radiation of circular polarization while maintaining a very low profile. The helix is formed using printed strips with straight-edge connections implemented by plated viaholes. The currents flowing on the strips and along via-holes of the helix contribute to the horizontal and vertical polarizations, respectively. Besides, the current on the ground plane is utilized to weaken the strong amplitude of the horizontal electric field generated by the one on the strips. Thus, a good circular polarization can be achieved. Furthermore, a tapered helix and conducting side-walls are employed to broaden the axial ratio (AR) bandwidth as well as to improve the end-fire radiation pattern. The designed antenna operates at the center frequency of 10 GHz. Simulated results show that the planar helical antenna achieves wide-impedance bandwidth (|S11| <; -10 dB) from 7.4 to 12.8 GHz (54%) and 3-dB AR bandwidth from 8.2 to 11.6 GHz (34%), while retaining a thickness of only 0.11λ0 at the center frequency. A prototype of the proposed antenna is fabricated and tested. Measured results are in good agreement with simulated ones.',\n",
       " 'befdf0eb1a3d2e0d404e7fbdb43438be7ae607e5': 'Context\\nCommon concerns when using low-calorie diets as a treatment for obesity are the reduction in fat-free mass, mostly muscular mass, that occurs together with the fat mass (FM) loss, and determining the best methodologies to evaluate body composition changes.\\n\\n\\nObjective\\nThis study aimed to evaluate the very-low-calorie ketogenic (VLCK) diet-induced changes in body composition of obese patients and to compare 3 different methodologies used to evaluate those changes.\\n\\n\\nDesign\\nTwenty obese patients followed a VLCK diet for 4 months. Body composition assessment was performed by dual-energy X-ray absorptiometry (DXA), multifrequency bioelectrical impedance (MF-BIA), and air displacement plethysmography (ADP) techniques. Muscular strength was also assessed. Measurements were performed at 4 points matched with the ketotic phases (basal, maximum ketosis, ketosis declining, and out of ketosis).\\n\\n\\nResults\\nAfter 4 months the VLCK diet induced a -20.2 ± 4.5 kg weight loss, at expenses of reductions in fat mass (FM) of -16.5 ± 5.1 kg (DXA), -18.2 ± 5.8 kg (MF-BIA), and -17.7 ± 9.9 kg (ADP). A substantial decrease was also observed in the visceral FM. The mild but marked reduction in fat-free mass occurred at maximum ketosis, primarily as a result of changes in total body water, and was recovered thereafter. No changes in muscle strength were observed. A strong correlation was evidenced between the 3 methods of assessing body composition.\\n\\n\\nConclusion\\nThe VLCK diet-induced weight loss was mainly at the expense of FM and visceral mass; muscle mass and strength were preserved. Of the 3 body composition techniques used, the MF-BIA method seems more convenient in the clinical setting.',\n",
       " '506d4ca228f81715946ed1ad8d9205fad20fddfe': 'According to art theory, pictorial balance acts to unify picture elements into a cohesive composition. For asymmetrical compositions, balancing elements is thought to be similar to balancing mechanical weights in a framework of symmetry axes. Assessment of preference for balance (APB), based on the symmetry-axes framework suggested in Arnheim R, 1974 Art and Visual Perception: A Psychology of the Creative Eye (Berkeley, CA: University of California Press), successfully matched subject balance ratings of images of geometrical shapes over unlimited viewing time. We now examine pictorial balance perception of Japanese calligraphy during first fixation, isolated from later cognitive processes, comparing APB measures with results from balance-rating and comparison tasks. Results show high between-task correlation, but low correlation with APB. We repeated the rating task, expanding the image set to include five rotations of each image, comparing balance perception of artist and novice participant groups. Rotation has no effect on APB balance computation but dramatically affects balance rating, especially for art experts. We analyze the variety of rotation effects and suggest that, rather than depending on element size and position relative to symmetry axes, first fixation balance processing derives from global processes such as grouping of lines and shapes, object recognition, preference for horizontal and vertical elements, closure, and completion, enhanced by vertical symmetry.',\n",
       " '772205182fbb6ad842df4a6cd937741145eeece0': 'Background: Smoking has long been suspected to be a risk factor for cervical cancer. However, not all previous studies have properly controlled for the effect of human papillomavirus (HPV) infection, which has now been established as a virtually necessary cause of cervical cancer. To evaluate the role of smoking as a cofactor of progression from HPV infection to cancer, we performed a pooled analysis of 10 previously published case–control studies. This analysis is part of a series of analyses of cofactors of HPV in the aetiology of cervical cancer. Methods: Data were pooled from eight case–control studies of invasive cervical carcinoma (ICC) and two of carcinoma in situ (CIS) from four continents. All studies used a similar protocol and questionnaires and included a PCR-based evaluation of HPV DNA in cytological smears or biopsy specimens. Only subjects positive for HPV DNA were included in the analysis. A total of 1463 squamous cell ICC cases were analyzed, along with 211 CIS cases, 124 adeno- or adeno-squamous ICC cases and 254 control women. Pooled odds ratios (OR) and 95% confidence intervals (CI) were estimated using logistic regression models controlling for sexual and non-sexual confounding factors. Results: There was an excess risk for ever smoking among HPV positive women (OR 2.17 95%CI 1.46–3.22). When results were analyzed by histological type, an excess risk was observed among cases of squamous cell carcinoma for current smokers (OR 2.30, 95%CI 1.31–4.04) and ex-smokers (OR 1.80, 95%CI 0.95–3.44). No clear pattern of association with risk was detected for adenocarcinomas, although the number of cases with this histologic type was limited. Conclusions: Smoking increases the risk of cervical cancer among HPV positive women. The results of our study are consistent with the few previously conducted studies of smoking and cervical cancer that have adequately controlled for HPV infection. Recent increasing trends of smoking among young women could have a serious impact on cervical cancer incidence in the coming years.',\n",
       " 'd2018e51b772aba852e54ccc0ba7f0b7c2792115': 'This paper analyzes the main challenges associated with noninvasive, continuous, wearable, and long-term breathing monitoring. The characteristics of an acoustic breathing signal from a miniature sensor are studied in the presence of sources of noise and interference artifacts that affect the signal. Based on these results, an algorithm has been devised to detect breathing. It is possible to implement the algorithm on a single integrated circuit, making it suitable for a miniature sensor device. The algorithm is tested in the presence of noise sources on five subjects and shows an average success rate of 91.3% (combined true positives and true negatives).',\n",
       " 'cc76f5d348ab6c3a20ab4adb285fc1ad96d3c009': 'We introduce a long short-term memory recurrent neural network (LSTM-RNN) approach for real-time facial animation, which automatically estimates head rotation and facial action unit activations of a speaker from just her speech. Specifically, the time-varying contextual non-linear mapping between audio stream and visual facial movements is realized by training a LSTM neural network on a large audio-visual data corpus. In this work, we extract a set of acoustic features from input audio, including Mel-scaled spectrogram, Mel frequency cepstral coefficients and chromagram that can effectively represent both contextual progression and emotional intensity of the speech. Output facial movements are characterized by 3D rotation and blending expression weights of a blendshape model, which can be used directly for animation. Thus, even though our model does not explicitly predict the affective states of the target speaker, her emotional manifestation is recreated via expression weights of the face model. Experiments on an evaluation dataset of different speakers across a wide range of affective states demonstrate promising results of our approach in real-time speech-driven facial animation.',\n",
       " '1b2a0e8af5c1f18e47e71244973ce4ace4ac6034': 'Hierarchical Pitman-Yor Process priors are compelling methods for learning language models, outperforming point-estimate based methods. However, these models remain unpopular due to computational and statistical inference issues, such as memory and time usage, as well as poor mixing of sampler. In this work we propose a novel framework which represents the HPYP model compactly using compressed suffix trees. Then, we develop an efficient approximate inference scheme in this framework that has a much lower memory footprint compared to full HPYP and is fast in the inference time. The experimental results illustrate that our model can be built on significantly larger datasets compared to previous HPYP models, while being several orders of magnitudes smaller, fast for training and inference, and outperforming the perplexity of the state-of-the-art Modified Kneser-Ney countbased LM smoothing by up to 15%.',\n",
       " 'c9d41f115eae5e03c5ed45c663d9435cb66ec942': 'This paper introduces a new compositional framework for classifying color correction methods according to their two main computational units. The framework was used to dissect fifteen among the best color correction algorithms and the computational units so derived, with the addition of four new units specifically designed for this work, were then reassembled in a combinatorial way to originate about one hundred distinct color correction methods, most of which never considered before. The above color correction methods were tested on three different existing datasets, including both real and artificial color transformations, plus a novel dataset of real image pairs categorized according to the kind of color alterations induced by specific acquisition setups. Differently from previous evaluations, special emphasis was given to effectiveness in real world applications, such as image mosaicing and stitching, where robustness with respect to strong image misalignments and light scattering effects is required. Experimental evidence is provided for the first time in terms of the most recent perceptual image quality metrics, which are known to be the closest to human judgment. Comparative results show that combinations of the new computational units are the most effective for real stitching scenarios, regardless of the specific source of color alteration. On the other hand, in the case of accurate image alignment and artificial color alterations, the best performing methods either use one of the new computational units, or are made up of fresh combinations of existing units.',\n",
       " 'b579366db457216b0548220bf369ab9eb183a0cc': 'Software even though intangible should undergo evolution to fit into the ever changing real world scenarios. Each issue faced by the development and service team directly reflects in the quality of the software product. According to the related work, very few research is going on in the field of ticket and its related incident; a part of corrective maintenance. In depth research on incident tickets should be viewed as critical since, it provides information related to the kind of maintenance activities that is performed in any timestamp. Therefore classifying and analyzing tickets becomes a critical task in managing the operations of the service since each incident will be having a service level agreement associated with it. Further, incident analysis is essential to identify the patterns associated. Due to the existence of huge population of software in each organization and millions of incidents get reported per software product every year, it is practically impossible to manually analyze all the tickets. This paper focuses on projecting the importance of ticket to maintain the quality of software products and also distinguish it from the defect associated with a software system. This paper projects the importance of identifying defects in software as well as handling the incident related tickets and resolving it when viewed from the perspective of quality. It also gives an overview of the scope defect analysis and ticket analytics provide to the researchers.',\n",
       " 'f69253e97f487b9d77b72553a9115fc814e3ed51': 'With the development of online advertisements, clickbait spread wider and wider. Clickbait dissatisfies users because the article content does not match their expectation. Thus, clickbait detection has attracted more and more attention recently. Traditional clickbait-detection methods rely on heavy feature engineering and fail to distinguish clickbait from normal headlines precisely because of the limited information in headlines. A convolutional neural network is useful for clickbait detection, since it utilizes pretrained Word2Vec to understand the headlines semantically, and employs different kernels to find various characteristics of the headlines. However, different types of articles tend to use different ways to draw users’ attention, and a pretrained Word2Vec model cannot distinguish these different ways. To address this issue, we propose a clickbait convolutional neural network (CBCNN) to consider not only the overall characteristics but also specific characteristics from different article types. Our experimental results show that our method outperforms traditional clickbait-detection algorithms and the TextCNN model in terms of precision, recall and accuracy.',\n",
       " '6c9bd4bd7e30470e069f8600dadb4fd6d2de6bc1': 'This paper describes a new language resource of events and semantic roles that characterize real-world situations. Narrative schemas contain sets of related events (edit and publish), a temporal ordering of the events (edit before publish), and the semantic roles of the participants (authors publish books). This type of world knowledge was central to early research in natural language understanding. Scripts were one of the main formalisms, representing common sequences of events that occur in the world. Unfortunately, most of this knowledge was hand-coded and time consuming to create. Current machine learning techniques, as well as a new approach to learning through coreference chains, has allowed us to automatically extract rich event structure from open domain text in the form of narrative schemas. The narrative schema resource described in this paper contains approximately 5000 unique events combined into schemas of varying sizes. We describe the resource, how it is learned, and a new evaluation of the coverage of these schemas over unseen documents.',\n",
       " 'a72daf1fc4b1fc16d3c8a2e33f9aac6e17461d9a': 'Recommender systems have been used in many domains to assist users\\' decision making by providing item recommendations and thereby reducing information overload. Context-aware recommender systems go further, incorporating the variability of users\\' preferences across contexts, and suggesting items that are appropriate in different contexts. In this paper, we present a novel recommendation task, \"Context Suggestion\", whereby the system recommends contexts in which items may be selected. We introduce the motivations behind the notion of context suggestion and discuss several potential solutions. In particular, we focus specifically on user-oriented context suggestion which involves recommending appropriate contexts based on a user\\'s profile. We propose extensions of well-known context-aware recommendation algorithms such as tensor factorization and deviation-based contextual modeling and adapt them as methods to recommend contexts instead of items. In our empirical evaluation, we compare the proposed solutions to several baseline algorithms using four real-world data sets.',\n",
       " '585da6b6355f3536e1b12b30ef4c3ea54b955f2d': 'Twitter, the popular microblogging site, has received increasing attention as a unique communication tool that facilitates electronic word-of-mouth (eWOM). To gain greater insight into this potential, this study investigates how consumers’ relationships with brands influence their engagement in retweeting brand messages on Twitter. Data from a survey of 315 Korean consumers who currently follow brands on Twitter show that those who retweet brand messages outscore those who do not on brand identification, brand trust, community commitment, community membership intention, Twitter usage frequency, and total number of postings. 2014 Elsevier Ltd. All rights reserved.',\n",
       " 'd18cc66f7f87e041dec544a0b843496085ab54e1': 'Theories on the functions of the hippocampal system are based largely on two fundamental discoveries: the amnestic consequences of removing the hippocampus and associated structures in the famous patient H.M. and the observation that spiking activity of hippocampal neurons is associated with the spatial position of the rat. In the footsteps of these discoveries, many attempts were made to reconcile these seemingly disparate functions. Here we propose that mechanisms of memory and planning have evolved from mechanisms of navigation in the physical world and hypothesize that the neuronal algorithms underlying navigation in real and mental space are fundamentally the same. We review experimental data in support of this hypothesis and discuss how specific firing patterns and oscillatory dynamics in the entorhinal cortex and hippocampus can support both navigation and memory.',\n",
       " '22fc3af1fb55d48f3c03cd96f277503e92541c60': 'In this work, a cost function design based on Lyapunov stability concepts for finite control set model predictive control is proposed. This predictive controller design allows one to characterize the performance of the controlled converter, while providing sufficient conditions for local stability for a class of power converters. Simulation and experimental results on a buck dc-dc converter and a two-level dc-ac inverter are conducted to validate the effectiveness of our proposal.',\n",
       " '4114c89bec92ebde7c20d12d0303281983ed1df8': 'This paper presents Swift, a packet filter for high-performance packet capture on commercial off-the-shelf hardware. The key features of the Swift include: 1) extremely lowfilter update latency for dynamic packet filtering, and 2) gigabits-per-second high-speed packet processing. Based on complex instruction set computer (CISC) instruction set architecture (ISA), Swift achieves the former with an instruction set design that avoids the need for compilation and security checking, and the latter by mainly utilizing single instruction, multiple data (SIMD). We implement Swift in the Linux 2.6 kernel for both i386 and ×86-64 architectures and extensively evaluate its dynamic and static filtering performance on multiple machines with different hardware setups. We compare Swift to BPF (the BSD packet filter)--the de facto standard for packet filtering in modern operating systems--and hand-coded optimized C filters that are used for demonstrating possible performance gains. For dynamic filtering tasks, Swift is at least three orders of magnitude faster than BPF in terms of filter update latency. For static filtering tasks, Swift outperforms BPF up to three times in terms of packet processing speed and achieves much closer performance to the optimized C filters. We also show that Swift can harness the processing power of hardware SIMD instructions by virtue of its SIMD-capable instruction set.',\n",
       " '8e508720cdb495b7821bf6e43c740eeb5f3a444a': 'Many applications in speech, robotics, finance, and biology deal with sequential data, where ordering matters and recurrent structures are common. However, this structure cannot be easily captured by standard kernel functions. To model such structure, we propose expressive closed-form kernel functions for Gaussian processes. The resulting model, GP-LSTM, fully encapsulates the inductive biases of long short-term memory (LSTM) recurrent networks, while retaining the non-parametric probabilistic advantages of Gaussian processes. We learn the properties of the proposed kernels by optimizing the Gaussian process marginal likelihood using a new provably convergent semi-stochastic gradient procedure, and exploit the structure of these kernels for scalable training and prediction. This approach provides a practical representation for Bayesian LSTMs. We demonstrate state-of-the-art performance on several benchmarks, and thoroughly investigate a consequential autonomous driving application, where the predictive uncertainties provided by GP-LSTM are uniquely valuable.',\n",
       " '110599f48c30251aba60f68b8484a7b0307bcb87': 'This report summarizes the objectives and evaluation of the SemEval 2015 task on the sentiment analysis of figurative language on Twitter (Task 11). This is the first sentiment analysis task wholly dedicated to analyzing figurative language on Twitter. Specifically, three broad classes of figurative language are considered: irony, sarcasm and metaphor. Gold standard sets of 8000 training tweets and 4000 test tweets were annotated using workers on the crowdsourcing platform CrowdFlower. Participating systems were required to provide a fine-grained sentiment score on an 11-point scale (-5 to +5, including 0 for neutral intent) for each tweet, and systems were evaluated against the gold standard using both a Cosinesimilarity and a Mean-Squared-Error measure.',\n",
       " '4b53f660eb6cfe9180f9e609ad94df8606724a3d': 'In this paper a novel approach is proposed to predict intraday directional-movements of a currency-pair in the foreign exchange market based on the text of breaking financial news-headlines. The motivation behind this work is twofold: First, although market-prediction through text-mining is shown to be a promising area of work in the literature, the text-mining approaches utilized in it at this stage are not much beyond basic ones as it is still an emerging field. This work is an effort to put more emphasis on the text-mining methods and tackle some specific aspects thereof that are weak in previous works, namely: the problem of high dimensionality as well as the problem of ignoring sentiment and semantics in dealing with textual language. This research assumes that addressing these aspects of text-mining have an impact on the quality of the achieved results. The proposed system proves this assumption to be right. The second part of the motivation is to research a specific market, namely, the foreign exchange market, which seems not to have been researched in the previous works based on predictive text-mining. Therefore, results of this work also successfully demonstrate a predictive relationship between this specific market-type and the textual data of news. Besides the above two main components of the motivation, there are other specific aspects that make the setup of the proposed system and the conducted experiment unique, for example, the use of news article-headlines only and not news article-bodies, which enables usage of short pieces of text rather than long ones; or the use of general financial breaking news without any further filtration. In order to accomplish the above, this work produces a multi-layer algorithm that tackles each of the mentioned aspects of the text-mining problem at a designated layer. The first layer is termed the Semantic Abstraction Layer and addresses the problem of co-reference in text mining that is contributing to sparsity. Co-reference occurs when two or more words in a text corpus refer to the same concept. This work produces a custom approach by the name of Heuristic-Hypernyms Feature-Selection which creates a way to recognize words with the same parent-word to be regarded as one entity. As a result, prediction accuracy increases significantly at this layer which is attributed to appropriate noise-reduction from the feature-space. The second layer is termed Sentiment Integration Layer, which integrates sentiment analysis capability into the algorithm by proposing a sentiment weight by the name of SumScore that reflects investors’ sentiment. Additionally, this layer reduces the dimensions by eliminating those that are of zero value in terms of sentiment and thereby improves prediction accuracy. The third layer encompasses a dynamic model creation algorithm, termed Synchronous Targeted Feature Reduction (STFR). It is suitable for the challenge at hand whereby the mining of a stream of text is concerned. It updates the models with the most recent information available and, more importantly, it ensures that the dimensions are reduced to the absolute minimum. The algorithm and each of its layers are extensively evaluated using real market data and news content across multiple years and have proven to be solid and superior to any other comparable solution. The proposed techniques implemented in the system, result in significantly high directional-accuracies of up to 83.33%. On top of a well-rounded multifaceted algorithm, this work contributes a much needed research framework for this context with a test-bed of data that must make future research endeavors more convenient. The produced algorithm is scalable and its modular design allows improvement in each of its layers in future research. This paper provides ample details to reproduce the entire system and the conducted experiments. 2014 Elsevier Ltd. All rights reserved. A. Khadjeh Nassirtoussi et al. / Expert Systems with Applications 42 (2015) 306–324 307',\n",
       " '7f90ef42f22d4f9b86d33b0ad7f16261273c8612': 'a r t i c l e i n f o a b s t r a c t We present an automatic approach to the construction of BabelNet, a very large, wide-coverage multilingual semantic network. Key to our approach is the integration of lexicographic and encyclopedic knowledge from WordNet and Wikipedia. In addition, Machine Translation is applied to enrich the resource with lexical information for all languages. We first conduct in vitro experiments on new and existing gold-standard datasets to show the high quality and coverage of BabelNet. We then show that our lexical resource can be used successfully to perform both monolingual and cross-lingual Word Sense Disambiguation: thanks to its wide lexical coverage and novel semantic relations, we are able to achieve state-of the-art results on three different SemEval evaluation tasks.',\n",
       " '033b62167e7358c429738092109311af696e9137': 'This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., “subtle nuances”) and a negative semantic orientation when it has bad associations (e.g., “very cavalier”). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “excellent” minus the mutual information between the given phrase and the word “poor”. A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews.',\n",
       " '105a0b3826710356e218685f87b20fe39c64c706': 'The Web has become an excellent source for gathering consumer opinions. There are now numerous Web sites containing such opinions, e.g., customer reviews of products, forums, discussion groups, and blogs. This paper focuses on online customer reviews of products. It makes two contributions. First, it proposes a novel framework for analyzing and comparing consumer opinions of competing products. A prototype system called Opinion Observer is also implemented. The system is such that with a single glance of its visualization, the user is able to clearly see the strengths and weaknesses of each product in the minds of consumers in terms of various product features. This comparison is useful to both potential customers and product manufacturers. For a potential customer, he/she can see a visual side-by-side and feature-by-feature comparison of consumer opinions on these products, which helps him/her to decide which product to buy. For a product manufacturer, the comparison enables it to easily gather marketing intelligence and product benchmarking information. Second, a new technique based on language pattern mining is proposed to extract product features from Pros and Cons in a particular type of reviews. Such features form the basis for the above comparison. Experimental results show that the technique is highly effective and outperform existing methods significantly.',\n",
       " '9ea16bc34448ca9d713f4501f1a6215a26746372': 'Software organizations have typically de-emphasized the importance of software testing. In this paper, the results of a regional survey of software testing and software quality assurance techniques are described. Researchers conducted the study during the summer and fall of 2002 by surveying software organizations in the Province of Alberta. Results indicate that Alberta-based organizations tend to test less than their counterparts in the United States. The results also indicate that Alberta software organizations tend to train fewer personnel on testing-related topics. This practice has the potential for a two-fold impact: first, the ability to detect trends that lead to reduced quality and to identify the root causes of reductions in product quality may suffer from the lack of testing. This consequence is serious enough to warrant consideration, since overall quality may suffer from the reduced ability to detect and eliminate process or product defects. Second, the organization may have a more difficult time adopting methodologies such as extreme programming. This is significant because other industry studies have concluded that many software organizations have tried or will in the next few years try some form of agile method. Newer approaches to software development like extreme programming increase the extent to which teams rely on testing skills. Organizations should consider their testing skill level as a key indication of their readiness for adopting software development techniques such as test-driven development, extreme programming, agile modelling, or other agile methods.',\n",
       " '746cafc676374114198c414d6426ec2f50e0ff80': 'This paper proposes a small-signal model for average current mode control based on an equivalent circuit. The model uses a three-terminal equivalent circuit model based on a linearized describing function method to include the feedback effect of the sideband frequency components of the inductor current. The model extends the results obtained in peak current mode control to average current mode control. The proposed small-signal model is accurate up to half switching frequency, predicting the subharmonic instability. The proposed model is verified using SIMPLIS simulation and hardware experiments, which show good agreement with the measurement results. Based on the proposed model, new feedback design guidelines are presented. The proposed design guidelines are compared with several conventional, widely used design criteria. By designing the external ramp following the proposed design guidelines, the quality factor of the double poles at half of the switching frequency in the control-to-output transfer function can be precisely controlled. This helps the feedback loop design to achieve wide control bandwidth and proper damping.',\n",
       " '2b337d6a72c8c2b1d97097dc24ec0e9a8d4c2186': 'Classifying short texts to one category or clustering semantically related texts is challenging, and the importance of both is growing due to the rise of microblogging platforms, digital news feeds, and the like. We can accomplish this classifying and clustering with the help of a deep neural network which produces compact binary representations of a short text, and can assign the same category to texts that have similar binary representations. But problems arise when there is little contextual information on the short texts, which makes it difficult for the deep neural network to produce similar binary codes for semantically related texts. We propose to address this issue using semantic enrichment. This is accomplished by taking the nouns, and verbs used in the short texts and generating the concepts and co-occurring words with the help of those terms. The nouns are used to generate concepts within the given short text, whereas the verbs are used to prune the ambiguous context (if any) present in the text. The enriched text then goes through a deep neural network to produce a prediction label for that short text representing it’s category.',\n",
       " '1d53a898850b8d055db80ba99c59c89b080dfc4c': 'Person detection and pose estimation is a key requirement to develop intelligent context-aware assistance systems. To foster the development of human pose estimation methods and their applications in the Operating Room (OR), we release the Multi-View Operating Room (MVOR) dataset, the first public dataset recorded during real clinical interventions. It consists of 732 synchronized multi-view frames recorded by three RGB-D cameras in a hybrid OR. It also includes the visual challenges present in such environments, such as occlusions and clutter. We provide camera calibration parameters, color and depth frames, human bounding boxes, and 2D/3D pose annotations. In this paper, we present the dataset, its annotations, as well as baseline results from several recent person detection and 2D/3D pose estimation methods. Since we need to blur some parts of the images to hide identity and nudity in the released dataset, we also present a comparative study of how the baselines have been impacted by the blurring. Results show a large margin for improvement and suggest that the MVOR dataset can be useful to compare the performance of the different methods.',\n",
       " '954d0346b5cdf3f1ec0fcc74ae5aadc5b733adc0': 'This mixed-method study focuses on online learning analytics, a research area of importance. Several important student attributes and their online activities are examined to identify what seems to work best to predict higher grades. The purpose is to explore the relationships between student grade and key learning engagement factors using a large sample from an online undergraduate business course at an accredited American university (n\\xa0=\\xa0228). Recent studies have discounted the ability to predict student learning outcomes from big data analytics but a few significant indicators have been found by some researchers. Current studies tend to use quantitative factors in learning analytics to forecast outcomes. This study extends that work by testing the common quantitative predictors of learning outcome, but qualitative data is also examined to triangulate the evidence. Pre and post testing of information technology understanding is done at the beginning of the course. First quantitative data is collected, and depending on the hypothesis test results, qualitative data is collected and analyzed with text analytics to uncover patterns. Moodle engagement analytics indicators are tested as predictors in the model. Data is also taken from the Moodle system logs. Qualitative data is collected from student reflection essays. The result was a significant General Linear Model with four online interaction predictors that captured 77.5\\xa0% of grade variance in an undergraduate business course.',\n",
       " '483b94374944293d2a6d36cc1c97f0544ce3c79c': 'A lot of effort has been made in the last decades to reveal, which hotel attributes guest care about. Due to the high costs that are typically involved with investments in the hotel industry, it makes a lot of sense to study, which product components the travellers appreciate. This study reveals that hotel attribute research turns out to be a wide and extremely heterogeneous field of research. The authors review empirical studies investigating the importance of hotel attributes, provide attribute rankings and suggest a framework for past and future research projects in the field, based on the dimensions “focus of research”, ”risk versus utility” and “trade-off versus no trade-off questioning situation”.',\n",
       " '54c377407242e74e7c08e4a49e61837fd9ce2b25': \"In this paper, a comprehensive model of the variable-speed constant-frequency aircraft electric power system is developed to study the performance characteristics of the system and, in particular, the system power quality over a frequency range of operation of 400 Hz to 800 Hz. A fully controlled active power filter is designed to regulate the load terminal voltage, eliminate harmonics, correct supply power factor, and minimize the effect of unbalanced loads. The control algorithm for the active power filter (APF) is based on the perfect harmonic cancellation method which provides a three-phase reference supply current in phase with its positive-sequence fundamental voltage. The proposed APF is integrated into the model of a 90-kVA advanced aircraft electric power system under VSCF operation. The performance characteristics of the system are studied with the frequency of the generator's output voltage varied from 400 Hz to 800 Hz under different loading conditions. Several case studies are presented including dc loads as well as passive and dynamic ac loads. The power quality characteristics of the studied aircraft electric power system with the proposed active filter are shown to be in compliance with the most recent military aircraft electrical standards MIL-STD-704F as well as with the IEEE Std. 519.\",\n",
       " '9d1940f843c448cc378214ff6bad3c1279b1911a': 'We address the problem of instance-level semantic segmentation, which aims at jointly detecting, segmenting and classifying every individual object in an image. In this context, existing methods typically propose candidate objects, usually as bounding boxes, and directly predict a binary mask within each such proposal. As a consequence, they cannot recover from errors in the object candidate generation process, such as too small or shifted boxes. In this paper, we introduce a novel object segment representation based on the distance transform of the object masks. We then design an object mask network (OMN) with a new residual-deconvolution architecture that infers such a representation and decodes it into the final binary object mask. This allows us to predict masks that go beyond the scope of the bounding boxes and are thus robust to inaccurate object candidates. We integrate our OMN into a Multitask Network Cascade framework, and learn the resulting shape-aware instance segmentation (SAIS) network in an end-to-end manner. Our experiments on the PASCAL VOC 2012 and the CityScapes datasets demonstrate the benefits of our approach, which outperforms the state-of-the-art in both object proposal generation and instance segmentation.',\n",
       " '4d0130e95925b00a2d1ecba931a1a05a74370f3f': 'There is a strong demand in many fields for practical robots, such as a porter robot and a personal mobility robot, that can move over rough terrain while carrying a load horizontally. We have developed a robot, called RT-Mover, which shows adequate mobility performance on targeted types of rough terrain. It has four drivable wheels and two leg-like axles but only five active shafts. A strength of this robot is that it realizes both a leg mode and a wheel mode in a simple mechanism. In this paper, the mechanical design concept is discussed. With an emphasis on minimizing the number of drive shafts, a mechanism is designed for a four-wheeled mobile body that is widely used in practical locomotive machinery. Also, strategies for moving on rough terrain are proposed. The kinematics, stability, and control of RT-Mover are also described in detail. Some typical cases of rough terrain for wheel mode and leg mode are selected, and the robot’s ability of locomotion is assessed through simulations and experiments. In each case, the robot is able to move over rough terrain while maintaining the horizontal orientation of its platform.',\n",
       " '0651f838d918586ec1df66450c3d324602c9f59e': 'Social-networking users unknowingly reveal certain kinds of personal information that malicious attackers could profit from to perpetrate significant privacy breaches. This paper quantitatively demonstrates how the simple act of tagging pictures on the social-networking site of Facebook could reveal private user attributes that are extremely sensitive. Our results suggest that photo tags can be used to help predicting some, but not all, of the analyzed attributes. We believe our analysis make users aware of significant breaches of their privacy and could inform the design of new privacy-preserving ways of tagging pictures on social-networking sites.',\n",
       " '4c9774c5e57a4b7535eb19f6584f75c8b9c2cdcc': 'Cloud computing is an emerging computing model in which resources of the computing communications are provided as services over the Internet. Privacy and security of cloud storage services are very important and become a challenge in cloud computing due to loss of control over data and its dependence on the cloud computing provider. While there is a huge amount of transferring data in cloud system, the risk of accessing data by attackers raises. Considering the problem of building a secure cloud storage service, current scheme is proposed which is based on combination of RSA and AES encryption methods to share the data among users in a secure cloud system. The proposed method allows providing difficulty for attackers as well as reducing the time of information transmission between user and cloud data storage.',\n",
       " 'e645cbd3aaeab56858f1e752677b8792d7377d14': 'The presented work aims at generating a systematically annotated corpus that can support the enhancement of sentiment analysis tasks in Telugu using wordlevel sentiment annotations. From OntoSenseNet, we extracted 11,000 adjectives, 253 adverbs, 8483 verbs and sentiment annotation is being done by language experts. We discuss the methodology followed for the polarity annotations and validate the developed resource. This work aims at developing a benchmark corpus, as an extension to SentiWordNet, and baseline accuracy for a model where lexeme annotations are applied for sentiment predictions. The fundamental aim of this paper is to validate and study the possibility of utilizing machine learning algorithms, word-level sentiment annotations in the task of automated sentiment identification. Furthermore, accuracy is improved by annotating the bi-grams extracted from the target corpus.',\n",
       " '40f5430ef326838d5b7ce018f62e51c188d7cdd7': 'This paper proposes quiz-style information presentation for interactive systems as a means to improve user understanding in educational tasks. Since the nature of quizzes can highly motivate users to stay voluntarily engaged in the interaction and keep their attention on receiving information, it is expected that information presented as quizzes can be better understood by users. To verify the effectiveness of the approach, we implemented read-out and quiz systems and performed comparison experiments using human subjects. In the task of memorizing biographical facts, the results showed that user understanding for the quiz system was significantly better than that for the read-out system, and that the subjects were more willing to use the quiz system despite the long duration of the quizzes. This indicates that quiz-style information presentation promotes engagement in the interaction with the system, leading to the improved user understanding.',\n",
       " '24bbff699187ad6bf37e447627de1ca25267a770': 'This paper presents the results of a bibliometric study about the evolution of research on Continuous Auditing. This study has as main motivation to find reasons for the very slow evolvement of research on this topic. In addition, Continuous Auditing is one of the features of the emerging concept of Continuous Assurance. Thus, considering that Continuous Assurance represents numerous advantages for the organizational performance, this study also intends to understand if there is a relation between the evolution of research on Continuous Auditing and the still very low maturity levels of continuous assurance solutions. This study shows that the number of publications is considerably reduced and that the slow growth of research on Continuous Auditing may be contributing to the lack of maturity of Continuous Assurance.',\n",
       " 'abd0478f1572d8ecdca4738df3e4b3bd116d7b42': 'This study directly tests the effect of personality and cognitive style on three measures of Internet use. The results support the use of personality—but not cognitive style—as an antecedent variable. After controlling for computer anxiety, selfefficacy, and gender, including the “Big Five” personality factors in the analysis significantly adds to the predictive capabilities of the dependent variables. Including cognitive style does not. The results are discussed in terms of the role of personality and cognitive style in models of technology adoption and use.',\n",
       " 'a64f48f9810c4788236f31dc2a9b87dd02977c3e': '• Averaged frequency responses at different 16, and 24 kHz. External sampling rate does not tell the internal sampling rate. • Supported signal bandwidth depends on bitrate, but no documentation exists bandwidths were found out expirementally • We tested 32kHz sampling with 16ms frame length. There is also 8 ms lookahead. • The results show that bitrates below 32 kbit/s are not useable for voice applications.The voice quality is much worse than with SILK or bitrates shown in steady state',\n",
       " '76eea8436996c7e9c8f7ad3dac34a12865edab24': 'Chain replication is a new approach to coordinating clusters of fail-stop storage servers. The approach is intended for supporting large-scale storage services that exhibit high throughput and availability without sacrificing strong consistency guarantees. Besides outlining the chain replication protocols themselves, simulation experiments explore the performance characteristics of a prototype implementation. Throughput, availability, and several objectplacement strategies (including schemes based on distributed hash table routing) are discussed.',\n",
       " '522a7178e501018e442c03f4b93e29f62ae1eb64': 'We introduce a technique for augmenting neural text-to-speech (TTS) with lowdimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.',\n",
       " 'ccbcaf528a222d04f40fd03b3cb89d5f78acbdc6': '-The huge amounts of data generated by healthcare transactions are too complex and voluminous to be processed and analyzed by traditional methods. Data mining provides the methodology and technology to transform these mounds of data into useful information for decision making. The Healthcare industry is generally “information rich”, which is not feasible to handle manually. These large amounts of data are very important in the field of data mining to extract useful information and generate relationships amongst the attributes. Kidney disease is a complex task which requires much experience and knowledge. Kidney disease is a silent killer in developed countries and one of the main contributors to disease burden in developing countries. In the health care industry the data mining is mainly used for predicting the diseases from the datasets. The Data mining classification techniques, namely Decision trees, ANN, Naive Bayes are analyzed on Kidney disease data set. Keywords--Data Mining, Kidney Disease, Decision tree, Naive Bayes, ANN, K-NN, SVM, Rough Set, Logistic Regression, Genetic Algorithms (GAs) / Evolutionary Programming (EP), Clustering',\n",
       " '30f46fdfe1fdab60bdecaa27aaa94526dfd87ac1': 'We propose a method which can perform real-time 3D reconstruction from a single hand-held event camera with no additional sensing, and works in unstructured scenes of which it has no prior knowledge. It is based on three decoupled probabilistic filters, each estimating 6-DoF camera motion, scene logarithmic (log) intensity gradient and scene inverse depth relative to a keyframe, and we build a real-time graph of these to track and model over an extended local workspace. We also upgrade the gradient estimate for each keyframe into an intensity image, allowing us to recover a real-time video-like intensity sequence with spatial and temporal super-resolution from the low bit-rate input event stream. To the best of our knowledge, this is the first algorithm provably able to track a general 6D motion along with reconstruction of arbitrary structure including its intensity and the reconstruction of grayscale video that exclusively relies on event camera data.',\n",
       " '892fea843d58852a835f38087bc3b5102123f567': 'A (t; k; n; S) ramp scheme is a protocol to distribute a secret s chosen inS among a setP of n participants in such a way that: 1) sets of participants of cardinality greater than or equal to k can reconstruct the secrets; 2) sets of participants of cardinality less than or equal tot have no information on s, whereas 3) sets of participants of cardinality greater than t and less thank might have “some” information on s. In this correspondence we analyze multiple ramp schemes, which are protocols to share many secrets among a set P of participants, using different ramp schemes. In particular, we prove a tight lower bound on the size of the shares held by each participant and on the dealer’s randomness in multiple ramp schemes.',\n",
       " 'ce148df015fc488ac6fc022dac3da53c141e0ea8': 'Precision medicine and personalized health efforts propose leveraging complex molecular, medical and family history, along with other types of personal data toward better life. We argue that this ambitious objective will require advanced and specialized machine learning solutions. Simply skimming some low-hanging results off the data wealth might have limited potential. Instead, we need to better understand all parts of the system to define medically relevant causes and effects: how do particular sequence variants affect particular proteins and pathways? How do these effects, in turn, cause the health or disease-related phenotype? Toward this end, deeper understanding will not simply diffuse from deeper machine learning, but from more explicit focus on understanding protein function, context-specific protein interaction networks, and impact of variation on both.',\n",
       " '38d34b02820020aac7f060e84bb6c01b4dee665a': 'Ž . Design management and process management are two important elements of total quality management TQM implementation. They are drastically different in their targets of improvement, visibility, and techniques. In this paper, we establish a framework for identifying the synergistic linkages of design and process management to the operational quality Ž . Ž . outcomes during the manufacturing process internal quality and upon the field usage of the products external quality . Through a study of quality practices in 418 manufacturing plants from multiple industries, we empirically demonstrate that both design and process management efforts have an equal positive impact on internal quality outcomes such as scrap, rework, defects, performance, and external quality outcomes such as complaints, warranty, litigation, market share. A detailed contingency analysis shows that the proposed model of synergies between design and process management holds true for large and small firms; for firms with different levels of TQM experience; and in different industries with varying levels of competition, logistical complexity of production, or production process characteristics. Finally, the results also suggest that organizational learning enables mature TQM firms to implement both design and process efforts more rigorously and their synergy helps these firms to attain better quality outcomes. These findings indicate that, to attain superior quality outcomes, firms need to balance their design and process management efforts and persevere with long-term Ž . implementation of these efforts. Because the study spans all of the manufacturing sectors SIC 20 through 39 , these conclusions should help firms in any industry revisit their priorities in terms of the relative efforts in design management and process management. q 2000 Elsevier Science B.V. All rights reserved.',\n",
       " 'b09b43cacd45fd922f7f85b1f8514cb4a775ca5d': 'Web services have attracted much attention from distributed application designers and developers because of their roles in abstraction and interoperability among heterogeneous software systems, and a growing number of distributed software applications have been published as Web services on the Internet. Faced with the increasing numbers of Web services and service users, researchers in the services computing field have attempted to address a challenging issue, i.e., how to quickly find the suitable ones according to user queries. Many previous studies have been reported towards this direction. In this paper, a novel Web service discovery approach based on topic models is presented. The proposed approach mines common topic groups from the service-topic distribution matrix generated by topic modeling, and the extracted common topic groups can then be leveraged to match user queries to relevant Web services, so as to make a better trade-off between the accuracy of service discovery and the number of candidate Web services. Experiment results conducted on two publicly-available data sets demonstrate that, compared with several widely used approaches, the proposed approach can maintain the performance of service discovery at an elevated level by greatly decreasing the number of candidate Web services, thus leading to faster response time.',\n",
       " 'c108437a57bd8f8eaed9e26360ee100074e3f3fc': 'In this paper, we will consider the approximation properties of a recently introduced neural network model called graph neural network (GNN), which can be used to process-structured data inputs, e.g., acyclic graphs, cyclic graphs, and directed or undirected graphs. This class of neural networks implements a function tau(G, n) isin R m that maps a graph G and one of its nodes n onto an m-dimensional Euclidean space. We characterize the functions that can be approximated by GNNs, in probability, up to any prescribed degree of precision. This set contains the maps that satisfy a property called preservation of the unfolding equivalence, and includes most of the practically useful functions on graphs; the only known exception is when the input graph contains particular patterns of symmetries when unfolding equivalence may not be preserved. The result can be considered an extension of the universal approximation property established for the classic feedforward neural networks (FNNs). Some experimental examples are used to show the computational capabilities of the proposed model.',\n",
       " '28d3ec156472c35ea8e1b7acad969b725111fe56': \"Sociological and technical difficulties, such as a lack of informal encounters, can make it difficult for new members of noncollocated software development teams to learn from their more experienced colleagues. To address this situation, we have developed a tool, named Hipikat that provides developers with efficient and effective access to the group memory for a software development project that is implicitly formed by all of the artifacts produced during the development. This project memory is built automatically with little or no change to existing work practices. After describing the Hipikat tool, we present two studies investigating Hipikat's usefulness in software modification tasks. One study evaluated the usefulness of Hipikat's recommendations on a sample of 20 modification tasks performed on the Eclipse Java IDE during the development of release 2.1 of the Eclipse software. We describe the study, present quantitative measures of Hipikat's performance, and describe in detail three cases that illustrate a range of issues that we have identified in the results. In the other study, we evaluated whether software developers who are new to a project can benefit from the artifacts that Hipikat recommends from the project memory. We describe the study, present qualitative observations, and suggest implications of using project memory as a learning aid for project newcomers.\",\n",
       " '334c4806912d851ef2117e67728cfa624dbec9a3': 'Given the central role that software development plays in the delivery and application of information technology, managers are increasingly focusing on process improvement in the software development area. This demand has spurred the provision of a number of new and/or improved approaches to software development, with perhaps the most prominent being object-orientation (OO). In addition, the focus on process improvement has increased the demand for software measures, or metrics with which to manage the process. The need for such metrics is particularly acute when an organization is adopting a new technology for which established practices have yet to be developed. This research addresses these needs through the development and implementation of a new suite of metrics for OO design. Metrics developed in previous research, while contributing to the field’s understanding of software development processes, have generally been subject to serious criticisms, including the lack of a theoretical base. Following Wand and Weber, the theoretical base chosen for the metrics was the ontology of Bunge. Six design metrics are developed, and then analytically evaluated against Weyuker’s proposed set of measurement principles. An automated data collection tool was then developed and implemented to collect an empirical sample of these metrics at two field sites in order to demonstrate their feasibility and suggest ways in which managers may use these metrics for process improvement. “A Metrics Suite For Object Oriented Design” Shyam R. Chidamber Chris F. Kemerer Index Terms CR',\n",
       " '383ca85aaca9f306ea7ae04fb0b6b76f1e393395': 'According to its proponents, open source style software development has the capacity to compete successfully, and perhaps in many cases displace, traditional commercial development methods. In order to begin investigating such claims, we examine data from two major open source projects, the Apache web server and the Mozilla browser. By using email archives of source code change history and problem reports we quantify aspects of developer participation, core team size, code ownership, productivity, defect density, and problem resolution intervals for these OSS projects. We develop several hypotheses by comparing the Apache project with several commercial projects. We then test and refine several of these hypotheses, based on an analysis of Mozilla data. We conclude with thoughts about the prospects for high-performance commercial/open source process hybrids.',\n",
       " '3ea9cd35f39e8c128f39f13148e91466715f4ee2': \"A file comparison program produces a list of differences between two files. These differences can be couched in terms of lines, e.g. by telling which lines must be inserted, deleted or moved to convert the first file to the second. Alternatively, the list of differences can identify individual bytes. Byte-oriented comparisons are useful with non-text files, such as compiled programs, that are not divided into lines. The approach adopted here is to generate only instructions to insert or delete entire lines. Since lines are treated as indivisible objects, files can be treated as containing lines consisting of a single symbol. In other words, an n-line file is modelled by a string of n symbols. In more formal terms, the file comparison problem can be rephrased as follows. The edit distance between two strings of symbols is the length of a shortest sequence of insertions and deletions that will convert the first string to the second. T h e goal, then, is to write a program that computes the edit distance between two arbitrary strings of symbols. In addition, the program must explicitly produce a shortest possible edit script (i.e. sequence of edit commands) for the given strings. Other approaches have been tried. For example, Tichy ' discusses a file-comparison tool that determines how one file can be constructed from another by copying blocks of lines and appending lines. However, the ability to economically generate shortestpossible edit scripts depends critically on the repertoire of instructions that are allowed in the scripts.2 File comparison algorithms have a number of potential uses beside merely producing a set of edit commands to be read by someone trying to understand the evolution of a program or document. For example, the edit scripts might be text editor instructions that are saved to avoid the expense of storing nearly identical files. Rather than storing\",\n",
       " '508119a50e3d4e8b7116c1b56a002de492b2270b': 'Finding basic objects on a daily basis is a difficult but common task for blind people. This paper demonstrates the implementation of a wearable, deep learning backed, object detection approach in the context of visual impairment or blindness. The prototype aims to substitute the impaired eye of the user and replace it with technical sensors. By scanning its surroundings, the prototype provides a situational overview of objects around the device. Object detection has been implemented using a near real-time, deep learning model named YOLOv2. The model supports the detection of 9000 objects. The prototype can display and read out the name of augmented objects which can be selected by voice commands and used as directional guides for the user, using 3D audio feedback. A distance announcement of a selected object is derived from the HoloLens’s spatial model. The wearable solution offers the opportunity to efficiently locate objects to support orientation without extensive training of the user. Preliminary evaluation covered the detection rate of speech recognition and the response times of the server.',\n",
       " 'c0a39b1b64100b929ec77d33232513ec72089a2e': 'PENG is a computer-processable controlled natural language designed for writing unambiguous and precise specifications. PENG covers a strict subset of standard English and is precisely defined by a controlled grammar and a controlled lexicon. In contrast to other controlled languages, the author does not need to know the grammatical restrictions explicitly. ECOLE, a look-ahead text editor, indicates the restrictions while the specification is written. The controlled lexicon contains domain-specific content words that can be defined by the author on the fly and predefined function words. Specifications written in PENG can be deterministically translated into discourse representations structures to cope with anaphora and presuppositions and also into first-order predicate logic. To test the formal properties of PENG, we reformulated Schubert’s steamroller puzzle in PENG, translated the resulting specification via discourse representation structures into first-order predicate logic with equality, and proved the steamroller’s conclusion with OTTER, a standard theorem prover.',\n",
       " 'f9cf246008d745f883914d925567bb36df806613': 'Airborne wind energy systems aim to harvest the power of winds blowing at altitudes higher than what conventional wind turbines reach. They employ a tethered flying structure, usually a wing, and exploit the aerodynamic lift to produce electrical power. In the case of ground-based systems, where the traction force on the tether is used to drive a generator on the ground, a two-phase power cycle is carried out: one phase to produce power, where the tether is reeled out under high traction force, and a second phase where the tether is recoiled under lower load. The problem of controlling a tethered wing in this second phase, the retraction phase, is addressed here, by proposing two possible control strategies. Theoretical analyses, numerical simulations, and experimental results are presented to show the performance of the two approaches. Finally, the experimental results of complete autonomous power generation cycles are reported and compared with those in first-principle models.',\n",
       " '53c544145d2fe5fe8c44584f44f36f74393b983e': 'This paper addresses the problem of simulating deformations between objects and the hand of a synthetic character during a grasping process. A numerical method based on finite element theory allows us to take into account the active forces of the fingers on the object and the reactive forces of the object on the fingers. The method improves control of synthetic human behavior in a task level animation system because it provides information about the environment of a synthetic human and so can be compared to the sense of touch. Finite element theory currently used in engineering seems one of the best approaches for modeling both elastic and plastic deformation of objects, as well as shocks with or without penetration between deformable objects. We show that intrinsic properties of the method based on composition/decomposition of elements have an impact in computer animation. We also state that the use of the same method for modeling both objects and human bodies improves the modeling both objects and human bodies improves the modeling of the contacts between them. Moreover, it allows a realistic envelope deformation of the human fingers comparable to existing methods. To show what we can expect from the method, we apply it to the grasping and pressing of a ball. Our solution to the grasping problem is based on displacement commands instead of force commands used in robotics and human behavior.',\n",
       " '0eaa75861d9e17f2c95bd3f80f48db95bf68a50c': 'Electromigration (EM) is one of the key concerns going forward for interconnect reliability in integrated circuit (IC) design. Although analog designers have been aware of the EM problem for some time, digital circuits are also being affected now. This talk addresses basic design issues and their effects on electromigration during interconnect physical design. The intention is to increase current density limits in the interconnect by adopting electromigration-inhibiting measures, such as short-length and reservoir effects. Exploitation of these effects at the layout stage can provide partial relief of EM concerns in IC design flows in future.',\n",
       " '24ff5027e7042aeead47ef3071f1a023243078bb': 'It is widely acknowledged that the development of traditional terrestrial communication technologies cannot provide all users with fair and high quality services due to the scarce network resource and limited coverage areas. To complement the terrestrial connection, especially for users in rural, disasterstricken, or other difficult-to-serve areas, satellites, unmanned aerial vehicles (UAVs), and balloons have been utilized to relay the communication signals. On the basis, Space-Air-Ground Integrated Networks (SAGINs) have been proposed to improve the users’ Quality of Experience (QoE). However, compared with existing networks such as ad hoc networks and cellular networks, the SAGINs are much more complex due to the various characteristics of three network segments. To improve the performance of SAGINs, researchers are facing many unprecedented challenges. In this paper, we propose the Artificial Intelligence (AI) technique to optimize the SAGINs, as the AI technique has shown its predominant advantages in many applications. We first analyze several main challenges of SAGINs and explain how these problems can be solved by AI. Then, we consider the satellite traffic balance as an example and propose a deep learning based method to improve the traffic control performance. Simulation results evaluate that the deep learning technique can be an efficient tool to improve the performance of SAGINs.',\n",
       " '2c6835e8bdb8c70a9c3aa9bd2578b01dd1b93114': 'We propose a virtual try-on method based on generative adversarial networks (GANs). By considering clothing regions, this method enables us to reflect the pattern of clothes better than Conditional Analogy GAN (CAGAN), an existing virtual try-on method based on GANs. Our method first obtains the clothing region on a person by using a human parsing model learned with a large-scale dataset. Next, using the acquired region, the clothing part is removed from a human image. A desired clothing image is added to the blank area. The network learns how to apply new clothing to the area of people’s clothing. Results demonstrate the possibility of reflecting a clothing pattern. Furthermore, an image of the clothes that the person is originally wearing becomes unnecessary during testing. In experiments, we generate images using images gathered from Zaland (a fashion E-commerce site).',\n",
       " '38a70884a93dd6912404519a779cc497965feff1': \"To explore possible reasons for low self-identification rates among undergraduates with learning disabilities (LD), we asked students (38 with LD, 100 without LD) attending two large, public, research-intensive universities to respond to a questionnaire designed to assess stereotypes about individuals with LD and conceptions of ability. Responses were coded into six categories of stereotypes about LD (low intelligence, compensation possible, process deficit, nonspecific insurmountable condition, working the system, and other), and into three categories of conceptions of intelligence (entity, incremental, neither). Consistent with past findings, the most frequent metastereotype reported by individuals in both groups related to generally low ability. In addition, students with LD were more likely to espouse views of intelligence as a fixed trait. As a whole, the study's findings have implications for our understanding of factors that influence self-identification and self-advocacy at the postsecondary level.\",\n",
       " 'cc6dc5a3e8a18a0aaab7cbe8cee22bf3ac92f0bf': \"In the last years, remarkable improvements have been made in the ability of distributed database systems performance. A distributed database is composed of some sites which are connected to each other through network connections. In this system, if good harmonization isn't made between different transactions, it may result in database incoherence. Nowadays, because of the complexity of many sites and their connection methods, it is difficult to extend different models in distributed database serially. The principle goal of concurrency control in distributed database is to ensure not interfering in accessibility of common database by different sites. Different concurrency control algorithms have been suggested to use in distributed database systems. In this paper, some available methods have been introduced and compared for concurrency control in distributed database.\",\n",
       " '45e2e2a327ea696411b212492b053fd328963cc3': 'BACKGROUND\\nMobile apps hold promise for serving as a lifestyle intervention in public health to promote wellness and attenuate chronic conditions, yet little is known about how individuals with chronic illness use or perceive mobile apps.\\n\\n\\nOBJECTIVE\\nThe objective of this study was to explore behaviors and perceptions about mobile phone-based apps for health among individuals with chronic conditions.\\n\\n\\nMETHODS\\nData were collected from a national cross-sectional survey of 1604 mobile phone users in the United States that assessed mHealth use, beliefs, and preferences. This study examined health app use, reason for download, and perceived efficacy by chronic condition.\\n\\n\\nRESULTS\\nAmong participants, having between 1 and 5 apps was reported by 38.9% (314/807) of respondents without a condition and by 6.6% (24/364) of respondents with hypertension. Use of health apps was reported 2 times or more per day by 21.3% (172/807) of respondents without a condition, 2.7% (10/364) with hypertension, 13.1% (26/198) with obesity, 12.3% (20/163) with diabetes, 12.0% (32/267) with depression, and 16.6% (53/319) with high cholesterol. Results of the logistic regression did not indicate a significant difference in health app download between individuals with and without chronic conditions (P>.05). Compared with individuals with poor health, health app download was more likely among those with self-reported very good health (odds ratio [OR] 3.80, 95% CI 2.38-6.09, P<.001) and excellent health (OR 4.77, 95% CI 2.70-8.42, P<.001). Similarly, compared with individuals who report never or rarely engaging in physical activity, health app download was more likely among those who report exercise 1 day per week (OR 2.47, 95% CI 1.6-3.83, P<.001), 2 days per week (OR 4.77, 95% CI 3.27-6.94, P<.001), 3 to 4 days per week (OR 5.00, 95% CI 3.52-7.10, P<.001), and 5 to 7 days per week (OR 4.64, 95% CI 3.11-6.92, P<.001). All logistic regression results controlled for age, sex, and race or ethnicity.\\n\\n\\nCONCLUSIONS\\nResults from this study suggest that individuals with poor self-reported health and low rates of physical activity, arguably those who stand to benefit most from health apps, were least likely to report download and use these health tools.',\n",
       " '71795f9f511f6948dd67aff7e9725c08ff1a4c94': 'Lately, programmers have started to take advantage of GPU capabilities of cloud-based machines. Using the GPUs can decrease the number of nodes required to perform the computation by increasing the productivity per node. We combine Hadoop, a widely-used MapReduce framework, with Aparapi, a new Java-to-OpenCL conversion tool from AMD. We propose an easy-to-use API which allows easy implementation of MapReduce algorithms that make use of the GPU. Our API improves upon Hadoop by further hiding the complexity of GPU programming, thus allowing the programmer to concentrate on her algorithm. We also propose an accompanying refactoring that allows the programmer to specify the GPU part of their map computation by using very lightweight annotation.',\n",
       " '8cfb12304856268ee438ccb16e4b87960c7349e0': 'Internet, a revolutionary invention, is always transforming into some new kind of hardware and software making it unavoidable for anyone. The form of communication that we see now is either human-human or human-device, but the Internet of Things (IoT) promises a great future for the internet where the type of communication is machine-machine (M2M). This paper aims to provide a comprehensive overview of the IoT scenario and reviews its enabling technologies and the sensor networks. Also, it describes a six-layered architecture of IoT and points out the related key challenges.',\n",
       " 'a39faa00248abb3984317f2d6830f485cb5e1a0d': 'Mobile health technologies offer great promise for reducing healthcare costs and improving patient care. Wearable and implantable technologies are contributing to a transformation in the mobile health era in terms of improving healthcare and health outcomes and providing real-time guidance on improved health management and tracking. In this article, we review the biomedical applications of wearable and implantable medical devices and sensors, ranging from monitoring to prevention of diseases, as well as the materials used in the fabrication of these devices and the standards for wireless medical devices and mobile applications. We conclude by discussing some of the technical challenges in wearable and implantable technology and possible solutions for overcoming these difficulties.',\n",
       " 'e749e6311e25eb8081672742e78c427ce5979552': 'Improving the operational effectiveness and efficiency of processes is a fundamental task of business process management (BPM). There exist many proposals of process improvement patterns (PIPs) as practices that aim at supporting this goal. Selecting and implementing relevant PIPs are therefore an important prerequisite for establishing process-aware information systems in enterprises. Nevertheless, there is still a gap regarding the validation of PIPs with respect to their actual business value for a specific application scenario before implementation investments are incurred. Based on empirical research as well as experiences from BPM projects, this paper proposes a method to tackle this challenge. Our approach toward the assessment of process improvement patterns considers real-world constraints such as the role of senior stakeholders or the cost of adapting available IT systems. In addition, it outlines process improvement potentials that arise from the information technology infrastructure available to organizations, particularly regarding the combination of enterprise resource planning with business process intelligence. Our approach is illustrated along a real-world business process from human resource management. The latter covers a transactional volume of about 29,000 process instances over a period of 1\\xa0year. Overall, our approach enables both practitioners and researchers to reasonably assess PIPs before taking any process implementation decision.',\n",
       " 'd5ecb372f6cbdfb52588fbb4a54be21d510009d0': 'This study aimed to investigate birth order effect on personality and academic performance amongst 120 Malaysians. Besides, it also aimed to examine the relationship between personality and academic achievement. Thirty firstborns, 30 middle children, 30 lastborns, and 30 only children, who shared the mean age of 20.0 years (SD= 1.85), were recruited into this study. Participants’ Sijil Pelajaran Malaysia (SPM) results were recorded and their personality was assessed by Ten Item Personality Inventory (TIPI). Results indicated that participants of different birth positions did not differ significantly in terms of personality and academic performance. However, Pearson’s correlation showed that extraversion correlated positively with academic performance. Keywordsbirth order; personality; academic achievement',\n",
       " '6193ece762c15b7d8a958dc64c37e858cd873b8a': 'A compact printed log-periodic dipole antenna (LPDA) with distributed inductive load has been presented in this paper. By adding a stub on the top of each element, the dimension of the LPAD can be reduced by 60%. The antenna has obtained an impedance bandwidth of 10GHz (8GHz-18GHz). According to the simulation results, the designed structure achieves stable radiation patterns throughout the operating frequency band.',\n",
       " '2d9416485091e6af3619c4bc9323a0887d450c8a': 'A major challenge in scaling object detection is the difficulty of obtaining labeled images for large numbers of categories. Recently, deep convolutional neural networks (CNNs) have emerged as clear winners on object classification benchmarks, in part due to training with 1.2M+ labeled classification images. Unfortunately, only a small fraction of those labels are available for the detection task. It is much cheaper and easier to collect large quantities of image-level labels from search engines than it is to collect detection data and label it with precise bounding boxes. In this paper, we propose Large Scale Detection through Adaptation (LSDA), an algorithm which learns the difference between the two tasks and transfers this knowledge to classifiers for categories without bounding box annotated data, turning them into detectors. Our method has the potential to enable detection for the tens of thousands of categories that lack bounding box annotations, yet have plenty of classification data. Evaluation on the ImageNet LSVRC-2013 detection challenge demonstrates the efficacy of our approach. This algorithm enables us to produce a >7.6K detector by using available classification data from leaf nodes in the ImageNet tree. We additionally demonstrate how to modify our architecture to produce a fast detector (running at 2fps for the 7.6K detector). Models and software are available at lsda.berkeleyvision.org.',\n",
       " '0f28cbfe0674e0af4899d21dd90f6f5d0d5c3f1b': 'In February 2013, Google Flu Trends (GFT) made headlines but not for a reason that Google executives or the creators of the flu tracking system would have hoped. Nature reported that GFT was predicting more than double the proportion of doctor visits for influenza-like illness (ILI) than the Centers for Disease Control and Prevention (CDC), which bases its estimates on surveillance reports from laboratories across the United States (1, 2). This happened despite the fact that GFT was built to predict CDC reports. Given that GFT is often held up as an exemplary use of big data (3, 4), what lessons can we draw from this error?',\n",
       " 'cc43c080340817029fd497536cc9bd39b0a76dd2': 'Human mobility is investigated using a continuum approach that allows to calculate the probability to observe a trip to any arbitrary region, and the fluxes between any two regions. The considered description offers a general and unified framework, in which previously proposed mobility models like the gravity model, the intervening opportunities model, and the recently introduced radiation model are naturally resulting as special cases. A new form of radiation model is derived and its validity is investigated using observational data offered by commuting trips obtained from the United States census data set, and the mobility fluxes extracted from mobile phone data collected in a western European country. The new modeling paradigm offered by this description suggests that the complex topological features observed in large mobility and transportation networks may be the result of a simple stochastic process taking place on an inhomogeneous landscape.',\n",
       " '15e8961e8f9d1fb5060c3284a5bdcc09f2fc1ba6': 'Glaucoma is a disease of the optic nerve caused by the increase in the intraocular pressure of the eye. Glaucoma mainly affects the optic disc by increasing the cup size. It can lead to the blindness if it is not detected and treated in proper time. The detection of glaucoma through Optical Coherence Tomography (OCT) and Heidelberg Retinal Tomography (HRT) is very expensive. This paper presents a novel method for glaucoma detection using digital fundus images. Digital image processing techniques, such as preprocessing, morphological operations and thresholding, are widely used for the automatic detection of optic disc, blood vessels and computation of the features. We have extracted features such as cup to disc (c/d) ratio, ratio of the distance between optic disc center and optic nerve head to diameter of the optic disc, and the ratio of blood vessels area in inferior-superior side to area of blood vessel in the nasal-temporal side. These features are validated by classifying the normal and glaucoma images using neural network classifier. The results presented in this paper indicate that the features are clinically significant in the detection of glaucoma. Our system is able to classify the glaucoma automatically with a sensitivity and specificity of 100% and 80% respectively.',\n",
       " '36b3865f944c74c6d782c26dfe7be04ef9664a67': 'Improving speech system performance in noisy environments remains a challenging task, and speech enhancement (SE) is one of the effective techniques to solve the problem. Motivated by the promising results of generative adversarial networks (GANs) in a variety of image processing tasks, we explore the potential of conditional GANs (cGANs) for SE, and in particular, we make use of the image processing framework proposed by Isola et al. [1] to learn a mapping from the spectrogram of noisy speech to an enhanced counterpart. The SE cGAN consists of two networks, trained in an adversarial manner: a generator that tries to enhance the input noisy spectrogram, and a discriminator that tries to distinguish between enhanced spectrograms provided by the generator and clean ones from the database using the noisy spectrogram as a condition. We evaluate the performance of the cGAN method in terms of perceptual evaluation of speech quality (PESQ), short-time objective intelligibility (STOI), and equal error rate (EER) of speaker verification (an example application). Experimental results show that the cGAN method overall outperforms the classical short-time spectral amplitude minimum mean square error (STSA-MMSE) SE algorithm, and is comparable to a deep neural network-based SE approach (DNN-SE).',\n",
       " 'bb192e0208548831de1475b11859f2114121c662': 'Along with privacy, discrimination is a very import ant issue when considering the legal and ethical aspects of data mini ng. It is more than obvious that most people do not want to be discriminated because of their gender, religion, nationality, age and so on, especially when those att ribu es are used for making decisions about them like giving them a job, loan, insu rance, etc. Discovering such potential biases and eliminating them from the traini ng data without harming their decision-making utility is therefore highly desirab le. For this reason, antidiscrimination techniques including discrimination discovery and prevention have been introduced in data mining. Discrimination prev ention consists of inducing patterns that do not lead to discriminatory decisio n even if the original training datasets are inherently biased. In this chapter, by focusing on the discrimination prevention, we present a taxonomy for classifying a d examining discrimination prevention methods. Then, we introduce a group of p re-processing discrimination prevention methods and specify the different featur es of each approach and how these approaches deal with direct or indirect discr imination. A presentation of metrics used to evaluate the performance of those app ro ches is also given. Finally, we conclude our study by enumerating interesting fu ture directions in this research body.',\n",
       " '1935e0986939ea6ef2afa01eeef94dbfea6fb6da': 'Mean-variance portfolio analysis provided the first quantitative treatment of the tradeoff between profit and risk. We describe in detail the interplay between objective and constraints in a number of single-period variants, including semivariance models. Particular emphasis is laid on avoiding the penalization of overperformance. The results are then used as building blocks in the development and theoretical analysis of multiperiod models based on scenario trees. A key property is the possibility of removing surplus money in future decisions, yielding approximate downside risk minimization.',\n",
       " '1ea03bc28a14ade633d5a7fe9af71328834d45ab': 'We propose a practical approach based on federated learning to solve out-of-domain issues with continuously running embedded speech-based models such as wake word detectors. We conduct an extensive empirical study of the federated averaging algorithm for the “Hey Snips” wake word based on a crowdsourced dataset that mimics a federation of wake word users. We empirically demonstrate that using an adaptive averaging strategy inspired from Adam in place of standard weighted model averaging highly reduces the number of communication rounds required to reach our target performance. The associated upstream communication costs per user are estimated at 8 MB, which is a reasonable in the context of smart home voice assistants. Additionally, the dataset used for these experiments is being open sourced with the aim of fostering further transparent research in the application of federated learning to speech data.',\n",
       " '55ca165fa6091973674b12ea8fa3f1a3a1e50a6d': 'Sample-based tree search (SBTS) is an approach to solving Markov decision problems based on constructing a lookahead search tree using random samples from a generative model of the MDP. It encompasses Monte Carlo tree search (MCTS) algorithms like UCT as well as algorithms such as sparse sampling. SBTS is well-suited to solving MDPs with large state spaces due to the relative insensitivity of SBTS algorithms to the size of the state space. The limiting factor in the performance of SBTS tends to be the exponential dependence of sample complexity on the depth of the search tree. The number of samples required to build a search tree is O((|A|B)), where |A| is the number of available actions, B is the number of possible random outcomes of taking an action, and d is the depth of the tree. State abstraction can be used to reduce B by aggregating random outcomes together into abstract states. Recent work has shown that abstract tree search often performs substantially better than tree search conducted in the ground state space. This paper presents a theoretical and empirical evaluation of tree search with both fixed and adaptive state abstractions. We derive a bound on regret due to state abstraction in tree search that decomposes abstraction error into three components arising from properties of the abstraction and the search algorithm. We describe versions of popular SBTS algorithms that use fixed state abstractions, and we introduce the Progressive Abstraction Refinement in Sparse Sampling (PARSS) algorithm, which adapts its abstraction during search. We evaluate PARSS as well as sparse sampling with fixed abstractions on 12 experimental problems, and find that PARSS outperforms search with a fixed abstraction and that search with even highly inaccurate fixed abstractions outperforms search without abstraction. These results establish progressive abstraction refinement as a promising basis for new tree search algorithms, and we propose directions for future work within the progressive refinement framework.',\n",
       " '2ae40898406df0a3732acc54f147c1d377f54e2a': 'We propose an algorithm called query by commitee, in which a committee of students is trained on the same data set. The next query is chosen according to the principle of maximal disagreement. The algorithm is studied for two toy models: the high-low game and perceptron learning of another perceptron. As the number of queries goes to infinity, the committee algorithm yields asymptotically finite information gain. This leads to generalization error that decreases exponentially with the number of examples. This in marked contrast to learning from randomly chosen inputs, for which the information gain approaches zero and the generalization error decreases with a relatively slow inverse power law. We suggest that asymptotically finite information gain may be an important characteristic of good query algorithms.',\n",
       " '49e85869fa2cbb31e2fd761951d0cdfa741d95f3': 'Manifold learning algorithms seek to find a low-dimensional parameterization of high-dimensional data. They heavily rely on the notion of what can be considered as local, how accurately the manifold can be approximated locally, and, last but not least, how the local structures can be patched together to produce the global parameterization. In this paper, we develop algorithms that address two key issues in manifold learning: 1) the adaptive selection of the local neighborhood sizes when imposing a connectivity structure on the given set of high-dimensional data points and 2) the adaptive bias reduction in the local low-dimensional embedding by accounting for the variations in the curvature of the manifold as well as its interplay with the sampling density of the data set. We demonstrate the effectiveness of our methods for improving the performance of manifold learning algorithms using both synthetic and real-world data sets.',\n",
       " 'bf07d60ba6d6c6b8cabab72dfce06f203782df8f': 'Advances in hyperspectral sensing provide new capability for characterizing spectral signatures in a wide range of physical and biological systems, while inspiring new methods for extracting information from these data. HSI data often lie on sparse, nonlinear manifolds whose geometric and topological structures can be exploited via manifold-learning techniques. In this article, we focused on demonstrating the opportunities provided by manifold learning for classification of remotely sensed data. However, limitations and opportunities remain both for research and applications. Although these methods have been demonstrated to mitigate the impact of physical effects that affect electromagnetic energy traversing the atmosphere and reflecting from a target, nonlinearities are not always exhibited in the data, particularly at lower spatial resolutions, so users should always evaluate the inherent nonlinearity in the data. Manifold learning is data driven, and as such, results are strongly dependent on the characteristics of the data, and one method will not consistently provide the best results. Nonlinear manifold-learning methods require parameter tuning, although experimental results are typically stable over a range of values, and have higher computational overhead than linear methods, which is particularly relevant for large-scale remote sensing data sets. Opportunities for advancing manifold learning also exist for analysis of hyperspectral and multisource remotely sensed data. Manifolds are assumed to be inherently smooth, an assumption that some data sets may violate, and data often contain classes whose spectra are distinctly different, resulting in multiple manifolds or submanifolds that cannot be readily integrated with a single manifold representation. Developing appropriate characterizations that exploit the unique characteristics of these submanifolds for a particular data set is an open research problem for which hierarchical manifold structures appear to have merit. To date, most work in manifold learning has focused on feature extraction from single images, assuming stationarity across the scene. Research is also needed in joint exploitation of global and local embedding methods in dynamic, multitemporal environments and integration with semisupervised and active learning.',\n",
       " '01996726f44253807537cec68393f1fce6a9cafa': 'We describe a probabilistic approach to the task of placing objects, described by high-dimensional vectors or by pairwise dissimilarities, in a low-dimensional space in a way that preserves neighbor identities. A Gaussian is centered on each object in the high-dimensional space and the densities under this Gaussian (or the given dissimilarities) are used to define a probability distribution over all the potential neighbors of the object. The aim of the embedding is to approximate this distribution as well as possible when the same operation is performed on the low-dimensional “images” of the objects. A natural cost function is a sum of Kullback-Leibler divergences, one per object, which leads to a simple gradient for adjusting the positions of the low-dimensional images. Unlike other dimensionality reduction methods, this probabilistic framework makes it easy to represent each object by a mixture of widely separated low-dimensional images. This allows ambiguous objects, like the document count vector for the word “bank”, to have versions close to the images of both “river” and “finance” without forcing the images of outdoor concepts to be located close to those of corporate concepts.',\n",
       " '0e1431fa42d76c44911b07078610d4b9254bd4ce': 'A new method for performing a nonlinear form of principal component analysis is proposed. By the use of integral operator kernel functions, one can efficiently compute principal components in high-dimensional feature spaces, related to input space by some nonlinear mapfor instance, the space of all possible five-pixel products in 16 16 images. We give the derivation of the method and present experimental results on polynomial feature extraction for pattern recognition.',\n",
       " '40cfac582cafeadb0e09e5f020e2febf5cbd4986': 'Adverse drug events (ADEs) constitute one of the leading causes of post-therapeutic death and their identification constitutes an important challenge of modern precision medicine. Unfortunately, the onset and effects of ADEs are often underreported complicating timely intervention. At over 500 million posts per day, Twitter is a commonly used social media platform. The ubiquity of day-to-day personal information exchange on Twitter makes it a promising target for data mining for ADE identification and intervention. Three technical challenges are central to this problem: (1) identification of salient medical keywords in (noisy) tweets, (2) mapping drug-effect relationships, and (3) classification of such relationships as adverse or non-adverse. We use a bipartite graph-theoretic representation called a drug-effect graph (DEG) for modeling drug and side effect relationships by representing the drugs and side effects as vertices. We construct individual DEGs on two data sources. The first DEG is constructed from the drug-effect relationships found in FDA package inserts as recorded in the SIDER database. The second DEG is constructed by mining the history of Twitter users. We use dictionary-based information extraction to identify medically-relevant concepts in tweets. Drugs, along with co-occurring symptoms are connected with edges weighted by temporal distance and frequency. Finally, information from the SIDER DEG is integrate with the Twitter DEG and edges are classified as either adverse or non-adverse using supervised machine learning. We examine both graph-theoretic and semantic features for the classification task. The proposed approach can identify adverse drug effects with high accuracy with precision exceeding 85\\xa0% and F1 exceeding 81\\xa0%. When compared with leading methods at the state-of-the-art, which employ un-enriched graph-theoretic analysis alone, our method leads to improvements ranging between 5 and 8\\xa0% in terms of the aforementioned measures. Additionally, we employ our method to discover several ADEs which, though present in medical literature and Twitter-streams, are not represented in the SIDER databases. We present a DEG integration model as a powerful formalism for the analysis of drug-effect relationships that is general enough to accommodate diverse data sources, yet rigorous enough to provide a strong mechanism for ADE identification.',\n",
       " '292eee24017356768f1f50b72701ea636dba7982': 'We present a method for automatic object localization and recognition in 3D point clouds representing outdoor urban scenes. The method is based on the implicit shape models (ISM) framework, which recognizes objects by voting for their center locations. It requires only few training examples per class, which is an important property for practical use. We also introduce and evaluate an improved version of the spin image descriptor, more robust to point density variation and uncertainty in normal direction estimation. Our experiments reveal a significant impact of these modifications on the recognition performance. We compare our results against the state-of-the-art method and get significant improvement in both precision and recall on the Ohio dataset, consisting of combined aerial and terrestrial LiDAR scans of 150,000 m of urban area in total.',\n",
       " 'ffd7ac9b4fff641d461091d5237321f83bae5216': 'In a world of global trading, maritime safety, security and efficiency are crucial issues. We propose a multi-task deep learning framework for vessel monitoring using Automatic Identification System (AIS) data streams. We combine recurrent neural networks with latent variable modeling and an embedding of AIS messages to a new representation space to jointly address key issues to be dealt with when considering AIS data streams: massive amount of streaming data, noisy data and irregular timesampling. We demonstrate the relevance of the proposed deep learning framework on real AIS datasets for a three-task setting, namely trajectory reconstruction, anomaly detection and vessel type identification.',\n",
       " '6385cd92746386c82a69ffdc3bc0a9da9f64f721': 'The Dysphagia Outcome and Severity Scale (DOSS) is a simple, easy-to-use, 7-point scale developed to systematically rate the functional severity of dysphagia based on objective assessment and make recommendations for diet level, independence level, and type of nutrition. Intra- and interjudge reliabilities of the DOSS was established by four clinicians on 135 consecutive patients who underwent a modified barium swallow procedure at a large teaching hospital. Patients were assigned a severity level, independence level, and nutritional level based on three areas most associated with final recommendations: oral stage bolus transfer, pharyngeal stage retention, and airway protection. Results indicate high interrater (90%) and intrarater (93%) agreement with this scale. Implications are suggested for use of the DOSS in documenting functional outcomes of swallowing and diet status based on objective assessment.',\n",
       " '1d18fba47004a4cf2643c41ca82f6b04904bb134': 'Accurate and high-quality depth maps are required in lots of 3D applications, such as multi-view rendering, 3D reconstruction and 3DTV. However, the resolution of captured depth image is much lower than that of its corresponding color image, which affects its application performance. In this paper, we propose a novel depth map super-resolution (SR) method by taking view synthesis quality into account. The proposed approach mainly includes two technical contributions. First, since the captured low-resolution (LR) depth map may be corrupted by noise and occlusion, we propose a credibility based multi-view depth maps fusion strategy, which considers the view synthesis quality and interview correlation, to refine the LR depth map. Second, we propose a view synthesis quality based trilateral depth-map up-sampling method, which considers depth smoothness, texture similarity and view synthesis quality in the up-sampling filter. Experimental results demonstrate that the proposed method outperforms state-of-the-art depth SR methods for both super-resolved depth maps and synthesized views. Furthermore, the proposed method is robust to noise and achieves promising results under noise-corruption conditions.',\n",
       " '922b5eaa5ca03b12d9842b7b84e0e420ccd2feee': 'AN IMPORTANT class of theoretical and practical problems in communication and control is of a statistical nature. Such problems are: (i) Prediction of random signals; (ii) separation of random signals from random noise; (iii) detection of signals of known form (pulses, sinusoids) in the presence of random noise. In his pioneering work, Wiener [1]3 showed that problems (i) and (ii) lead to the so-called Wiener-Hopf integral equation; he also gave a method (spectral factorization) for the solution of this integral equation in the practically important special case of stationary statistics and rational spectra. Many extensions and generalizations followed Wiener’s basic work. Zadeh and Ragazzini solved the finite-memory case [2]. Concurrently and independently of Bode and Shannon [3], they also gave a simplified method [2) of solution. Booton discussed the nonstationary Wiener-Hopf equation [4]. These results are now in standard texts [5-6]. A somewhat different approach along these main lines has been given recently by Darlington [7]. For extensions to sampled signals, see, e.g., Franklin [8], Lees [9]. Another approach based on the eigenfunctions of the WienerHopf equation (which applies also to nonstationary problems whereas the preceding methods in general don’t), has been pioneered by Davis [10] and applied by many others, e.g., Shinbrot [11], Blum [12], Pugachev [13], Solodovnikov [14]. In all these works, the objective is to obtain the specification of a linear dynamic system (Wiener filter) which accomplishes the prediction, separation, or detection of a random signal.4 ——— 1 This research was supported in part by the U. S. Air Force Office of Scientific Research under Contract AF 49 (638)-382. 2 7212 Bellona Ave. 3 Numbers in brackets designate References at end of paper. 4 Of course, in general these tasks may be done better by nonlinear filters. At present, however, little or nothing is known about how to obtain (both theoretically and practically) these nonlinear filters. Contributed by the Instruments and Regulators Division and presented at the Instruments and Regulators Conference, March 29– Apri1 2, 1959, of THE AMERICAN SOCIETY OF MECHANICAL ENGINEERS. NOTE: Statements and opinions advanced in papers are to be understood as individual expressions of their authors and not those of the Society. Manuscript received at ASME Headquarters, February 24, 1959. Paper No. 59-IRD—11. A New Approach to Linear Filtering and Prediction Problems',\n",
       " '9ee859bec32fa8240de0273faff6f20e16e21cc6': 'Diabetic retinopathy is one of the leading causes of preventable blindness in the developed world. Early diagnosis of diabetic retinopathy enables timely treatment and in order to achieve it a major effort will have to be invested into screening programs and especially into automated screening programs. Detection of exudates is very important for early diagnosis of diabetic retinopathy. Deep neural networks have proven to be a very promising machine learning technique, and have shown excellent results in different compute vision problems. In this paper we show that convolutional neural networks can be effectively used in order to detect exudates in color fundus photographs.',\n",
       " '13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986': 'Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.',\n",
       " 'aeb7eaf29e16c82d9c0038a10d5b12d32b26ab60': 'Multi-task learning (MTL) involves the simultaneous training of two or more related tasks over shared representations. In this work, we apply MTL to audio-visual automatic speech recognition(AV-ASR). Our primary task is to learn a mapping between audio-visual fused features and frame labels obtained from acoustic GMM/HMM model. This is combined with an auxiliary task which maps visual features to frame labels obtained from a separate visual GMM/HMM model. The MTL model is tested at various levels of babble noise and the results are compared with a base-line hybrid DNN-HMM AVASR model. Our results indicate that MTL is especially useful at higher level of noise. Compared to base-line, upto 7% relative improvement in WER is reported at -3 SNR dB1.',\n",
       " '45fe9fac928c9e64c96b5feda318a09f1b0228dd': 'We have advanced an effort to develop vision based human understanding technologies for realizing human-friendly machine interfaces. Visual information, such as gender, age ethnicity, and facial expression play an important role in face-to-face communication. This paper addresses a novel approach for ethnicity classification with facial images. In this approach, the Gabor wavelets transformation and retina sampling are combined to extract key facial features, and support vector machines that are used for ethnicity classification. Our system, based on this approach, has achieved approximately 94% for ethnicity estimation under various lighting conditions.',\n",
       " '2dbaedea8ac09b11d9da8767eb73b6b821890661': \"From an initial sample of 1278 Italian students, the authors selected 537 on the basis of their responses to a self-report bully and victim questionnaire. Participants' ages ranged from 13 to 20 years (M = 15.12 years, SD = 1.08 years). The authors compared the concurrent psychological symptoms of 4 participant groups (bullies, victims, bully/victims [i.e., bullies who were also victims of bullying], and uninvolved students). Of participants, 157 were in the bullies group, 140 were in the victims group, 81 were in the bully/victims group, and 159 were in the uninvolved students group. The results show that bullies reported a higher level of externalizing problems, victims reported more internalizing symptoms, and bully/victims reported both a higher level of externalizing problems and more internalizing symptoms. The authors divided the sample into 8 groups on the basis of the students' recollection of their earlier school experiences and of their present role. The authors classified the participants as stable versus late bullies, victims, bully/victims, or uninvolved students. The authors compared each stable group with its corresponding late group and found that stable victims and stable bully/victims reported higher degrees of anxiety, depression, and withdrawal than did the other groups. The authors focus their discussion on the role of chronic peer difficulties in relation to adolescents' symptoms and well-being.\",\n",
       " 'd26ce29a109f8ccb42d7b0d312c70a6a750018ce': \"This paper presents a novel side gate HiGT (High-conductivity IGBT) that incorporates historical changes of gate structures for planar and trench gate IGBTs. Side gate HiGT has a side-wall gate, and the opposite side of channel region for side-wall gate is covered by a thick oxide layer to reduce Miller capacitance (Cres). In addition, side gate HiGT has no floating p-layer, which causes the excess Vge overshoot. The proposed side gate HiGT has 75% smaller Cres than the conventional trench gate IGBT. The excess Vge overshoot during turn-on is effectively suppressed, and Eon + Err can be reduced by 34% at the same diode's recovery dv/dt. Furthermore, side gate HiGT has sufficiently rugged RBSOA and SCSOA.\",\n",
       " '0db78d548f914135ad16c0d6618890de52c0c065': 'Current dialogue systems generally operate in a pipelined, modular fashion on one complete utterance at a time. Evidence from human language understanding shows that human understanding operates incrementally and makes use of multiple sources of information during the parsing process, including traditionally “later” components such as pragmatics. In this paper we describe a spoken dialogue system that understands language incrementally, provides visual feedback on possible referents during the course of the user’s utterance, and allows for overlapping speech and actions. We further present findings from an empirical study showing that the resulting dialogue system is faster overall than its nonincremental counterpart. Furthermore, the incremental system is preferred to its nonincremental counterpart – beyond what is accounted for by factors such as speed and accuracy. These results indicate that successful incremental understanding systems will improve both performance and usability.',\n",
       " 'eec44862b2d58434ca7706224bc0e9437a2bc791': 'Ž . The balanced scorecard BSC has emerged as a decision support tool at the strategic management level. Many business leaders now evaluate corporate performance by supplementing financial accounting data with goal-related measures from the following perspectives: customer, internal business process, and learning and growth. It is argued that the BSC concept can be adapted to assist those managing business functions, organizational units and individual projects. This article develops a Ž . balanced scorecard for information systems IS that measures and evaluates IS activities from the following perspectives: business value, user orientation, internal process, and future readiness. Case study evidence suggests that a balanced IS scorecard can be the foundation for a strategic IS management system provided that certain development guidelines are followed, appropriate metrics are identified, and key implementation obstacles are overcome. q 1999 Elsevier Science B.V. All rights reserved.',\n",
       " '4828a00f623651a9780b945980530fb6b3cb199a': 'Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model’s parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet (Russakovsky et al., 2014). Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than singlestep attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a “label leaking” effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.',\n",
       " 'f7d3b7986255f2e5e2402e84d7d7c5e583d7cb05': 'This paper presents a novel online-capable interaction-aware intention and maneuver prediction framework for dynamic environments. The main contribution is the combination of model-based interaction-aware intention estimation with maneuver-based motion prediction based on supervised learning. The advantages of this framework are twofold. On one hand, expert knowledge in the form of heuristics is integrated, which simplifies the modeling of the interaction. On the other hand, the difficulties associated with the scalability and data sparsity of the algorithm due to the so-called curse of dimensionality can be reduced, as a reduced feature space is sufficient for supervised learning. The proposed algorithm can be used for highly automated driving or as a prediction module for advanced driver assistance systems without the need of intervehicle communication. At the start of the algorithm, the motion intention of each driver in a traffic scene is predicted in an iterative manner using the game-theoretic idea of stochastic multiagent simulation. This approach provides an interpretation of what other drivers intend to do and how they interact with surrounding traffic. By incorporating this information into a Bayesian network classifier, the developed framework achieves a significant improvement in terms of reliable prediction time and precision compared with other state-of-the-art approaches. By means of experimental results in real traffic on highways, the validity of the proposed concept and its online capability is demonstrated. Furthermore, its performance is quantitatively evaluated using appropriate statistical measures.',\n",
       " '7616624dd230c42f6397a9a48094cf4611c02ab8': 'This study investigates a phase-locked loop (PLL) controlled parallel resonant converter (PRC) for a pulse power capacitor charging application. The dynamic nature of the capacitor charging is such that it causes a shift in the resonant frequency of the PRC. Using the proposed control method, the PRC can be optimized to operate with its maximum power capability and guarantee ZVS operation, even when the input voltage and resonant tank parameters vary. The detailed implementation of the PLL controller, as well as the determination of dead-time and leading time, is presented in this paper. Simulation and experimental results verify the performance of the proposed control method.',\n",
       " '4902805fe1e2f292f6beed7593154e686d7f6dc2': \"Recognizing lines of unconstrained handwritten text is a challenging task. The difficulty of segmenting cursive or overlapping characters, combined with the need to exploit surrounding context, has led to low recognition rates for even the best current recognizers. Most recent progress in the field has been made either through improved preprocessing or through advances in language modeling. Relatively little work has been done on the basic recognition algorithms. Indeed, most systems rely on the same hidden Markov models that have been used for decades in speech and handwriting recognition, despite their well-known shortcomings. This paper proposes an alternative approach based on a novel type of recurrent neural network, specifically designed for sequence labeling tasks where the data is hard to segment and contains long-range bidirectional interdependencies. In experiments on two large unconstrained handwriting databases, our approach achieves word recognition accuracies of 79.7 percent on online data and 74.1 percent on offline data, significantly outperforming a state-of-the-art HMM-based system. In addition, we demonstrate the network's robustness to lexicon size, measure the individual influence of its hidden layers, and analyze its use of context. Last, we provide an in-depth discussion of the differences between the network and HMMs, suggesting reasons for the network's superior performance.\",\n",
       " 'e48df18774fbaff8b70b0231a02c3ccf1ebdf784': 'Recently, automatic face recognition has been applied in many web and mobile applications. Developers integrate and implement face recognition as an access control into these applications. However, face recognition authentication is vulnerable to several attacks especially when an attacker presents a 2-D printed image or recorded video frames in front of the face sensor system to gain access as a legitimate user. This paper introduces a non-intrusive method to detect face spoofing attacks that utilize a single frame of sequenced frames. We propose a specialized deep convolution neural network to extract complex and high features of the input diffused frame. We tested our method on the Replay Attack dataset which consists of 1200 short videos of both real-access and spoofing attacks. An extensive experimental analysis was conducted that demonstrated better results when compared to previous static algorithms results.',\n",
       " '1663f1b811c0ea542c1d128ff129cdf5cd7f9c44': 'ISAR imaging is described for general motion of a radar target. ISAR imaging may be seen as a 3D to 2D projection, and the importance of the ISAR image projection plane is stated. For general motion, ISAR images are often smeared when using FFT processing. Time frequency methods are used to analyze such images, and to form sharp images. A given smeared image is shown to be the result of changes both in scale and in the projection plane orientation.',\n",
       " '792e9c588e3426ec55f630fffefa439fc17e0406': \"To support the systematic improvement of business intelligence (BI) in organizations, we have designed and refined a BI maturity model (BIMM) and a respective measurement instrument (MI) in prior research. In this study, we devise an evaluation strategy, and evaluate the validity of the designed measurement artifact. Through cluster analysis of maturity assessments of 92 organizations, we identify characteristic BI maturity scenarios and representative cases for the relevant scenarios. For evaluating the designed instrument, we compare its results with insights obtained from in-depth interviews in the respective companies. A close match between our model's quantitative maturity assessments and the maturity levels from the qualitative analyses indicates that the MI correctly assesses BI maturity. The applied evaluation approach has the potential to be re-used in other design research studies where the validity of utility claims is often hard to prove.\",\n",
       " '2907cde029f349948680a3690500d4cf09b5be96': 'This thesis describes MultiSphinx, a concurrent architecture for scalable, low-latency automatic speech recognition. We first consider the problem of constructing a universal “core” speech recognizer on top of which domain and task specific adaptation layers can be constructed. We then show that when this problem is restricted to that of expanding the search space from a “core” vocabulary to a superset of this vocabulary across multiple passes of search, it allows us to effectively “factor” a recognizer into components of roughly equal complexity. We present simple but effective algorithms for constructing the reduced vocabulary and associated statistical language model from an existing system. Finally, we describe the MultiSphinx decoder architecture, which allows multiple passes of recognition to operate concurrently and incrementally, either in multiple threads in the same process, or across multiple processes on separate machines, and which allows the best possible partial results, including confidence scores, to be obtained at any time during the recognition process.',\n",
       " '86436e9d0c98e7133c7d00d8875bcf0720ad3882': '(USA) respectively. Since graduation, they have been associated with this project in voluntary and individual capacity. Sandeep Singh Gujral, an occupational therapist was an intern at the IIT Delhi from August-November 2009 and conducted user training for the trials.',\n",
       " 'ee548bd6b96cf1d748def335c1517c2deea1b3f5': \"This paper tests whether accurate sales forecasts for Nike are possible from Facebook data and how events related to Nike affect the activity on Nike's Facebook pages. The paper draws from the AIDA sales framework (Awareness, Interest, Desire, and Action) from the domain of marketing and employs the method of social set analysis from the domain of computational social science to model sales from Big Social Data. The dataset consists of (a) selection of Nike's Facebook pages with the number of likes, comments, posts etc. that have been registered for each page per day and (b) business data in terms of quarterly global sales figures published in Nike's financial reports. An event study is also conducted using the Social Set Visualizer (SoSeVi). The findings suggest that Facebook data does have informational value. Some of the simple regression models have a high forecasting accuracy. The multiple regressions have a lower forecasting accuracy and cause analysis barriers due to data set characteristics such as perfect multicollinearity. The event study found abnormal activity around several Nike specific events but inferences about those activity spikes, whether they are purely event-related or coincidences, can only be determined after detailed case-by-case text analysis. Our findings help assess the informational value of Big Social Data for a company's marketing strategy, sales operations and supply chain.\",\n",
       " 'dcbebb8fbd3ebef2816ebe0f7da12340a5725a8b': 'Vectorial representations of words have grown remarkably popular in natural language processing and machine translation. The recent surge in deep learning-inspired methods for producing distributed representations has been widely noted even outside these fields. Existing representations are typically trained on large monolingual corpora using context-based prediction models. In this paper, we propose extending pre-existing word representations by exploiting Wiktionary. This process results in a substantial extension of the original word vector representations, yielding a large multilingual dictionary of word embeddings. We believe that this resource can enable numerous monolingual and cross-lingual applications, as evidenced in a series of monolingual and cross-lingual semantic experiments that we have conducted.',\n",
       " '356f5f4224ee090a11e83a7e3cc130b2fdb0e612': 'Query expansion methods have been studied for a long time - with debatable success in many instances. In this paper we present a probabilistic query expansion model based on a similarity thesaurus which was constructed automatically. A similarity thesaurus reflects domain knowledge about the particular collection from which it is constructed. We address the two important issues with query expansion: the selection and the weighting of additional search terms. In contrast to earlier methods, our queries are expanded by adding those terms that are most similar to the concept of the query, rather than selecting terms that are similar to the query terms. Our experiments show that this kind of query expansion results in a notable improvement in the retrieval effectiveness when measured using both recall-precision and usefulness.',\n",
       " '37b0f219c1f2fbc4b432b24a5fe91dd733f19b7f': 'Although commonly used in both commercial and experimental information retrieval systems, thesauri have not demonstrated consistent beneets for retrieval performance, and it is diicult to construct a thesaurus automatically for large text databases. In this paper, an approach, called PhraseFinder, is proposed to construct collection-dependent association thesauri automatically using large full-text document collections. The association thesaurus can be accessed through natural language queries in INQUERY, an information retrieval system based on the probabilistic inference network. Experiments are conducted in IN-QUERY to evaluate diierent types of association thesauri, and thesauri constructed for a variety of collections.',\n",
       " 'ca56018ed7042d8528b5a7cd8f332c5737b53b1f': \"A well constructed thesaurus has long been recognized as a valuable tool in the effective operation of an information retrieval system. This paper reports the results of experiments designed to determine the validity of an approach to the automatic construction of global thesauri (described originally by Crouch in [1] and [2] based on a clustering of the document collection. The authors validate the approach by showing that the use of thesauri generated by this method results in substantial improvements in retrieval effectiveness in four test collections. The term discrimination value theory, used in the thesaurus generation algorithm to determine a term's membership in a particular thesaurus class, is found not to be useful in distinguishing a “good” from an “indifferent” or “poor” thesaurus class). In conclusion, the authors suggest an alternate approach to automatic thesaurus construction which greatly simplifies the work of producing viable thesaurus classes. Experimental results show that the alternate approach described herein in some cases produces thesauri which are comparable in retrieval effectiveness to those produced by the first method at much lower cost.\",\n",
       " 'e50a316f97c9a405aa000d883a633bd5707f1a34': 'The experimental evidence accumulated over the past 20 years indicates that text indexing systems based on the assignment of appropriately weighted single terms produce retrieval results that are superior to those obtainable with other more elaborate text representations. These results depend crucially on the choice of effective termweighting systems. This article summarizes the insights gained in automatic term weighting, and provides baseline single-term-indexing models with which other more elaborate content analysis procedures can be compared. 1. AUTOMATIC TEXT ANALYSIS In the late 195Os, Luhn [l] first suggested that automatic text retrieval systems could be designed based on a comparison of content identifiers attached both to the stored texts and to the users’ information queries. Typically, certain words extracted from the texts of documents and queries would be used for content identification; alternatively, the content representations could be chosen manually by trained indexers familiar with the subject areas under consideration and with the contents of the document collections. In either case, the documents would be represented by term vectors of the form D= (ti,tj,...ytp) (1) where each tk identifies a content term assigned to some sample document D. Analogously, the information requests, or queries, would be represented either in vector form, or in the form of Boolean statements. Thus, a typical query Q might be formulated as Q = (qa,qbr.. . ,4r) (2)',\n",
       " 'fac511cc5079d432c7e010eee2d5a8d67136ecdd': 'The build-to-order supply chain management (BOSC) strategy has recently attracted the attention of both researchers and practitioners, given its successful implementation in many companies including Dell computers, Compaq, and BMW. The growing number of articles on BOSC in the literature is an indication of the importance of the strategy and of its role in improving the competitiveness of an organization. The objective of a BOSC strategy is to meet the requirements of individual customers by leveraging the advantages of outsourcing and information technology. There are not many research articles that provide an overview of BOSC, despite the fact that this strategy is being promoted as the operations paradigm of the future. The main objective of this research is to (i) review the concepts of BOSC, (ii) develop definitions of BOSC, (iii) classify the literature based on a suitable classification scheme, leading to some useful insights into BOSC and some future research directions, (iv) review the selected articles on BOSC for their contribution to the development and operations of BOSC, (v) develop a framework for BOSC, and (vi) suggest some future research directions. The literature has been reviewed based on the following four major areas of decision-making: organizational competitiveness, the development and implementation of BOSC, the operations of BOSC, and information technology in BOSC. Some of the important observations are: (a) there is a lack of adequate research on the design and control of BOSC, (b) there is a need for further research on the implementation of BOSC, (c) human resource issues in BOSC have been ignored, (d) issues of product commonality and modularity from the perspective of partnership or supplier development require further attention and (e) the trade-off between responsiveness and the cost of logistics needs further study. The paper ends with concluding remarks. # 2004 Elsevier B.V. All rights reserved.',\n",
       " '6ac15e819701cd0d077d8157711c4c402106722c': \"This technical report describes Team MIT's approach to the DARPA Urban Challenge. We have developed a novel strategy for using many inexpensive sensors, mounted on the vehicle periphery, and calibrated with a new cross\\xadmodal calibration technique. Lidar, camera, and radar data streams are processed using an innovative, locally smooth state representation that provides robust perception for real\\xad time autonomous control. A resilient planning and control architecture has been developed for driving in traffic, comprised of an innovative combination of well\\xadproven algorithms for mission planning, situational planning, situational interpretation, and trajectory control. These innovations are being incorporated in two new robotic vehicles equipped for autonomous driving in urban environments, with extensive testing on a DARPA site visit course. Experimental results demonstrate all basic navigation and some basic traffic behaviors, including unoccupied autonomous driving, lane following using pure\\xadpursuit control and our local frame perception strategy, obstacle avoidance using kino\\xaddynamic RRT path planning, U\\xadturns, and precedence evaluation amongst other cars at intersections using our situational interpreter. We are working to extend these approaches to advanced navigation and traffic scenarios. † Executive Summary This technical report describes Team MIT's approach to the DARPA Urban Challenge. We have developed a novel strategy for using many inexpensive sensors, mounted on the vehicle periphery, and calibrated with a new cross-modal calibration technique. Lidar, camera, and radar data streams are processed using an innovative, locally smooth state representation that provides robust perception for real-time autonomous control. A resilient planning and control architecture has been developed for driving in traffic, comprised of an innovative combination of well-proven algorithms for mission planning, situational planning, situational interpretation, and trajectory control. These innovations are being incorporated in two new robotic vehicles equipped for autonomous driving in urban environments, with extensive testing on a DARPA site visit course. Experimental results demonstrate all basic navigation and some basic traffic behaviors, including unoccupied autonomous driving, lane following using pure-pursuit control and our local frame perception strategy, obstacle avoidance using kino-dynamic RRT path planning, U-turns, and precedence evaluation amongst other cars at intersections using our situational interpreter. We are working to extend these approaches to advanced navigation and traffic scenarios. DISCLAIMER: The information contained in this paper does not represent the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency (DARPA) or the Department of Defense. DARPA does not guarantee the accuracy or reliability of the information in this paper. Additional support …\",\n",
       " '035e9cd81d5dd3b1621fb14e00b452959daffddf': 'Healthcare innovation has made progressive strides. Innovative solutions now tend to incorporate device integration, data collection and data analysis linked across a diverse range of actors building platform-centric healthcare ecosystems. The interconnectedness and inter-disciplinarity of the ecosystems bring with it a number of vital issues around how to strategically manage such a complex system. This paper highlights the importance of innovation intermediaries particularly in a platform-centric ecosystem such as the healthcare industry. It serves as a reminder of why it is important for healthcare technologists to consider proactive ways to contribute to the innovation ecosystem by creating devices with the platform perspective in mind.',\n",
       " 'f4968d7a9a330edf09f95c45170ad67d2f340fc7': 'Sensors in a data fusion environment over hostile territory are geographically dispersed and change location with time. In order to collect and process data from these sensors an equally flexible network of fusion beds (i.e., clusterheads) is required. To account for the hostile environment, we allow communication links between sensors and clusterheads to be unreliable. We develop a mixed integer linear programming (MILP) model to determine the clusterhead location strategy that maximizes the expected data covered minus the clusterhead reassignments, over a time horizon. A column generation (CG) heuristic is developed for this problem. Computational results show that CG performs much faster than a standard commercial solver and the typical optimality gap for large problems is less than 5%. Improvements to the basic model in the areas of modeling link failure, consideration of bandwidth capacity, and clusterhead changeover cost estimation are also discussed.',\n",
       " 'e5a837696c7a5d329a6a540a0d2de912a010ac47': 'This document is a complete draft of a chapter by Rajeev Gor e on \\\\Tableau Methods for Modal and Temporal Logics\" which is part of the \\\\Handbook of Tableau Methods\", edited',\n",
       " 'f0fcb064acefae20d0a7293a868c1a9822f388bb': 'We present a camera lens simulation model capable of producing advanced photographic phenomena in a general spectral Monte Carlo image rendering system. Our approach incorporates insights from geometrical diffraction theory, from optical engineering and from glass science. We show how to efficiently simulate all five monochromatic aberrations, spherical and coma aberration, astigmatism, field curvature and distortion. We also consider chromatic aberration, lateral colour and aperture diffraction. The inclusion of Fresnel reflection generates correct lens flares and we present an optimized sampling method for path generation.',\n",
       " '412b93429b7b7b307cc64702cdfa91630210a52e': 'In this paper we present a polynomial time technology mapping algorithm, called Flow-Map, that optimally solves the LUT-based FPGA technology mapping problem for depth minimization for general Boolean networks. This theoretical breakthrough makes a sharp contrast with the fact that conventional technology mapping problem in library-based designs is NP-hard. A key step in Flow-Map is to compute a minimum height K-feasible cut in a network, solved by network flow computation. Our algorithm also effectively minimizes the number of LUTs by maximizing the volume of each cut and by several postprocessing operations. We tested the Flow-Map algorithm on a set of benchmarks and achieved reductions on both the network depth and the number of LUTs in mapping solutions as compared with previous algorithms.',\n",
       " '10727ad1dacea19dc813e50b014ca08e27dcd065': 'Mobile devices provide a continuous data stream of contextual behavioral information which can be leveraged for a variety of user services, such as in personalizing ads and customizing home screens. In this paper, we aim to better understand gender-related behavioral patterns found in application, Bluetooth, and Wi-Fi usage. Using a dataset which consists of approximately 19 months of data collected from 189 subjects, gender classification is performed using 1,000 features related to the frequency of events yielding up to 91.8% accuracy. Then, we present a behavioral analysis of application traffic using techniques commonly used for web browsing activity as an alternative data exploration approach. Finally, we conclude with a discussion on impersonation attacks, where we aim to determine if one gender is less vulnerable to unauthorized access on their mobile device.',\n",
       " 'e275f643c97ca1f4c7715635bb72cf02df928d06': '',\n",
       " '5f85b28af230828675997aea48e939576996e16e': 'The use of educational games in learning environments is an increasingly relevant trend. The motivational and immersive traits of game-based learning have been deeply studied in the literature, but the systematic design and implementation of educational games remain an elusive topic. In this study some relevant requirements for the design of educational games in online education are analyzed, and a general game design method that includes adaptation and assessment features is proposed. Finally, a particular implementation of that design is described in light of its applicability to other implementations and environments. 2008 Elsevier Ltd. All rights reserved.',\n",
       " '328bfd1d0229bc4973277f893abd1eb288159fc9': 'This paper is a summary of findings of adult age-related craniofacial morphological changes. Our aims are two-fold: (1) through a review of the literature we address the factors influencing craniofacial aging, and (2) the general ways in which a head and face age in adulthood. We present findings on environmental and innate influences on face aging, facial soft tissue age changes, and bony changes in the craniofacial and dentoalveolar skeleton. We then briefly address the relevance of this information to forensic science research and applications, such as the development of computer facial age-progression and face recognition technologies, and contributions to forensic sketch artistry.',\n",
       " '7ecaca8db190608dc4482999e19b1593cc6ad4e5': 'Concrete Figure 5 Features of the body of a domain: (A) patterns and (B) logic Language Paradigm Value Specification Element Creation Implicit Explicit Logic Constraint Object-Oriented Functional Procedural Logic Value Binding Imperative Assignment IBM SYSTEMS JOURNAL, VOL 45, NO 3, 2006 CZARNECKI AND HELSEN 629 operating on one model from the parts operating on other models. For example, classical rewrite rules have an LHS operating on the source model and an RHS operating on the target model. In other approaches, such as a rule implemented as a Java program, there might not be any such syntactic distinction. Multidirectionality Multidirectionality refers to the ability to execute a rule in different directions (see Figure 4A). Rules supporting multidirectionality are usually defined over in/out-domains. Multidirectional rules are available in MTF and QVT Relations. Application condition Transformation rules in some approaches may have an application condition (see Figure 4A) that must be true in order for the rule to be executed. An example is the when-clause in QVT Relations (Example 1). Intermediate structure The execution of a rule may require the creation of some additional intermediate structures (see Figure 4A) which are not part of the models being transformed. These structures are often temporary and require their own metamodel. A particular example of intermediate structures are traceability links. In contrast to other intermediate structures, traceability links are usually persisted. Even if traceability links are not persisted, some approaches, such as AGG and VIATRA, rely on them to prevent multiple ‘‘firings’’ of a rule for the same input element. Parameterization The simplest kind of parameterization is the use of control parameters that allow passing values as control flags (Figure 7). Control parameters are useful for implementing policies. For example, a transformation from class models to relational schemas could have a control parameter specifying which of the alternative patterns of object-relational mapping should be used in a given execution. 7 Generics allow passing data types, including model element types, as parameters. Generics can help make transformation rules more reusable. Generic transformations have been described by Varró and Pataricza. 17 Finally, higher-order rules take other rules as parameters and may provide even higher levels of reuse and abstraction. Stratego 64 is an example of a term rewriting language for program transformation supporting higher-order rules. We are currently not aware of any model transformation approaches with a first class support for higherorder rules. Reflection and aspects Some authors advocate the support for reflection and aspects (Figure 4) in transformation languages. Reflection is supported in ATL by allowing reflective access to transformation rules during the execution of transformations. An aspect-oriented extension of MTL was proposed by Silaghi et al. 65 Reflection and aspects can be used to express concerns that crosscut several rules, such as custom traceability management policies. 66 Rule application control: Location determination A rule needs to be applied to a specific location within its source scope. As there may be more than one match for a rule within a given source scope, we need a strategy for determining the application locations (Figure 8A). The strategy could be deterministic, nondeterministic, or interactive. For example, a deterministic strategy could exploit some standard traversal strategy (such as depth first) over the containment hierarchy in the source. Stratego 64 is an example of a term rewriting language with a rich mechanism for expressing traversal in tree structures. Examples of nondeterministic strategies include one-point application, where a rule is applied to one nondeterministically selected location, and concurrent application, where one rule is Figure 6 Typing Untyped Syntactically Typed Semantically Typed Typing Figure 7 Parameterization Control Parameters Generics Higher-Order Rules Parameterization CZARNECKI AND HELSEN IBM SYSTEMS JOURNAL, VOL 45, NO 3, 2006 630 Figure 8 Model transformation approach features: (A) location determination, (B) rule scheduling, (C) rule organization, (D) source-target relationship, (E) incrementality, (F) directionality, and (G) tracing Concurrent One-Point Non-Deterministic Deterministic Interactive Rule Application Strategy A',\n",
       " '4f0a21f94152f68f102d90c6f63f2eb8638eacc6': 'OBJECTIVES\\nTo critically review the currently available evidence of studies comparing robotic partial nephrectomy (RPN) and open partial nephrectomy (OPN).\\n\\n\\nMATERIALS AND METHODS\\nA comprehensive review of the literature from Pubmed, Web of Science and Scopus was performed in October 2013. All relevant studies comparing RPN with OPN were included for further screening. A cumulative meta-analysis of all comparative studies was performed and publication bias was assessed by a funnel plot.\\n\\n\\nRESULTS\\nEight studies were included for the analysis, including a total of 3418 patients (757 patients in the robotic group and 2661 patients in the open group). Although RPN procedures had a longer operative time (weighted mean difference [WMD]: 40.89; 95% confidence interval [CI], 14.39-67.40; p\\u200a=\\u200a0.002), patients in this group benefited from a lower perioperative complication rate (19.3% for RPN and 29.5% for OPN; odds ratio [OR]: 0.53; 95%CI, 0.42-0.67; p<0.00001), shorter hospital stay (WMD: -2.78; 95%CI, -3.36 to -1.92; p<0.00001), less estimated blood loss(WMD: -106.83; 95%CI, -176.4 to -37.27; p\\u200a=\\u200a0.003). Transfusions, conversion to radical nephrectomy, ischemia time and estimated GFR change, margin status, and overall cost were comparable between the two techniques. The main limitation of the present meta-analysis is the non-randomization of all included studies.\\n\\n\\nCONCLUSIONS\\nRPN appears to be an efficient alternative to OPN with the advantages of a lower rate of perioperative complications, shorter length of hospital stay and less blood loss. Nevertheless, high quality prospective randomized studies with longer follow-up period are needed to confirm these findings.',\n",
       " 'd464735cc3f8cb515a96f1ac346d42e8d7a28671': 'This paper presents a digital nonlinearity calibration technique for ADCs with strong input–output discontinuities between adjacent codes, such as pipeline, algorithmic, and SAR ADCs with redundancy. In this kind of converter, the ADC transfer function often involves multivalued regions, where conventional integral-nonlinearity (INL)-based calibration methods tend to miscalibrate, negatively affecting the ADC’s performance. As a solution to this problem, this paper proposes a novel INL-based calibration which incorporates information from the ADC’s internal signals to provide a robust estimation of static nonlinear errors for multivalued ADCs. The method is fully generalizable and can be applied to any existing design as long as there is access to internal digital signals. In pipeline or subranging ADCs, this implies access to partial subcodes before digital correction; for algorithmic or SAR ADCs, conversion bit/bits per cycle are used. As a proof-of-concept demonstrator, the experimental results for a 1.2 V 23 mW 130 nm-CMOS pipeline ADC with a SINAD of 58.4 dBc (in nominal conditions without calibration) is considered. In a stressed situation with 0.95 V of supply, the ADC has SINAD values of 47.8 dBc and 56.1 dBc, respectively, before and after calibration (total power consumption, including the calibration logic, being 15.4 mW).',\n",
       " 'b1be4ea2ce9edcdcb4455643d0d21094f4f0a772': 'We present WebSelF, a framework for web scraping which models the process of web scraping and decomposes it into four conceptually independent, reusable, and composable constituents. We have validated our framework through a full parameterized implementation that is flexible enough to capture previous work on web scraping. We conducted an experiment that evaluated several qualitatively different web scraping constituents (including previous work and combinations hereof) on about 11,000 HTML pages on daily versions of 17 web sites over a period of more than one year. Our framework solves three concrete problems with current web scraping and our experimental results indicate that composition of previous and our new techniques achieve a higher degree of accuracy, precision and specificity than existing tech-',\n",
       " 'a7e2814ec5db800d2f8c4313fd436e9cf8273821': 'The k-Nearest-Neighbours (kNN) is a simple but effective method for classification. The major drawbacks with respect to kNN are (1) its low efficiency being a lazy learning method prohibits it in many applications such as dynamic web mining for a large repository, and (2) its dependency on the selection of a “good value” for k. In this paper, we propose a novel kNN type method for classification that is aimed at overcoming these shortcomings. Our method constructs a kNN model for the data, which replaces the data to serve as the basis of classification. The value of k is automatically determined, is varied for different data, and is optimal in terms of classification accuracy. The construction of the model reduces the dependency on k and makes classification faster. Experiments were carried out on some public datasets collected from the UCI machine learning repository in order to test our method. The experimental results show that the kNN based model compares well with C5.0 and kNN in terms of classification accuracy, but is more efficient than the standard kNN.',\n",
       " '0fafcc5fc916ac93e73f3a708f4023720bbe2faf': 'This research work proposes a new method for garment retexturing using a single static image along with depth information obtained using the Microsoft Kinect 2 camera. First the garment is segmented out from the image and texture domain coordinates are computed for each pixel of the shirt using 3D information. After that shading is applied on the new colours from the texture image by applying linear stretching of the luminance of the segmented garment. The proposed method is colour and pattern invariant and results in to visually realistic retexturing. The proposed method has been tested on various images and it is shown that it generally performs better and produces more realistic results compared to the state-of-the-art methods. The proposed method can be an application for the virtual fitting room.',\n",
       " 'b323c4d8f284dd27b9bc8c8be5bee3cd30e2c8ca': 'Since the nurse scheduling problem (NSP) is a problem of finding a feasible solution, the solution space must include infeasible solutions to solve it using a local search algorithm. However, the solution space consisting of all the solutions is so large that the search requires much CPU time. In the NSP, some constraints have higher priority. Thus, we can define the solution space to be the set of solutions satisfying some of the important constraints, which are called the elementary constraints. The connectivity of the solution space is also important for the performance. However, the connectivity is not obvious when the solution space consists only of solutions satisfying the elementary constraints and is composed of small neighborhoods. This paper gives theoretical support for using 4-opt-type neighborhood operations by discussing the connectivity of its solution space and the size of the neighborhood. Another interesting point in our model is a special case of the NSP corresponds to the bipartite transportation problem, and our result also applies to it.',\n",
       " '00dbf46a7a4ba6222ac5d44c1a8c09f261e5693c': 'The massive parallelism of graphics processing units (GPUs) offers tremendous performance in many high-performance computing applications. While dense linear algebra readily maps to such platforms, harnessing this potential for sparse matrix computations presents additional challenges. Given its role in iterative methods for solving sparse linear systems and eigenvalue problems, sparse matrix-vector multiplication (SpMV) is of singular importance in sparse linear algebra. In this paper we discuss data structures and algorithms for SpMV that are efficiently implemented on the CUDA platform for the fine-grained parallel architecture of the GPU. Given the memory-bound nature of SpMV, we emphasize memory bandwidth efficiency and compact storage formats. We consider a broad spectrum of sparse matrices, from those that are well-structured and regular to highly irregular matrices with large imbalances in the distribution of nonzeros per matrix row. We develop methods to exploit several common forms of matrix structure while offering alternatives which accommodate greater irregularity. On structured, grid-based matrices we achieve performance of 36 GFLOP/s in single precision and 16 GFLOP/s in double precision on a GeForce GTX 280 GPU. For unstructured finite-element matrices, we observe performance in excess of 15 GFLOP/s and 10 GFLOP/s in single and double precision respectively. These results compare favorably to prior state-of-the-art studies of SpMV methods on conventional multicore processors. Our double precision SpMV performance is generally two and a half times that of a Cell BE with 8 SPEs and more than ten times greater than that of a quad-core Intel Clovertown system.',\n",
       " '0cab85c646bc4572101044cb22d944e3685732b5': 'This paper presents novel high-speed architectures for the hardware implementation of the Advanced Encryption Standard (AES) algorithm. Unlike previous works which rely on look-up tables to implement the SubBytes and InvSubBytes transformations of the AES algorithm, the proposed design employs combinational logic only. As a direct consequence, the unbreakable delay incurred by look-up tables in the conventional approaches is eliminated, and the advantage of subpipelining can be further explored. Furthermore, composite field arithmetic is employed to reduce the area requirements, and different implementations for the inversion in subfield GF(2/sup 4/) are compared. In addition, an efficient key expansion architecture suitable for the subpipelined round units is also presented. Using the proposed architecture, a fully subpipelined encryptor with 7 substages in each round unit can achieve a throughput of 21.56 Gbps on a Xilinx XCV1000 e-8 bg560 device in non-feedback modes, which is faster and is 79% more efficient in terms of equivalent throughput/slice than the fastest previous FPGA implementation known to date.',\n",
       " '29e65e682764c6dcc33cdefd8150521893fc2c94': \"We take a qualitative approach to understanding deaf and hard of hearing (DHH) students' experiences with real-time captioning as an access technology in mainstream university classrooms. We consider both existing human-based captioning as well as new machine-based solutions that use automatic speech recognition (ASR). We employed a variety of qualitative research methods to gather data about students' captioning experiences including in-class observations, interviews, diary studies, and usability evaluations. We also conducted a co-design workshop with 8 stakeholders after our initial research findings. Our results show that accuracy and reliability of the technology are still the most important issues across captioning solutions. However, we additionally found that current captioning solutions tend to limit students' autonomy in the classroom and present a variety of user experience shortcomings, such as complex setups, poor feedback and limited control over caption presentation. Based on these findings, we propose design requirements and recommend features for real-time captioning in mainstream classrooms.\",\n",
       " '704885bae7e9c5a37528be854b8bd2f24d1e641c': 'Personalization agents are incorporated in many websites to tailor content and interfaces for individual users. But in contrast to the proliferation of personalized web services worldwide, empirical research studying the effects of web personalization is scant. How does exposure to personalized offers affect subsequent product consideration and choice outcome? Drawing on the literature in HCI and consumer behavior, this research examines the effects of web personalization on users’ information processing and expectations through different decision-making stages. A study using a personalized ring-tone download website was conducted. Our findings provide empirical evidence of the effects of web personalization. In particular, when consumers are forming their consideration sets, the agents have the capacity to help them discover new products and generate demand for unfamiliar products. Once the users have arrived at their choice, however, the persuasive effects from the personalization agent diminish. These results establish that adaptive role of personalization agents at different stages of decision-making.',\n",
       " '6e91e18f89027bb01c791ce54b2f7a1fc8d8ce9c': 'BACKGROUND\\nPacific Kids DASH for Health (PacDASH) aimed to improve child diet and physical activity (PA) level and prevent excess weight gain and elevation in blood pressure (BP) at 9 months.\\n\\n\\nMETHODS\\nPacDASH was a two-arm, randomized, controlled trial (ClinicalTrials.gov: NCT00905411). Eighty-five 5- to 8-year-olds in the 50th-99th percentile for BMI were randomly assigned to treatment (n=41) or control (n=44) groups; 62 completed the 9-month trial. Sixty-two percent were female. Mean age was 7.1±0.95 years. Race/ethnicity was Asian (44%), Native Hawaiian or Other Pacific Islander (28%), white (21%), or other race/ethnicity (7%). Intervention was provided at baseline and 3, 6 and 9 months, with monthly supportive mailings between intervention visits, and a follow-up visit at 15 months to observe maintenance. Diet and PA were assessed by 2-day log. Body size, composition, and BP were measured. The intervention effect on diet and PA, body size and composition, and BP by the end of the intervention was tested using an F test from a mixed regression model, after adjustment for sex, age, and ethnic group.\\n\\n\\nRESULTS\\nFruit and vegetable (FV) intake decreased less in the treatment than control group (p=0.04). Diastolic BP (DBP) was 12 percentile units lower in the treatment than control group after 9 months of intervention (p=0.01). There were no group differences in systolic BP (SBP) or body size/composition.\\n\\n\\nCONCLUSIONS\\nThe PacDASH trial enhanced FV intake and DBP, but not SBP or body size/composition.',\n",
       " '18d5fe1bd87db26b7037d81f4170cf4ebe320e00': 'Due to the lack of an appropriate symmetry in the acquisition geometry, general bistatic synthetic aperture radar (SAR) cannot benefit from the two main properties of low-to-moderate resolution monostatic SAR: azimuth-invariance and topography-insensitivity. The precise accommodation of azimuth-variance and topography is a real challenge for efficent image formation algorithms working in the Fourier domain, but can be quite naturally handled by time-domain approaches. We present an efficient and practical implementation of a generalised bistatic SAR image formation algorithm with an accurate accommodation of these two effects. The algorithm has a common structure with the monostatic fast-factorised backprojection (FFBP), and is therefore based on subaperture processing. The images computed over the different subapertures are displayed in an advantageous elliptical coordinate system capable of incorporating the topographic information of the imaged scene in an analogous manner as topography-dependent monostatic SAR algorithms do. Analytical expressions for the Nyquist requirements using this coordinate system are derived. The overall discussion includes practical implementation hints and a realistic computational burden estimation. The algorithm is tested with both simulated and actual bistatic SAR data. The actual data correspond to the spaceborne-airborne experiment between TerraSAR-X and F-SAR performed in 2007 and to the DLR-ONERA airborne experiment carried out in 2003. The presented approach proves its suitability for the precise SAR focussing of the data acquired in general bistatic configurations.',\n",
       " '2f585530aea2968d799a9109aeb202b03ba977f4': 'Machine comprehension plays an essential role in NLP and has been widely explored with dataset like MCTest. However, this dataset is too simple and too small for learning true reasoning abilities. (Hermann et al., 2015) therefore release a large scale news article dataset and propose a deep LSTM reader system for machine comprehension. However, the training process is expensive. We therefore try feature-engineered approach with semantics on the new dataset to see how traditional machine learning technique and semantics can help with machine comprehension. Meanwhile, our proposed L2R reader system achieves good performance with efficiency and less training data.',\n",
       " '5dd2b359fa6a0f7ebe3ca123df01b852bef08421': 'The large-scale mobile underwater wireless sensor network (UWSN) is a novel networking paradigm to explore aqueous environments. However, the characteristics of mobile UWSNs, such as low communication bandwidth, large propagation delay, floating node mobility, and high error probability, are significantly different from ground-based wireless sensor networks. The novel networking paradigm poses interdisciplinary challenges that will require new technological solutions. In particular, in this article we adopt a top-down approach to explore the research challenges in mobile UWSN design. Along the layered protocol stack, we proceed roughly from the top application layer to the bottom physical layer. At each layer, a set of new design intricacies is studied. The conclusion is that building scalable mobile UWSNs is a challenge that must be answered by interdisciplinary efforts of acoustic communications, signal processing, and mobile acoustic network protocol design.',\n",
       " '0435ef56d5ba8461021cf9e5d811a014ed4e98ac': 'We study the problem of media access control in the novel regime of sensor networks, where unique application behavior and tight constraints in computation power, storage, energy resources, and radio technology have shaped this design space to be very different from that found in traditional mobile computing regime. Media access control in sensor networks must not only be energy efficient but should also allow fair bandwidth allocation to the infrastructure for all nodes in a multihop network. We propose an adaptive rate control mechanism aiming to support these two goals and find that such a scheme is most effective in achieving our fairness goal while being energy efficient for both low and high duty cycle of network traffic.',\n",
       " '11897106a75d5ba3bf5fb4677943bd6f33daadf7': 'Ocean bottom sensor nodes can be used for oceanographic data collection, pollution monitoring, offshore exploration and tactical surveillance applications. Moreover, Unmanned or Autonomous Underwater Vehicles (UUVs, AUVs), equipped with sensors, will find application in exploration of natural undersea resources and gathering of scientific data in collaborative monitoring missions. Underwater acoustic networking is the enabling technology for these applications. Underwater Networks consist of a variable number of sensors and vehicles that are deployed to perform collaborative monitoring tasks over a given area.In this paper, several fundamental key aspects of underwater acoustic communications are investigated. Different architectures for two-dimensional and three-dimensional underwater sensor networks are discussed, and the underwater channel is characterized. The main challenges for the development of efficient networking solutions posed by the underwater environment are detailed at all layers of the protocol stack. Furthermore, open research issues are discussed and possible solution approaches are outlined.',\n",
       " '1e7e4d2b2be08c01f1347dee53e99473d2a9e83d': 'The micro-modem is a compact, low-power, underwater acoustic communications and navigation subsystem. It has the capability to perform low-rate frequency-hopping frequency-shift keying (FH-FSK), variable rate phase-coherent keying (PSK), and two different types of long base line navigation, narrow-band and broadband. The system can be configured to transmit in four different bands from 3 to 30 kHz, with a larger board required for the lowest frequency. The user interface is based on the NMEA standard, which is a serial port specification. The modem also includes a simple built-in networking capability which supports up to 16 units in a polled or random-access mode and has an acknowledgement capability which supports guaranteed delivery transactions. The paper contains a detailed system description and results from several tests are also presented',\n",
       " '93074961701ca6983a6c0fdc2964d11e282b8bae': 'Progress in underwater acoustic telemetry since 1982 is reviewed within a framework of six current research areas: (1) underwater channel physics, channel simulations, and measurements; (2) receiver structures; (3) diversity exploitation; (4) error control coding; (5) networked systems; and (6) alternative modulation strategies. Advances in each of these areas as well as perspectives on the future challenges facing them are presented. A primary thesis of this paper is that increased integration of high-fidelity channel models into ongoing underwater telemetry research is needed if the performance envelope (defined in terms of range, rate, and channel complexity) of underwater modems is to expand.',\n",
       " '5e296b53252db11667011b55e78bd4a67cd3e547': 'This paper empirically studies consumer choice behavior in regard to store brands in the US, UK and Spain. Store brand market shares differ by country and they are usually much higher in Europe than in the US. However, there is surprisingly little work in marketing that empirically studies the reasons that underlie higher market shares associated with store brands in Europe over the US. In this paper, we empirically study the notion that the differential success of store brands in the US versus in Europe is the higher brand equity that store brands command in Europe over the US. We use a framework based on previous work to conduct our analysis: consumer brand choice under uncertainty, and brands as signals of product positions. More specifically, we examine whether uncertainty about quality (or, the positioning of the brand in the product space), perceived quality of store brands versus national brands, consistency in store brand offerings over time, as well as consumer attitudes towards risk, quality and price underlie the differential success of store brands at least partially in the US versus Europe. We propose and estimate a model that explicitly incorporates the impact of uncertainty on consumer behavior. We compare 1) levels of uncertainty associated with store brands versus national brands, 2) consistency in product positions over time for both national and store brands, 3) relative quality levels of national versus store brands, and 4) consumer sensitivity to price, quality and risk across the three countries we study. The model is estimated on scanner panel data on detergent in the US, UK and Spanish markets, and on toilet paper and margarine in the US and Spain. We find that consumer learning and perceived risk (and the associated brand equity), as well as consumer attitude towards risk, quality and price, play an important role in consumers’ store versus national brand choices and contribute to the differences in relative success of store brands across the countries we study.',\n",
       " 'f5dbb5a0aeb5823d704077b372d0dd989d4ebe1c': 'Wireless sensor networks (WSNs) have attracted much attention in recent years for their unique characteristics and wide use in many different applications. Especially, in military networks, all sensor motes are deployed randomly and densely. How can we optimize the number of deployed nodes (sensor node and sink) with a QoS guarantee (minimal end-to-end delay and packet drop)? In this paper, using the M/M/1 queuing model we propose a deployment optimization model for non-beacon-mode 802.15.4 sensor networks. The simulation results show that the proposed model is a promising approach for deploying the sensor network.',\n",
       " '1fbb66a9407470e1da332c4ef69cdc34e169a3d7': 'Deep learning is bringing breakthroughs to many computer vision subfields including Optical Music Recognition (OMR), which has seen a series of improvements to musical symbol detection achieved by using generic deep learning models. However, so far, each such proposal has been based on a specific dataset and different evaluation criteria, which made it difficult to quantify the new deep learning-based state-of-the-art and assess the relative merits of these detection models on music scores. In this paper, a baseline for general detection of musical symbols with deep learning is presented. We consider three datasets of heterogeneous typology but with the same annotation format, three neural models of different nature, and establish their performance in terms of a common evaluation standard. The experimental results confirm that the direct music object detection with deep learning is indeed promising, but at the same time illustrates some of the domain-specific shortcomings of the general detectors. A qualitative comparison then suggests avenues for OMR improvement, based both on properties of the detection model and how the datasets are defined. To the best of our knowledge, this is the first time that competing music object detection systems from the machine learning paradigm are directly compared to each other. We hope that this work will serve as a reference to measure the progress of future developments of OMR in music object detection.',\n",
       " '4b07c5cda3dab4e48bd39bee91c5eba2642037b2': 'This paper presents a measurement system of cathodic currents for copper electrorefining processes using multicircuital technology with optibar intercell bars. The proposed system is based on current estimation using 55 magnetic field sensors per intercell bar. Current values are sampled and stored every 5 min for seven days in a compatible SQL database. The method does not affect the normal operation of the process and does not require any structural modifications. The system for online measurement of 40 cells involving 2090 sensors is in operation in an electrorefinery site.',\n",
       " 'a2c1cf2de9e822c2d662d520083832853ac39f9d': 'In this paper, the analog front end (AFE) for an inductive position sensor in an automotive electromagnetic resonance gear control applications is presented. To improve the position detection accuracy, a coil driver with an automatic two-step impedance calibration is proposed which, despite the load variation, provides the desired driving capability by controlling the main driver size. Also, a time shared analog-to-digital converter (ADC) is proposed to convert eight-phase signals while reducing the current consumption and area to 1/8 of the conventional structure. A relaxation oscillator with temperature compensation is proposed to generate a constant clock frequency in vehicle temperature conditions. This chip is fabricated using a 0.18-<inline-formula> <tex-math notation=\"LaTeX\">$\\\\mu \\\\text{m}$ </tex-math></inline-formula> CMOS process and the die area is 2 mm <inline-formula> <tex-math notation=\"LaTeX\">$\\\\times 1.5$ </tex-math></inline-formula> mm. The power consumption of the AFE is 23.1 mW from the supply voltage of 3.3 V to drive one transmitter (Tx) coil and eight receiver (Rx) coils. The measured position detection accuracy is greater than 99.8 %. The measurement of the Tx shows a driving capability higher than 35 mA with respect to the load change.',\n",
       " '2cfac4e999538728a8e11e2b7784433e80ca6a38': 'We have determined how most of the transcriptional regulators encoded in the eukaryote Saccharomyces cerevisiae associate with genes across the genome in living cells. Just as maps of metabolic networks describe the potential pathways that may be used by a cell to accomplish metabolic processes, this network of regulator-gene interactions describes potential pathways yeast cells can use to regulate global gene expression programs. We use this information to identify network motifs, the simplest units of network architecture, and demonstrate that an automated process can use motifs to assemble a transcriptional regulatory network structure. Our results reveal that eukaryotic cellular functions are highly connected through networks of transcriptional regulators that regulate other transcriptional regulators.',\n",
       " '526ece284aacc3ab8e3d4e839a9512dbbd27867b': \"Labeling large datasets has become faster, cheaper, and easier with the advent of crowdsourcing services like Amazon Mechanical Turk. How can one trust the labels obtained from such services? We propose a model of the labeling process which includes label uncertainty, as well a multi-dimensional measure of the annotators' ability. From the model we derive an online algorithm that estimates the most likely value of the labels and the annotator abilities. It finds and prioritizes experts when requesting labels, and actively excludes unreliable annotators. Based on labels already obtained, it dynamically chooses which images will be labeled next, and how many labels to request in order to achieve a desired level of confidence. Our algorithm is general and can handle binary, multi-valued, and continuous annotations (e.g. bounding boxes). Experiments on a dataset containing more than 50,000 labels show that our algorithm reduces the number of labels required, and thus the total cost of labeling, by a large factor while keeping error rates low on a variety of datasets.\",\n",
       " '55b4b9b303da1b0e8f938a80636ec95f7af235fd': 'This paper describes our submission to the 2017 BioASQ challenge. We participated in Task B, Phase B which is concerned with biomedical question answering (QA). We focus on factoid and list question, using an extractive QA model, that is, we restrict our system to output substrings of the provided text snippets. At the core of our system, we use FastQA, a state-ofthe-art neural QA system. We extended it with biomedical word embeddings and changed its answer layer to be able to answer list questions in addition to factoid questions. We pre-trained the model on a large-scale open-domain QA dataset, SQuAD, and then fine-tuned the parameters on the BioASQ training set. With our approach, we achieve state-of-the-art results on factoid questions and competitive results on list questions.',\n",
       " '6653956ab027cadb035673055c91ce1c7767e140': 'A class of the novel compact-size branch-line couplers using the quasi-lumped elements approach with symmetrical or nonsymmetrical T-shaped structures is proposed in this paper. The design equations have been derived, and two circuits using the quasi-lumped elements approach were realized for physical measurements. This novel design occupies only 29% of the area of the conventional approach at 2.4 GHz. In addition, a third circuit was designed by using the same formula implementing a symmetrical T-shaped structure and occupied both the internal and external area of the coupler. This coupler achieved 500-MHz bandwidth while the phase difference between S21 and S31 is within 90degplusmn1deg. Thus, the bandwidth is not only 25% wider than that of the conventional coupler, but occupies only 70% of the circuit area compared to the conventional design. All three proposed couplers can be implemented by using the standard printed-circuit-board etching processes without any implementation of lumped elements, bonding wires, and via-holes, making it very useful for wireless communication systems',\n",
       " 'fa27e993f88c12ef3cf5bbc6eb3b0b4f9de15e86': 'There have been few attempts to bring evolutionary theory to the study of human motivation. From this perspective motives can be considered psychological mechanisms to produce behavior that solves evolutionarily important tasks in the human niche. From the dimensions of the human niche we deduce eight human needs: optimize the number and survival of gene copies; maintain bodily integrity; avoid external threats; optimize sexual, environmental, and social capital; and acquire reproductive and survival skills. These needs then serve as the foundation for a necessary and sufficient list of 15 human motives, which we label: lust, hunger, comfort, fear, disgust, attract, love, nurture, create, hoard, affiliate, status, justice, curiosity, and play. We show that these motives are consistent with evidence from the current literature. This approach provides us with a precise vocabulary for talking about motivation, the lack of which has hampered progress in behavioral science. Developing testable theories about the structure and function of motives is essential to the project of understanding the organization of animal cognition and learning, as well as for the applied behavioral sciences.',\n",
       " '6de09e48d42d14c4079e5d8a6a58485341b41cad': 'The purpose of this article is to examine students’ views on the blended learning method and its use in relation to the students’ individual learning style. The study was conducted with 31 senior students. Web based media together with face to face classroom settings were used in the blended learning framework. A scale of Students’ Views on Blended Learning and its implementation, Kolb’s Learning Style Inventory, Pre-Information Form and open ended questions were used to gather data. The majority of the students’ fell into assimilators, accommodators and convergers learning styles. Results revealed that students’ views on blended learning method and its use are quite positive.',\n",
       " 'b4d4a78ecc68fd8fe9235864e0b1878cb9e9f84b': 'An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. This has two important consequences:Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intended recipient. Only he can decipher the message, since only he knows the corresponding decryption key.\\nA message can be “signed” using a privately held decryption key. Anyone can verify this signature using the corresponding publicly revealed encryption key. Signatures cannot be forged, and a signer cannot later deny the validity of his signature. This has obvious applications in “electronic mail” and “electronic funds transfer” systems. A message is encrypted by representing it as a number M, raising M to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product, n, of two large secret prime numbers p and q. Decryption is similar; only a different, secret, power d is used, where e * d = 1(mod (p - 1) * (q - 1)). The security of the system rests in part on the difficulty of factoring the published divisor, n.',\n",
       " '628c2bcfbd6b604e2d154c7756840d3a5907470f': 'Internet of Things (IoT) are being adopted for industrial and manufacturing applications such as manufacturing automation, remote machine diagnostics, prognostic health management of industrial machines and supply chain management. CloudBased Manufacturing is a recent on-demand model of manufacturing that is leveraging IoT technologies. While Cloud-Based Manufacturing enables on-demand access to manufacturing resources, a trusted intermediary is required for transactions between the users who wish to avail manufacturing services. We present a decentralized, peer-to-peer platform called BPIIoT for Industrial Internet of Things based on the Block chain technology. With the use of Blockchain technology, the BPIIoT platform enables peers in a decentralized, trustless, peer-to-peer network to interact with each other without the need for a trusted intermediary.',\n",
       " '0fd0e3854ee696148e978ec33d5c042554cd4d23': 'This document provides additional details about the experiments described in (Heilman and Smith, 2010). Note that while this document provides information about the datasets and experimental methods, it does not provide further results. If you have any further questions, please feel free to contact the first author. The preprocessed datasets (i.e., tagged and parsed) will be made available for research purposes upon request.',\n",
       " '3e27e210c9a41d9901164fd4c24e549616d1958a': 'It is often the case that the performance of a neural network can be improved by adding layers. In real-world practices, we always train dozens of neural network architectures in parallel which is a wasteful process. We explored CompNet, in which case we morph a well-trained neural network to a deeper one where network function can be preserved and the added layer is compact. The work of the paper makes two contributions: a). The modified network can converge fast and keep the same functionality so that we do not need to train from scratch again; b). The layer size of the added layer in the neural network is controlled by removing the redundant parameters with sparse optimization. This differs from previous network morphism approaches which tend to add more neurons or channels beyond the actual requirements and result in redundance of the model. The method is illustrated using several neural network structures on different data sets including MNIST and CIFAR10.',\n",
       " 'c11de783900e118e3d3de74efca5435c98b11e7c': 'Sleepiness or fatigue in drivers driving for long hours is the major cause of accidents on highways worldwide. The International statistics shows that a large number of road accidents are caused by driver fatigue. Therefore, a system that can detect oncoming driver fatigue and issue timely warning could help in preventing many accidents, and consequently save money and reduce personal suffering. The authors have made an attempt to design a system that uses video camera that points directly towards the driver‟s face in order to detect fatigue. If the fatigue is detected a warning signal is issued to alert the driver. The authors have worked on the video files recorded by the camera. Video file is converted into frames.Once the eyes are located from each frame, by determining the energy value of each frame one can determine whether the eyes are open or close. A particular condition is set for the energy values of open and close eyes. If the average of the energy value for 5 consecutive frames falls in a given condition then the driver will be detected as drowsy and issues a warning signal. The algorithm is proposed, implemented, tested, and found working satisfactorily.',\n",
       " '1e55bb7c095d3ea15bccb3df920c546ec54c86b5': '',\n",
       " '23a3dc2af47b13fe63189df63dbdda068b854cdd': 'We define distances between geometric curves by the square root of the minimal energy required to transform one curve into the other. The energy is formally defined from a left invariant Riemannian distance on an infinite dimensional group acting on the curves, which can be explicitly computed. The obtained distance boils down to a variational problem for which an optimal matching between the curves has to be computed. An analysis of the distance when the curves are polygonal leads to a numerical procedure for the solution of the variational problem, which can efficiently be implemented, as illustrated by experiments.',\n",
       " '37726c30352bd235c2a832b6c16633c2b11b8913': 'Locally linear embedding (LLE) is a recently proposed method for unsupervised nonlinear dimensionality reduction. It has a number of attractive features: it does not require an iterative algorithm, and just a few parameters need to be set. Two extensions of LLE to supervised feature extraction were independently proposed by the authors of this paper. Here, both methods are unified in a common framework and applied to a number of benchmark data sets. Results show that they perform very well on high-dimensional data which exhibits a manifold structure.',\n",
       " '8213dc79a49f6bd8e1f396e66db1d8503d85f566': 'Differential privacy has recently become a de facto standard for private statistical data release. Many algorithms have been proposed to generate differentially private histograms or synthetic data. However, most of them focus on \"one-time\" release of a static dataset and do not adequately address the increasing need of releasing series of dynamic datasets in real time. A straightforward application of existing histogram methods on each snapshot of such dynamic datasets will incur high accumulated error due to the composibility of differential privacy and correlations or overlapping users between the snapshots. In this paper, we address the problem of releasing series of dynamic datasets in real time with differential privacy, using a novel adaptive distance-based sampling approach. Our first method, DSFT, uses a fixed distance threshold and releases a differentially private histogram only when the current snapshot is sufficiently different from the previous one, i.e., with a distance greater than a predefined threshold. Our second method, DSAT, further improves DSFT and uses a dynamic threshold adaptively adjusted by a feedback control mechanism to capture the data dynamics. Extensive experiments on real and synthetic datasets demonstrate that our approach achieves better utility than baseline methods and existing state-of-the-art methods.',\n",
       " 'd318b1cbb00282eea7fc5789f97d859181fc165e': 'Improved automatic speech recognition (ASR) in babble noise conditions continues to pose major challenges. In this paper, we propose a new local binary pattern (LBP) based speech presence indicator (SPI) to distinguish speech and non-speech components. Babble noise is subsequently estimated using recursive averaging. In the speech enhancement system optimally-modified log-spectral amplitude (OMLSA) uses the estimated noise spectrum obtained from the LBP based recursive averaging (LRA). The performance of the LRA speech enhancement system is compared to the conventional improved minima controlled recursive averaging (IMCRA). Segmental SNR improvements and perceptual evaluations of speech quality (PESQ) scores show that LRA offers superior babble noise reduction compared to the IMCRA system. Hidden Markov model (HMM) based word recognition results show a corresponding improvement.',\n",
       " 'f7b3544567c32512be1bef4093a78075e59bdc11': 'Flipped classroom approach has been increasingly adopted in higher institutions. Although this approach has many advantages, there are also many challenges that should be considered. In this paper, we discuss the suitability of this approach to teach computer programming, and we report on our pilot experience of using this approach at Qatar University to teach one subject of computer programming course. It is found that students has positive attitude to this approach, it improves their learning. However, the main challenge was how to involve some of the students in online learning activities.',\n",
       " 'ba965d68ea7a58f6a5676c47a5c81fad21959ef6': 'In this article some formal, content-related and procedural considerations towards the sense of humor are articulated and the analysis of both everyday humor behavior and of comic styles leads to the initial proposal of a four factormodel of humor (4FMH). This model is tested in a new dataset and it is also examined whether two forms of comic styles (benevolent humor and moral mockery) do fit in. The model seems to be robust but further studies on the structure of the sense of humor as a personality trait are required.',\n",
       " '3e7aabaffdb4b05e701c544451dce55ad96e9401': 'The growing commercial interest in indoor location-based services (ILBS) has spurred recent development of many indoor positioning techniques. Due to the absence of Global Positioning System (GPS) signal, many other signals have been proposed for indoor usage. Among them, Wi-Fi (802.11) emerges as a promising one due to the pervasive deployment of wireless LANs (WLANs). In particular, Wi-Fi fingerprinting has been attracting much attention recently because it does not require line-of-sight measurement of access points (APs) and achieves high applicability in complex indoor environment. This survey overviews recent advances on two major areas of Wi-Fi fingerprint localization: advanced localization techniques and efficient system deployment. Regarding advanced techniques to localize users, we present how to make use of temporal or spatial signal patterns, user collaboration, and motion sensors. Regarding efficient system deployment, we discuss recent advances on reducing offline labor-intensive survey, adapting to fingerprint changes, calibrating heterogeneous devices for signal collection, and achieving energy efficiency for smartphones. We study and compare the approaches through our deployment experiences, and discuss some future directions.',\n",
       " '8acaebdf9569adafb03793b23e77bf4ac8c09f83': 'We present the analysis and design of fixed physical length, spoof Surface Plasmon Polariton based waveguides with adjustable delay at terahertz frequencies. The adjustable delay is obtained using Corrugated Planar Goubau Lines (CPGL) by changing its corrugation depth without changing the total physical length of the waveguide. Our simulation results show that electrical lengths of 237.9°, 220.6°, and 310.6° can be achieved by physical lengths of 250 μm and 200 μm at 0.25, 0.275, and 0.3 THz, respectively, for demonstration purposes. These simulations results are also consistent with our analytical calculations using the physical parameter and material properties. When we combine pairs of same length delay lines as if they are two branches of a terahertz phase shifter, we achieved an error rate of relative phase shift estimation better than 5.8%. To the best of our knowledge, this is the first-time demonstration of adjustable spoof Surface Plasmon Polariton based CPGL delay lines. The idea can be used for obtaining tunable delay lines with fixed lengths and phase shifters for the terahertz band circuitry.',\n",
       " '10d710c01acb10c4aea702926d21697935656c3d': 'This paper proposes a method for transferring the RGB color spectrum to near-infrared (NIR) images using deep multi-scale convolutional neural networks. A direct and integrated transfer between NIR and RGB pixels is trained. The trained model does not require any user guidance or a reference image database in the recall phase to produce images with a natural appearance. To preserve the rich details of the NIR image, its high frequency features are transferred to the estimated RGB image. The presented approach is trained and evaluated on a real-world dataset containing a large amount of road scene images in summer. The dataset was captured by a multi-CCD NIR/RGB camera, which ensures a perfect pixel to pixel registration.',\n",
       " '325d145af5f38943e469da6369ab26883a3fd69e': 'Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a “colorization Turing test,” asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.',\n",
       " '326a0914dcdf7f42b5e1c2887174476728ca1b9d': 'The problem this paper is concerned with is that of unsupervised learning. Mainly, what does it mean to learn a probability distribution? The classical answer to this is to learn a probability density. This is often done by defining a parametric family of densities (Pθ)θ∈Rd and finding the one that maximized the likelihood on our data: if we have real data examples {x}i=1, we would solve the problem',\n",
       " '5287d8fef49b80b8d500583c07e935c7f9798933': 'Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories, such as faces, album covers, and room interiors. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.',\n",
       " '57bbbfea63019a57ef658a27622c357978400a50': '',\n",
       " '6ec02fb5bfc307911c26741fb3804f16d8ad299c': 'In recent years, active learning has emerged as a powerful tool in building robust systems for object detection using computer vision. Indeed, active learning approaches to on-road vehicle detection have achieved impressive results. While active learning approaches for object detection have been explored and presented in the literature, few studies have been performed to comparatively assess costs and merits. In this study, we provide a cost-sensitive analysis of three popular active learning methods for on-road vehicle detection. The generality of active learning findings is demonstrated via learning experiments performed with detectors based on histogram of oriented gradient features and SVM classification (HOG–SVM), and Haar-like features and Adaboost classification (Haar–Adaboost). Experimental evaluation has been performed on static images and real-world on-road vehicle datasets. Learning approaches are assessed in terms of the time spent annotating, data required, recall, and precision.',\n",
       " 'c2b6755543c3f7c71adb3e14eb06179f27b6ad5d': \"AIM\\nTo analyse the type and location of defects in HyFlex CM instruments after clinical use in a graduate endodontic programme and to examine the impact of clinical use on their metallurgical properties.\\n\\n\\nMETHODOLOGY\\nA total of 468 HyFlex CM instruments discarded from a graduate endodontic programme were collected after use in three teeth. The incidence and type of instrument defects were analysed. The lateral surfaces of the defect instruments were examined by scanning electron microscopy. New and clinically used instruments were examined by differential scanning calorimetry (DSC) and x-ray diffraction (XRD). Vickers hardness was measured with a 200-g load near the flutes for new and clinically used axially sectioned instruments. Data were analysed using one-way anova or Tukey's multiple comparison test.\\n\\n\\nRESULTS\\nOf the 468 HyFlex instruments collected, no fractures were observed and 16 (3.4%) revealed deformation. Of all the unwound instruments, size 20, .04 taper unwound the most often (n\\xa0=\\xa05) followed by size 25, .08 taper (n\\xa0=\\xa04). The trend of DSC plots of new instruments and clinically used (with and without defects) instruments groups were very similar. The DSC analyses showed that HyFlex instruments had an austenite transformation completion or austenite-finish (Af ) temperature exceeding 37\\xa0°C. The Af temperatures of HyFlex instruments (with or without defects) after multiple clinical use were much lower than in new instruments (P\\xa0<\\xa00.05). The enthalpy values for the transformation from martensitic to austenitic on deformed instruments were smaller than in the new instruments at the tip region (P\\xa0<\\xa00.05). XRD results showed that NiTi instruments had austenite and martensite structure on both new and used HyFlex instruments at room temperature. No significant difference in microhardness was detected amongst new and used instruments (with and without defects).\\n\\n\\nCONCLUSIONS\\nThe risk of HyFlex instruments fracture in the canal is very low when instruments are discarded after three cases of clinical use. New HyFlex instruments were a mixture of martensite and austenite structure at body temperature. Multiple clinical use caused significant changes in the microstructural properties of HyFlex instruments. Smaller instruments should be considered as single-use.\",\n",
       " 'a25f6d05c8191be01f736073fa2bc20c03ad7ad8': 'In this study, we propose integrated control of a robotic hand and arm using only proximity sensing from the fingertips. An integrated control scheme for the fingers and for the arm enables quick control of the position and posture of the arm by placing the fingertips adjacent to the surface of an object to be grasped. The arm control scheme enables adjustments based on errors in hand position and posture that would be impossible to achieve by finger motions alone, thus allowing the fingers to grasp an object in a laterally symmetric grasp. This can prevent grasp failures such as a finger pushing the object out of the hand or knocking the object over. Proposed control of the arm and hand allowed correction of position errors on the order of several centimeters. For example, an object on a workbench that is in an uncertain positional relation with the robot, with an inexpensive optical sensor such as a Kinect, which only provides coarse image data, would be sufficient for grasping an object.',\n",
       " '13317a497f4dc5f62a15dbdc135dd3ea293474df': 'Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations. Yet while ‘multi-sense’ methods have been proposed and tested on artificial wordsimilarity tasks, we don’t know if they improve real natural language understanding tasks. In this paper we introduce a multisense embedding model based on Chinese Restaurant Processes that achieves state of the art performance on matching human word similarity judgments, and propose a pipelined architecture for incorporating multi-sense embeddings into language un-',\n",
       " '7ec6b06b0f421b80ca25994c7aa106106c7bfb50': 'This work presents new three-level unidirectional single-phase PFC rectifier topologies well-suited for applications targeting high efficiency and/or high power density. The characteristics of a selected novel rectifier topology, including its principles of operation, modulation strategy, PID control scheme, and a power circuit design related analysis are presented. Finally, a 220-V/3-kW laboratory prototype is constructed and used in order to verify the characteristics of the new converter, which include remarkably low switching losses and single ac-side boost inductor, that allow for a 98.6% peak efficiency with a switching frequency of 140 kHz.',\n",
       " 'c62ba57869099f20c8bcefd9b38ce5d8b4b3db56': 'Many recent studies of trust and reputation are made in the context of commercial reputation or rating systems for online communities. Most of these systems have been constructed without a formal rating model or much regard for our sociological understanding of these concepts. We first provide a critical overview of the state of research on trust and reputation. We then propose a formal quantitative model for the rating process. Based on this model, we formulate two personalized rating schemes and demonstrate their effectiveness at inferring trust experimentally using a simulated dataset and a real world movie-rating dataset. Our experiments show that the popular global rating scheme widely used in commercial electronic communities is inferior to our personalized rating schemes when sufficient ratings among members are available. The level of sufficiency is then discussed. In comparison with other models of reputation, we quantitatively show that our framework provides significantly better estimations of reputation. \"Better\" is discussed with respect to a rating process and specific games as defined in this work. Secondly, we propose a mathematical framework for modeling trust and reputation that is rooted in findings from the social sciences. In particular, our framework makes explicit the importance of social information (i.e., indirect channels of inference) in aiding members of a social network choose whom they want to partner with or to avoid. Rating systems that make use of such indirect channels of inference are necessarily personalized in nature, catering to the individual context of the rater. Finally, we have extended our trust and reputation framework toward addressing a fundamental problem for social science and biology: evolution of cooperation. We show that by providing an indirect inference mechanism for the propagation of trust and reputation, cooperation among selfish agents can be explained for a set of game theoretic simulations. For these simulations in particular, our proposal is shown to have provided more cooperative agent communities than existing schemes are able to. Thesis Supervisor: Peter Szolovits Title: Professor of Electrical Engineering and Computer Science',\n",
       " '61df37b2c1f731e2b6bcb1ae2c2b7670b917284c': 'NASA Ames Research Center, in cooperation with the FAA, has completed research and development of a proof-ofconcept Surface Management System (SMS). This paper reports on two recent SMS field tests as well as final performance and benefits analyses. Field tests and analysis support the conclusion that substantial portions of SMS technology are ready for transfer to the FAA and deployment throughout the National Airspace System (NAS). Other SMS capabilities were accepted in concept but require additional refinement for inclusion in subsequent development spirals. SMS is a decision support tool that helps operational specialists at Air Traffic Control (ATC) and NAS user facilities to collaboratively manage the movements of aircraft on the surface of busy airports, thereby improving capacity, efficiency, and flexibility. SMS provides accurate predictions of the future demand and how that demand will affect airport resources – information that is not currently available. The resulting shared awareness enables the Air Traffic Control Tower (ATCT), Terminal Radar Approach Control (TRACON), Air Route Traffic Control Center (ARTCC), and air carriers to coordinate traffic management decisions. Furthermore, SMS uses its ability to predict how future demand will play out on the surface to evaluate the effect of various traffic management decisions in advance of implementing them, to plan and advise surface operations. The SMS concept, displays, and algorithms were evaluated through a series of field tests at Memphis International Airport (MEM). An operational trial in September, 2003 evaluated SMS traffic management components, such as runway configuration change planning; shadow testing in January, 2004 tested tactical components (e.g., Approval Request (APREQ) coordination, sequencing for departure, and Expected Departure Clearance Time (EDCT) compliance). Participants in these evaluations rated the SMS concept and many of the traffic management displays very positively. Local and Ground controller displays will require integration with other automation systems. Feedback from FAA and NAS user participants support the conclusion that SMS algorithms currently provide information that has acceptable and beneficial accuracy for traffic management applications. Performance analysis results document the current accuracy of SMS algorithms. Benefits/cost analysis of delay cost reduction due to SMS provides the business case for SMS deployment.',\n",
       " 'dcff311940942dcf81db5073e551a87e1710e52a': 'Generally, theintrudermustperformseveralactions,organizedin anintrusionscenario, to achieve hisor hermaliciousobjective.Wearguethatintrusionscenarioscan bemodelledasa planningprocessandwesuggestmodellinga maliciousobjectiveas anattemptto violatea givensecurityrequirement. Our proposalis thento extendthe definitionof attackcorrelationpresentedin [CM02] to correlateattackswith intrusion objectivesThis notionis usefulto decideif a sequenceof correlatedactionscanlead to a securityrequirementviolation.This approachprovidesthesecurityadministrator with aglobalview of whathappensin thesystem.In particular, it controlsunobserved actionsthroughhypothesisgeneration,clustersrepeatedactionsin a singlescenario, recognizesintrudersthatarechangingtheir intrusionobjectivesandis efficient to detectvariationsof anintrusionscenario.Thisapproachcanalsobeusedto eliminatea category of falsepositivesthatcorrespondto falseattacks,that is actionsthatarenot furthercorrelatedto anintrusionobjective.',\n",
       " '7ffdf4d92b4bc5690249ed98e51e1699f39d0e71': 'For the first time, a fully integrated phased array antenna with radio frequency microelectromechanical systems (RF MEMS) switches on a flexible, organic substrate is demonstrated above 10 GHz. A low noise amplifier (LNA), MEMS phase shifter, and 2 times 2 patch antenna array are integrated into a system-on-package (SOP) on a liquid crystal polymer substrate. Two antenna arrays are compared; one implemented using a single-layer SOP and the second with a multilayer SOP. Both implementations are low-loss and capable of 12deg of beam steering. The design frequency is 14 GHz and the measured return loss is greater than 12 dB for both implementations. The use of an LNA allows for a much higher radiated power level. These antennas can be customized to meet almost any size, frequency, and performance needed. This research furthers the state-of-the-art for organic SOP devices.',\n",
       " '3aa41f8fdb6a4523e2cd95365bb6c7499ad29708': 'This paper introduces a novel system that allows users to generate customized cartoon avatars through a sketching interface. The rise of social media and personalized gaming has given a need for personalized virtual appearances. Avatars, self-curated and customized images to represent oneself, have become a common means of expressing oneself in these new media. Avatar creation platforms face the challenge of granting user significant control over the avatar creation, and the challenge of encumbering the user with too many choices in their avatar customization. This paper demonstrates a sketch-guided avatar customization system and its potential to simplify the avatar creation process. Author',\n",
       " 'ba02b6125ba47ff3629f1d09d1bada28169c2b32': 'Existing entailment datasets mainly pose problems which can be answered without attention to grammar or word order. Learning syntax requires comparing examples where different grammar and word order change the desired classification. We introduce several datasets based on synthetic transformations of natural entailment examples in SNLI or FEVER, to teach aspects of grammar and word order. We show that without retraining, popular entailment models are unaware that these syntactic differences change meaning. With retraining, some but not all popular entailment models can learn to compare the syntax properly.',\n",
       " 'cb25c33ba56db92b7da4d5080f73fba07cb914a3': 'As the rapid progress of science and technology, the free-form surface optical component has played an important role in spaceflight, aviation, national defense, and other areas of the technology. While the technology of fast tool servo (FTS) is the most promising method for the machining of free-form surface optical component. However, the shortcomings of short-stroke of fast tool servo device have constrained the development of free-form surface optical component. To address this problem, a new large-stroke flexible FTS device is proposed in this paper. A series of mechanism modeling and optimal designs are carried out via compliance matrix theory, pseudo-rigid body theory, and Particle Swarm Optimization (PSO) algorithm, respectively. The mechanism performance of the large-stroke FTS device is verified by the Finite Element Analysis (FEA) method. For this study, a piezoelectric (PZT) actuator P-840.60 that can travel to 90 µm under open-loop control is employed, the results of experiment indicate that the maximum of output displacement can achieve 258.3µm, and the bandwidth can achieve around 316.84 Hz. Both theoretical analysis and the test results of prototype uniformly verify that the presented FTS device can meet the demand of the actual microstructure processing.',\n",
       " '3d4bae33c2ccc0a6597f80e27cbeed64990b95bd': 'Therapeutic interventions that incorporate training in mindfulness meditation have become increasingly popular, but to date little is known about neural mechanisms associated with these interventions. Mindfulness-Based Stress Reduction (MBSR), one of the most widely used mindfulness training programs, has been reported to produce positive effects on psychological well-being and to ameliorate symptoms of a number of disorders. Here, we report a controlled longitudinal study to investigate pre-post changes in brain gray matter concentration attributable to participation in an MBSR program. Anatomical magnetic resonance (MR) images from 16 healthy, meditation-naïve participants were obtained before and after they underwent the 8-week program. Changes in gray matter concentration were investigated using voxel-based morphometry, and compared with a waiting list control group of 17 individuals. Analyses in a priori regions of interest confirmed increases in gray matter concentration within the left hippocampus. Whole brain analyses identified increases in the posterior cingulate cortex, the temporo-parietal junction, and the cerebellum in the MBSR group compared with the controls. The results suggest that participation in MBSR is associated with changes in gray matter concentration in brain regions involved in learning and memory processes, emotion regulation, self-referential processing, and perspective taking.',\n",
       " '217af49622a4e51b6d1b9b6c75726eaf1355a903': 'In this paper, we explore the problem of enhancing still pictures with subtly animated motions. We limit our domain to scenes containing passive elements that respond to natural forces in some fashion. We use a semi-automatic approach, in which a human user segments the scene into a series of layers to be individually animated. Then, a \"stochastic motion texture\" is automatically synthesized using a spectral method, i.e., the inverse Fourier transform of a filtered noise spectrum. The motion texture is a time-varying 2D displacement map, which is applied to each layer. The resulting warped layers are then recomposited to form the animated frames. The result is a looping video texture created from a single still image, which has the advantages of being more controllable and of generally higher image quality and resolution than a video texture created from a video source. We demonstrate the technique on a variety of photographs and paintings.',\n",
       " '10d79507f0f2e2d2968bf3a962e1daffc8bd44f0': 'Most previously proposed statistical models for the indoor multipath channel include only time of arrival characteristics. However, in order to use statistical models in simulating or analyzing the performance of systems employing spatial diversity combining, information about angle of arrival statistics is also required. Ideally, it would be desirable to characterize the full spare-time nature of the channel. In this paper, a system is described that was used to collect simultaneous time and angle of arrival data at 7 GHz. Data processing methods are outlined, and results obtained from data taken in two different buildings are presented. Based on the results, a model is proposed that employs the clustered \"double Poisson\" time-of-arrival model proposed by Saleh and Valenzuela (1987). The observed angular distribution is also clustered with uniformly distributed clusters and arrivals within clusters that have a Laplacian distribution.',\n",
       " 'd00ef607a10e5be00a9e05504ab9771c0b05d4ea': 'High-voltage-rated solid-state switches such as insulated-gate bipolar transistors (IGBTs) are commercially available up to 6.5 kV. Such voltage ratings are attractive for pulsed power and high-voltage switch-mode converter applications. However, as the IGBT voltage ratings increase, the rate of current rise and fall are generally reduced. This tradeoff is difficult to avoid as IGBTs must maintain a low resistance in the epitaxial or drift region layer. For high-voltage-rated IGBTs with thick drift regions to support the reverse voltage, the required high carrier concentrations are injected at turn on and removed at turn off, which slows the switching speed. An option for faster switching is to series multiple, lower voltage-rated IGBTs. An IGBT-stack prototype with six, 1200 V rated IGBTs in series has been experimentally tested. The six-series IGBT stack consists of individual, optically isolated, gate drivers and aluminum cooling plates for forced air cooling which results in a compact package. Each IGBT is overvoltage protected by transient voltage suppressors. The turn-on current rise time of the six-series IGBT stack and a single 6.5 kV rated IGBT has been experimentally measured in a pulsed resistive-load, capacitor discharge circuit. The IGBT stack has also been compared to two IGBT modules in series, each rated at 3.3 kV, in a boost circuit application switching at 9 kHz and producing an output of 5 kV. The six-series IGBT stack results in improved turn-on switching speed, and significantly higher power boost converter efficiency due to a reduced current tail during turn off. The experimental test parameters and the results of the comparison tests are discussed in the following paper',\n",
       " 'b63a60e666c4c0335d8de4581eaaa3f71e8e0e54': 'DC-bus voltage control is an important task in the operation of a dc or a hybrid ac/dc microgrid system. To improve the dc-bus voltage control dynamics, traditional approaches attempt to measure and feedforward the load or source power in the dc-bus control scheme. However, in a microgrid system with distributed dc sources and loads, the traditional feedforward-based methods need remote measurement with communications. In this paper, a nonlinear disturbance observer (NDO) based dc-bus voltage control is proposed, which does not need the remote measurement and enables the important “plug-and-play” feature. Based on this observer, a novel dc-bus voltage control scheme is developed to suppress the transient fluctuations of dc-bus voltage and improve the power quality in such a microgrid system. Details on the design of the observer, the dc-bus controller and the pulsewidth-modulation (PWM) dead-time compensation are provided in this paper. The effects of possible dc-bus capacitance variation are also considered. The performance of the proposed control strategy has been successfully verified in a 30 kVA hybrid microgrid including ac/dc buses, battery energy storage system, and photovoltaic (PV) power generation system.',\n",
       " '6b557c35514d4b6bd75cebdaa2151517f5e820e2': 'Recent developments in wind energy research including wind speed prediction, wind turbine control, operations of hybrid power systems, as well as condition monitoring and fault detection are surveyed. Approaches based on statistics, physics, and data mining for wind speed prediction at different time scales are reviewed. Comparative analysis of prediction results reported in the literature is presented. Studies of classical and intelligent control of wind turbines involving different objectives and strategies are reported. Models for planning operations of different hybrid power systems including wind generation for various objectives are addressed. Methodologies for condition monitoring and fault detection are discussed. Future research directions in wind energy are proposed. 2013 Elsevier Ltd. All rights reserved.',\n",
       " 'e81f115f2ac725f27ea6549f4de0a71b3a3f6a5c': 'The purpose of this research was to develop, standardize, and test the reliability of a short neuropsychological test battery in the Spanish language. This neuropsychological battery was named \"NEUROPSI,\" and was developed to assess briefly a wide spectrum of cognitive functions, including orientation, attention, memory, language, visuoperceptual abilities, and executive functions. The NEUROPSI includes items that are relevant for Spanish-speaking communities. It can be applied to illiterates and low educational groups. Administration time is 25 to 30 min. Normative data were collected from 800 monolingual Spanish-speaking individuals, ages 16 to 85 years. Four age groups were used: (1) 16 to 30 years, (2) 31 to 50 years, (3) 51 to 65 years, and (4) 66 to 85 years. Data also are analyzed and presented within 4 different educational levels that were represented in this sample; (1) illiterates (zero years of school); (2) 1 to 4 years of school; (2) 5 to 9 years of school; and (3) 10 or more years of formal education. The effects of age and education, as well as the factor structure of the NEUROPSI are analyzed. The NEUROPSI may fulfill the need for brief, reliable, and objective evaluation of a broad range of cognitive functions in Spanish-speaking populations.',\n",
       " '382c057c0be037340e7d6494fc3a580b9d6b958c': 'The nonprofit phenomenon “TED,” the brand name for the concepts of Technology Education and Design, was born in 1984. It launched into pop culture stardom in 2006 when the organization’s curators began offering short, free, unrestricted, and educational video segments. Known as “TEDTalks,” these informational segments are designed to be no longer than 18 minutes in length and provide succinct, targeted enlightenment on various topics or ideas that are deemed “worth spreading.” TED Talks, often delivered in sophisticated studios with trendy backdrops, follow a format that focuses learners on the presenter and limited, extremely purposeful visual aids. Topics range from global warming to running to the developing world. Popular TED Talks, such as Sir Ken Robinson’s “Schools Kill Creatively” or Dan Gilbert’s “Why Are We Happy?” can easily garner well over a million views. TED Talks are a curious phenomenon for educators to observe. They are in many ways the antithesis of traditional lectures, which are typically 60-120 minutes in length and delivered in cavernous halls by faculty members engaged in everyday academic lives. Perhaps the formality of the lecture is the biggest superficial difference in comparison to casual TEDTalks (Table 1). However, TED Talks are not as unstructured as they may appear. Presenters are well coached and instructed to follow a specific presentation formula, whichmaximizes storyboarding and highlights passion for the subject. While learning is not formally assessed, TED Talks do seem to accomplish their goals of spreading ideas while sparking curiosity within the learner. The fact that some presentations have been viewed more than 16 million times points to the effectiveness of the platform in at least reaching learners and stimulating a desire to click, listen, and learn.Moreover, the TEDTalks website is the fourth most popular technology website and the single most popular conference and events website in the world. The TED phenomenon may have both direct and subliminal messages for academia. Perhaps an initial question to ponder is whether the TED phenomenon is a logical grassroots educational evolution or a reaction to the digital generation and their preference for learning that occurs “wherever, whenever.” The diverse cross-section of TED devotees ranging in background and age would seem to provide evidence that the platform does not solely appeal to younger generations of learners. Instead, it suggests that adult learners are either more drawn to digital learning than they think they are or than they are likely to admit. The perceived efficacy of TED once again calls into question the continued reliance of academia on the lecture as the primary currency of learning. TED Talks do not convey large chunks of information but rather present grander ideas. Would TED-like educational modules or blocks of 18-20 minutes be more likely to pique student curiosity across a variety of pharmacy topics, maintain attention span, and improve retention? Many faculty members who are recognized as outstanding teachers or lecturers might confess that they already teach through a TED-like lens. Collaterally, TED Talks or TED-formatted learning experiences might be ideal springboards for incorporation into inverted or flipped classroom environments where information is gathered and learned at home, while ideas are analyzed, debated, and assimilated within the classroom. Unarguably, TED Talks have given scientists and other researchers a real-time, mass media driven opportunity to disseminate their research, ideas, and theories that might otherwise have gone unnoticed. Similar platforms or approaches may be able to provide opportunities for the academy to further transmit research to the general public. The TED approach to idea dissemination is not without its critics. Several authors have criticized TED for flattening or dumbing down ideas so they fit into a preconceived, convenient format that is primarily designed to entertain. Consequently, the oversimplified ideas and conceptsmay provoke little effort from the learner to analyze data, theory, or controversy. Some Corresponding Author: Frank Romanelli, PharmD, MPH, 789 South Limestone Road, University of Kentucky College of Pharmacy, Lexington, KY 40536. E-mail: froma2@email. uky.edu American Journal of Pharmaceutical Education 2014; 78 (6) Article 113.',\n",
       " '36eff99a7f23cec395e4efc80ff7f937934c7be6': 'Geometry and Meaning is an interesting book about a relationship between geometry and logic defined on certain types of abstract spaces and how that intimate relationship might be exploited when applied in computational linguistics. It is also about an approach to information retrieval, because the analysis of natural language, especially negation, is applied to problems in IR, and indeed illustrated throughout the book by simple examples using search engines. It is refreshing to see IR issues tackled from a different point of view than the standard vector space (Salton, 1968). It is an enjoyable read, as intended by the author, and succeeds as a sort of tourist guide to the subject in hand. The early part of the book concentrates on the introduction of a number of elementary concepts from mathematics: graph theory, linear algebra (especially vector spaces), lattice theory, and logic. These concepts are well motivated and illustrated with good examples, mostly of a classificatory or taxonomic kind. One of the major goals of the book is to argue that non-classical logic, in the form of a quantum logic, is a candidate for analyzing language and its underlying logic, with a promise that such an approach could lead to improved search engines. The argument for this is aided by copious references to early philosophers, scientists, and mathematicians, creating the impression that when Aristotle, Descartes, Boole, and Grassmann were laying the foundations for taxonomy, analytical geometry, logic, and vector spaces, they had a more flexible and broader view of these subjects than is current. This is especially true of logic. Thus the historical approach taken to introducing quantum logic (chapter 7) is to show that this particular kind of logic and its interpretation in vector space were inherent in some of the ideas of these earlier thinkers. Widdows claims that Aristotle was never respected for his mathematics and that Grassmann’s Ausdehnungslehre was largely ignored and left in obscurity. Whether Aristotle was never admired for his mathematics I am unable to judge, but certainly Alfred North Whitehead (1925) was not complimentary when he said:',\n",
       " 'f0d82cbac15c4379677d815c9d32f7044b19d869': \"Neuroengineering is faced with unique challenges in repairing or replacing complex neural systems that are composed of many interacting parts. These interactions form intricate patterns over large spatiotemporal scales and produce emergent behaviors that are difficult to predict from individual elements. Network science provides a particularly appropriate framework in which to study and intervene in such systems by treating neural elements (cells, volumes) as nodes in a graph and neural interactions (synapses, white matter tracts) as edges in that graph. Here, we review the emerging discipline of network neuroscience, which uses and develops tools from graph theory to better understand and manipulate neural systems from micro- to macroscales. We present examples of how human brain imaging data are being modeled with network analysis and underscore potential pitfalls. We then highlight current computational and theoretical frontiers and emphasize their utility in informing diagnosis and monitoring, brain-machine interfaces, and brain stimulation. A flexible and rapidly evolving enterprise, network neuroscience provides a set of powerful approaches and fundamental insights that are critical for the neuroengineer's tool kit.\",\n",
       " 'f7d5f8c60972c18812925715f685ce8ae5d5659d': 'The two-dimensional orthogonal packing problem (2OPP ) consists of determining if a set of rectangles (items) can be packed into one rectangle of fixed size (bin). In this paper we propose two exact algorithms for solving this problem. The first algorithm is an improvement on a classical branch&bound method, whereas the second algorithm is based on a two-step enumerative method. We also describe reduction procedures and lower bounds which can be used within the branch&bound method. We report computational experiments for randomly generated benchmarks, which demonstrate the efficiency of both methods.',\n",
       " '90c1104142203c8ead18882d49bfea8aec23e758': 'The objectives of the present study were: a) to investigate three continuous variants of the NASA-Task Load Index (TLX) (standard NASA (CNASA), average NASA (C1NASA) and principal component NASA (PCNASA)) and five different variants of the simplified subjective workload assessment technique (SSWAT) (continuous standard SSWAT (CSSWAT), continuous average SSWAT (C1SSWAT), continuous principal component SSWAT (PCSSWAT), discrete event-based SSWAT (D1SSWAT) and discrete standard SSWAT (DSSWAT)) in terms of their sensitivity and diagnosticity to assess the mental workload associated with agricultural spraying; b) to compare and select the best variants of NASA-TLX and SSWAT for future mental workload research in the agricultural domain. A total of 16 male university students (mean 30.4 +/- 12.5 years) participated in this study. All the participants were trained to drive an agricultural spraying simulator. Sensitivity was assessed by the ability of the scales to report the maximum change in workload ratings due to the change in illumination and difficulty levels. In addition, the factor loading method was used to quantify sensitivity. The diagnosticity was assessed by the ability of the scale to diagnose the change in task levels from single to dual. Among all the variants of NASA-TLX and SSWAT, PCNASA and discrete variants of SSWAT showed the highest sensitivity and diagnosticity. Moreover, among all the variants of NASA and SSWAT, the discrete variants of SSWAT showed the highest sensitivity and diagnosticity but also high between-subject variability. The continuous variants of both scales had relatively low sensitivity and diagnosticity and also low between-subject variability. Hence, when selecting a scale for future mental workload research in the agricultural domain, a researcher should decide what to compromise: 1) between-subject variability or 2) sensitivity and diagnosticity. STATEMENT OF RELEVANCE: The use of subjective workload scales is very popular in mental workload research. The present study investigated the different variants of two popular workload rating scales (i.e. NASA-TLX and SSWAT) in terms of their sensitivity and diagnositicity and selected the best variants of each scale for future mental workload research.',\n",
       " 'b1cbfd6c1e7f8a77e6c1e6db6cd0625e3bd785ef': \"Hashing is one of the most fundamental operations that provides a means for a program to obtain fast access to large amounts of data. Despite the emergence of GPUs as many-threaded general purpose processors, high performance parallel data hashing solutions for GPUs are yet to receive adequate attention. Existing hashing solutions for GPUs not only impose restrictions (e.g., inability to concurrently execute insertion and retrieval operations, limitation on the size of key-value data pairs) that limit their applicability, their performance does not scale to large hash tables that must be kept out-of-core in the host memory. In this paper we present Stadium Hashing (Stash) that is scalable to large hash tables and practical as it does not impose the aforementioned restrictions. To support large out-of-core hash tables, Stash uses a compact data structure named ticket-board that is separate from hash table buckets and is held inside GPU global memory. Ticket-board locally resolves significant portion of insertion and lookup operations and hence, by reducing accesses to the host memory, it accelerates the execution of these operations. Split design of the ticket-board also enables arbitrarily large keys and values. Unlike existing methods, Stash naturally supports concurrent insertions and retrievals due to its use of double hashing as the collision resolution strategy. Furthermore, we propose Stash with collaborative lanes (clStash) that enhances GPU's SIMD resource utilization for batched insertions during hash table creation. For concurrent insertion and retrieval streams, Stadium hashing can be up to 2 and 3 times faster than GPU Cuckoo hashing for in-core and out-of-core tables respectively.\",\n",
       " '20f5b475effb8fd0bf26bc72b4490b033ac25129': 'We present a robust and real time approach to lane marker detection in urban streets. It is based on generating a top view of the road, filtering using selective oriented Gaussian filters, using RANSAC line fitting to give initial guesses to a new and fast RANSAC algorithm for fitting Bezier Splines, which is then followed by a post-processing step. Our algorithm can detect all lanes in still images of the street in various conditions, while operating at a rate of 50 Hz and achieving comparable results to previous techniques.',\n",
       " '27edbcf8c6023905db4de18a4189c2093ab39b23': 'A lane-detection system is an important component of many intelligent transportation systems. We present a robust lane-detection-and-tracking algorithm to deal with challenging scenarios such as a lane curvature, worn lane markings, lane changes, and emerging, ending, merging, and splitting lanes. We first present a comparative study to find a good real-time lane-marking classifier. Once detection is done, the lane markings are grouped into lane-boundary hypotheses. We group left and right lane boundaries separately to effectively handle merging and splitting lanes. A fast and robust algorithm, based on random-sample consensus and particle filtering, is proposed to generate a large number of hypotheses in real time. The generated hypotheses are evaluated and grouped based on a probabilistic framework. The suggested framework effectively combines a likelihood-based object-recognition algorithm with a Markov-style process (tracking) and can also be applied to general-part-based object-tracking problems. An experimental result on local streets and highways shows that the suggested algorithm is very reliable.',\n",
       " '4d2cd0b25c5b0f69b6976752ebca43ec5f04a461': 'In this paper, we proposed a B-Snake based lane detection and tracking algorithm without any cameras’ parameters. Compared with other lane models, the B-Snake based lane model is able to describe a wider range of lane structures since B-Spline can form any arbitrary shape by a set of control points. The problems of detecting both sides of lane markings (or boundaries) have been merged here as the problem of detecting the mid-line of the lane, by using the knowledge of the perspective parallel lines. Furthermore, a robust algorithm, called CHEVP, is presented for providing a good initial position for the B-Snake. Also, a minimum error method by Minimum Mean Square Error (MMSE) is proposed to determine the control points of the B-Snake model by the overall image forces on two sides of lane. Experimental results show that the proposed method is robust against noise, shadows, and illumination variations in the captured road images. It is also applicable to the marked and the unmarked roads, as well as the dash and the solid paint line roads. q 2003 Elsevier B.V. All rights reserved.',\n",
       " '1c0f7854c14debcc34368e210568696a01c40573': 'In this article a new method for the calibration of a vision system which consists of two (or more) cameras is presented. The proposed method, which uses simple properties of vanishing points, is divided into two steps. In the first step, the intrinsic parameters of each camera, that is, the focal length and the location of the intersection between the optical axis and the image plane, are recovered from a single image of a cube. In the second step, the extrinsic parameters of a pair of cameras, that is, the rotation matrix and the translation vector which describe the rigid motion between the coordinate systems fixed in the two cameras are estimated from an image stereo pair of a suitable planar pattern. Firstly, by matching the corresponding vanishing points in the two images the rotation matrix can be computed, then the translation vector is estimated by means of a simple triangulation. The robustness of the method against noise is discussed, and the conditions for optimal estimation of the rotation matrix are derived. Extensive experimentation shows that the precision that can be achieved with the proposed method is sufficient to efficiently perform machine vision tasks that require camera calibration, like depth from stereo and motion from image sequence.',\n",
       " '235aff8bdb65654163110b35f268de6933814c49': 'A lane detection system is an important component of many intelligent transportation systems. We present a robust realtime lane tracking algorithm for a curved local road. First, we present a comparative study to find a good realtime lane marking classifier. Once lane markings are detected, they are grouped into many lane boundary hypotheses represented by constrained cubic spline curves. We present a robust hypothesis generation algorithm using a particle filtering technique and a RANSAC (random sample concensus) algorithm. We introduce a probabilistic approach to group lane boundary hypotheses into left and right lane boundaries. The proposed grouping approach can be applied to general part-based object tracking problems. It incorporates a likelihood-based object recognition technique into a Markov-style process. An experimental result on local streets shows that the suggested algorithm is very reliable',\n",
       " '514ee2a4d6dec51d726012bd74b32b1e05f13271': 'Philosophers have studied ontologies for centuries in their search for a systematic explanation of existence: “What kind of things exist?” Recently, ontologies have emerged as a major research topic in the fields of artificial intelligence and knowledge management where they address the content issue: “What kind of things should we represent?” The answer to that question differs with the scope of the ontology. Ontologies that are subject-independent are called upper-level ontologies, and they attempt to define concepts that are shared by all domains, such as time and space. Domain ontologies, on the other hand, attempt to define the things that are relevant to a specific application domain. Both types of ontologies are becoming increasingly important in the era of the Internet where consistent and machine-readable semantic definitions of economic phenomena become the language of e-commerce. In this paper, we propose the conceptual accounting framework of the Resource-Event-Agent (REA) model of McCarthy (1982) as an enterprise domain ontology, and we build upon the initial ontology work of Geerts and McCarthy (2000) which explored REA with respect to the ontological categorizations of John Sowa (1999). Because of its conceptual modeling heritage, REA already resembles an established ontology in many declarative (categories) and procedural (axioms) respects, and we also propose here to extend formally that framework both (1) vertically in terms of entrepreneurial logic (value chains) and workflow detail, and (2) horizontally in terms of type and commitment images of enterprise economic phenomena. A strong emphasis throughout the paper is given to the microeconomic foundations of the category definitions.',\n",
       " '944692d5d33fbc5f42294a8310380e0b057a1320': 'A wide band patch antenna fed by an L-probe can be designed for dual- and multi-band application by cutting U-slots on the patch. Simulation and measurement results are presented to illustrate this design.',\n",
       " '6800fbe3314be9f638fb075e15b489d1aadb3030': 'The collaborative filtering (CF) approach to recommenders has recently enjoyed much interest and progress. The fact that it played a central role within the recently completed Netflix competition has contributed to its popularity. This chapter surveys the recent progress in the field. Matrix factorization techniques, which became a first choice for implementing CF, are described together with recent innovations. We also describe several extensions that bring competitive accuracy into neighborhood methods, which used to dominate the field. The chapter demonstrates how to utilize temporal models and implicit feedback to extend models accuracy. In passing, we include detailed descriptions of some the central methods developed for tackling the challenge of the Netflix Prize competition.',\n",
       " '12bbec48c8fde83ea276402ffedd2e241e978a12': 'VirtualTable is a projection augmented reality installation where users are engaged in an interactive tower defense game. The installation runs continuously and is designed to attract people to a table, which the game is projected onto. Any number of players can join the game for an optional period of time. The goal is to prevent the virtual stylized soot balls, spawning on one side of the table, from reaching the cheese. To stop them, the players can place any kind of object on the table, that then will become part of the game. Depending on the object, it will become either a wall, an obstacle for the soot balls, or a tower, that eliminates them within a physical range. The number of enemies is dependent on the number of objects in the field, forcing the players to use strategy and collaboration and not the sheer number of objects to win the game.',\n",
       " 'ffd76d49439c078a6afc246e6d0638a01ad563f8': 'Mobile healthcare is a fast growing area of research that capitalizes on mobile technologies and wearables to provide realtime and continuous monitoring and analysis of vital signs of users. Yet, most of the current applications are developed for general population without taking into consideration the context and needs of different user groups. Designing and developing mobile health applications and diaries according to the user context can significantly improve the quality of user interaction and encourage the application use. In this paper, we propose a user context model and a set of usability attributes for developing mobile applications in healthcare. The proposed model and the selected attributes are integrated into a mobile application development framework to provide user-centered and context-aware guidelines. To validate our framework, a mobile diary was implemented for patients undergoing Peritoneal Dialysis (PD) and tested with real users.',\n",
       " '8deafc34941a79b9cfc348ab63ec51752c7b1cde': 'The exponential growth in the amount of data gathered from various sources has resulted in the need for more efficient algorithms to quickly analyze large datasets. Clustering techniques, like K-Means are useful in analyzing data in a parallel fashion. K-Means largely depends upon a proper initialization to produce optimal results. K-means++ initialization algorithm provides solution based on providing an initial set of centres to the K-Means algorithm. However, its inherent sequential nature makes it suffer from various limitations when applied to large datasets. For instance, it makes k iterations to find k centres. In this paper, we present an algorithm that attempts to overcome the drawbacks of previous algorithms. Our work provides a method to select a good initial seeding in less time, facilitating fast and accurate cluster analysis over large datasets.',\n",
       " '455d562bf02dcb5161c98668a5f5e470d02b70b8': 'Neural network-based clustering has recently gained popularity, and in particular a constrained clustering formulation has been proposed to perform transfer learning and image category discovery using deep learning. The core idea is to formulate a clustering objective with pairwise constraints that can be used to train a deep clustering network; therefore the cluster assignments and their underlying feature representations are jointly optimized end-toend. In this work, we provide a novel clustering formulation to address scalability issues of previous work in terms of optimizing deeper networks and larger amounts of categories. The proposed objective directly minimizes the negative log-likelihood of cluster assignment with respect to the pairwise constraints, has no hyper-parameters, and demonstrates improved scalability and performance on both supervised learning and unsupervised transfer learning.',\n",
       " 'e6bef595cb78bcad4880aea6a3a73ecd32fbfe06': 'The exponential increase in the availability of online reviews and recommendations makes sentiment classification an interesting topic in academic and industrial research. Reviews can span so many different domains that it is difficult to gather annotated training data for all of them. Hence, this paper studies the problem of domain adaptation for sentiment classifiers, hereby a system is trained on labeled reviews from one source domain but is meant to be deployed on another. We propose a deep learning approach which learns to extract a meaningful representation for each review in an unsupervised fashion. Sentiment classifiers trained with this high-level feature representation clearly outperform state-of-the-art methods on a benchmark composed of reviews of 4 types of Amazon products. Furthermore, this method scales well and allowed us to successfully perform domain adaptation on a larger industrial-strength dataset of 22 domains.',\n",
       " 'd77d2ab03f891d8f0822083020486a6de1f2900f': 'The task of discriminating the motor imagery of different movements within the same limb using electroencephalography (EEG) signals is challenging because these imaginary movements have close spatial representations on the motor cortex area. There is, however, a pressing need to succeed in this task. The reason is that the ability to classify different same-limb imaginary movements could increase the number of control dimensions of a brain-computer interface (BCI). In this paper, we propose a 3-class BCI system that discriminates EEG signals corresponding to rest, imaginary grasp movements, and imaginary elbow movements. Besides, the differences between simple motor imagery and goal-oriented motor imagery in terms of their topographical distributions and classification accuracies are also being investigated. To the best of our knowledge, both problems have not been explored in the literature. Based on the EEG data recorded from 12 able-bodied individuals, we have demonstrated that same-limb motor imagery classification is possible. For the binary classification of imaginary grasp and elbow (goal-oriented) movements, the average accuracy achieved is 66.9%. For the 3-class problem of discriminating rest against imaginary grasp and elbow movements, the average classification accuracy achieved is 60.7%, which is greater than the random classification accuracy of 33.3%. Our results also show that goal-oriented imaginary elbow movements lead to a better classification performance compared to simple imaginary elbow movements. This proposed BCI system could potentially be used in controlling a robotic rehabilitation system, which can assist stroke patients in performing task-specific exercises.',\n",
       " 'de0f84359078ec9ba79f4d0061fe73f6cac6591c': 'A resonant single-stage single-switch four-output LED driver with high power factor and passive current balancing is proposed. By controlling one output current, the other output currents of four-output LED driver can be controlled via passive current balancing, which makes its control simple. When magnetizing inductor current operates in critical conduction mode, unity power factor is achieved. The proposed LED driver uses only one active switch and one magnetic component, thus it benefits from low cost, small volume, and light weight. Moreover, high-efficiency performance is achieved due to single-stage power conversion and soft-switching characteristics. The characteristics of the proposed LED driver are studied in this paper and experimental results of two 110-W four-output isolated LED drivers are provided to verify the studied results.',\n",
       " '1924ae6773f09efcfc791454d42a3ec53207a815': 'Natural language software requirements descriptions enable end users to formulate their wishes and expectations for a future software product without much prior knowledge in requirements engineering. However, these descriptions are susceptible to linguistic inaccuracies such as ambiguities and incompleteness that can harm the development process. There is a number of software solutions that can detect deficits in requirements descriptions and partially solve them, but they are often hard to use and not suitable for end users. For this reason, we develop a software system that helps end-users to create unambiguous and complete requirements descriptions by combining existing expert tools and controlling them using automatic compensation strategies. In order to recognize the necessity of individual compensation methods in the descriptions, we have developed linguistic indicators, which we present in this paper. Based on these indicators, the whole text analysis pipeline is ad-hoc configured and thus adapted to the individual circumstances of a requirements description.',\n",
       " '727774c3a911d45ea6fe2d4ad66fd3b453a18c99': \"In this paper, we report a study that examines the relationship between image-based computational analyses of web pages and users' aesthetic judgments about the same image material. Web pages were iteratively decomposed into quadrants of minimum entropy (quadtree decomposition) based on low-level image statistics, to permit a characterization of these pages in terms of their respective organizational symmetry, balance and equilibrium. These attributes were then evaluated for their correlation with human participants' subjective ratings of the same web pages on four aesthetic and affective dimensions. Several of these correlations were quite large and revealed interesting patterns in the relationship between low-level (i.e., pixel-level) image statistics and design-relevant dimensions.\",\n",
       " '21c76cc8ebfb9c112c2594ce490b47e458b50e31': 'In this paper, we present an American Sign Language recognition system using a compact and affordable 3D motion sensor. The palm-sized Leap Motion sensor provides a much more portable and economical solution than Cyblerglove or Microsoft kinect used in existing studies. We apply k-nearest neighbor and support vector machine to classify the 26 letters of the English alphabet in American Sign Language using the derived features from the sensory data. The experiment result shows that the highest average classification rate of 72.78% and 79.83% was achieved by k-nearest neighbor and support vector machine respectively. We also provide detailed discussions on the parameter setting in machine learning methods and accuracy of specific alphabet letters in this paper.',\n",
       " '519f5892938d4423cecc999b6e489b72fc0d0ca7': 'BACKGROUND\\nEhlers-Danlos syndrome (EDS) hypermobility-type is the most common hereditary disorder of the connective tissue. The tissue fragility characteristic of this condition leads to multi-systemic symptoms in which pain, often severe, chronic, and disabling, is the most experienced. Clinical observations suggest that the complex patient with EDS hypermobility-type is refractory toward several biomedical and physical approaches. In this context and in accordance with the contemporary conceptualization of pain (biopsychosocial perspective), the identification of psychological aspects involved in the pain experience can be useful to improve interventions for this under-recognized pathology.\\n\\n\\nPURPOSE\\nReview of the literature on joint hypermobility and EDS hypermobility-type concerning psychological factors linked to pain chronicity and disability.\\n\\n\\nMETHODS\\nA comprehensive search was performed using scientific online databases and references lists, encompassing publications reporting quantitative and qualitative research as well as unpublished literature.\\n\\n\\nRESULTS\\nDespite scarce research, psychological factors associated with EDS hypermobility-type that potentially affect pain chronicity and disability were identified. These are cognitive problems and attention to body sensations, negative emotions, and unhealthy patterns of activity (hypo/hyperactivity).\\n\\n\\nCONCLUSIONS\\nAs in other chronic pain conditions, these aspects should be more explored in EDS hypermobility-type, and integrated into chronic pain prevention and management programs. Implications for Rehabilitation Clinicians should be aware that joint hypermobility may be associated with other health problems, and in its presence suspect a heritable disorder of connective tissue such as the Ehlers-Danlos syndrome (EDS) hypermobility-type, in which chronic pain is one of the most frequent and invalidating symptoms. It is necessary to explore the psychosocial functioning of patients as part of the overall chronic pain management in the EDS hypermobility-type, especially when they do not respond to biomedical approaches as psychological factors may be operating against rehabilitation. Further research on the psychological factors linked to pain chronicity and disability in the EDS hypermobility-type is needed.',\n",
       " '7d2fda30e52c39431dbb90ae065da036a55acdc7': 'Research has indicated that multiple sets are superior to single sets for maximal strength development. However, whether maximal strength gains are achieved may depend on the ability to sustain a consistent number of repetitions over consecutive sets. A key factor that determines the ability to sustain repetitions is the length of rest interval between sets. The length of the rest interval is commonly prescribed based on the training goal, but may vary based on several other factors. The purpose of this review was to discuss these factors in the context of different training goals. When training for muscular strength, the magnitude of the load lifted is a key determinant of the rest interval prescribed between sets. For loads less than 90% of 1 repetition maximum, 3-5 minutes rest between sets allows for greater strength increases through the maintenance of training intensity. However, when testing for maximal strength, 1-2 minutes rest between sets might be sufficient between repeated attempts. When training for muscular power, a minimum of 3 minutes rest should be prescribed between sets of repeated maximal effort movements (e.g., plyometric jumps). When training for muscular hypertrophy, consecutive sets should be performed prior to when full recovery has taken place. Shorter rest intervals of 30-60 seconds between sets have been associated with higher acute increases in growth hormone, which may contribute to the hypertrophic effect. When training for muscular endurance, an ideal strategy might be to perform resistance exercises in a circuit, with shorter rest intervals (e.g., 30 seconds) between exercises that involve dissimilar muscle groups, and longer rest intervals (e.g., 3 minutes) between exercises that involve similar muscle groups. In summary, the length of the rest interval between sets is only 1 component of a resistance exercise program directed toward different training goals. Prescribing the appropriate rest interval does not ensure a desired outcome if other components such as intensity and volume are not prescribed appropriately.',\n",
       " 'fe0643f3405c22fe7ca0b7d1274a812d6e3e5a11': \"Since Cree, Inc.'s 2<sup>nd</sup> generation 4H-SiC MOSFETs were commercially released with a specific on-resistance (R<sub>ON, SP</sub>) of 5 mΩ·cm<sup>2</sup> for a 1200 V-rating in early 2013, we have further optimized the device design and fabrication processes as well as greatly expanded the voltage ratings from 900 V up to 15 kV for a much wider range of high-power, high-frequency, and high-voltage energy-conversion and transmission applications. Using these next-generation SiC MOSFETs, we have now achieved new breakthrough performance for voltage ratings from 900 V up to 15 kV with a R<sub>ON, SP</sub> as low as 2.3 mΩ·cm<sup>2</sup> for a breakdown voltage (BV) of 1230 V and 900 V-rating, 2.7 mΩ·cm<sup>2</sup> for a BV of 1620 V and 1200 V-rating, 3.38 mΩ·cm<sup>2</sup> for a BV of 1830 V and 1700 V-rating, 10.6 mΩ·cm<sup>2</sup> for a BV of 4160 V and 3300 V-rating, 123 mΩ·cm<sup>2</sup> for a BV of 12 kV and 10 kV-rating, and 208 mΩ·cm<sup>2</sup> for a BV of 15.5 kV and 15 kV-rating. In addition, due to the lack of current tailing during the bipolar device switching turn-off, the SiC MOSFETs reported in this work exhibit incredibly high frequency switching performance over their silicon counter parts.\",\n",
       " '011d4ccb74f32f597df54ac8037a7903bd95038b': 'Skin color is one of the most conspicuous ways in which humans vary and has been widely used to define human races. Here we present new evidence indicating that variations in skin color are adaptive, and are related to the regulation of ultraviolet (UV) radiation penetration in the integument and its direct and indirect effects on fitness. Using remotely sensed data on UV radiation levels, hypotheses concerning the distribution of the skin colors of indigenous peoples relative to UV levels were tested quantitatively in this study for the first time. The major results of this study are: (1) skin reflectance is strongly correlated with absolute latitude and UV radiation levels. The highest correlation between skin reflectance and UV levels was observed at 545 nm, near the absorption maximum for oxyhemoglobin, suggesting that the main role of melanin pigmentation in humans is regulation of the effects of UV radiation on the contents of cutaneous blood vessels located in the dermis. (2) Predicted skin reflectances deviated little from observed values. (3) In all populations for which skin reflectance data were available for males and females, females were found to be lighter skinned than males. (4) The clinal gradation of skin coloration observed among indigenous peoples is correlated with UV radiation levels and represents a compromise solution to the conflicting physiological requirements of photoprotection and vitamin D synthesis. The earliest members of the hominid lineage probably had a mostly unpigmented or lightly pigmented integument covered with dark black hair, similar to that of the modern chimpanzee. The evolution of a naked, darkly pigmented integument occurred early in the evolution of the genus Homo. A dark epidermis protected sweat glands from UV-induced injury, thus insuring the integrity of somatic thermoregulation. Of greater significance to individual reproductive success was that highly melanized skin protected against UV-induced photolysis of folate (Branda & Eaton, 1978, Science201, 625-626; Jablonski, 1992, Proc. Australas. Soc. Hum. Biol.5, 455-462, 1999, Med. Hypotheses52, 581-582), a metabolite essential for normal development of the embryonic neural tube (Bower & Stanley, 1989, The Medical Journal of Australia150, 613-619; Medical Research Council Vitamin Research Group, 1991, The Lancet338, 31-37) and spermatogenesis (Cosentino et al., 1990, Proc. Natn. Acad. Sci. U.S.A.87, 1431-1435; Mathur et al., 1977, Fertility Sterility28, 1356-1360).As hominids migrated outside of the tropics, varying degrees of depigmentation evolved in order to permit UVB-induced synthesis of previtamin D(3). The lighter color of female skin may be required to permit synthesis of the relatively higher amounts of vitamin D(3)necessary during pregnancy and lactation. Skin coloration in humans is adaptive and labile. Skin pigmentation levels have changed more than once in human evolution. Because of this, skin coloration is of no value in determining phylogenetic relationships among modern human groups.',\n",
       " 'd87d70ecd0fdf0976cebbeaeacf25ad9872ffde1': 'Watermarking is used to protect the copyrighted materials from being misused and help us to know the lawful ownership. The security of any watermarking scheme is always a prime concern for the developer. In this work, the robustness and security issue of IWT (integer wavelet transform) and SVD (singular value decomposition) based watermarking is explored. Generally, SVD based watermarking techniques suffer with an issue of false positive problem. This leads to even authenticating the wrong owner. We are proposing a novel solution to this false positive problem; that arises in SVD based approach. Firstly, IWT is employed on the host image and then SVD is performed on this transformed host. The properties of IWT and SVD help in achieving high value of robustness. Singular values are used for the watermark embedding. In order to further improve the quality of watermarking, the optimization of scaling factor (mixing ratio) is performed with the help of artificial bee colony (ABC) algorithm. A comparison with other schemes is performed to show the superiority of proposed scheme. & 2015 Elsevier Ltd. All rights reserved.',\n",
       " 'ae3ebe6c69fdb19e12d3218a5127788fae269c10': 'Test functions are important to validate and compare the performance of optimization algorithms. There have been many test or benchmark functions reported in the literature; however, there is no standard list or set of benchmark functions. Ideally, test functions should have diverse properties so that can be truly useful to test new algorithms in an unbiased way. For this purpose, we have reviewed and compiled a rich set of 175 benchmark functions for unconstrained optimization problems with diverse properties in terms of modality, separability, and valley landscape. This is by far the most complete set of functions so far in the literature, and tt can be expected this complete set of functions can be used for validation of new optimization in the future.',\n",
       " 'd28235adc2c8c6fdfaa474bc2bab931129149fd6': 'In this article, three approaches are proposed for measuring difficulty that can be useful in developing Dynamic Difficulty Adjustment (DDA) systems in different game genres. Our analysis of the existing DDA systems shows that there are three ways to measure the difficulty of the game: using the formal model of gameplay, using the features of the game, and direct examination of the player. These approaches are described in this article and supplemented by appropriate examples of DDA implementations. In addition, the article describes the distinction between task complexity and task difficulty in DDA systems. Separating task complexity (especially the structural one) is suggested, which is an objective characteristic of the task, and task difficulty, which is related to the interaction between the task and the task performer.',\n",
       " '5c881260bcc64070b2b33c10d28f23f793b8344f': 'The demand for low voltage, low drop-out (LDO) regulators is increasing because of the growing demand for portable electronics, i.e., cellular phones, pagers, laptops, etc. LDOs are used coherently with dc-dc converters as well as standalone parts. In power supply systems, they are typically cascaded onto switching regulators to suppress noise and provide a low noise output. The need for low voltage is innate to portable low power devices and corroborated by lower breakdown voltages resulting from reductions in feature size. Low quiescent current in a battery operated system is an intrinsic performance parameter because it partially determines battery life. This paper discusses some techniques that enable the practical realizations of low quiescent current LDOs at low voltages and in existing technologies. The proposed circuit exploits the frequency response dependence on load-current to minimize quiescent current flow. Moreover, the output current capabilities of MOS power transistors are enhanced and drop-out voltages are decreased for a given device size. Other applications, like dc-dc converters, can also reap the benefits of these enhanced MOS devices. An LDO prototype incorporating the aforementioned techniques was fabricated. The circuit was operable down to input voltages of 1 V with a zero-load quiescent current flow of 23 μA. Moreover, the regulator provided 18 and 50 mA of output current at input voltages of 1 and 1.2 V respectively.',\n",
       " '950ff860dbc8a24fc638ac942ce9c1f51fb24899': 'Next Point-of-Interest (POI) recommendation is of great value for both location-based service providers and users. Recently Recurrent Neural Networks (RNNs) have been proved to be effective on sequential recommendation tasks. However, existing RNN solutions rarely consider the spatio-temporal intervals between neighbor checkins, which are essential for modeling user check-in behaviors in next POI recommendation. In this paper, we propose a new variant of LSTM, named STLSTM, which implements time gates and distance gates into LSTM to capture the spatio-temporal relation between successive check-ins. Specifically, one time gate and one distance gate are designed to control short-term interest update, and another time gate and distance gate are designed to control long-term interest update. Furthermore, to reduce the number of parameters and improve efficiency, we further integrate coupled input and forget gates with our proposed model. Finally, we evaluate the proposed model using four real-world datasets from various location-based social networks. Our experimental results show that our model significantly outperforms the state-of-the-art approaches for next POI recommendation.',\n",
       " 'f99a50ce62845c62d9fcdec277e0857350534cc9': 'A novel design of an absorptive frequency-selective transmission structure (AFST) is proposed. This structure is based on the design of a frequency-dependent lossy layer with square-loop hybrid resonator (SLHR). The parallel resonance provided by the hybrid resonator is utilized to bypass the lossy path and improve the insertion loss. Meanwhile, the series resonance of the hybrid resonator is used for expanding the upper absorption bandwidth. Furthermore, the absorption for out-of-band frequencies is achieved by using four metallic strips with lumped resistors, which are connected with the SLHR. The quantity of lumped elements required in a unit cell can be reduced by at least 50% compared to previous structures. The design guidelines are explained with the aid of an equivalent circuit model. Both simulation and experiment results are presented to demonstrate the performance of our AFST. It is shown that an insertion loss of 0.29 dB at 6.1 GHz and a 112.4% 10 dB reflection reduction bandwidth are obtained under the normal incidence.',\n",
       " '26f70336acf7247a35d3c0be6308fe29f25d2872': 'Evaluation of the Advanced Encryption Standard (AES) algorithm in FPGA is proposed here. This Evaluation is compared with other works to show the efficiency. Here we are concerned about two major purposes. The first is to define some of the terms and concepts behind basic cryptographic methods, and to offer a way to compare the myriad cryptographic schemes in use today. The second is to provide some real examples of cryptography in use today. The design uses an iterative looping approach with block and key size of 128 bits, lookup table implementation of S-box. This gives low complexity architecture and easily achieves low latency as well as high throughput. Simulation results, performance results are presented and compared with previous reported designs. Since its acceptance as the adopted symmetric-key algorithm, the Advanced Encryption Standard (AES) and its recently standardized authentication Galois/Counter Mode (GCM) have been utilized in various security-constrained applications. Many of the AES-GCM applications are power and resource constrained and requires efficient hardware implementations. In this project, AES-GCM algorithms are evaluated and optimized to identify the high-performance and low-power architectures. The Advanced Encryption Standard (AES) is a specification for the encryption of electronic data. The Cipher Block Chaining (CBC) mode is a confidentiality mode whose encryption process features the combining (“chaining”) of the plaintext blocks with the previous Cipher text blocks. The CBC mode requires an IV to combine with the first plaintext block. The IV need not be secret, but it must be unpredictable. Also, the integrity of the IV should be protected. Galois/Counter Mode (GCM) is a block cipher mode of operation that uses universal hashing over a binary Galois field to provide authenticated encryption. Galois Hash is used for authentication, and the Advanced Encryption Standard (AES) block cipher is used for encryption in counter mode of operation. To obtain the least-complexity S-box, the formulations for the Galois Field (GF) sub-field inversions in GF (24) are optimized By conducting exhaustive simulations for the input transitions, we analyze the synthesis of the AES S-boxes considering the switching activities, gate-level net lists, and parasitic information. Finally, by implementation of AES-GCM the high-performance GF (2128) multiplier architectures, gives the detailed information of its performance. An optimized coding for the implementation of Advanced Encryption Standard-Galois Counter Mode has been developed. The speed factor of the algorithm implementation has been targeted and a software code in Verilog HDL has been developed. This implementation is useful in wireless security like military communication and mobile telephony where there is a grayer emphasis on the speed of communication.',\n",
       " '03f64a5989e4d2ecab989d9724ad4cc58f976daf': 'Most of the existing multi-document summarization methods decompose the documents into sentences and work directly in the sentence space using a term-sentence matrix. However, the knowledge on the document side, i.e. the topics embedded in the documents, can help the context understanding and guide the sentence selection in the summarization procedure. In this paper, we propose a new Bayesian sentence-based topic model for summarization by making use of both the term-document and term-sentence associations. An efficient variational Bayesian algorithm is derived for model parameter estimation. Experimental results on benchmark data sets show the effectiveness of the proposed model for the multi-document summarization task.',\n",
       " '9a1b3247fc7f0abf892a40884169e0ed10d3b684': 'The popularity of using Internet contains some risks of network attacks. Intrusion detection is one major research problem in network security, whose aim is to identify unusual access or attacks to secure internal networks. In literature, intrusion detection systems have been approached by various machine learning techniques. However, there is no a review paper to examine and understand the current status of using machine learning techniques to solve the intrusion detection problems. This chapter reviews 55 related studies in the period between 2000 and 2007 focusing on developing single, hybrid, and ensemble classifiers. Related studies are compared by their classifier design, datasets used, and other experimental setups. Current achievements and limitations in developing intrusion detection systems by machine learning are present and discussed. A number of future research directions are also provided. 2009 Elsevier Ltd. All rights reserved.',\n",
       " 'a10d128fd95710308dfee83953c5b26293b9ede7': 'Software Defined Networks (SDNs) based on the OpenFlow (OF) protocol export controlplane programmability of switched substrates. As a result, rich functionality in traffic management, load balancing, routing, firewall configuration, etc. that may pertain to specific flows they control, may be easily developed. In this paper we extend these functionalities with an efficient and scalable mechanism for performing anomaly detection and mitigation in SDN architectures. Flow statistics may reveal anomalies triggered by large scale malicious events (typically massive Distributed Denial of Service attacks) and subsequently assist networked resource owners/operators to raise mitigation policies against these threats. First, we demonstrate that OF statistics collection and processing overloads the centralized control plane, introducing scalability issues. Second, we propose a modular architecture for the separation of the data collection process from the SDN control plane with the employment of sFlow monitoring data. We then report experimental results that compare its performance against native OF approaches that use standard flow table statistics. Both alternatives are evaluated using an entropy-based method on high volume real network traffic data collected from a university campus network. The packet traces were fed to hardware and software OF devices in order to assess flow-based datagathering and related anomaly detection options. We subsequently present experimental results that demonstrate the effectiveness of the proposed sFlow-based mechanism compared to the native OF approach, in terms of overhead imposed on usage of system resources. Finally, we conclude by demonstrating that once a network anomaly is detected and identified, the OF protocol can effectively mitigate it via flow table modifications. 2013 Elsevier B.V. All rights reserved.',\n",
       " 'c84b10c01a84f26fe8a1c978c919fbe5a9f9a661': 'The Internet has led to the creation of a digital society, where (almost) everything is connected and is accessible from anywhere. However, despite their widespread adoption, traditional IP networks are complex and very hard to manage. It is both difficult to configure the network according to predefined policies, and to reconfigure it to respond to faults, load, and changes. To make matters even more difficult, current networks are also vertically integrated: the control and data planes are bundled together. Software-defined networking (SDN) is an emerging paradigm that promises to change this state of affairs, by breaking vertical integration, separating the network’s control logic from the underlying routers and switches, promoting (logical) centralization of network control, and introducing the ability to program the network. The separation of concerns, introduced between the definition of network policies, their implementation in switching hardware, and the forwarding of traffic, is key to the desired flexibility: by breaking the network control problem into tractable pieces, SDN makes it easier to create and introduce new abstractions in networking, simplifying network management and facilitating network evolution. In this paper, we present a comprehensive survey on SDN. We start by introducing the motivation for SDN, explain its main concepts and how it differs from traditional networking, its roots, and the standardization activities regarding this novel paradigm. Next, we present the key building blocks of an SDN infrastructure using a bottom-up, layered approach. We provide an in-depth analysis of the hardware infrastructure, southbound and northbound application programming interfaces (APIs), network virtualization layers, network operating systems (SDN controllers), network programming languages, and network applications. We also look at cross-layer problems such as debugging and troubleshooting. In an effort to anticipate the future evolution of this new paradigm, we discuss the main ongoing research efforts and challenges of SDN. In particular, we address the design of switches and control platforms with a focus on aspects such as resiliency, scalability, performance, security, and dependabilityVas well as new opportunities for carrier transport networks and cloud providers. Last but not least, we analyze the position of SDN as a key enabler of a software-defined',\n",
       " '1821fbfc03a45af816a8d7aef50321654b0aeec0': 'Despite their exponential growth, home and small office/home office networks continue to be poorly managed. Consequently, security of hosts in most home networks is easily compromised and these hosts are in turn used for largescale malicious activities without the home users’ knowledge. We argue that the advent of Software Defined Networking (SDN) provides a unique opportunity to effectively detect and contain network security problems in home and home office networks. We show how four prominent traffic anomaly detection algorithms can be implemented in an SDN context using Openflow compliant switches and NOX as a controller. Our experiments indicate that these algorithms are significantly more accurate in identifying malicious activities in the home networks as compared to the ISP. Furthermore, the efficiency analysis of our SDN implementations on a programmable home network router indicates that the anomaly detectors can operate at line rates without introducing any performance penalties for the home network traffic.',\n",
       " '3192a953370bc8bf4b906261e8e2596355d2b610': 'Today\\'s data networks are surprisingly fragile and difficult to manage. We argue that the root of these problems lies in the complexity of the control and management planes--the software and protocols coordinating network elements--and particularly the way the decision logic and the distributed-systems issues are inexorably intertwined. We advocate a complete refactoring of the functionality and propose three key principles--network-level objectives, network-wide views, and direct control--that we believe should underlie a new architecture. Following these principles, we identify an extreme design point that we call \"4D,\" after the architecture\\'s four planes: decision, dissemination, discovery, and data. The 4D architecture completely separates an AS\\'s decision logic from pro-tocols that govern the interaction among network elements. The AS-level objectives are specified in the decision plane, and en-forced through direct configuration of the state that drives how the data plane forwards packets. In the 4D architecture, the routers and switches simply forward packets at the behest of the decision plane, and collect measurement data to aid the decision plane in controlling the network. Although 4D would involve substantial changes to today\\'s control and management planes, the format of data packets does not need to change; this eases the deployment path for the 4D architecture, while still enabling substantial innovation in network control and management. We hope that exploring an extreme design point will help focus the attention of the research and industrial communities on this crucially important and intellectually challenging area.',\n",
       " '883e3a3950968ebf8d03d3281076671538660c7c': 'Urban land use information plays an essential role in a wide variety of urban planning and environmental monitoring processes. During the past few decades, with the rapid technological development of remote sensing (RS), geographic information systems (GIS) and geospatial big data, numerous methods have been developed to identify urban land use at a fine scale. Points-of-interest (POIs) have been widely used to extract information pertaining to urban land use types and functional zones. However, it is difficult to quantify the relationship between spatial distributions of POIs and regional land use types due to a lack of reliable models. Previous methods may ignore abundant spatial features that can be extracted from POIs. In this study, we establish an innovative framework that detects urban land use distributions at the scale of traffic analysis zones (TAZs) by integrating Baidu POIs and a Word2Vec model. This framework was implemented using a Google open-source model of a deep-learning language in 2013. First, data for the Pearl River Delta (PRD) are transformed into a TAZ-POI corpus using a greedy algorithmby considering the spatial distributions of TAZs and inner POIs. Then, high-dimensional characteristic vectors of POIs and TAZs are extracted using the Word2Vec model. Finally, to validate the reliability of the POI/TAZ vectors, we implement a K-Means-based clustering model to analyze correlations between the POI/TAZ vectors and deploy TAZ vectors to identify urban land use types using a random forest algorithm (RFA) model. Compared with some state-of-the-art probabilistic topic models (PTMs), the proposed method can efficiently obtain the highest accuracy (OA = 0.8728, kappa = 0.8399). Moreover, the results can be used to help urban planners to monitor dynamic urban land use and evaluate the impact of urban planning schemes. ARTICLE HISTORY Received 21 March 2016 Accepted 28 September 2016',\n",
       " 'b0f7423f93e7c6e506c115771ef82440077a732a': 'As the number of electronic components of avionics systems is significantly increasing, it is desirable to run several avionics software on a single computing device. In such system, providing a seamless way to integrate separate applications on a computing device is a very critical issue as the Integrated Modular Avionics (IMA) concept addresses. In this context, the ARINC 653 standard defines resource partitioning of avionics application software. The virtualization technology has very high potential of providing an optimal implementation of the partition concept. In this paper, we study supports for full virtualization based ARINC 653 partitioning. The supports include extension of XML-based configuration file format and hierarchical scheduler for temporal partitioning. We show that our implementation can support well-known VMMs, such as VirtualBox and VMware and present basic performance numbers.',\n",
       " '5fa463ad51c0fda19cf6a32d851a12eec5e872b1': 'With the increase of terrorist threats around the world, human identification research has become a sought after area of research. Unlike standard biometric recognition techniques, gait recognition is a non-intrusive technique. Both data collection and classification processes can be done without a subject’s cooperation. In this paper, we propose a new model-based gait recognition technique called postured-based gait recognition. It consists of two elements: posture-based features and posture-based classification. Posture-based features are composed of displacements of all joints between current and adjacent frames and center-of-body (CoB) relative coordinates of all joints, where the coordinates of each joint come from its relative position to four joints: hip-center, hip-left, hip-right, and spine joints, from the front forward. The CoB relative coordinate system is a critical part to handle the different observation angle issue. In posture-based classification, postured-based gait features of all frames are considered. The dominant subject becomes a classification result. The postured-based gait recognition technique outperforms the existing techniques in both fixed direction and freestyle walk scenarios, where turning around and changing directions are involved. This suggests that a set of postures and quick movements are sufficient to identify a person. The proposed technique also performs well under the gallery-size test and the cumulative match characteristic test, which implies that the postured-based gait recognition technique is not gallery-size sensitive and is a good potential tool for forensic and surveillance use.',\n",
       " '602f775577a5458e8b6c5d5a3cdccc7bb183662c': 'This study compared the nature of text comprehension as measured by multiple-choice format and open-ended format questions. Participants read a short text while explaining preselected sentences. After reading the text, participants answered open-ended and multiple-choice versions of the same questions based on their memory of the text content. The results indicated that performance on open-ended questions was correlated with the quality of self-explanations, but performance on multiple-choice questions was correlated with the level of prior knowledge related to the text. These results suggest that open-ended and multiple-choice format questions measure different aspects of comprehension processes. The results are discussed in terms of dual process theories of text comprehension.',\n",
       " 'ebeca41ac60c2151137a45fcc5d1a70a419cad65': 'Availability of large volume of community contributed location data enables a lot of location providing services and these services have attracted many industries and academic researchers by its importance. In this paper we propose the new recommender system that recommends the new POI for next hours. First we find the users with similar check-in sequences and depict their check-in sequences as a directed graph, then find the users current location. To recommend the new POI recommendation for next hour we refer to the directed graph we have created. Our algorithm considers both the temporal factor i.e., recommendation time, and the spatial(distance) at the same time. We conduct an experiment on random data collected from Foursquare and Gowalla. Experiment results show that our proposed model outperforms the collaborative-filtering based state-of-the-art recommender techniques.',\n",
       " '08952d434a9b6f1dc9281f2693b2dd855edcda6b': 'This paper presents SiRiUS, a secure file system designed to be layered over insecure network and P2P file systems such as NFS, CIFS, OceanStore, and Yahoo! Briefcase. SiRiUS assumes the network storage is untrusted and provides its own read-write cryptographic access control for file level sharing. Key management and revocation is simple with minimal out-of-band communication. File system freshness guarantees are supported by SiRiUS using hash tree constructions. SiRiUS contains a novel method of performing file random access in a cryptographic file system without the use of a block server. Extensions to SiRiUS include large scale group sharing using the NNL key revocation construction. Our implementation of SiRiUS performs well relative to the underlying file system despite using cryptographic operations.',\n",
       " 'adeca3a75008d92cb52f5f2561dda7005a8814a4': '0957-4174/$ see front matter 2012 Elsevier Ltd. A http://dx.doi.org/10.1016/j.eswa.2012.12.089 ⇑ Corresponding author. Tel.: +44 23 92 844171. E-mail addresses: Alessio.Ishizaka@port.ac.uk (A. I com (N.H. Nguyen). Fuzzy AHP is a hybrid method that combines Fuzzy Set Theory and AHP. It has been developed to take into account uncertainty and imprecision in the evaluations. Fuzzy Set Theory requires the definition of a membership function. At present, there are no indications of how these membership functions can be constructed. In this paper, a way to calibrate the membership functions with comparisons given by the decision-maker on alternatives with known measures is proposed. This new technique is illustrated in a study measuring the most important factors in selecting a student current account. 2012 Elsevier Ltd. All rights reserved.',\n",
       " '539b15c0215582d12e2228d486374651c21ac75d': 'The automation of the overtaking maneuver is considered to be one of the toughest challenges in the development of autonomous vehicles. This operation involves two vehicles (the overtaking and the overtaken) cooperatively driving, as well as the surveillance of any other vehicles that are involved in the maneuver. This operation consists of two lane changes-one from the right to the left lane of the road, and the other is to return to the right lane after passing. Lane-change maneuvers have been used to move into or out of a circulation lane or platoon; however, overtaking operations have not received much coverage in the literature. In this paper, we present an overtaking system for autonomous vehicles equipped with path-tracking and lane-change capabilities. The system uses fuzzy controllers that mimic human behavior and reactions during overtaking maneuvers. The system is based on the information that is supplied by a high-precision Global Positioning System and a wireless network environment. It is able to drive an automated vehicle and overtake a second vehicle that is driving in the same lane of the road.',\n",
       " 'a306754e556446a5199e258f464fd6e26be547fe': 'Liposuction alone is not always sufficient to correct the shape of the lower leg, and muscle reduction may be necessary. To assess the outcomes of a new technique of selective neurectomy of the gastrocnemius muscle to correct calf hypertrophy. Between October 2007 and May 2010, 300 patients underwent neurectomy of the medial and lateral heads of the gastrocnemius muscle at the Department of\\xa0Cosmetic and Plastic Surgery, the Second People’s Hospital of Guangdong Province (Guangzhou, China) to correct the shape of their lower legs. Follow-up data from these 300 patients were analyzed retrospectively. Cosmetic results were evaluated independently by the surgeon, the patient, and a third party. Preoperative and postoperative calf circumferences were compared. The Fugl-Meyer motor function assessment was evaluated 3\\xa0months after surgery. The average reduction in calf circumference was 3.2\\xa0±\\xa01.2\\xa0cm. The Fugl-Meyer scores were normal in all patients both before and 3\\xa0months after surgery. A normal calf shape was achieved in all patients. Six patients complained of fatigue while walking and four of scar pigmentation, but in all cases, this resolved within 6\\xa0months. Calf asymmetry was observed in only two patients. The present series suggests that neurectomy of the medial and lateral heads of the gastrocnemius muscle may be safe and effective for correcting the shape of the calves. This journal requires that authors assign a level of evidence to each article. For a full description of these Evidence-Based Medicine ratings, please refer to the Table of Contents or the online Instructions to Authors www.springer.com/00266 .',\n",
       " 'f31e0932a2f35a6d7feff20977ce08b5b5398c60': 'Tendons consist of collagen (mostly type I collagen) and elastin embedded in a proteoglycan-water matrix with collagen accounting for 65-80% and elastin approximately 1-2% of the dry mass of the tendon. These elements are produced by tenoblasts and tenocytes, which are the elongated fibroblasts and fibrocytes that lie between the collagen fibers, and are organized in a complex hierarchical scheme to form the tendon proper. Soluble tropocollagen molecules form cross-links to create insoluble collagen molecules which then aggregate progressively into microfibrils and then into electronmicroscopically clearly visible units, the collagen fibrils. A bunch of collagen fibrils forms a collagen fiber, which is the basic unit of a tendon. A fine sheath of connective tissue called endotenon invests each collagen fiber and binds fibers together. A bunch of collagen fibers forms a primary fiber bundle, and a group of primary fiber bundles forms a secondary fiber bundle. A group of secondary fiber bundles, in turn, forms a tertiary bundle, and the tertiary bundles make up the tendon. The entire tendon is surrounded by a fine connective tissue sheath called epitenon. The three-dimensional ultrastructure of tendon fibers and fiber bundles is complex. Within one collagen fiber, the fibrils are oriented not only longitudinally but also transversely and horizontally. The longitudinal fibers do not run only parallel but also cross each other, forming spirals. Some of the individual fibrils and fibril groups form spiral-type plaits. The basic function of the tendon is to transmit the force created by the muscle to the bone, and, in this way, make joint movement possible. The complex macro- and microstructure of tendons and tendon fibers make this possible. During various phases of movements, the tendons are exposed not only to longitudinal but also to transversal and rotational forces. In addition, they must be prepared to withstand direct contusions and pressures. The above-described three-dimensional internal structure of the fibers forms a buffer medium against forces of various directions, thus preventing damage and disconnection of the fibers.',\n",
       " '6939327c1732e027130f0706b6279f78b8ecd2b7': 'Cloud computing is expected to be a promising solution for scientific computing. In this paper, we propose a flexible container-based computing platform to run scientific workflows on cloud. We integrate Galaxy, a popular biology workflow system, with four famous container cluster systems. Preliminary evaluation shows that container cluster systems introduce negligible performance overhead for data intensive scientific workflows, meanwhile, they are able to solve tool installation problem, guarantee reproducibility and improve resource utilization. Moreover, we implement four ways of using Docker, the most popular container tool, for our platform. Docker in Docker and Sibling Docker, which run everything within containers, both help scientists easily deploy our platform on any clouds in a few minutes.',\n",
       " '545dd72cd0357995144bb19bef132bcc67a52667': 'Voiced-Unvoiced classification (V-UV) is a well understood but still not perfectly solved problem. It tackles the problem of determining whether a signal frame contains harmonic content or not. This paper presents a new approach to this problem using a conventional multi-layer perceptron neural network trained with linear predictive coding (LPC) coefficients. LPC is a method that results in a number of coefficients that can be transformed to the envelope of the spectrum of the input frame. As a spectrum is suitable for determining the harmonic content, so are the LPC-coefficients. The proposed neural network works reasonably well compared to other approaches and has been evaluated on a small dataset of 4 different speakers.',\n",
       " '89cbcc1e740a4591443ff4765a6ae8df0fdf5554': \"What is the difference between Piaget's constructivism and Papert’s “constructionism”? Beyond the mere play on the words, I think the distinction holds, and that integrating both views can enrich our understanding of how people learn and grow. Piaget’s constructivism offers a window into what children are interested in, and able to achieve, at different stages of their development. The theory describes how children’s ways of doing and thinking evolve over time, and under which circumstance children are more likely to let go of—or hold onto— their currently held views. Piaget suggests that children have very good reasons not to abandon their worldviews just because someone else, be it an expert, tells them they’re wrong. Papert’s constructionism, in contrast, focuses more on the art of learning, or ‘learning to learn’, and on the significance of making things in learning. Papert is interested in how learners engage in a conversation with [their own or other people’s] artifacts, and how these conversations boost self-directed learning, and ultimately facilitate the construction of new knowledge. He stresses the importance of tools, media, and context in human development. Integrating both perspectives illuminates the processes by which individuals come to make sense of their experience, gradually optimizing their interactions with the world\",\n",
       " '19c05a149bb20f27dd0eca0ec3ac847390b2d100': 'Distant speech recognition (DSR) holds out the promise of providing a natural human computer interface in that it enables verbal interactions with computers without the necessity of donning intrusive body- or head-mounted devices. Recognizing distant speech robustly, however, remains a challenge. This paper provides a overview of DSR systems based on microphone arrays. In particular, we present recent work on acoustic beamforming for DSR, along with experimental results verifying the effectiveness of the various algorithms described here; beginning from a word error rate (WER) of 14.3% with a single microphone of a 64-channel linear array, our state-of-the-art DSR system achieved a WER of 5.3%, which was comparable to that of 4.2% obtained with a lapel microphone. Furthermore, we report the results of speech recognition experiments on data captured with a popular device, the Kinect [1]. Even for speakers at a distance of four meters from the Kinect, our DSR system achieved acceptable recognition performance on a large vocabulary task, a WER of 24.1%, beginning from a WER of 42.5% with a single array channel.',\n",
       " '142bd1d4e41e5e29bdd87e0d5a145f3c708a3f44': 'This paper describes a data set collected by an autonomous ground vehicle testbed, based upon a modified Ford F-250 pickup truck. The vehicle is outfitted with a professional (Applanix POS-LV) and consumer (Xsens MTi-G) inertial measurement unit (IMU), a Velodyne 3D-lidar scanner, two push-broom forward looking Riegl lidars, and a Point Grey Ladybug3 omnidirectional camera system. Here we present the time-registered data from these sensors mounted on the vehicle, collected while driving the vehicle around the Ford Research campus and downtown Dearborn, Michigan during NovemberDecember 2009. The vehicle path trajectory in these data sets contain several large and small-scale loop closures, which should be useful for testing various state of the art computer vision and simultaneous localization and mapping (SLAM) algorithms. Fig. 1. The modified Ford F-250 pickup truck.',\n",
       " '1de3c8ddf30b9d6389aebc3bfa8a02a169a7368b': 'Graph mining is a challenging task by itself, and even more so when processing data streams which evolve in real-time. Data stream mining faces hard constraints regarding time and space for processing, and also needs to provide for concept drift detection. In this paper we present a framework for studying graph pattern mining on time-varying streams. Three new methods for mining frequent closed subgraphs are presented. All methods work on coresets of closed subgraphs, compressed representations of graph sets, and maintain these sets in a batch-incremental manner, but use different approaches to address potential concept drift. An evaluation study on datasets comprising up to four million graphs explores the strength and limitations of the proposed methods. To the best of our knowledge this is the first work on mining frequent closed subgraphs in non-stationary data streams.',\n",
       " '31ea3186aa7072a9e25218efe229f5ee3cca3316': 'Sequence-to-sequence models have shown promising improvements on the temporal task of video captioning, but they optimize word-level cross-entropy loss during training. First, using policy gradient and mixed-loss methods for reinforcement learning, we directly optimize sentence-level task-based metrics (as rewards), achieving significant improvements over the baseline, based on both automatic metrics and human evaluation on multiple datasets. Next, we propose a novel entailment-enhanced reward (CIDEnt) that corrects phrase-matching based metrics (such as CIDEr) to only allow for logically-implied partial matches and avoid contradictions, achieving further significant improvements over the CIDEr-reward model. Overall, our CIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.',\n",
       " '4b944d518b88beeb9b2376975400cabd6e919957': 'The imminent arrival of the Internet of Things (IoT), which consists of a vast number of devices with heterogeneous characteristics, means that future networks need a new architecture to accommodate the expected increase in data generation. Software defined networking (SDN) and network virtualization (NV) are two technologies that promise to cost-effectively provide the scale and versatility necessary for IoT services. In this paper, we survey the state of the art on the application of SDN and NV to IoT. To the best of our knowledge, we are the first to provide a comprehensive description of every possible IoT implementation aspect for the two technologies. We start by outlining the ways of combining SDN and NV. Subsequently, we present how the two technologies can be used in the mobile and cellular context, with emphasis on forthcoming 5G networks. Afterward, we move to the study of wireless sensor networks, arguably the current foremost example of an IoT network. Finally, we review some general SDN-NV-enabled IoT architectures, along with real-life deployments and use-cases. We conclude by giving directions for future research on this topic.',\n",
       " 'fa16642fe405382cbd407ce1bc22213561185aba': 'This study helps in monitoring blood glucose level of a patient with the aid of an android device non-invasively. Diabetes is a metabolic disease characterized by high level of sugar in the blood, and considered as the fastest growing long-term disease affecting millions of people globally. The study measures the blood glucose level using sensor patch through diffused reflectance spectra on the inner side of the forearm. The Arduino microcontroller does the processing of the information from the sensor patch while the Bluetooth module wirelessly transmits to the android device the measured glucose level for storing, interpreting and displaying. Results showed that there is no significant between the measured values using the commercially available glucose meter and the created device. Based on ISO 15197 standard 39 of the 40 trials conducted, or 97.5% fell within the acceptable range.',\n",
       " 'a360a526794df3aa8de96f83df171769a4022642': 'Learning a good representation of text is key to many recommendation applications. Examples include news recommendation where texts to be recommended are constantly published everyday. However, most existing recommendation techniques, such as matrix factorization based methods, mainly rely on interaction histories to learn representations of items. While latent factors of items can be learned e\\x82ectively from user interaction data, in many cases, such data is not available, especially for newly emerged items. In this work, we aim to address the problem of personalized recommendation for completely new items with text information available. We cast the problem as a personalized text ranking problem and propose a general framework that combines text embedding with personalized recommendation. Users and textual content are embedded into latent feature space. \\x8ce text embedding function can be learned end-to-end by predicting user interactions with items. To alleviate sparsity in interaction data, and leverage large amount of text data with li\\x8ale or no user interactions, we further propose a joint text embedding model that incorporates unsupervised text embedding with a combination module. Experimental results show that our model can signi\\x80cantly improve the e\\x82ectiveness of recommendation systems on real-world datasets.',\n",
       " '1aa60b5ae893cd93a221bf71b6b264f5aa5ca6b8': 'As humans, we have expectations for the results of any action, e.g. we expect at least one student to be returned when we query a university database for student records. When these expectations are not met, traditional database users often explore datasets via a series of slightly altered SQL queries. Yet most database access is via limited interfaces that deprive end users of the ability to alter their query in any way to garner better understanding of the dataset and result set. Users are unable to question why a particular data item is Not in the result set of a given query. In this work, we develop a model for answers to WHY NOT? queries. We show through a user study the usefulness of our answers, and describe two algorithms for finding the manipulation that discarded the data item of interest. Moreover, we work through two different methods for tracing the discarded data item that can be used with either algorithm. Using our algorithms, it is feasible for users to find the manipulation that excluded the data item of interest, and can eliminate the need for exhausting debugging.',\n",
       " 'f39e21382458bf723e207d0ac649680f9b4dde4a': \"Due to the complex structure and handwritten deformation, the offline handwritten Chinese characters recognition has been one of the most challenging problems. In this paper, an offline handwritten Chinese character recognition tool has been developed based on the Tesseract open source OCR engine. The tool mainly contributes on the following two points: First, a handwritten Chinese character features library is generated, which is independent of a specific user's writing style, Second, by preprocessing the input image and adjusting the Tesseract engine, multiple candidate recognition results are output based on weight ranking. The recognition accuracy rate of this tool is above 88% for both known user test set and unknown user test set. It has shown that the Tesseract engine is feasible for offline handwritten Chinese character recognition to a certain degree.\",\n",
       " '3cc0c9a9917f9ed032376fa467838e720701e783': \"The modular Gal4 system has proven to be an extremely useful tool for conditional gene expression in Drosophila. One limitation has been the inability of the system to work in the female germline. A modified Gal4 system that works throughout oogenesis is presented here. To achieve germline expression, it was critical to change the basal promoter and 3'-UTR in the Gal4-responsive expression vector (generating UASp). Basal promoters and heterologous 3'-UTRs are often considered neutral, but as shown here, can endow qualitative tissue-specificity to a chimeric transcript. The modified Gal4 system was used to investigate the role of the Drosophila FGF homologue branchless, ligand for the FGF receptor breathless, in border cell migration. FGF signaling guides tracheal cell migration in the embryo. However, misexpression of branchless in the ovary had no effect on border cell migration. Thus border cells and tracheal cells appear to be guided differently.\",\n",
       " 'e79b34f6779095a73ba4604291d84bc26802b35e': 'Current state-of-the-art relation extraction methods typically rely on a set of lexical, syntactic, and semantic features, explicitly computed in a pre-processing step. Training feature extraction models requires additional annotated language resources, which severely restricts the applicability and portability of relation extraction to novel languages. Similarly, pre-processing introduces an additional source of error. To address these limitations, we introduce TRE, a Transformer for Relation Extraction. Unlike previous relation extraction models, TRE uses pre-trained deep language representations instead of explicit linguistic features to inform the relation classification and combines it with the self-attentive Transformer architecture to effectively model long-range dependencies between entity mentions. TRE allows us to learn implicit linguistic features solely from plain text corpora by unsupervised pre-training, before fine-tuning the learned language representations on the relation extraction task. TRE obtains a new state-of-the-art result on the TACRED and SemEval 2010 Task 8 datasets, achieving a test F1 of 67.4 and 87.1, respectively. Furthermore, we observe a significant increase in sample efficiency. With only 20% of the training examples, TRE matches the performance of our baselines and our model trained from scratch on 100% of the TACRED dataset. We open-source our trained models, experiments, and source code.',\n",
       " 'bad43ffc1c7d07db5990f631334bfa3157a6b134': 'A corporate feed slotted waveguide array antenna with broadband characteristics in term of gain in the 350 GHz band is achieved by measurement for the first time. The etching accuracy for thin laminated plates of the diffusion bonding process with conventional chemical etching is limited to ±20μm. This limits the use of this process for antenna fabrication in the submillimeter wave band where the fabrication tolerances are very severe. To improve the etching accuracy of the thin laminated plates, a new fabrication process has been developed. Each silicon wafer is etched by DRIE (deep reactive ion etcher) and is plated by gold on the surface. This new fabrication process provides better fabrication tolerances about ±5 μm using wafer bond aligner. The thin laminated wafers are then bonded with the diffusion bonding process under high temperature and high pressure. To validate the proposed antenna concepts, an antenna prototype has been designed and fabricated in the 350 GHz band. The 3dB-down gain bandwidth is about 44.6 GHz by this silicon process while it was about 15GHz by the conventional process using metal plates in measurement.',\n",
       " '0dacd4593ba6bce441bae37fc3ff7f3b70408ee1': \"Convex empirical risk minimization is a basic tool in machine learning and statistics. We provide new algorithms and matching lower bounds for differentially private convex empirical risk minimization assuming only that each data point's contribution to the loss function is Lipschitz and that the domain of optimization is bounded. We provide a separate set of algorithms and matching lower bounds for the setting in which the loss functions are known to also be strongly convex. Our algorithms run in polynomial time, and in some cases even match the optimal nonprivate running time (as measured by oracle complexity). We give separate algorithms (and lower bounds) for (ε, 0)and (ε, δ)-differential privacy; perhaps surprisingly, the techniques used for designing optimal algorithms in the two cases are completely different. Our lower bounds apply even to very simple, smooth function families, such as linear and quadratic functions. This implies that algorithms from previous work can be used to obtain optimal error rates, under the additional assumption that the contributions of each data point to the loss function is smooth. We show that simple approaches to smoothing arbitrary loss functions (in order to apply previous techniques) do not yield optimal error rates. In particular, optimal algorithms were not previously known for problems such as training support vector machines and the high-dimensional median.\",\n",
       " '24d800e6681a129b7787cbb05d0e224acad70e8d': 'A wide range of malicious activities rely on the domain name service (DNS) to manage their large, distributed networks of infected machines. As a consequence, the monitoring and analysis of DNS queries has recently been proposed as one of the most promising techniques to detect and blacklist domains involved in malicious activities (e.g., phishing, spam, botnets command-and-control, etc.). EXPOSURE is a system we designed to detect such domains in real time, by applying 15 unique features grouped in four categories.\\n We conducted a controlled experiment with a large, real-world dataset consisting of billions of DNS requests. The extremely positive results obtained in the tests convinced us to implement our techniques and deploy it as a free, online service. In this article, we present the Exposure system and describe the results and lessons learned from 17 months of its operation. Over this amount of time, the service detected over 100K malicious domains. The statistics about the time of usage, number of queries, and target IP addresses of each domain are also published on a daily basis on the service Web page.',\n",
       " '32334506f746e83367cecb91a0ab841e287cd958': 'We consider a statistical database in which a trusted administrator introduces noise to the query responses with the goal of maintaining privacy of individual database entries. In such a database, a query consists of a pair (S, f) where S is a set of rows in the database and f is a function mapping database rows to {0, 1}. The true answer is ΣiεS f(di), and a noisy version is released as the response to the query. Results of Dinur, Dwork, and Nissim show that a strong form of privacy can be maintained using a surprisingly small amount of noise -- much less than the sampling error -- provided the total number of queries is sublinear in the number of database rows. We call this query and (slightly) noisy reply the SuLQ (Sub-Linear Queries) primitive. The assumption of sublinearity becomes reasonable as databases grow increasingly large.We extend this work in two ways. First, we modify the privacy analysis to real-valued functions f and arbitrary row types, as a consequence greatly improving the bounds on noise required for privacy. Second, we examine the computational power of the SuLQ primitive. We show that it is very powerful indeed, in that slightly noisy versions of the following computations can be carried out with very few invocations of the primitive: principal component analysis, k means clustering, the Perceptron Algorithm, the ID3 algorithm, and (apparently!) all algorithms that operate in the in the statistical query learning model [11].',\n",
       " '49934d08d42ed9e279a82cbad2086377443c8a75': 'Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the ε-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.',\n",
       " '61efdc56bc6c034e9d13a0c99d0b651a78bfc596': 'Many resource allocation problems can be formulated as an optimization problem whose constraints contain sensitive information about participating users. This paper concerns a class of resource allocation problems whose objective function depends on the aggregate allocation (i.e., the sum of individual allocations); in particular, we investigate distributed algorithmic solutions that preserve the privacy of participating users. Without privacy considerations, existing distributed algorithms normally consist of a central entity computing and broadcasting certain public coordination signals to participating users. However, the coordination signals often depend on user information, so that an adversary who has access to the coordination signals can potentially decode information on individual users and put user privacy at risk. We present a distributed optimization algorithm that preserves differential privacy, which is a strong notion that guarantees user privacy regardless of any auxiliary information an adversary may have. The algorithm achieves privacy by perturbing the public signals with additive noise, whose magnitude is determined by the sensitivity of the projection operation onto user-specified constraints. By viewing the differentially private algorithm as an implementation of stochastic gradient descent, we are able to derive a bound for the suboptimality of the algorithm. We illustrate the implementation of our algorithm via a case study of electric vehicle charging. Specifically, we derive the sensitivity and present numerical simulations for the algorithm. Through numerical simulations, we are able to investigate various aspects of the algorithm when being used in practice, including the choice of step size, number of iterations, and the trade-off between privacy level and suboptimality.',\n",
       " 'c7788c34ba1387f1e437a2f83e1931f0c64d8e4e': \"Recommender Systems act as a personalized decision guides, aiding users in decisions on matters related to personal taste. Most previous research on Recommender Systems has focused on the statistical accuracy of the algorithms driving the systems, with little emphasis on interface issues and the user's perspective. The goal of this research was to examine the role of transprency (user understanding of why a particular recommendation was made) in Recommender Systems. To explore this issue, we conducted a user study of five music Recommender Systems. Preliminary results indicate that users like and feel more confident about recommendations that they perceive as transparent.\",\n",
       " '7731c8a1c56fdfa149759a8bb7b81464da0b15c1': 'The work presented here applies deep learning to the task of automated cardiac auscultation, i.e. recognizing abnormalities in heart sounds. We describe an automated heart sound classification algorithm that combines the use of time-frequency heat map representations with a deep convolutional neural network (CNN). Given the cost-sensitive nature of misclassification, our CNN architecture is trained using a modified loss function that directly optimizes the trade-off between sensitivity and specificity. We evaluated our algorithm at the 2016 PhysioNet Computing in Cardiology challenge where the objective was to accurately classify normal and abnormal heart sounds from single, short, potentially noisy recordings. Our entry to the challenge achieved a final specificity of 0.95, sensitivity of 0.73 and overall score of 0.84. We achieved the greatest specificity score out of all challenge entries and, using just a single CNN, our algorithm differed in overall score by only 0.02 compared to the top place finisher, which used an ensemble approach.',\n",
       " '17a00f26b68f40fb03e998a7eef40437dd40e561': 'Active safety systems are based upon the accurate and fast estimation of the value of important dynamical variables such as forces, load transfer, actual tire-road friction (kinetic friction) muk, and maximum tire-road friction available (potential friction) mup. Measuring these parameters directly from tires offers the potential for improving significantly the performance of active safety systems. We present a distributed architecture for a data-acquisition system that is based on a number of complex intelligent sensors inside the tire that form a wireless sensor network with coordination nodes placed on the body of the car. The design of this system has been extremely challenging due to the very limited available energy combined with strict application requirements for data rate, delay, size, weight, and reliability in a highly dynamical environment. Moreover, it required expertise in multiple engineering disciplines, including control-system design, signal processing, integrated-circuit design, communications, real-time software design, antenna design, energy scavenging, and system assembly.',\n",
       " '190dcdb71a119ec830d6e7e6e01bb42c6c10c2f3': 'Just-in-time (JIT) compilation of running programs provides more optimization opportunities than offline compilation. Modern JIT compilers, such as those in virtual machines like Oracle\\'s HotSpot for Java or Google\\'s V8 for JavaScript, rely on dynamic profiling as their key mechanism to guide optimizations. While these JIT compilers offer good average performance, their behavior is a black box and the achieved performance is highly unpredictable.\\n In this paper, we propose to turn JIT compilation into a precision tool by adding two essential and generic metaprogramming facilities: First, allow programs to invoke JIT compilation explicitly. This enables controlled specialization of arbitrary code at run-time, in the style of partial evaluation. It also enables the JIT compiler to report warnings and errors to the program when it is unable to compile a code path in the demanded way. Second, allow the JIT compiler to call back into the program to perform compile-time computation. This lets the program itself define the translation strategy for certain constructs on the fly and gives rise to a powerful JIT macro facility that enables \"smart\" libraries to supply domain-specific compiler optimizations or safety checks.\\n We present Lancet, a JIT compiler framework for Java bytecode that enables such a tight, two-way integration with the running program. Lancet itself was derived from a high-level Java bytecode interpreter: staging the interpreter using LMS (Lightweight Modular Staging) produced a simple bytecode compiler. Adding abstract interpretation turned the simple compiler into an optimizing compiler. This fact provides compelling evidence for the scalability of the staged-interpreter approach to compiler construction.\\n In the case of Lancet, JIT macros also provide a natural interface to existing LMS-based toolchains such as the Delite parallelism and DSL framework, which can now serve as accelerator macros for arbitrary JVM bytecode.',\n",
       " 'f5888af5e5353eb74d37ec50e9840e58b1992953': 'Recommendation of scientific papers is a task aimed to support researchers in accessing relevant articles from a large pool of unseen articles. When writing a paper, a researcher focuses on the topics related to her/his scientific domain, by using a technical language. The core idea of this paper is to exploit the topics related to the researchers scientific production (authored articles) to formally define her/his profile; in particular we propose to employ topic modeling to formally represent the user profile, and language modeling to formally represent each unseen paper. The recommendation technique we propose relies on the assessment of the closeness of the language used in the researchers papers and the one employed in the unseen papers. The proposed approach exploits a reliable knowledge source for building the user profile, and it alleviates the cold-start problem, typical of collaborative filtering techniques. We also present a preliminary evaluation of our approach on the DBLP.',\n",
       " '1f8be49d63c694ec71c2310309cd02a2d8dd457f': 'In this paper, we focus on developing a novel mechanism to preserve differential privacy in deep neural networks, such that: (1) The privacy budget consumption is totally independent of the number of training steps; (2) It has the ability to adaptively inject noise into features based on the contribution of each to the output; and (3) It could be applied in a variety of different deep neural networks. To achieve this, we figure out a way to perturb affine transformations of neurons, and loss functions used in deep neural networks. In addition, our mechanism intentionally adds \"more noise\" into features which are \"less relevant\" to the model output, and vice-versa. Our theoretical analysis further derives the sensitivities and error bounds of our mechanism. Rigorous experiments conducted on MNIST and CIFAR-10 datasets show that our mechanism is highly effective and outperforms existing solutions.',\n",
       " '31e9d9458471b4a0cfc6cf1de219b10af0f37239': 'Massively multiplayer online role-playing games (MMORPGs) are video games in which players create an avatar that evolves and interacts with other avatars in a persistent virtual world. Motivations to play MMORPGs are heterogeneous (e.g. achievement, socialisation, immersion in virtual worlds). This study investigates in detail the relationships between self-reported motives and actual in-game behaviours. We recruited a sample of 690 World of Warcraft players (the most popular MMORPG) who agreed to have their avatar monitored for 8 months. Participants completed an initial online survey about their motives to play. Their actual in-game behaviours were measured through the game’s official database (the Armory website). Results showed specific associations between motives and in-game behaviours. Moreover, longitudinal analyses revealed that teamworkand competition-oriented motives are the most accurate predictors of fast progression in the game. In addition, although specific associations exist between problematic use and certain motives (e.g. advancement, escapism), longitudinal analyses showed that high involvement in the game is not necessarily associated with a negative impact upon daily living. 2012 Elsevier Ltd. All rights reserved.',\n",
       " '33127e014cf537192c33a5b0e4b62df2a7b1869f': 'It is not sufficient to merely check the syntax of new policies before they are deployed in a system; policies need to be analyzed for their interactions with each other and with their local environment. That is, policies need to go through a ratification process. We believe policy ratification becomes an essential part of system management as the number of policies in the system increases and as the system administration becomes more decentralized. In this paper, we focus on the basic tasks involved in policy ratification. To a large degree, these basic tasks can be performed independent of policy model and language and require little domain-specific knowledge. We present algorithms from constraint, linear, and logic programming disciplines to help perform ratification tasks. We provide an algorithm to efficiently assign priorities to the policies based on relative policy preferences indicated by policy administrators. Finally, with an example, we show how these algorithms have been integrated with our policy system to provide feedback to a policy administrator regarding potential interactions of policies with each other and with their deployment environment.',\n",
       " 'c6b5c1cc565c878db50ad20aafd804284558ad02': 'A new measure of centrality, C,, is introduced. It is based on the concept of network flows. While conceptually similar to Freeman’s original measure, Ca, the new measure differs from the original in two important ways. First, C, is defined for both valued and non-valued graphs. This makes C, applicable to a wider variety of network datasets. Second, the computation of C, is not based on geodesic paths as is C, but on all the independent paths between all pairs of points in the network.',\n",
       " '2ccca721c20ad1d8503ede36fe310626070de640': 'Distributed energy resources (DERs), such as photovoltaic, wind, and gas generators, are connected to the grid more than ever before, which introduces tremendous changes in the distribution grid. Due to these changes, it is important to understand where these DERs are connected in order to sustainably operate the distribution grid. But the exact distribution system topology is difficult to obtain due to frequent distribution grid reconfigurations and insufficient knowledge about new components. In this paper, we propose a methodology that utilizes new data from sensor-equipped DER devices to obtain the distribution grid topology. Specifically, a graphical model is presented to describe the probabilistic relationship among different voltage measurements. With power flow analysis, a mutual information-based identification algorithm is proposed to deal with tree and partially meshed networks. Simulation results show highly accurate connectivity identification in the IEEE standard distribution test systems and Electric Power Research Institute test systems.',\n",
       " '8eb3ebd0a1d8a26c7070543180d233f841b79850': 'IEEE 802.11 Medium Access Control(MAC) is proposed to support asynchronous and time bounded delivery of radio data packets in infrastructure and ad hoc networks. The basis of the IEEE 802.11 WLAN MAC protocol is Distributed Coordination Function(DCF), which is a Carrier Sense Multiple Access with Collision Avoidance(CSMA/CA) with binary slotted exponential back-off scheme. Since IEEE 802.11 MAC has its own characteristics that are different from other wireless MAC protocols, the performance of reliable transport protocol over 802.11 needs further study. This paper proposes a scheme named DCF+, which is compatible with DCF, to enhance the performance of reliable transport protocol over WLAN. To analyze the performance of DCF and DCF+, this paper also introduces an analytical model to compute the saturated throughput of WLAN. Comparing with other models, this model is shown to be able to predict the behaviors of 802.11 more accurately. Moreover, DCF+ is able to improve the performance of TCP over WLAN, which is verified by modeling and elaborate simulation results.',\n",
       " '5574763d870bae0fd3fd6d3014297942a045f60a': 'Data mining now-a-days plays an important role in prediction of diseases in health care industry. The Health care industry utilizes data mining Techniques and finds out the information which is hidden in the data set. Many diagnoses have been done for predicting diseases. Without knowing the knowledge of profound medicine and clinical experience the treatment goes wrong. The time taken to recover from diseases depends on patients&apos; severity. For finding out the disease, number of test needs to be taken by patient. In most cases not all test become more effective. And at last it leads to the death of the patient. Many experiments have been conducted by comparing the performance of predictive data mining for reducing the number of test taken by the patient indirectly. This research paper is to present a survey on predicting the presence of life threatening diseases which causes to death and list out the various classification algorithms that has been used with number of attributes for prediction.',\n",
       " '6273df9def7c011bc21cd42a4029d4b7c7c48c2e': 'A 45GHz Doherty power amplifier is implemented in 45nm SOI CMOS. Two-stack FET amplifiers are used as main and auxiliary amplifiers, allowing a supply voltage of 2.5V and high output power. The use of slow-wave coplanar waveguides (CPW) improves the PAE and gain by approximately 3% and 1dB, and reduces the die area by 20%. This amplifier exhibits more than 18dBm saturated output power, with peak power gain of 7dB. It occupies 0.64mm2 while achieving a peak PAE of 23%; at 6dB back-off the PAE is 17%.',\n",
       " '1e396464e440e6032be3f035a9a6837c32c9d2c0': 'Used for thermal energy harvesting, thermoelectric generator (TEG) can convert heat into electricity directly. Structurally, the main part of TEG is the thermopile, which consists of thermocouples connected in series electrically and in parallel thermally. Benefiting from massive progress achieved in a microelectromechanical systems technology, micro TEG (<inline-formula> <tex-math notation=\"LaTeX\">$\\\\mu$ </tex-math></inline-formula>-TEG) with advantages of small volume and high output voltage has obtained attention in recent 20 years. The review gives a comprehensive survey of the development and current status of <inline-formula> <tex-math notation=\"LaTeX\">$\\\\mu$ </tex-math></inline-formula>-TEG. First, the principle of operation is introduced and some key parameters used for characterizing the performance of <inline-formula> <tex-math notation=\"LaTeX\">$\\\\mu$ </tex-math></inline-formula>-TEG are highlighted. Next, <inline-formula> <tex-math notation=\"LaTeX\">$\\\\mu$ </tex-math></inline-formula>-TEGs are classified from the perspectives of structure, material, and fabrication technology. Then, almost all the relevant works are summarized for the convenience of comparison and reference. Summarized information includes the structure, material property, fabrication technology, output performance, and so on. This will provide readers with an overall evaluation of different studies and guide them in choosing the suitable <inline-formula> <tex-math notation=\"LaTeX\">$\\\\mu$ </tex-math></inline-formula>-TEGs for their applications. In addition, the existing and potential applications of <inline-formula> <tex-math notation=\"LaTeX\">$\\\\mu$ </tex-math></inline-formula>-TEG are shown, especially the applications in the Internet of things. Finally, we summarize the challenges encountered in improving the output power of <inline-formula> <tex-math notation=\"LaTeX\">$\\\\mu$ </tex-math></inline-formula>-TEG and predicted that more researchers would focus their efforts on the flexible structure <inline-formula> <tex-math notation=\"LaTeX\">$\\\\mu$ </tex-math></inline-formula>-TEG, and combination of <inline-formula> <tex-math notation=\"LaTeX\">$\\\\mu$ </tex-math></inline-formula>-TEG and other energy harvestings. With the emergence of more low-power devices and the gradual improvement of <italic>ZT</italic> value of the thermoelectric material, <inline-formula> <tex-math notation=\"LaTeX\">$\\\\mu$ </tex-math></inline-formula>-TEG is promising for applications in various fields. [2017-0610]',\n",
       " '4c11a7b668dee651cc2d8eb2eaf8665449b1738f': \"The release engineering process is the process that brings high quality code changes from a developer's workspace to the end user, encompassing code change integration, continuous integration, build system specifications, infrastructure-as-code, deployment and release. Recent practices of continuous delivery, which bring new content to the end user in days or hours rather than months or years, have generated a surge of industry-driven interest in the release engineering pipeline. This paper argues that the involvement of researchers is essential, by providing a brief introduction to the six major phases of the release engineering pipeline, a roadmap of future research, and a checklist of three major ways that the release engineering process of a system under study can invalidate the findings of software engineering studies. The main take-home message is that, while release engineering technology has flourished tremendously due to industry, empirical validation of best practices and the impact of the release engineering process on (amongst others) software quality is largely missing and provides major research opportunities.\",\n",
       " '9f6db3f5809a9d1b9f1c70d9d30382a0bd8be8d0': 'Cloud computing has emerged as a very important commercial infrastructure that promises to reduce the need for maintaining costly computing facilities by organizations and institutes. Through the use of virtualization and time sharing of resources, clouds serve with a single set of physical resources as a large user base with altogether different needs. Thus, the clouds have the promise to provide to their owners the benefits of an economy of calibration and, at the same time, become a substitute for scientists to clusters, grids, and parallel production conditions. However, the present commercial clouds have been built to support web and small database workloads, which are very different from common scientific computing workloads. Furthermore, the use of virtualization and resource time sharing may introduce significant performance penalties for the demanding scientific computing workloads. In this paper, we analyze the performance of cloud computing services for scientific computing workloads. This paper evaluate the presence in real scientific computing workloads of Many-Task Computing users, that is, of users who employ loosely coupled applications comprising many tasks to achieve their scientific goals. Our effective method demonstrates to yield comparative and even better results than the more complex state-of-the-art techniques but has the advantage to be appropriate for real-time applications.',\n",
       " '6fcccd6def46a4dd50f85df4d4c011bd9f1855af': 'Language Oriented Programming (LOP) is a paradigm that puts domain specific programming languages (DSLs) at the center of the software development process. Currently, there are three main approaches to LOP: (1) the use of internal DSLs, implemented as libraries in a given host language; (2) the use of external DSLs, implemented as interpreters or compilers in an external language; and (3) the use of language workbenches, which are integrated development environments (IDEs) for defining and using external DSLs. In this paper, we contribute: (4) a novel language-oriented approach to LOP for defining and using internal DSLs. While language workbenches adapt internal DSL features to overcome some of the limitations of external DSLs, our approach adapts language workbench features to overcome some of the limitations of internal DSLs. We introduce Cedalion, an LOP host language for internal DSLs, featuring static validation and projectional editing. To validate our approach we present a case study in which Cedalion was used by biologists in designing a DNA microarray for molecular Biology research.',\n",
       " '7cbbe0025b71a265c6bee195b5595cfad397a734': 'People interact with chairs frequently, making them a potential location to perform implicit health sensing that requires no additional effort by users. We surveyed 550 participants to understand how people sit in chairs and inform the design of a chair that detects heart and respiratory rate from the armrests and backrests of the chair respectively. In a laboratory study with 18 participants, we evaluated a range of common sitting positions to determine when heart rate and respiratory rate detection was possible (32% of the time for heart rate, 52% for respiratory rate) and evaluate the accuracy of the detected rate (83% for heart rate, 73% for respiratory rate). We discuss the challenges of moving this sensing to the wild by evaluating an in-situ study totaling 40 hours with 11 participants. We show that, as an implicit sensor, the chair can collect vital signs data from its occupant through natural interaction with the chair.',\n",
       " 'a00a757b26d5c4f53b628a9c565990cdd0e51876': 'We motivate and describe a new freely available human-human dialogue data set for interactive learning of visually grounded word meanings through ostensive definition by a tutor to a learner. The data has been collected using a novel, character-by-character variant of the DiET chat tool (Healey et al., 2003; Mills and Healey, submitted) with a novel task, where a Learner needs to learn invented visual attribute words (such as “burchak” for square) from a tutor. As such, the text-based interactions closely resemble face-to-face conversation and thus contain many of the linguistic phenomena encountered in natural, spontaneous dialogue. These include selfand other-correction, mid-sentence continuations, interruptions, overlaps, fillers, and hedges. We also present a generic n-gram framework for building user (i.e. tutor) simulations from this type of incremental data, which is freely available to researchers. We show that the simulations produce outputs that are similar to the original data (e.g. 78% turn match similarity). Finally, we train and evaluate a Reinforcement Learning dialogue control agent for learning visually grounded word meanings, trained from the BURCHAK corpus. The learned policy shows comparable performance to a rulebased system built previously.',\n",
       " 'b49e31fe5948b3ca4552ac69dd7a735607467f1c': 'In many applications, optimization of a collection of problems is required where each problem is structurally the same, but in which some or all of the data defining the instance is updated. Such models are easily specified within modern modeling systems, but have often been slow to solve due to the time needed to regenerate the instance, and the inability to use advance solution information (such as basis factorizations) from previous solves as the collection is processed. We describe a new language extension, GUSS, that gathers data from different sources/symbols to define the collection of models (called scenarios), updates a base model instance with this scenario data and solves the updated model instance and scatters the scenario results to symbols in the GAMS database. We demonstrate the utility of this approach in three applications, namely data envelopment analysis, cross validation and stochastic dual dynamic programming. The language extensions are available for general use in all versions of GAMS starting with release 23.7.',\n",
       " '5914781bde18606e55e8f7683f55889df91576ec': 'Article history: Received 7 January 2008 Received in revised form 24 June 2008 Accepted 31 July 2008 Available online 5 April 2009 Outsourcing is a phenomenon that as a practice originated in the 1950s, but it was not until the 1980s when the strategy became widely adopted in organizations. Since then, the strategy has evolved from a strictly cost focused approach towards more cooperative nature, in which cost is only one, often secondary, decision-making criterion. In the development of the strategy, three broad and somewhat overlapping, yet distinct phases can be identified: the era of the Big Bang, the era of the Bandwagon, and the era of Barrierless Organizations. This paper illustrates that the evolution of the practice has caused several contradictions among researchers, as well as led to the situation where the theoretical background of the phenomenon has recently become much richer. Through examining existing research, this paper intends to identify the development of outsourcing strategy from a practical as well as a theoretical perspective from its birth up to today. In addition, through providing insights from managers in the information technology industry, this paper aims at providing a glimpse from the future – that is – what may be the future directions and research issues in this complex phenomenon? © 2009 Elsevier Inc. All rights reserved.',\n",
       " '423455ad8afb9b2534c0954a5e61c95bea611801': 'Virtual machines were developed by IBM in the 1960’s to provide concurrent, interactive access to a mainframe computer. Each virtual machine is a replica of the underlying physical machine and users are given the illusion of running directly on the physical machine. Virtual machines also provide benefits like isolation and resource sharing, and the ability to run multiple flavors and configurations of operating systems. VMware Workstation brings such mainframe-class virtual machine technology to PC-based desktop and workstation computers. This paper focuses on VMware Workstation’s approach to virtualizing I/O devices. PCs have a staggering variety of hardware, and are usually pre-installed with an operating system. Instead of replacing the pre-installed OS, VMware Workstation uses it to host a user-level application (VMApp) component, as well as to schedule a privileged virtual machine monitor (VMM) component. The VMM directly provides high-performance CPU virtualization while the VMApp uses the host OS to virtualize I/O devices and shield the VMM from the variety of devices. A crucial question is whether virtualizing devices via such a hosted architecture can meet the performance required of high throughput, low latency devices. To this end, this paper studies the virtualization and performance of an Ethernet adapter on VMware Workstation. Results indicate that with optimizations, VMware Workstation’s hosted virtualization architecture can match native I/O throughput on standard PCs. Although a straightforward hosted implementation is CPU-limited due to virtualization overhead on a 733 MHz Pentium R III system on a 100 Mb/s Ethernet, a series of optimizations targeted at reducing CPU utilization allows the system to match native network throughput. Further optimizations are discussed both within and outside a hosted architecture.',\n",
       " 'c5788be735f3caadc7d0d3147aa52fd4a6036ec4': 'Genome-wide association studies (GWASs) have become the focus of the statistical analysis of complex traits in humans, successfully shedding light on several aspects of genetic architecture and biological aetiology. Single-nucleotide polymorphisms (SNPs) are usually modelled as having additive, cumulative and independent effects on the phenotype. Although evidently a useful approach, it is often argued that this is not a realistic biological model and that epistasis (that is, the statistical interaction between SNPs) should be included. The purpose of this Review is to summarize recent directions in methodology for detecting epistasis and to discuss evidence of the role of epistasis in human complex trait variation. We also discuss the relevance of epistasis in the context of GWASs and potential hazards in the interpretation of statistical interaction terms.',\n",
       " 'd3569f184b7083c0433bf00fa561736ae6f8d31e': \"Databases often contain uncertain and imprecise references to real-world entities. Entity resolution, the process of reconciling multiple references to underlying real-world entities, is an important data cleaning process required before accurate visualization or analysis of the data is possible. In many cases, in addition to noisy data describing entities, there is data describing the relationships among the entities. This relational data is important during the entity resolution process; it is useful both for the algorithms which determine likely database references to be resolved and for visual analytic tools which support the entity resolution process. In this paper, we introduce a novel user interface, D-Dupe, for interactive entity resolution in relational data. D-Dupe effectively combines relational entity resolution algorithms with a novel network visualization that enables users to make use of an entity's relational context for making resolution decisions. Since resolution decisions often are interdependent, D-Dupe facilitates understanding this complex process through animations which highlight combined inferences and a history mechanism which allows users to inspect chains of resolution decisions. An empirical study with 12 users confirmed the benefits of the relational context visualization on the performance of entity resolution tasks in relational data in terms of time as well as users' confidence and satisfaction.\",\n",
       " 'c630196c34533903b48e546897d46df27c844bc2': 'This paper introduces a large air-gap capacitive wireless power transfer (WPT) system for electric vehicle charging that achieves a power transfer density exceeding the state-of-the-art by more than a factor of four. This high power transfer density is achieved by operating at a high switching frequency (6.78 MHz), combined with an innovative approach to designing matching networks that enable effective power transfer at this high frequency. In this approach, the matching networks are designed such that the parasitic capacitances present in a vehicle charging environment are absorbed and utilized as part of the wireless power transfer mechanism. A new modeling approach is developed to simplify the complex network of parasitic capacitances into equivalent capacitances that are directly utilized as the matching network capacitors. A systematic procedure to accurately measure these equivalent capacitances is also presented. A prototype capacitive WPT system with 150 cm2 coupling plates, operating at 6.78 MHz and incorporating matching networks designed using the proposed approach, is built and tested. The prototype system transfers 589 W of power across a 12-cm air gap, achieving a power transfer density of 19.6 kW/m2.',\n",
       " '1750a3716a03aaacdfbb0e25214beaa5e1e2b6ee': '1 Why develop an ontology? In recent years the development of ontologies—explicit formal specifications of the terms in the domain and relations among them (Gruber 1993)—has been moving from the realm of ArtificialIntelligence laboratories to the desktops of domain experts. Ontologies have become common on the World-Wide Web. The ontologies on the Web range from large taxonomies categorizing Web sites (such as on Yahoo!) to categorizations of products for sale and their features (such as on Amazon.com). The WWW Consortium (W3C) is developing the Resource Description Framework (Brickley and Guha 1999), a language for encoding knowledge on Web pages to make it understandable to electronic agents searching for information. The Defense Advanced Research Projects Agency (DARPA), in conjunction with the W3C, is developing DARPA Agent Markup Language (DAML) by extending RDF with more expressive constructs aimed at facilitating agent interaction on the Web (Hendler and McGuinness 2000). Many disciplines now develop standardized ontologies that domain experts can use to share and annotate information in their fields. Medicine, for example, has produced large, standardized, structured vocabularies such as SNOMED (Price and Spackman 2000) and the semantic network of the Unified Medical Language System (Humphreys and Lindberg 1993). Broad general-purpose ontologies are emerging as well. For example, the United Nations Development Program and Dun & Bradstreet combined their efforts to develop the UNSPSC ontology which provides terminology for products and services (www.unspsc.org). An ontology defines a common vocabulary for researchers who need to share information in a domain. It includes machine-interpretable definitions of basic concepts in the domain and relations among them. Why would someone want to develop an ontology? Some of the reasons are:',\n",
       " '7c459c36e19629ff0dfb4bd0e541cc5d2d3f03e0': 'Social engineering is a type of attack that allows unauthorized access to a system to achieve specific objective. Commonly, the purpose is to obtain information for social engineers. Some successful social engineering attacks get victims’ information via human based retrieval approach, example technique terms as dumpster diving or shoulder surfing attack to get access to password. Alternatively, victims’ information also can be stolen using technical-based method such as from pop-up windows, email or web sites to get the password or other sensitive information. This research performed a preliminary analysis on social engineering attack taxonomy that emphasized on types of technical-based social engineering attack. Results from the analysis become a guideline in proposing a new generic taxonomy of Social Engineering Attack (SEA).',\n",
       " 'bf003bb2d52304fea114d824bc0bf7bfbc7c3106': '',\n",
       " '10466df2b511239674d8487101229193c011a657': 'Trusted people can fail to be trustworthy when it comes to protecting their aperture of access to secure computer systems due to inadequate education, negligence, and various social pressures. People are often the weakest link in an otherwise secure computer system and, consequently, are targeted for social engineering attacks. Social Engineering is a technique used by hackers or other attackers to gain access to information technology systems by getting the needed information (for example, a username and password) from a person rather than breaking into the system through electronic or algorithmic hacking methods. Such attacks can occur on both a physical and psychological level. The physical setting for these attacks occurs where a victim feels secure: often the workplace, the phone, the trash, and even on-line. Psychology is often used to create a rushed or officious ambiance that helps the social engineer to cajole information about accessing the system from an employee.\\n Data privacy legislation in the United States and international countries that imposes privacy standards and fines for negligent or willful non-compliance increases the urgency to measure the trustworthiness of people and systems. One metric for determining compliance is to simulate, by audit, a social engineering attack upon an organization required to follow data privacy standards. Such an organization commits to protect the confidentiality of personal data with which it is entrusted.\\n This paper presents the results of an approved social engineering audit made without notice within an organization where data security is a concern. Areas emphasized include experiences between the Social Engineer and the audited users, techniques used by the Social Engineer, and other findings from the audit. Possible steps to mitigate exposure to the dangers of Social Engineering through improved user education are reviewed.',\n",
       " '24b4076e2f58325f5d86ba1ca1f00b08a56fb682': \"This paper is intended to serve as a comprehensive introduction to the emerging eld concerned with the design and use of ontologies. We observe that disparate backgrounds, languages, tools, and techniques are a major barrier to e ective communication among people, organisations, and/or software systems. We show how the development and implementation of an explicit account of a shared understanding (i.e. an `ontology') in a given subject area, can improve such communication, which in turn, can give rise to greater reuse and sharing, inter-operability, and more reliable software. After motivating their need, we clarify just what ontologies are and what purposes they serve. We outline a methodology for developing and evaluating ontologies, rst discussing informal techniques, concerning such issues as scoping, handling ambiguity, reaching agreement and producing de nitions. We then consider the bene ts of and describe, a more formal approach. We re-visit the scoping phase, and discuss the role of formal languages and techniques in the speci cation, implementation and evaluation of ontologies. Finally, we review the state of the art and practice in this emerging eld, considering various case studies, software tools for ontology development, key research issues and future prospects. AIAI-TR-191 Ontologies Page i\",\n",
       " '90b16f97715a18a52b6a00b69411083bdb0460a0': 'We report a flexible and wearable pressure sensor based on the giant piezocapacitive effect of a three-dimensional (3-D) microporous dielectric elastomer, which is capable of highly sensitive and stable pressure sensing over a large tactile pressure range. Due to the presence of micropores within the elastomeric dielectric layer, our piezocapacitive pressure sensor is highly deformable by even very small amounts of pressure, leading to a dramatic increase in its sensitivity. Moreover, the gradual closure of micropores under compression increases the effective dielectric constant, thereby further enhancing the sensitivity of the sensor. The 3-D microporous dielectric layer with serially stacked springs of elastomer bridges can cover a much wider pressure range than those of previously reported micro-/nanostructured sensing materials. We also investigate the applicability of our sensor to wearable pressure-sensing devices as an electronic pressure-sensing skin in robotic fingers as well as a bandage-type pressure-sensing device for pulse monitoring at the human wrist. Finally, we demonstrate a pressure sensor array pad for the recognition of spatially distributed pressure information on a plane. Our sensor, with its excellent pressure-sensing performance, marks the realization of a true tactile pressure sensor presenting highly sensitive responses to the entire tactile pressure range, from ultralow-force detection to high weights generated by human activity.',\n",
       " '8f24560a66651fdb94eef61339527004fda8283b': 'Humans are able to understand and perform complex tasks by strategically structuring the tasks into incremental steps or subgoals. For a robot attempting to learn to perform a sequential task with critical subgoal states, such states can provide a natural opportunity for interaction with a human expert. This paper analyzes the benefit of incorporating a notion of subgoals into Inverse Reinforcement Learning (IRL) with a Human-In-The-Loop (HITL) framework. The learning process is interactive, with a human expert first providing input in the form of full demonstrations along with some subgoal states. These subgoal states define a set of subtasks for the learning agent to complete in order to achieve the final goal. The learning agent queries for partial demonstrations corresponding to each subtask as needed when the agent struggles with the subtask. The proposed Human Interactive IRL (HI-IRL) framework is evaluated on several discrete path-planning tasks. We demonstrate that subgoal-based interactive structuring of the learning task results in significantly more efficient learning, requiring only a fraction of the demonstration data needed for learning the underlying reward function with the baseline IRL model.',\n",
       " '747a58918524d15aca29885af3e1bc87313eb312': 'Emotions have a powerful impact on behavior and beliefs. The goal of our research is to create general computational models of this interplay of emotion, cognition and behavior to inform the design of virtual humans. Here, we address an aspect of emotional behavior that has been studied extensively in the psychological literature but largely ignored by computational approaches, emotion-focused coping. Rather than motivating external action, emotion-focused coping strategies alter beliefs in response to strong emotions. For example an individual may alter beliefs about the importance of a goal that is being threatened, thereby reducing their distress. We present a preliminary model of emotion-focused coping and discuss how coping processes, in general, can be coupled to emotions and behavior. The approach is illustrated within a virtual reality training environment where the models are used to create virtual human characters in high-stress social situations.',\n",
       " '332648a09d6ded93926829dbd81ac9dddf31d5b9': 'For aerial robots, maintaining a high vantage point for an extended time is crucial in many applications. However, available on-board power and mechanical fatigue constrain their flight time, especially for smaller, battery-powered aircraft. Perching on elevated structures is a biologically inspired approach to overcome these limitations. Previous perching robots have required specific material properties for the landing sites, such as surface asperities for spines, or ferromagnetism. We describe a switchable electroadhesive that enables controlled perching and detachment on nearly any material while requiring approximately three orders of magnitude less power than required to sustain flight. These electroadhesives are designed, characterized, and used to demonstrate a flying robotic insect able to robustly perch on a wide range of materials, including glass, wood, and a natural leaf.',\n",
       " '1facb3308307312789e1db7f0a0904ac9c9e7179': 'The complex behavior of Steel Plate Shear Walls (SPSW) is investigated herein through nonlinear FE simulations. A 3D detailed FE model is developed and validated utilizing experimental results available in the literature. The influence of key parameters on the structural behavior is investigated. The considered parameters are: the infill plate thickness, the beam size, the column size, the infill plate material grade and the frame material grade. Several structural responses are used as criteria to quantify their influence on the SPSW behavior. The evaluated structural responses are: yield strength, yield displacement, ultimate strength, initial stiffness and secondary stiffness. The results show that, overall the most influential parameter is the infill plate thickness followed by the beam size. Also, it was found that the least influential parameter is the frame material grade.',\n",
       " '236f183be06d824122da59ffb79e501d1a537486': 'Design for Reliability of Low-voltage, Switched-capacitor Circuits by Andrew Masami Abo Doctor of Philosophy in Engineering University of California, Berkeley Professor Paul R. Gray, Chair Analog, switched-capacitor circuits play a critical role in mixed-signal, analogto-digital interfaces. They implement a large class of functions, such as sampling, filtering, and digitization. Furthermore, their implementation makes them suitable for integration with complex, digital-signal-processing blocks in a compatible, low-cost technology–particularly CMOS. Even as an increasingly larger amount of signal processing is done in the digital domain, this critical, analogto-digital interface is fundamentally necessary. Examples of some integrated applications include camcorders, wireless LAN transceivers, digital set-top boxes, and others. Advances in CMOS technology, however, are driving the operating voltage of integrated circuits increasingly lower. As device dimensions shrink, the applied voltages will need to be proportionately scaled in order to guarantee long-term reliability and manage power density. The reliability constraints of the technology dictate that the analog circuitry operate at the same low voltage as the digital circuitry. Furthermore, in achieving low-voltage operation, the reliability constraints of the technology must not be violated. This work examines the voltage limitations of CMOS technology and how analog circuits can maximize the utility of MOS devices without degrading relia-',\n",
       " '83834cd33996ed0b00e3e0fca3cda413d7ed79ff': 'Data Warehouses and Business Intelligence have become popular fields of research in recent years. Unfortunately, in daily practice many Data Warehouse and Business Intelligence solutions still fail to help organizations make better decisions and increase their profitability, due to intransparent complexities and project interdependencies. In addition, emerging application domains such as Mobile Learning & Analytics heavily depend on a wellstructured data foundation with a longitudinally prepared architecture. Therefore, this research presents the Data Warehouse Capability Maturity Model (DWCMM) which encompasses both technical and organizational aspects involved in developing a Data Warehouse environment. The DWCMM can be used to help organizations assess their current Data Warehouse solution and provide them with guidelines for future improvements. The DWCMM consists of a maturity matrix and a maturity assessment questionnaire with 60 questions. The DWCMM has been evaluated empirically through expert interviews and case studies. We conclude that the DWCMM can be successfully applied in practice and that organizations can intelligibly utilize the DWCMM as a quickscan instrument to jumpstart their Data Warehouse and Business Intelligence improvement processes.',\n",
       " '89a9ad85d8343a622aaa8c072beacaf8df1f0464': 'This article describes a class of recently developed multiple-mode-resonator-based bandpass filters for ultra-wide-band (UWB) transmission systems. These filters have many attractive features, including a simple design, compact size, low loss and good linearity in the UWB, enhanced out-of-band rejection, and easy integration with other circuits/antennas. In this article, we present a variety of multiple-mode resonators with stepped-impedance or stub-loaded nonuniform configurations and analyze their properties based on the transmission line theory. Along with the frequency dispersion of parallel-coupled transmission lines, we design and implement various filter structures on planar, uniplanar, and hybrid transmission line geometries.',\n",
       " '1ca75a68d6769df095ac3864d86bca21e9650985': 'In this letter, an enhanced version of Address Resolution Protocol (ARP) is proposed to prevent ARP poisoning-based Man-in-the-Middle (MITM) attacks. The proposed mechanism is based on the following concept. When a node knows the correct Media Access Control (MAC) address for a given IP address, if it retains the IP/MAC address mapping while that machine is alive, then MITM attack is impossible for that IP address. In order to prevent MITM attacks even for a new IP address, a voting-based resolution mechanism is proposed. The proposed scheme is backward compatible with existing ARP and incrementally deployable.',\n",
       " '9c13e54760455a50482cda070c70448ecf30d68c': 'Several alternative distance measures for comparing time series have recently been proposed and evaluated on time series classification (TSC) problems. These include variants of dynamic time warping (DTW), such as weighted and derivative DTW, and edit distance-based measures, including longest common subsequence, edit distance with real penalty, time warp with edit, and move–split–merge. These measures have the common characteristic that they operate in the time domain and compensate for potential localised misalignment through some elastic adjustment. Our aim is to experimentally test two hypotheses related to these distance measures. Firstly, we test whether there is any significant difference in accuracy for TSC problems between nearest neighbour classifiers using these distance measures. Secondly, we test whether combining these elastic distance measures through simple ensemble schemes gives significantly better accuracy. We test these hypotheses by carrying out one of the largest experimental studies ever conducted into time series classification. Our first key finding is that there is no significant difference between the elastic distance measures in terms of classification accuracy on our data sets. Our second finding, and the major contribution of this work, is to define an ensemble classifier that significantly outperforms the individual classifiers. We also demonstrate that the ensemble is more accurate than approaches not based in the time domain. Nearly all TSC papers in the data mining literature cite DTW (with warping window set through cross validation) as the benchmark for comparison. We believe that our ensemble is the first ever classifier to significantly outperform DTW and as such raises the bar for future work in this area.',\n",
       " '8c76872375aa79acb26871c93da76d90dfb0a950': 'This paper shows results of recovering punctuation over speech transcriptions for a Portuguese broadcast news corpus. The approach is based on maximum entropy models and uses word, part-of-speech, time and speaker information. The contribution of each type of feature is analyzed individually. Separate results for each focus condition are given, making it possible to analyze the differences of performance between planned and spontaneous speech.',\n",
       " '7e2eb3402ea7eacf182bccc3f8bb685636098d2c': 'In this paper we present for the first time, the development of a new system for the off-line optical recognition of the characters used in the Orthodox Hellenic Byzantine Music Notation, that has been established since 1814. We describe the structure of the new system and propose algorithms for the recognition of the 71 distinct character classes, based on Wavelets, 4-projections and other structural and statistical features. Using a Nearest Neighbor classifier, combined with a post classification schema and a tree-structured classification philosophy, an accuracy of 99.4 % was achieved, in a database of about 18,000 byzantine character patterns that have been developed for the needs of the system. Optical music recognition Off-line character recognition, Byzantine Music, Byzantine Music Notation, Wavelets, Projections, Neural Networks Contour processing, Nearest Neighbor Classifier Byzantine Music Data Base',\n",
       " '26d4ab9b60b91bb610202b58fa1766951fedb9e9': 'This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.',\n",
       " '4805aee558489b5413ce5434737043148537f62f': 'With the increase in mobile device use, there is a greater need for increasingly sophisticated malware detection algorithms. The research presented in this paper examines two types of features of Android applications, permission requests and system calls, as a way to detect malware. We are able to differentiate between benign and malicious apps by applying a machine learning algorithm. The model that is presented here achieved a classification accuracy of around 80% using permissions and 60% using system calls for a relatively small dataset. In the future, different machine learning algorithms will be examined to see if there is a more suitable algorithm. More features will also be taken into account and the training set will be expanded.',\n",
       " '608ec914e356ff5e5782c908016958bf650a946f': 'This paper describes our system submission to the CogALex-2016 Shared Task on Corpus-Based Identification of Semantic Relations. Our system won first place for Task-1 and second place for Task-2. The evaluation results of our system on the test set is 88.1% (79.0% for TRUE only) f-measure for Task-1 on detecting semantic similarity, and 76.0% (42.3% when excluding RANDOM) for Task-2 on identifying fine-grained semantic relations. In our experiments, we try word analogy, linear regression, and multi-task Convolutional Neural Networks (CNNs) with word embeddings from publicly available word vectors. We found that linear regression performs better in the binary classification (Task-1), while CNNs have better performance in the multi-class semantic classification (Task-2). We assume that word analogy is more suited for deterministic answers rather than handling the ambiguity of one-to-many and many-to-many relationships. We also show that classifier performance could benefit from balancing the distribution of labels in the training data.',\n",
       " '5ecbb84d51e2a23dadd496d7c6ab10cf277d4452': 'This paper introduces a novel 5-DOF parallel manipulator with a rotation-symmetric arm system. The manipulator is unorthodox since one degree of freedom of its manipulated platform is unconstrained. Such a manipulator is still useful in a wide range of applications utilizing a rotation-symmetric tool. The manipulator workspace is analyzed for singularities and collisions. The rotation-symmetric arm system leads to a large positional workspace in relation to the footprint of the manipulator. With careful choice of structural parameters, the rotational workspace of the tool is also sizeable.',\n",
       " 'c79ddcef4bdf56c5467143b32e53b23825c17eff': 'In this paper, we describe a new approach for managing service function chains in scenarios where data from Internet of Things (IoT) devices is partially processed at the network edge. Our framework is enabled by two emerging technologies, Software-Defined Networking (SDN) and container based virtualization, which ensure several benefits in terms of flexibility, easy programmability, and versatility. These features are well suitable with the increasingly stringent requirements of IoT applications, and allow a dynamic and automated network service chaining. An extensive performance evaluation, which has been carried out by means of a testbed, seeks to understand how our proposed framework performs in terms of computational overhead, network bandwidth, and energy consumption. By accounting for the constraints of typical IoT gateways, our evaluation tries to shed light on the actual deployability of the framework on low-power nodes.',\n",
       " '489555f05e316015d24d2a1fdd9663d4b85eb60f': \"The aim of the present study was to assess the diagnostic accuracy of 7 clinical tests for Morton's neuroma (MN) compared with ultrasonography (US). Forty patients (54 feet) were diagnosed with MN using predetermined clinical criteria. These patients were subsequently referred for US, which was performed by a single, experienced musculoskeletal radiologist. The clinical test results were compared against the US findings. MN was confirmed on US at the site of clinical diagnosis in 53 feet (98%). The operational characteristics of the clinical tests performed were as follows: thumb index finger squeeze (96% sensitivity, 96% accuracy), Mulder's click (61% sensitivity, 62% accuracy), foot squeeze (41% sensitivity, 41% accuracy), plantar percussion (37% sensitivity, 36% accuracy), dorsal percussion (33% sensitivity, 26% accuracy), and light touch and pin prick (26% sensitivity, 25% accuracy). No correlation was found between the size of MN on US and the positive clinical tests, except for Mulder's click. The size of MN was significantly larger in patients with a positive Mulder's click (10.9 versus 8.5 mm, p = .016). The clinical assessment was comparable to US in diagnosing MN. The thumb index finger squeeze test was the most sensitive screening test for the clinical diagnosis of MN.\",\n",
       " '206723950b10580ced733cbacbfc23c85b268e13': 'The goal of this paper is to address the question: ‘why do lurkers lurk?’ Lurkers reportedly makeup the majority of members in online groups, yet little is known about them. Without insight into lurkers, our understanding of online groups is incomplete. Ignoring, dismissing, or misunderstanding lurking distorts knowledge of life online and may lead to inappropriate design of online environments. To investigate lurking, the authors carried out a study of lurking using in-depth, semi-structured interviews with ten members of online groups. 79 reasons for lurking and seven lurkers’ needs are identified from the interview transcripts. The analysis reveals that lurking is a strategic activity involving more than just reading posts. Reasons for lurking are categorized and a gratification model is proposed to explain lurker behavior.',\n",
       " '22d185c7ba066468f9ff1df03f1910831076e943': 'There are two main types of word representations: low-dimensional embeddings and high-dimensional distributional vectors, in which each dimension corresponds to a context word. In this paper, we initialize an embedding-learning model with distributional vectors. Evaluation on word similarity shows that this initialization significantly increases the quality of embeddings for rare words.',\n",
       " 'f1b3400e49a929d9f5bd1b15081a13120abc3906': 'This paper describes a technique to compare large text sources using word vector representations (word2vec) and dimensionality reduction (tSNE) and how it can be implemented using Python. The technique provides a bird’s-eye view of text sources, e.g. text summaries and their source material, and enables users to explore text sources like a geographical map. Word vector representations capture many linguistic properties such as gender, tense, plurality and even semantic concepts like \"capital city of\". Using dimensionality reduction, a 2D map can be computed where semantically similar words are close to each other. The technique uses the word2vec model from the gensim Python library and t-SNE from scikit-learn.',\n",
       " 'e49d662652885e9b71622713838c840cca9d33ed': 'The objective of technology-assisted review (\"TAR\") is to find as much relevant information as possible with reasonable effort. Quality is a measure of the extent to which a TAR method achieves this objective, while reliability is a measure of how consistently it achieves an acceptable result. We are concerned with how to define, measure, and achieve high quality and high reliability in TAR. When quality is defined using the traditional goal-post method of specifying a minimum acceptable recall threshold, the quality and reliability of a TAR method are both, by definition, equal to the probability of achieving the threshold. Assuming this definition of quality and reliability, we show how to augment any TAR method to achieve guaranteed reliability, for a quantifiable level of additional review effort. We demonstrate this result by augmenting the TAR method supplied as the baseline model implementation for the TREC 2015 Total Recall Track, measuring reliability and effort for 555 topics from eight test collections. While our empirical results corroborate our claim of guaranteed reliability, we observe that the augmentation strategy may entail disproportionate effort, especially when the number of relevant documents is low. To address this limitation, we propose stopping criteria for the model implementation that may be applied with no additional review effort, while achieving empirical reliability that compares favorably to the provably reliable method. We further argue that optimizing reliability according to the traditional goal-post method is inconsistent with certain subjective aspects of quality, and that optimizing a Taguchi quality loss function may be more apt.',\n",
       " 'e75cb14344eaeec987aa571d0009d0e02ec48a63': 'Increased requirements regarding ergonomic comfort, limited space, weight reduction, and electronic automation of functions and safety features are on the rise for future automotive gear levers. At the same time, current mechanical gear levers have restrictions to achieve this. In this paper, we present a monostable, miniaturized mechatronic gear lever to fulfill these requirements for automotive applications. This solution describes a gear lever for positioning in the center console of a car to achieve optimal ergonomics for dynamic driving, which enables both automatic and manual gear switching. In this paper, we describe the sensor and actuator concept, safety concept, recommended shift pattern, mechanical design, and the electronic integration of this shift-by-wire system in a typical automotive bus communication network. The main contribution of this paper is a successful system design and the integration of a mechatronic system in new applications for optimizing the human-machine interface inside road vehicles.',\n",
       " '66c410a2567e96dcff135bf6582cb26c9df765c4': 'Secure access is one of the fundamental problems in wireless mobile networks. Digital signature is a widely used technique to protect messages’ authenticity and nodes’ identities. From the practical perspective, to ensure the quality of services in wireless mobile networks, ideally the process of signature verification should introduce minimum delay. Batch cryptography technique is a powerful tool to reduce verification time. However, most of the existing works focus on designing batch verification algorithms for wireless mobile networks without sufficiently considering the impact of invalid signatures, which can lead to verification failures and performance degradation. In this paper, we propose a Batch Identification Game Model (BIGM) in wireless mobile networks, enabling nodes to find invalid signatures with reasonable delay no matter whether the game scenario is complete information or incomplete information. Specifically, we analyze and prove the existence of Nash Equilibriums (NEs) in both scenarios, to select the dominant algorithm for identifying invalid signatures. To optimize the identification algorithm selection, we propose a self-adaptive auto-match protocol which estimates the strategies and states of attackers based on historical information. Comprehensive simulation results in terms of NE reasonability, algorithm selection accuracy, and identification delay are provided to demonstrate that BIGM can identify invalid signatures more efficiently than existing algorithms.',\n",
       " '9a59a3719bf08105d4632898ee178bd982da2204': 'The  autonomous  vehicle  is  a  mobile  robot integrating  multi‐sensor  navigation  and  positioning, intelligent decision making and control  technology. This paper  presents  the  control  system  architecture  of  the autonomous vehicle, called “Intelligent Pioneer”, and the path  tracking  and  stability  of  motion  to  effectively navigate  in unknown  environments  is discussed.  In  this approach,  a  two  degree‐of‐freedom  dynamic  model  is developed to formulate the path‐tracking problem in state space format. For controlling the instantaneous path error, traditional  controllers  have  difficulty  in  guaranteeing performance and stability over a wide range of parameter changes and disturbances. Therefore, a newly developed adaptive‐PID  controller  will  be  used.  By  using  this approach the flexibility of the vehicle control system will be  increased  and  achieving  great  advantages. Throughout,  we  provide  examples  and  results  from Intelligent Pioneer and the autonomous vehicle using this approach  competed  in  the  2010  and  2011  Future Challenge of China. Intelligent Pioneer finished all of the competition programmes and won  first position  in 2010 and third position in 2011.',\n",
       " '17ebe1eb19655543a6b876f91d41917488e70f55': \"The brain processes information through multiple layers of neurons. This deep architecture is representationally powerful, but complicates learning because it is difficult to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame by multiplying error signals with all the synaptic weights on each neuron's axon and further downstream. However, this involves a precise, symmetric backward connectivity pattern, which is thought to be impossible in the brain. Here we demonstrate that this strong architectural constraint is not required for effective error propagation. We present a surprisingly simple mechanism that assigns blame by multiplying errors by even random synaptic weights. This mechanism can transmit teaching signals across multiple layers of neurons and performs as effectively as backpropagation on a variety of tasks. Our results help reopen questions about how the brain could use error signals and dispel long-held assumptions about algorithmic constraints on learning.\",\n",
       " '57b199e1d22752c385c34191c1058bcabb850d9f': \"Recent neurophysiological studies reveal that neurons in certain brain structures carry specific signals about past and future rewards. Dopamine neurons display a short-latency, phasic reward signal indicating the difference between actual and predicted rewards. The signal is useful for enhancing neuronal processing and learning behavioral reactions. It is distinctly different from dopamine's tonic enabling of numerous behavioral processes. Neurons in the striatum, frontal cortex, and amygdala also process reward information but provide more differentiated information for identifying and anticipating rewards and organizing goal-directed behavior. The different reward signals have complementary functions, and the optimal use of rewards in voluntary behavior would benefit from interactions between the signals. Addictive psychostimulant drugs may exert their action by amplifying the dopamine reward signal.\",\n",
       " '7592f8a1d4fa2703b75cad6833775da2ff72fe7b': 'The competitive MNIST handwritten digit recognition benchmark has a long history of broken records since 1998. The most recent advancement by others dates back 8 years (error rate 0.4%). Good old on-line back-propagation for plain multi-layer perceptrons yields a very low 0.35% error rate on the MNIST handwritten digits benchmark with a single MLP and 0.31% with a committee of seven MLP. All we need to achieve this until 2011 best result are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning.',\n",
       " '9539a0c4f8766c08dbaf96561cf6f1f409f5d3f9': 'Changes in neural responses based on spatial attention have been demonstrated in many areas of visual cortex, indicating that the neural correlate of attention is an enhanced response to stimuli at an attended location and reduced responses to stimuli elsewhere. Here we demonstrate non-spatial, feature-based attentional modulation of visual motion processing, and show that attention increases the gain of direction-selective neurons in visual cortical area MT without narrowing the direction-tuning curves. These findings place important constraints on the neural mechanisms of attention and we propose to unify the effects of spatial location, direction of motion and other features of the attended stimuli in a ‘feature similarity gain model’ of attention.',\n",
       " 'cbcd9f32b526397f88d18163875d04255e72137f': '',\n",
       " '19e2ad92d0f6ad3a9c76e957a0463be9ac244203': 'Based on cross-bispectrum, quadratic-nonlinearity coupling between two vibration signals is proposed and used to assess health conditions of rotating shafts in an AH-64D helicopter tail rotor drive train. Vibration data are gathered from two bearings supporting the shaft in an experimental helicopter drive train simulating different shaft conditions, namely, baseline, misalignment, imbalance, and combination of misalignment and imbalance. The proposed metric shows better capabilities in distinguishing different shaft settings than the conventional linear coupling based on cross-power spectrum.',\n",
       " '7b24aa024ca2037b097cfcb2ea73a60ab497b80e': 'Fear of security breaches has been a major r eason f or the business world’ s reluctance to embrace the Inter net as a viable means of communication. A widely adopted solution consists of physically separating private networks from the rest of Internet using firewalls. This paper discusses the curr ent cryptographic security measures available for the Internet infrastructur e as an alternative to physical segregation. First the IPsec ar chitecture including security protocols in the Internet Layer and the related key management pr oposals are introduced. The transport layer security protocol and security issues in the netw ork control and management are then presented. The paper is addr essed to r eaders with a basic understanding of common security mechanisms including encryption, authentication and key exchange techniques.',\n",
       " '525dc4242b21df23ba4e1ec0748cf46de0e8f5c0': 'OBJECTIVE\\nWe examined the associations between client attachment, client attachment to the therapist, and symptom change, as well as the effects of client-therapist attachment match on outcome. Clients (n = 67) and their therapists (n = 27) completed the ECR to assess attachment.\\n\\n\\nMETHOD\\nClients completed also the Client Attachment to Therapist scale three times (early, middle, and late sessions) and the OQ-45 at intake and four times over the course of a year of psychodynamic psychotherapy.\\n\\n\\nRESULTS\\nClients characterized by avoidant attachment and by avoidant attachment to their therapist showed the least improvement. A low-avoidant client-therapist attachment match led to a greater decrease in symptom distress than when a low-avoidant therapist treated a high-avoidant client.\\n\\n\\nCONCLUSIONS\\nThese findings suggest the importance of considering client-therapist attachment matching and the need to pay attention to the special challenges involved in treating avoidant clients in order to facilitate progress in psychotherapy.',\n",
       " '476edaffb4e613303012e7321dd319ba23abd0c3': 'We propose a new approach for dynamic control of redundant manipulators to deal with multiple prioritized tasks at the same time by utilizing null space projection techniques. The compliance control law is based on a new representation of the dynamics wherein specific null space velocity coordinates are introduced. These allow to efficiently exploit the kinematic redundancy according to the task hierarchy and lead to a dynamics formulation with block-diagonal inertia matrix. The compensation of velocity-dependent coupling terms between the tasks by an additional passive feedback action facilitates a stability analysis for the complete hierarchy based on semi-definite Lyapunov functions. No external forces have to be measured. Finally, the performance of the control approach is evaluated in experiments on a torque-controlled robot. © 2015 Elsevier Ltd. All rights reserved.',\n",
       " 'fdb1a478c6c566729a82424b3d6b37ca76c8b85e': 'What constitutes a good life? Few questions are of more fundamental importance to a positive psychology. Flow research has yielded one answer, providing an understanding of experiences during which individuals are fully involved in the present moment. Viewed through the experiential lens of flow, a good life is one that is characterized by complete absorption in what one does. In this chapter, we describe the flow model of optimal experience and optimal development, explain how flow and related constructs have been measured, discuss recent work in this area, and identify some promising directions for future research.',\n",
       " '7817db7b898a3458035174d914a7570d0b0efb7b': 'Purpose – The purpose of this paper is to explore the relationship between corporate social responsibility (CSR) and customer outcomes. Design/methodology/approach – This paper reviews the literature on CSR effects and satisfaction, noting gaps in the literature. Findings – A series of propositions is put forward to guide future research endeavours. Research limitations/implications – By understanding the likely impact on customer satisfaction of CSR initiatives vis-à-vis customer-centric initiatives, the academic research community can assist managers to understand how to best allocate company resources in situations of low customer satisfaction. Such endeavours are managerially relevant and topical. Researchers seeking to test the propositions put forward in this paper would be able to gain links with, and possibly attract funding from, banks to conduct their research. Such endeavours may assist researchers to redefine the stakeholder view by placing customers at the centre of a network of stakeholders. Practical implications – An understanding of how to best allocate company resources to increase the proportion of satisfied customers will allow bank marketers to reduce customer churn and hence increase market share and profits. Originality/value – Researchers have not previously conducted a comparative analysis of the effects of different CSR initiatives on customer satisfaction, nor considered whether more customer-centric initiatives are likely to be more effective in increasing the proportion of satisfied customers.',\n",
       " '60cc377d4d2b885594906d58bacb5732e8a04eb9': 'After a period where implementation speed was more important than integration, consistency and reduction of complexity, architectural considerations have become a key issue of information management in recent years again. Enterprise architecture is widely accepted as an essential mechanism for ensuring agility and consistency, compliance and efficiency. Although standards like TOGAF and FEAF have developed, however, there is no common agreement on which architecture layers, which artifact types and which dependencies constitute the essence of enterprise architecture. This paper contributes to the identification of essential elements of enterprise architecture by (1) specifying enterprise architecture as a hierarchical, multilevel system comprising aggregation hierarchies, architecture layers and views, (2) discussing enterprise architecture frameworks with regard to essential elements, (3) proposing interfacing requirements of enterprise architecture with other architecture models and (4) matching these findings with current enterprise architecture practice in several large companies.',\n",
       " '46e0faacf50c8053d38fb3cf2da7fbbfb2932977': 'Central to the vision of the smart grid is the deployment of smart meters that will allow autonomous software agents, representing the consumers, to optimise their use of devices and heating in the smart home while interacting with the grid. However, without some form of coordination, the population of agents may end up with overly-homogeneous optimised consumption patterns that may generate significant peaks in demand in the grid. These peaks, in turn, reduce the efficiency of the overall system, increase carbon emissions, and may even, in the worst case, cause blackouts. Hence, in this paper, we introduce a novel model of a Decentralised Demand Side Management (DDSM) mechanism that allows agents, by adapting the deferment of their loads based on grid prices, to coordinate in a decentralised manner. Specifically, using average UK consumption profiles for 26M homes, we demonstrate that, through an emergent coordination of the agents, the peak demand of domestic consumers in the grid can be reduced by up to 17% and carbon emissions by up to 6%. We also show that our DDSM mechanism is robust to the increasing electrification of heating in UK homes (i.e., it exhibits a similar efficiency).',\n",
       " '3d9e919a4de74089f94f5a1b2a167c66c19a241d': 'OBJECTIVE\\nTo determine the value of measuring maxillary length at 11-14 weeks of gestation in screening for trisomy 21.\\n\\n\\nMETHODS\\nIn 970 fetuses ultrasound examination was carried out for measurement of crown-rump length (CRL), nuchal translucency and maxillary length, and to determine if the nasal bone was present or absent, immediately before chorionic villus sampling for karyotyping at 11-14 weeks of gestation. In 60 cases the maxillary length was measured twice by the same operator to calculate the intraobserver variation in measurements.\\n\\n\\nRESULTS\\nThe median gestation was 12 (range, 11-14) weeks. The maxilla was successfully examined in all cases. The mean difference between paired measurements of maxillary length was -0.012 mm and the 95% limits of agreement were -0.42 (95% CI, -0.47 to -0.37) to 0.40 (95% CI, 0.35 to 0.44) mm. The fetal karyotype was normal in 839 pregnancies and abnormal in 131, including 88 cases of trisomy 21. In the chromosomally normal group the maxillary length increased significantly with CRL from a mean of 4.8 mm at a CRL of 45 mm to 8.3 mm at a CRL of 84 mm. In the trisomy 21 fetuses the maxillary length was significantly shorter than normal by 0.7 mm and in the trisomy 21 fetuses with absent nasal bone the maxilla was shorter than in those with present nasal bone by 0.5 mm. In fetuses with other chromosomal defects there were no significant differences from normal in the maxillary length.\\n\\n\\nCONCLUSION\\nAt 11-14 weeks of gestation, maxillary length in trisomy 21 fetuses is significantly shorter than in normal fetuses.',\n",
       " 'db9531c2677ab3eeaaf434ccb18ca354438560d6': 'E-commerce is undergoing an evolution through the adoption of Web 2.0 capabilities to enhance customer participation and achieve greater economic value. This new phenomenon is commonly referred to as social commerce, however it has not yet been fully understood. In addition to the lack of a stable and agreed-upon definition, there is little research on social commerce and no significant research dedicated to the design of social commerce platforms. This study offers literature review to explain the concept of social commerce, tracks its nascent state-of-the-art, and discusses relevant design features as they relate to e-commerce and Web 2.0. We propose a new model and a set of principles for guiding social commerce design. We also apply the model and guidelines to two leading social commerce platforms, Amazon and Starbucks on Facebook. The findings indicate that, for any social commerce website, it is critical to achieve a minimum set of social commerce design features. These design features must cover all the layers of the proposed model, including the individual, conversation, community and commerce levels. 2012 Elsevier B.V. All rights reserved.',\n",
       " '9d420ad78af7366384f77b29e62a93a0325ace77': 'This paper presents a novel audio fingerprinting method that is highly robust to a variety of audio distortions. It is based on an unconventional audio fingerprint generation scheme. The robustness is achieved by generating different versions of the spectrogram matrix of the audio signal by using a threshold based on the average of the spectral values to prune this matrix. We transform each version of this pruned spectrogram matrix into a 2-D binary image. Multiple versions of these 2-D images suppress noise to a varying degree. This varying degree of noise suppression improves likelihood of one of the images matching a reference image. To speed up matching, we convert each image into an n-dimensional vector, and perform a nearest neighbor search based on this n-dimensional vector. We give results with two different feature parameters and their combination. We test this method on TRECVID 2010 content-based copy detection evaluation dataset, and we validate the performance on TRECVID 2009 dataset also. Experimental results show the effectiveness of these features even when the audio is distorted. We compare the proposed method to two state-of-the-art audio copy detection systems, namely NN-based and Shazam systems. Our method by far outperforms Shazam system for all audio transformations (or distortions) in terms of detection performance, number of missed queries and localization accuracy. Compared to NN-based system, our approach reduces minimal Normalized Detection Cost Rate (min NDCR) by 23\\xa0% and improves localization accuracy by 24\\xa0%.',\n",
       " '8c15753cbb921f1b0ce4cd09b83415152212dbef': 'Gender dysphoria (also known as \"transsexualism\") is characterized as a discrepancy between anatomical sex and gender identity. Research points towards neurobiological influences. Due to the sexually dimorphic characteristics of the human voice, voice gender perception provides a biologically relevant function, e.g. in the context of mating selection. There is evidence for a better recognition of voices of the opposite sex and a differentiation of the sexes in its underlying functional cerebral correlates, namely the prefrontal and middle temporal areas. This fMRI study investigated the neural correlates of voice gender perception in 32 male-to-female gender dysphoric individuals (MtFs) compared to 20 non-gender dysphoric men and 19 non-gender dysphoric women. Participants indicated the sex of 240 voice stimuli modified in semitone steps in the direction to the other gender. Compared to men and women, MtFs showed differences in a neural network including the medial prefrontal gyrus, the insula, and the precuneus when responding to male vs. female voices. With increased voice morphing men recruited more prefrontal areas compared to women and MtFs, while MtFs revealed a pattern more similar to women. On a behavioral and neuronal level, our results support the feeling of MtFs reporting they cannot identify with their assigned sex.',\n",
       " '9281495c7ffc4d6d6e5305281c200f9b02ba70db': 'Complex IT outsourcing arrangements promise numerous benefits such as increased cost predictability and reduced costs, higher flexibility and scalability upon demand. Organizations trying to realize these benefits, however, face several security and compliance challenges. In this article, we investigate the pressure to take action with respect to such challenges and discuss avenues toward promising responses. We collected perceptions on security and compliance challenges from multiple stakeholders by means of a series of interviews and an online survey, first, to analyze the current and future relevance of the challenges as well as potential adverse effects on organizational performance and, second, to discuss the nature and scope of potential responses. The survey participants confirmed the current and future relevance of the six challenges auditing clouds, managing heterogeneity of services, coordinating involved parties, managing relationships between clients and vendors, localizing and migrating data and coping with lack of security awareness. Additionally, they perceived these challenges as affecting organizational performance adversely in case they are not properly addressed. Responses in form of organizational measures were considered more promising than technical ones concerning all challenges except localizing and migrating data, for which the opposite was true. Balancing relational and contractual governance as well as employing specific client and vendor capabilities is essential for the success of IT outsourcing arrangements, yet do not seem sufficient to overcome the investigated challenges. Innovations connecting the technical perspective of utility software with the business perspective of application software relevant for security and compliance management, however, nourish the hope that the benefits associated with complex IT outsourcing arrangements can be realized in the foreseeable future whilst addressing the security and compliance challenges. a 2013 Elsevier Ltd. All rights reserved. 61. .fraunhofer.de (D. Bachlechner), stefan.thalmann@uibk.ac.at (S. Thalmann), ronald.maier@ ier Ltd. All rights reserved. c om p u t e r s & s e c u r i t y 4 0 ( 2 0 1 4 ) 3 8e5 9 39',\n",
       " '919fa5c3a4f9c3c1c7ba407ccbac8ab72ba68566': 'The advancement of the next-generation sequencing technology enables mapping gene expression at the single-cell level, capable of tracking cell heterogeneity and determination of cell subpopulations using single-cell RNA sequencing (scRNA-seq). Unlike the objectives of conventional RNA-seq where differential expression analysis is the integral component, the most important goal of scRNA-seq is to identify highly variable genes across a population of cells, to account for the discrete nature of single-cell gene expression and uniqueness of sequencing library preparation protocol for single-cell sequencing. However, there is lack of generic expression variation model for different scRNA-seq data sets. Hence, the objective of this study is to develop a gene expression variation model (GEVM), utilizing the relationship between coefficient of variation (CV) and average expression level to address the over-dispersion of single-cell data, and its corresponding statistical significance to quantify the variably expressed genes (VEGs). We have built a simulation framework that generated scRNA-seq data with different number of cells, model parameters, and variation levels. We implemented our GEVM and demonstrated the robustness by using a set of simulated scRNA-seq data under different conditions. We evaluated the regression robustness using root-mean-square error (RMSE) and assessed the parameter estimation process by varying initial model parameters that deviated from homogeneous cell population. We also applied the GEVM on real scRNA-seq data to test the performance under distinct cases. In this paper, we proposed a gene expression variation model that can be used to determine significant variably expressed genes. Applying the model to the simulated single-cell data, we observed robust parameter estimation under different conditions with minimal root mean square errors. We also examined the model on two distinct scRNA-seq data sets using different single-cell protocols and determined the VEGs. Obtaining VEGs allowed us to observe possible subpopulations, providing further evidences of cell heterogeneity. With the GEVM, we can easily find out significant variably expressed genes in different scRNA-seq data sets.',\n",
       " 'd4caec47eeabb2eca3ce9e39b1fae5424634c731': 'Many robotic hands or prosthetic hands have been developed in the last several decades, and many use tendon-driven mechanisms for their transmissions. Robotic hands are now built with underactuated mechanisms, which have fewer actuators than degrees of freedom, to reduce mechanical complexity or to realize a biomimetic motion such as flexion of an index finger. The design is heuristic and it is useful to develop design methods for the underactuated mechanisms. This paper classifies mechanisms driven by tendons into three classes, and proposes a design method for them. The two classes are related to underactuated tendon-driven mechanisms, and these have been used without distinction so far. An index finger robot, which has four active tendons and two passive tendons, is developed and controlled with the proposed method.',\n",
       " '8d6ca2dae1a6d1e71626be6167b9f25d2ce6dbcc': 'Semi-supervised learning algorithms reduce the high cost of acquiring labeled training data by using both labeled and unlabeled data during learning. Deep Convolutional Networks (DCNs) have achieved great success in supervised tasks and as such have been widely employed in the semi-supervised learning. In this paper we leverage the recently developed Deep Rendering Mixture Model (DRMM), a probabilistic generative model that models latent nuisance variation, and whose inference algorithm yields DCNs. We develop an EM algorithm for the DRMM to learn from both labeled and unlabeled data. Guided by the theory of the DRMM, we introduce a novel nonnegativity constraint and a variational inference term. We report state-of-the-art performance on MNIST and SVHN and competitive results on CIFAR10. We also probe deeper into how a DRMM trained in a semi-supervised setting represents latent nuisance variation using synthetically rendered images. Taken together, our work provides a unified framework for supervised, unsupervised, and semisupervised learning.',\n",
       " '9bfc34ca3d3dd17ecdcb092f2a056da6cb824acd': \"This article may be used for research, teaching, and private study purposes. Any substantial or systematic reproduction, redistribution, reselling, loan, sub-licensing, systematic supply, or distribution in any form to anyone is expressly forbidden. The publisher does not give any warranty express or implied or make any representation that the contents will be complete or accurate or up to date. The accuracy of any instructions, formulae, and drug doses should be independently verified with primary sources. The publisher shall not be liable for any loss, actions, claims, proceedings, demand, or costs or damages whatsoever or howsoever caused arising directly or indirectly in connection with or arising out of the use of this material. Population mobility, i.e. the movement and contact of individuals across geographic space, is one of the essential factors that determine the course of a pandemic disease spread. This research views both individual-based daily activities and a pandemic spread as spatial interaction problems, where locations interact with each other via the visitors that they share or the virus that is transmitted from one place to another. The research proposes a general visual analytic approach to synthesize very large spatial interaction data and discover interesting (and unknown) patterns. The proposed approach involves a suite of visual and computational techniques, including (1) a new graph partitioning method to segment a very large interaction graph into a moderate number of spatially contiguous subgraphs (regions); (2) a reorderable matrix, with regions 'optimally' ordered on the diagonal, to effectively present a holistic view of major spatial interaction patterns; and (3) a modified flow map, interactively linked to the reorderable matrix, to enable pattern interpretation in a geographical context. The implemented system is able to visualize both people's daily movements and a disease spread over space in a similar way. The discovered spatial interaction patterns provide valuable insight for designing effective pandemic mitigation strategies and supporting decision-making in time-critical situations.\",\n",
       " '0428c79e5be359ccd13d63205b5e06037404967b': 'Stochastic bandit problems have been analyzed from two different perspectives: a frequentist view, where the parameter is a deterministic unknown quantity, and a Bayesian approach, where the parameter is drawn from a prior distribution. We show in this paper that methods derived from this second perspective prove optimal when evaluated using the frequentist cumulated regret as a measure of performance. We give a general formulation for a class of Bayesian index policies that rely on quantiles of the posterior distribution. For binary bandits, we prove that the corresponding algorithm, termed BayesUCB, satisfies finite-time regret bounds that imply its asymptotic optimality. More generally, Bayes-UCB appears as an unifying framework for several variants of the UCB algorithm addressing different bandit problems (parametric multi-armed bandits, Gaussian bandits with unknown mean and variance, linear bandits). But the generality of the Bayesian approach makes it possible to address more challenging models. In particular, we show how to handle linear bandits with sparsity constraints by resorting to Gibbs sampling.',\n",
       " 'e030aa1ea57ee47d3f3a0ce05b7e983f95115f1a': 'BACKGROUND\\nGiven the importance of regular physical activity, it is crucial to evaluate the factors favoring participation in physical activity. We aimed to report the psychometric analysis of the Farsi version of the Physical Activity and Leisure Motivation Scale (PALMS).\\n\\n\\nMETHODS\\nThe Farsi version of PALMS was completed by 406 healthy adult individuals to test its factor structure and concurrent validity and reliability.\\n\\n\\nRESULTS\\nConducting the exploratory factor analysis revealed nine factors that accounted for 64.6% of the variances. The PALMS reliability was supported with a high internal consistency of 0.91 and a high test-retest reliability of 0.97 (95% CI: 0.97-0.98). The association between the PALMS and its previous version Recreational Exercise Motivation Measure scores was strongly significant (r= 0.86, P < 0.001).\\n\\n\\nCONCLUSION\\nWe have shown that the Farsi version of the PALMS appears to be a valuable instrument to measure motivation for physical activity and leisure.',\n",
       " '6570489a6294a5845adfd195a50a226f78a139c1': 'This article focuses on examining the determinants and mediators of the purchase intention of nononline purchasers between ages 31 and 60 who mostly have strong purchasing power. It propose anew online purchase intention model by integrating the technology acceptance model with additional determinants and adding habitual online usage as a new mediator. Based on a sample of more than 300 middle-aged non-online purchasers, beyond some situationally-specific predictor variables, online purchasing attitude and habitual online usage are key mediators. Personal awareness of security only affects habitual online usage, t indicating a concern of middle-aged users. Habitual online usage is a',\n",
       " '1eb92d883dab2bc6a408245f4766f4c5d52f7545': \"Spatial crowdsourcing has gained emerging interest from both research communities and industries. Most of current spatial crowdsourcing frameworks assume independent and atomic tasks. However, there could be some cases that one needs to crowdsource a spatial complex task which consists of some spatial sub-tasks (i.e., tasks related to a specific location). The spatial complex task's assignment requires assignments of all of its sub-tasks. The currently available frameworks are inapplicable to such kind of tasks. In this paper, we introduce a novel approach to crowdsource spatial complex tasks. We first formally define the Maximum Complex Task Assignment (MCTA) problem and propose alternative solutions. Subsequently, we perform various experiments using both real and synthetic datasets to investigate and verify the usability of our proposed approach.\",\n",
       " '44298a4cf816fe8d55c663337932724407ae772b': 'Most policy search algorithms require thousands of training episodes to find an effective policy, which is often infeasible with a physical robot. This survey article focuses on the extreme other end of the spectrum: how can a robot adapt with only a handful of trials (a dozen) and a few minutes? By analogy with the word “big-data”, we refer to this challenge as “micro-data reinforcement learning”. We show that a first strategy is to leverage prior knowledge on the policy structure (e.g., dynamic movement primitives), on the policy parameters (e.g., demonstrations), or on the dynamics (e.g., simulators). A second strategy is to create data-driven surrogate models of the expected reward (e.g., Bayesian optimization) or the dynamical model (e.g., model-based policy search), so that the policy optimizer queries the model instead of the real system. Overall, all successful micro-data algorithms combine these two strategies by varying the kind of model and prior knowledge. The current scientific challenges essentially revolve around scaling up to complex robots (e.g., humanoids), designing generic priors, and optimizing the computing time.',\n",
       " '35318f1dcc88c8051911ba48815c47d424626a92': \"TED Talks are short, powerful talks given by some of the world's brightest minds - scientists, philanthropists, businessmen, artists, and many others. Funded by members and advertising, these talks are free to access by the public on the TED website and TED YouTube channel, and many videos have become viral phenomena. In this research project, we perform a visual analysis of TED Talk videos and playlists to gain a good understanding of the trends and relationships between TED Talk topics.\",\n",
       " 'a42569c671b5f9d0fe2007af55199d668dae491b': 'To unlock the wealth of the healthcare data, we often need to link the real-world text snippets to the referred medical concepts described by the canonical descriptions. However, existing healthcare concept linking methods, such as dictionary-based and simple machine learning methods, are not effective due to the word discrepancy between the text snippet and the canonical concept description, and the overlapping concept meaning among the fine-grained concepts. To address these challenges, we propose a Neural Concept Linking (NCL) approach for accurate concept linking using systematically integrated neural networks. We call the novel neural network architecture as the COMposite AttentIonal encode-Decode neural network (COM-AID). COM-AID performs an encode-decode process that encodes a concept into a vector and decodes the vector into a text snippet with the help of two devised contexts. On the one hand, it injects the textual context into the neural network through the attention mechanism, so that the word discrepancy can be overcome from the semantic perspective. On the other hand, it incorporates the structural context into the neural network through the attention mechanism, so that minor concept meaning differences can be enlarged and effectively differentiated. Empirical studies on two real-world datasets confirm that the NCL produces accurate concept linking results and significantly outperforms state-of-the-art techniques.',\n",
       " '30f2b6834d6f2322da204f36ad24ddf43cc45d33': 'Classification of large, static collections of XML data has been intensively studied in the last several years. Recently however, the data processing paradigm is shifting from static to streaming data, where documents have to be processed online using limited memory and class definitions can change with time in an event called concept drift. As most existing XML classifiers are capable of processing only static data, there is a need to develop new approaches dedicated for streaming environments. In this paper, we propose a new classification algorithm for XML data streams called XSC. The algorithm uses incrementally mined frequent subtrees and a tree-subtree similarity measure to classify new documents in an associative manner. The proposed approach is experimentally evaluated against eight state-of-the-art stream classifiers on real and synthetic data. The results show that XSC performs significantly better than competitive algorithms in terms of accuracy and memory usage.',\n",
       " '14829636fee5a1cf8dee9737849a8e2bdaf9a91f': 'Bitcoin is a distributed digital currency which has attracted a substantial number of users. We perform an in-depth investigation to understand what made Bitcoin so successful, while decades of research on cryptographic e-cash has not lead to a large-scale deployment. We ask also how Bitcoin could become a good candidate for a long-lived stable currency. In doing so, we identify several issues and attacks of Bitcoin, and propose suitable techniques to address them.',\n",
       " '35fe18606529d82ce3fc90961dd6813c92713b3c': \"Bit coin has emerged as the most successful cryptographic currency in history. Within two years of its quiet launch in 2009, Bit coin grew to comprise billions of dollars of economic value despite only cursory analysis of the system's design. Since then a growing literature has identified hidden-but-important properties of the system, discovered attacks, proposed promising alternatives, and singled out difficult future challenges. Meanwhile a large and vibrant open-source community has proposed and deployed numerous modifications and extensions. We provide the first systematic exposition Bit coin and the many related crypto currencies or 'altcoins.' Drawing from a scattered body of knowledge, we identify three key components of Bit coin's design that can be decoupled. This enables a more insightful analysis of Bit coin's properties and future stability. We map the design space for numerous proposed modifications, providing comparative analyses for alternative consensus mechanisms, currency allocation mechanisms, computational puzzles, and key management tools. We survey anonymity issues in Bit coin and provide an evaluation framework for analyzing a variety of privacy-enhancing proposals. Finally we provide new insights on what we term disinter mediation protocols, which absolve the need for trusted intermediaries in an interesting set of applications. We identify three general disinter mediation strategies and provide a detailed comparison.\",\n",
       " '3d16ed355757fc13b7c6d7d6d04e6e9c5c9c0b78': '',\n",
       " '5e86853f533c88a1996455d955a2e20ac47b3878': \"Bitcoin is a digital currency that unlike traditional currencies does not rely on a centralized authority. Instead Bitcoin relies on a network of volunteers that collectively implement a replicated ledger and verify transactions. In this paper we analyze how Bitcoin uses a multi-hop broadcast to propagate transactions and blocks through the network to update the ledger replicas. We then use the gathered information to verify the conjecture that the propagation delay in the network is the primary cause for blockchain forks. Blockchain forks should be avoided as they are symptomatic for inconsistencies among the replicas in the network. We then show what can be achieved by pushing the current protocol to its limit with unilateral changes to the client's behavior.\",\n",
       " '5fb1285e05bbd78d0094fe8061c644ea09d9da8d': 'Bitcoin is a decentralized payment system that relies on Proof-of-Work (PoW) to verify payments. Nowadays, Bitcoin is increasingly used in a number of fast payment scenarios, where the time between the exchange of currency and goods is short (in the order of few seconds). While the Bitcoin payment verification scheme is designed to prevent double-spending, our results show that the system requires tens of minutes to verify a transaction and is therefore inappropriate for fast payments. An example of this use of Bitcoin was recently reported in the media: Bitcoins were used as a form of \\\\emph{fast} payment in a local fast-food restaurant. Until now, the security of fast Bitcoin payments has not been studied. In this paper, we analyze the security of using Bitcoin for fast payments. We show that, unless appropriate detection techniques are integrated in the current Bitcoin implementation, double-spending attacks on fast payments succeed with overwhelming probability and can be mounted at low cost. We further show that the measures recommended by Bitcoin developers for the use of Bitcoin in fast payments are not always effective in detecting double-spending; we show that if those recommendations are integrated in future Bitcoin implementations, double-spending attacks on Bitcoin will still be possible. Finally, we propose and implement a modification to the existing Bitcoin implementation that ensures the detection of double-spending attacks against fast payments.',\n",
       " 'd2920567fb66bc69d92ab2208f6455e37ce6138b': 'The objectives of this research are to co-create understanding and knowledge on the phenomenon of disruptive innovation in order to provide pragmatic clarity on the term’s meaning, impact and implications. This will address the academic audience’s gap in knowledge and provide help to practitioners wanting to understand how disruptive innovation can be fostered as part of a major competitive strategy. This paper reports on the first eighteen months of a three year academic and industrial investigation. It presents a new pragmatic definition drawn from the literature and an overview of the conceptual framework for disruptive innovation that was co-created via the collaborative efforts of academia and industry. The barriers to disruptive innovation are presented and a best practice case study of how one company is overcoming these barriers is described. The remainder of the research, which is supported by a European Commission co-sponsored project called Disrupt-it, will focus on developing and validating tools to help overcome these barriers. Thomond, P., Herzberg, T. and Lettice, F. (2003). \"Disruptive Innovation: Removing the Innovators’ Dilemma\". Knowledge into Practice British Academy of Management Annual Conference, Harrogate, UK, September 2003. 2 1.0. Introduction and Background. In his ground breaking book “The Innovator’s Dilemma: When New Technologies Cause Great Firms to Fail”, Clayton Christensen first coined the phrase ‘disruptive technologies’. He showed that time and again almost all the organisations that have ‘died’ or been displaced from their industries because of a new paradigm of customer offering could see the disruption coming but did nothing until it was too late (Christensen, 1997). They assess the new approaches or technologies and frame them as either deficient or as an unlikely threat much to the managers’ regret and the organisation’s demise (Christensen 2002). In the early 1990s, major airlines such as British Airways decided that the opportunities afforded by a low-cost, point-to point no frills strategy such as that introduced by the newly formed Ryanair was an unlikely threat. By the mid-1990’s other newcomers such as easyJet had embraced Ryanair’s foresight and before long, the ‘low cost’ approach had captured a large segment of the market. Low-cost no frills proved a hit with European travellers but not with the established airlines who had either ignored the threat or failed to capitalise on the approach. Today DVD technology and Charles Schwab are seen to be having a similar impact upon the VHS industry and Merrill Lynch respectively, however, disruption is not just a recent phenomenon it has firm foundations as a trend in the past that will also inevitably occur in the future. Examples of past disruptive innovations would include the introduction of the telegraph and its impact upon businesses like Pony Express and the transistor’s impact upon the companies that produced cathode ray tubes. Future predictions include the impact of Light Emitting Diode’ (L.E.D.) technology and its potential to completely disrupt the traditional light bulb sector and its supporting industries. More optimistically, Christensen (2002) further shows that the process of disruptive innovation has been one of the fundamental causal mechanisms through which access to life improving products and services has been increased and the basis on which long term organisational survival could be ensured (Christensen, 1997). In spite of the proclaimed importance of disruptive innovation and the ever increasing interest from both the business and academic press alike, there still appears to be a disparity between rhetoric and reality. To date, the multifaceted and interrelated issues of disruptive innovation have not been investigated in depth. The phenomenon with examples has been described by a number of authors (Christensen, 1997, Moore, 1995 Gilbert and Bower, 2002) and practitioner orientated writers have begun to offer strategies for responding to disruptive Thomond, P., Herzberg, T. and Lettice, F. (2003). \"Disruptive Innovation: Removing the Innovators’ Dilemma\". Knowledge into Practice British Academy of Management Annual Conference, Harrogate, UK, September 2003. 3 change (Charitou and Markides, 2003, Rigby and Corbett, 2002, Rafi and Kampas, 2002). However, a deep integrated understanding of the entire subject is missing. In particular, there is an industrial need and academic gap knowledge in the pragmatic comprehension of how organisations can understand and foster disruptive innovation as part of a major competitive strategy. The objectives of this research are to co-create understanding and knowledge on the phenomenon of disruptive innovation in order to provide pragmatic clarity on the term’s meaning, impact and implications. This will address the academic audience’s gap in knowledge and provide help to practitioners wanting to understand how disruptive innovation can be fostered as part of a major competitive strategy. The current paper reports on the first eighteen months of a three year academic and industrial investigation. It presents a new pragmatic definition drawn from the literature and an overview of the conceptual framework for disruptive innovation that was co-created via the collaborative efforts of academia and industry. The barriers to disruptive innovation are presented and a best practice case study of how one company is overcoming these barriers is described. The research contributes to “Disrupt-it”, a €3million project for the Information Society Technologies Commission under the 5th Framework Program of the European Union, which will focus on developing and validating tools to help organisations foster disruptive innovation. 2.0 Understanding the Phenomenon of Disruptive Innovation. ‘Disruptive Innovation’, ‘Disruptive Technologies’ and ‘Disruptive Business Strategies’ are emerging and increasingly prominent business terms that are used to describe a form of revolutionary change. They are receiving ever more academic and industrial attention, yet these terms are still poorly defined and not well understood. A key objective of this research is to improve the understanding of disruptive innovation by drawing together multiple perspectives on the topic, as shown in Figure 1, into a more holistic and comprehensive definition. Much of the past investigation into discontinuous and disruptive innovation has been path dependent upon the researchers’ investigative history. For example, Hamel’s strategy background leads him to see disruptive innovation through the lens of the ‘business model’; whereas Christensen’s technologically orientated past leads to a focus on ‘disruptive technologies’. What many researchers share is the view that firms need to periodically engage in the process of revolutionary change for long-term survival and this is not a new Thomond, P., Herzberg, T. and Lettice, F. (2003). \"Disruptive Innovation: Removing the Innovators’ Dilemma\". Knowledge into Practice British Academy of Management Annual Conference, Harrogate, UK, September 2003. 4 phenomenon (Christensen, 1997; Christensen and Rosenbloom, 1995; Hamel, 2000; Schumpeter, 1975, Tushman and Anderson, 1986; Tushman and Nadler, 1986, Gilbert and Bower, 2002; Rigby and Corbett, 2002; Charitou and Markides, 2003; Foster and Kaplan, 2001; Thomond and Lettice, 2002). Disruptive innovation has also been defined as “a technology, product or process that creeps up from below an existing business and threatens to displace it. Typically, the disrupter offers lower performance and less functionality... The product or process is good enough for a meaningful number of customers – indeed some don’t buy the older version’s higher functionality and welcome the disruption’s simplicity. And gradually, the new product or process improves to the point where it displaces the incumbent.” (Rafi and Kampas p 8, 2002). This definition borrows heavily from the work of Christensen (1997), which in turn has some of its origins in the findings of Dosi (1982). For example, each of the cases of disruptive innovation mentioned thus far represents a new paradigm of customer offering. Dosi (1982) claims that these can be represented as discontinuities in trajectories of progress as defined within earlier paradigms where a technological paradigm is a pattern of solutions for selected technological problems. In fact, new paradigms redefine the future meaning of progress and a new class of problems becomes the target of normal incremental innovation (Dosi, 1982). Therefore, disruptive innovations appear to typify a particular type of ‘discontinuous innovation’ (a term which has received much more academic attention). The same characteristics are found, except that disruptive innovations first establish their commercial footing in new or simple market niches by enabling customers to do things that only specialists could do before (e.g. low cost European airlines are opening up air travel to thousands that did not fly before) and that these new offerings, through a period of exploitation, migrate upmarket and eventually redefine the paradigms and value propositions on which the existing industry is based (Christensen, 1997, 2002; Moore, 1995; Business Model Charitou & Markides, 2003 Hamel, 2000',\n",
       " '5e90e57fccafbc78ecbac1a78c546b7db9a468ce': 'Most technical and scientific terms are comprised of complex, multi-word noun phrases but certainly not all noun phrases are technical or scientific terms. The distinction of specific terminology from common non-specific noun phrases can be based on the observation that terms reveal a much lesser degree of distributional variation than non-specific noun phrases. We formalize the limited paradigmatic modifiability of terms and, subsequently, test the corresponding algorithm on bigram, trigram and quadgram noun phrases extracted from a 104-million-word biomedical text corpus. Using an already existing and community-wide curated biomedical terminology as an evaluation gold standard, we show that our algorithm significantly outperforms standard term identification measures and, therefore, qualifies as a high-performant building block for any terminology identification system. We also provide empirical evidence that the superiority of our approach, beyond a 10-million-word threshold, is essentially domain- and corpus-size-independent.',\n",
       " '991891e3aa226766dcb4ad7221045599f8607685': 'Hybrid and electric vehicles have been the focus of many academic and industrial studies to reduce transport pollution; they are now established products. In hybrid and electric vehicles, the drive motor should have high torque density, high power density, high efficiency, strong physical structure and variable speed range. An axial flux induction motor is an interesting solution, where the motor is a double sided axial flux machine. This can significantly increase torque density. In this paper a review of the axial flux motor for automotive applications, and the different possible topologies for the axial field motor, are presented.',\n",
       " '11b111cbe79e5733fea28e4b9ff99fe7b4a4585c': 'The discovery of vulnerabilities in source code is a key for securing computer systems. While specific types of security flaws can be identified automatically, in the general case the process of finding vulnerabilities cannot be automated and vulnerabilities are mainly discovered by manual analysis. In this paper, we propose a method for assisting a security analyst during auditing of source code. Our method proceeds by extracting abstract syntax trees from the code and determining structural patterns in these trees, such that each function in the code can be described as a mixture of these patterns. This representation enables us to decompose a known vulnerability and extrapolate it to a code base, such that functions potentially suffering from the same flaw can be suggested to the analyst. We evaluate our method on the source code of four popular open-source projects: LibTIFF, FFmpeg, Pidgin and Asterisk. For three of these projects, we are able to identify zero-day vulnerabilities by inspecting only a small fraction of the code bases.',\n",
       " '0dbed89ea3296f351eb986cc02678c7a33d50945': 'Quantum computers (QCs) have many potential hardware implementations ranging from solid-state silicon-based structures to electron-spin qubits on liquid helium. However, all QCs must contend with gate infidelity and qubit state decoherence over time. Quantum error correcting codes (QECCs) have been developed to protect program qubit states from such noise. Previously, Monte Carlo noise simulators have been developed to model the effectiveness of QECCs in combating decoherence. The downside to this random sampling approach is that it may take days or weeks to produce enough samples for an accurate measurement. We present an alternative noise modeling approach that performs combinatorial analysis rather than random sampling. This model tracks the progression of the most likely error states of the quantum program through its course of execution. This approach has the potential for enormous speedups versus the previous Monte Carlo methodology. We have found speedups with the combinatorial model on the order of 100X-1,000X over the Monte Carlo approach when analyzing applications utilizing the [[7,1,3]] QECC. The combinatorial noise model has significant memory requirements, and we analyze its scaling properties relative to the size of the quantum program. Due to its speedup, this noise model is a valuable alternative to traditional Monte Carlo simulation.',\n",
       " '47f0455d65a0823c70ce7cce9749f3abd826e0a7': 'Given a large graph, how can we calculate the relevance between nodes fast and accurately? Random walk with restart (RWR) provides a good measure for this purpose and has been applied to diverse data mining applications including ranking, community detection, link prediction, and anomaly detection. Since calculating RWR from scratch takes a long time, various preprocessing methods, most of which are related to inverting adjacency matrices, have been proposed to speed up the calculation. However, these methods do not scale to large graphs because they usually produce large dense matrices that do not fit into memory. In addition, the existing methods are inappropriate when graphs dynamically change because the expensive preprocessing task needs to be computed repeatedly.\\n In this article, we propose Bear, a fast, scalable, and accurate method for computing RWR on large graphs. Bear has two versions: a preprocessing method BearS for static graphs and an incremental update method BearD for dynamic graphs. BearS consists of the preprocessing step and the query step. In the preprocessing step, BearS reorders the adjacency matrix of a given graph so that it contains a large and easy-to-invert submatrix, and precomputes several matrices including the Schur complement of the submatrix. In the query step, BearS quickly computes the RWR scores for a given query node using a block elimination approach with the matrices computed in the preprocessing step. For dynamic graphs, BearD efficiently updates the changed parts in the preprocessed matrices of BearS based on the observation that only small parts of the preprocessed matrices change when few edges are inserted or deleted. Through extensive experiments, we show that BearS significantly outperforms other state-of-the-art methods in terms of preprocessing and query speed, space efficiency, and accuracy. We also show that BearD quickly updates the preprocessed matrices and immediately computes queries when the graph changes.',\n",
       " '239222aead65a66be698036d04e4af6eaa24b77b': 'Clustering provides an effective way for prolonging the lifetime of a wireless sensor network. Current clustering algorithms usually utilize two techniques, selecting cluster heads with more residual energy and rotating cluster heads periodically, to distribute the energy consumption among nodes in each cluster and extend the network lifetime. However, they rarely consider the hot spots problem in multihop wireless sensor networks. When cluster heads cooperate with each other to forward their data to the base station, the cluster heads closer to the base station are burdened with heavy relay traffic and tend to die early, leaving areas of the network uncovered and causing network partition. To address the problem, we propose an energy-efficient unequal clustering (EEUC) mechanism for periodical data gathering in wireless sensor networks. It partitions the nodes into clusters of unequal size, and clusters closer to the base station have smaller sizes than those farther away from the base station. Thus cluster heads closer to the base station can preserve some energy for the inter-cluster data forwarding. We also propose an energy-aware multihop routing protocol for the inter-cluster communication. Simulation results show that our unequal clustering mechanism balances the energy consumption well among all sensor nodes and achieves an obvious improvement on the network lifetime',\n",
       " 'd19f938c790f0ffd8fa7fccc9fd7c40758a29f94': '',\n",
       " '7a5ae36df3f08df85dfaa21fead748f830d5e4fa': 'We consider a transfer-learning problem by using the parameter transfer approach, where a suitable parameter of feature mapping is learned through one task and applied to another objective task. Then, we introduce the notion of the local stability and parameter transfer learnability of parametric feature mapping, and thereby derive a learning bound for parameter transfer algorithms. As an application of parameter transfer learning, we discuss the performance of sparse coding in selftaught learning. Although self-taught learning algorithms with plentiful unlabeled data often show excellent empirical performance, their theoretical analysis has not been studied. In this paper, we also provide the first theoretical learning bound for self-taught learning.',\n",
       " '2b695f4060e78f9977a3da1c01a07a05a3f94b28': 'Intelligent tutoring systems research aims to produce systems that meet or exceed the effectiveness of one on one expert human tutoring. Theory and empirical study suggest that affective states of the learner must be addressed to achieve this goal. While many affective measures can be utilized, posture offers the advantages of non intrusiveness and ease of interpretation. This paper presents an accurate posture estimation algorithm applied to a computer mediated tutoring corpus of depth recordings. Analyses of posture and session level student reports of engagement and cognitive load identified significant patterns. The results indicate that disengagement and frustration may coincide with closer postural positions and more movement, while focused attention and less frustration occur with more distant, stable postural positions. It is hoped that this work will lead to intelligent tutoring systems that recognize a greater breadth of affective expression through channels of posture and gesture.',\n",
       " 'c28bcaab43e57b9b03f09fd2237669634da8a741': 'The neural basis of decision making has been an elusive concept largely due to the many subprocesses associated with it. Recent efforts involving neuroimaging, neuropsychological studies, and animal work indicate that the prefrontal cortex plays a central role in several of these subprocesses. The frontal lobes are involved in tasks ranging from making binary choices to making multi-attribute decisions that require explicit deliberation and integration of diverse sources of information. In categorizing different aspects of decision making, a division of the prefrontal cortex into three primary regions is proposed. (1) The orbitofrontal and ventromedial areas are most relevant to deciding based on reward values and contribute affective information regarding decision attributes and options. (2) Dorsolateral prefrontal cortex is critical in making decisions that call for the consideration of multiple sources of information, and may recruit separable areas when making well defined versus poorly defined decisions. (3) The anterior and ventral cingulate cortex appear especially relevant in sorting among conflicting options, as well as signaling outcome-relevant information. This topic is broadly relevant to cognitive neuroscience as a discipline, as it generally comprises several aspects of cognition and may involve numerous brain regions depending on the situation. The review concludes with a summary of how these regions may interact in deciding and possible future research directions for the field.',\n",
       " '3ec40e4f549c49b048cd29aeb0223e709abc5565': 'With the development of Web 2.0 and cyber city modeling, an increasing number of 3D models have been available on web-based model-sharing platforms with many applications such as navigation, urban planning, and virtual reality. Based on the concept of data reuse, a 3D model retrieval system is proposed to retrieve building models similar to a user-specified query. The basic idea behind this system is to reuse these existing 3D building models instead of reconstruction from point clouds. To efficiently retrieve models, the models in databases are compactly encoded by using a shape descriptor generally. However, most of the geometric descriptors in related works are applied to polygonal models. In this study, the input query of the model retrieval system is a point cloud acquired by Light Detection and Ranging (LiDAR) systems because of the efficient scene scanning and spatial information collection. Using Point clouds with sparse, noisy, and incomplete sampling as input queries is more difficult than that by using 3D models. Because that the building roof is more informative than other parts in the airborne LiDAR point cloud, an image-based approach is proposed to encode both point clouds from input queries and 3D models in databases. The main goal of data encoding is that the models in the database and input point clouds can be consistently encoded. Firstly, top-view depth images of buildings are generated to represent the geometry surface of a building roof. Secondly, geometric features are extracted from depth images based on height, edge and plane of building. Finally, descriptors can be extracted by spatial histograms and used in 3D model retrieval system. For data retrieval, the models are retrieved by matching the encoding coefficients of point clouds and building models. In experiments, a database including about 900,000 3D models collected from the Internet is used for evaluation of data retrieval. The results of the proposed method show a clear superiority over related methods.',\n",
       " '2433254a9df37729159daa5eeec56123e122518e': 'This article reviews recently published research about consumers in digital and social media marketing settings. Five themes are identified: (i) consumer digital culture, (ii) responses to digital advertising, (iii) effects of digital environments on consumer behavior, (iv) mobile environments, and (v) online word of mouth (WOM). Collectively these articles shed light from many different angles on how consumers experience, influence, and are influenced by the digital environments in which they are situated as part of their daily lives. Much is still to be understood, and existing knowledge tends to be disproportionately focused on WOM, which is only part of the digital consumer experience. Several directions for future research are advanced to encourage researchers to consider a broader range of phenomena.',\n",
       " '399bc455dcbaf9eb0b4144d0bc721ac4bb7c8d59': 'A spreadsheet-like \"direct manipulation\" interface is more intuitive for many non-technical database users compared to traditional alternatives, such as visual query builders. The construction of such a direct manipulation interfacemay appear straightforward, but there are some significant challenges. First, individual direct manipulation operations cannot be too complex, so expressive power has to be achieved through composing (long) sequences of small operations. Second, all intermediate results are visible to the user, so grouping and ordering are material after every small step. Third, users often find the need to modify previously specified queries. Since manipulations are specified one step at a time, there is no actual queryexpression to modify. Suitable means must be provided to address this need. Fourth, the order in which manipulations are performed by the user should not affect the results obtained, to avoid user confusion. We address the aforementioned challenges by designing a new spreadsheet algebra that: i) operates on recursively grouped multi-sets, ii) contains a selectively designed set of operators capable of expressing at least all single-block SQL queries and can be intuitively implemented in a spreadsheet, iii) enables query modification by the notion of modifiable query state, and iv) requires no ordering in unary data manipulation operators since they are all designed to commute. We built a prototype implementation of the spreadsheet algebra and show, through user studies with non-technical subjects, that the resultant query interface is easier to use than a standard commercial visual query builder.',\n",
       " '1eff385c88fd1fdd1c03fd3fb573de2530b73f99': 'Objective self-awareness theory has undergone fundamental changes in the 3 decades since Duval and Wicklund\\'s (1972) original formulation. We review new evidence that bears on the basic tenets of the theory. Many of the assumptions of self-awareness theory require revision, particularly how expectancies influence approach and avoidance of self-standard discrepancies; the nature of standards, especially when they are changed; and the role of causal attribution in directing discrepancy reduction. However, several unresolved conceptual issues remain; future theoretical and empirical directions are discussed. Article: The human dilemma is that which arises out of a man\\'s capacity to experience himself as both subject and object at the same time. Both are necessary--for the science of psychology, for therapy, and for gratifying living. (May, 1967, p. 8) Although psychological perspectives on the self have a long history (e.g., Cooley, 1902; James, 1890; Mead, 1934), experimental research on the self has emerged only within the last 40 years. One of the earliest \"self theories\" was objective self-awareness (OSA) theory (Duval & Wicklund, 1972). OSA theory was concerned with the self-reflexive quality of the consciousness. Just as people can apprehend the existence of environmental stimuli, they can be aware of their own existence: \"When attention is directed inward and the individual\\'s consciousness is focused on himself, he is the object of his own consciousness--hence \\'objective\\' self awareness\" (Duval & Wicklund, 1972, p. 2). This is contrasted with \"subjective self-awareness\" that results when attention is directed away from the self and the person \"experiences himself as the source of perception and action\" (Duval & Wicklund, 1972, p. 3). By this, Duval and Wicklund (1972,chap. 3) meant consciousness of one\\'s existence on an organismic level, in which such existence is undifferentiated as a separate and distinct object in the world. OSA theory has stimulated a lot of research and informed basic issues in social psychology, such as emotion (Scheier & Carver, 1977), attribution (Duval & Wicklund, 1973), attitude--behavior consistency (Gibbons, 1983), self-standard comparison (Duval & Lalwani, 1999), prosocial behavior (Froming, Nasby, & McManus, 1998), deindividuation (Diener, 1979), stereotyping (Macrae, Bodenhausen, & Milne, 1998), self-assessment (Silvia & Gendolla, in press), terror management (Arndt, Greenberg, Simon, Pyszczynski, & Solomon, 1998; Silvia, 2001), and group dynamics (Duval, 1976; Mullen, 1983). Self-focused attention is also fundamental to a host of clinical and health phenomena (Hull, 1981; Ingram, 1990; Pyszczynski, Hamilton, Greenberg, & Becker, 1991; Wells & Matthews, 1994). The study of self-focused attention continues to be a dynamic and active research area. A lot of research relevant to basic theoretical issues has been conducted since the last maj or review (Gibbons, 1990). Recent research has made progress in understanding links between self-awareness and causal attribution, the effects of expectancies on self-standard discrepancy reduction, and the nature of standards--the dynamics of selfawareness are now viewed quite differently. We review these recent developments[1] and hope that a conceptual integration of new findings will further stimulate research on self-focused attention. However, there is still much conceptual work left to be done, and many basic issues remain murky and controversial. We discuss these unresolved issues and sketch the beginnings of some possible solutions. Original Theory The original statement of OSA theory (Duval & Wicklund, 1972) employed only a few constructs, relations, and processes. The theory assumed that the orientation of conscious attention was the essence of selfevaluation. Focusing attention on the self brought about objective self-awareness, which initiated an automatic comparison of the self against standards. The self was defined very broadly as the person\\'s knowledge of the person. A standard was \"defined as a mental representation of correct behavior, attitudes, and traits ... All of the standards of correctness taken together define what a \\'correct\\' person is\" (Duval & Wicklund, 1972, pp. 3, 4). This simple system consisting of self, standards, and attentional focus was assumed to operate according to gestalt consistency principles (Heider, 1960). If a discrepancy was found between self and standards, negative affect was said to arise. This aversive state then motivated the restoration of consistency. Two behavioral routes were proposed. People could either actively change their actions, attitudes, or traits to be more congruent with the representations of the standard or could avoid the self-focusing stimuli and circumstances. Avoidance effectively terminates the comparison process and hence all self-evaluation. Early research found solid support for these basic ideas (Carver, 1975; Gibbons & Wicklund, 1976; Wicklund & Duval, 1971). Duval and Wicklund (1972) also assumed that objective self-awareness would generally be an aversive state--the probability that at least one self-standard discrepancy exists is quite high. This was the first assumption to be revised. Later work found that self-awareness can be a positive state when people are congruent with their standards (Greenberg & Musham, 1981; Ickes, Wicklund, & Ferris, 1973). New Developments OSA theory has grown considerably from the original statement (Duval & Wicklund, 1972). Our review focuses primarily on core theoretical developments since the last review (Gibbons, 1990). Other interesting aspects, such as interpersonal processes and interoceptive accuracy, have not changed significantly since previous reviews (Gibbons, 1990; Silvia & Gendolla, in press). We will also overlook the many clinical consequences of self-awareness; these have been exhaustively reviewed elsewhere (Pyszczynski et al., 1991; Wells & Matthews, 1994). Expectancies and Avoiding Self-Awareness Reducing a discrepancy or avoiding self-focus are equally effective ways of reducing the negative affect resulting from a discrepancy. When do people do one or the other? The original theory was not very specific about when approach versus avoidance would occur. Duval and Wicklund (1972) did, however, speculate that two factors should be relevant. The first was whether people felt they could effectively reduce the discrepancy; the second was whether the discrepancy was small or large. In their translation of OSA theory into a \"test--operate--test--exit\" (TOTE) feedback system, Carver, Blaney, and Scheier (1979a, 1979b) suggested that expectancies regarding outcome favorability determine approach versus avoidance behavior. When a self-standard discrepancy is recognized, people implicitly appraise their likelihood of reducing the discrepancy (cf. Bandura, 1977; Lazarus, 1966). If eventual discrepancy reduction is perceived as likely, people will try to achieve the standard. When expectations regarding improvement are unfavorable, however, people will try to avoid self-focus. Later research and theory (Duval, Duval, & Mulilis, 1992) refined Duval and Wicklund\\'s (1972) speculations and the notion of outcome favorability. Expectancies are not simply and dichotomously favorable or unfavorable--they connote a person\\'s rate of progress in discrepancy reduction relative to the magnitude of the discrepancy. More specifically, people will try to reduce a discrepancy to the extent they believe that their rate of progress is sufficient relative to the magnitude of the problem. Those who believe their rate of progress to be insufficient will avoid. To test this hypothesis, participants were told they were either highly (90%) or mildly (10%) discrepant from an experimental standard (Duval et al., 1992, Study 1). Participants were then given the opportunity to engage in a remedial task that was guaranteed by the experimenter to totally eliminate their deficiency provided that they worked on the task for 2 hr and 10 min. However, the rate at which working on the task would reduce the discrepancy was varied. In the low rate of progress conditions individuals were shown a performance curve indicating no progress until the last 20 min of working on the remedial task. During the last 20 min discrepancy was reduced to zero. In the constant rate of progress condition, participants were shown a performance curve in which progress toward discrepancy reduction began immediately and continued throughout such efforts with 30% of the deficiency being reduced in the first 30 min of activity and totally eliminated after 2 hr and 10 min. Results indicated that persons who believed the discrepancy to be mild and progress to be constant worked on the remedial task; those who perceived the problem to be mild but the rate of progress to be low avoided this activity. However, participants who thought that the discrepancy was substantial and the rate of progress only constant avoided working on the remedial task relative to those in the mild discrepancy and constant rate of progress condition. These results were conceptually replicated in a second experiment (Duval et al., 1992) using participants\\' time to complete the total 2 hr and 10 min of remedial work as the dependent measure. This pattern suggests that the rate of progress was sufficient relative to the magnitude of the discrepancy in the mild discrepancy and constant rate of progress condition; this in turn promoted approaching the problem. In the high discrepancy and constant rate of progress condition and the high and mild discrepancy and low rate of progress conditions, rate of progress was insufficient and promoted avoiding the problem. In a third experiment (Duval et al., 1992), people were again led to believe that they were either highly or mildly discrepant from a standard on an intellectual dimension and then given the opportunity to reduce that deficiency by working on a remedial task. A',\n",
       " '0341cd2fb49a56697edaf03b05734f44d0e41f89': 'A dependence cluster is a set of mutually inter-dependent program elements. Prior studies have found that large dependence clusters are prevalent in software systems. It has been suggested that dependence clusters have potentially harmful effects on software quality. However, little empirical evidence has been provided to support this claim. The study presented in this paper investigates the relationship between dependence clusters and software quality at the function-level with a focus on effort-aware fault-proneness prediction. The investigation first analyzes whether or not larger dependence clusters tend to be more fault-prone. Second, it investigates whether the proportion of faulty functions inside dependence clusters is significantly different from the proportion of faulty functions outside dependence clusters. Third, it examines whether or not functions inside dependence clusters playing a more important role than others are more fault-prone. Finally, based on two groups of functions (i.e., functions inside and outside dependence clusters), the investigation considers a segmented fault-proneness prediction model. Our experimental results, based on five well-known open-source systems, show that (1) larger dependence clusters tend to be more fault-prone; (2) the proportion of faulty functions inside dependence clusters is significantly larger than the proportion of faulty functions outside dependence clusters; (3) functions inside dependence clusters that play more important roles are more fault-prone; (4) our segmented prediction model can significantly improve the effectiveness of effort-aware fault-proneness prediction in both ranking and classification scenarios. These findings help us better understand how dependence clusters influence software quality.',\n",
       " 'b0f16acfa4efce9c24100ec330b82fb8a28feeec': 'Many traditional reinforcement-learning algorithms have been designed for problems with small finite state and action spaces. Learn ing in such discrete problems can been difficult, due to noise and delayed reinfor cements. However, many real-world problems have continuous state or action sp aces, which can make learning a good decision policy even more involved. In this c apter we discuss how to automatically find good decision policies in continuous d omains. Because analytically computing a good policy from a continuous model c an be infeasible, in this chapter we mainly focus on methods that explicitly up date a representation of a value function, a policy or both. We discuss conside rations in choosing an appropriate representation for these functions and disc uss gradient-based and gradient-free ways to update the parameters. We show how to a pply these methods to reinforcement-learning problems and discuss many speci fic algorithms. Amongst others, we cover gradient-based temporal-difference lear ning, evolutionary strategies, policy-gradient algorithms and (natural) actor-cri ti methods. We discuss the advantages of different approaches and compare the perform ance of a state-of-theart actor-critic method and a state-of-the-art evolutiona ry strategy empirically.',\n",
       " '2bf8acb0bd8b0fde644b91c5dd4bef2e8119e61e': 'The problem of selecting determinant features generating appropriate model structure is a challenge in epidemiological modelling. Disease spread is highly complex, and experts develop their understanding of its dynamic over years. There is an increasing variety and volume of epidemiological data which adds to the potential confusion. The authors propose here to make use of that data to better understand disease systems. Decision tree techniques have been extensively used to extract pertinent information and improve decision making. In this paper, the authors propose an innovative structured approach combining decision tree induction with Bio-PEPA computational modelling, and illustrate the approach through application to tuberculosis. By using decision tree induction, the enhanced Bio-PEPA model shows considerable improvement over the initial model with regard to the simulated results matching observed data. The key finding is that the developer expresses a realistic predictive model using relevant features, thus considering this approach as decision support, empowers the epidemiologist in his policy decision making. KEywoRDS Bio-PEPA Modelling, Data Mining, Decision Support, Decision Tree Induction, Epidemiology, Modelling and Simulation, Optimisation, Refinement, Tuberculosis',\n",
       " 'e3ab7a95af2c0efc92f146f8667ff95e46da84f1': 'The evolving explosion in high data rate services and applications will soon require the use of untapped, abundant unregulated spectrum of the visible light for communications to adequately meet the demands of the fifth-generation (5G) mobile technologies. Radio-frequency (RF) networks are proving to be scarce to cover the escalation in data rate services. Visible light communication (VLC) has emerged as a great potential solution, either in replacement of, or complement to, existing RF networks, to support the projected traffic demands. Despite of the prolific advantages of VLC networks, VLC faces many challenges that must be resolved in the near future to achieve a full standardization and to be integrated to future wireless systems. Here, we review the new, emerging research in the field of VLC networks and lay out the challenges, technological solutions, and future work predictions. Specifically, we first review the VLC channel capacity derivation, discuss the performance metrics and the associated variables; the optimization of VLC networks are also discussed, including resources and power allocation techniques, user-to-access point (AP) association and APs-toclustered-users-association, APs coordination techniques, nonorthogonal multiple access (NOMA) VLC networks, simultaneous energy harvesting and information transmission using the visible light, and the security issue in VLC networks. Finally, we propose several open research problems to optimize the various VLC networks by maximizing either the sum rate, fairness, energy efficiency, secrecy rate, or harvested energy.',\n",
       " 'b1b5646683557b38468344dff09ae921a5a4b345': 'The IoT protocols used in the application layer, namely the Constraint Application Protocol (CoAP) and Message Queue Telemetry Transport (MQTT) have dependencies to the transport layer. The choice of transport, Transmission Control Protocol (TCP) or the User Datagram Protocol(UDP), on the other hand, has an impact on the Internet of Things (IoT) application level performance, especially over a wireless medium. The motivation of this work is to look at the impact of the protocol stack on performance over two different wireless medium realizations, namely Bluetooth Low Energy and Wi-Fi. The use case studied is infrequent small reports sent from the sensor device to a central cloud storage over a last mile radio access link. We find that while CoAP/UDP based transport performs consistently better both in terms of latency and power consumption over both links, MQTT/TCP may also work when the use requirements allow for longerlatency providing better reliability. All in all, the full connectivity stack needs to be considered when designing an IoT deployment.',\n",
       " 'cd5b7d8fb4f8dc3872e773ec24460c9020da91ed': 'This paper presents a new design concept of a beam steerable high gain phased array antenna based on WR28 waveguide at 29 GHz frequency for fifth generation (5G) full dimension multiple input multiple output (FD-MIMO) system. The 8×8 planar phased array is fed by a three dimensional beamformer to obtain volumetric beam scanning ranging from −60 to +60 degrees both in azimuth and elevation direction. Beamforming network (BFN) is designed using 16 set of 8×8 Butler matrix beamformer to get 64 beam states, which control the horizontal and vertical angle. This is a new concept to design waveguide based high power three-dimensional beamformer for volumetric multibeam in Ka band for 5G application. The maximum gain of phased array is 28.5 dBi that covers 28.9 GHz to 29.4 GHz frequency band.',\n",
       " 'b4cbe50b8988e7c9c1a7b982bfb6c708bb3ce3e8': 'The use of the commercial video games as rehabilitation tools, such as the Nintendo WiiFit, has recently gained much interest in the physical therapy arena. Motion tracking controllers such as the Nintendo Wiimote are not sensitive enough to accurately measure performance in all components of balance. Additionally, users can figure out how to \"cheat\" inaccurate trackers by performing minimal movement (e.g. wrist twisting a Wiimote instead of a full arm swing). Physical rehabilitation requires accurate and appropriate tracking and feedback of performance. To this end, we are developing applications that leverage recent advances in commercial video game technology to provide full-body control of animated virtual characters. A key component of our approach is the use of newly available low cost depth sensing camera technology that provides markerless full-body tracking on a conventional PC. The aim of this research was to develop and assess an interactive game-based rehabilitation tool for balance training of adults with neurological injury.',\n",
       " '6a2311d02aea97f7fe4e78c8bd2a53091364dc3b': '1 Consultant, Maplewood, MN USA 4 * Correspondence: sahyun@infionline.net; 1-(651)-927-9686 5 6 Abstract: We examined a series of real-world, pictorial photographs with varying 7 characteristics, along with their modification by noise addition and unsharp masking. As 8 response metrics we used three different versions of the aesthetic measure originally 9 proposed by Birkhoff. The first aesthetic measure, which has been used in other studies, 10 and which we used in our previous work as well, showed a preference for the least 11 complex of the images. It provided no justification for noise addition, but did reveal 12 enhancement on unsharp masking. Optimum level of unsharp masking varied with the 13 image, but was predictable from the individual image’s GIF compressibility. We expect this 14 result to be useful for guiding the processing of pictorial photographic imagery. The 15 second aesthetic measure, that of informational aesthetics based on entropy alone failed 16 to provide useful discrimination among the images or the conditions of their modification. 17 A third measure, derived from the concepts of entropy maximization, as well as the 18 hypothesized preference of observers for “simpler”, i.e., more compressible, images, 19 yielded qualitatively the same results as the more traditional version of the measure. 20 Differences among the photographs and the conditions of their modification were more 21 clearly defined with this metric, however. 22',\n",
       " '97aef787d63aef75e6f8055cdac3771f8649f21a': 'Word embedding has become a fundamental component to many NLP tasks such as named entity recognition and machine translation. However, popular models that learn such embeddings are unaware of the morphology of words, so it is not directly applicable to highly agglutinative languages such as Korean. We propose a syllable-based learning model for Korean using a convolutional neural network, in which word representation is composed of trained syllable vectors. Our model successfully produces morphologically meaningful representation of Korean words compared to the original Skip-gram embeddings. The results also show that it is quite robust to the Out-of-Vocabulary problem.',\n",
       " 'd9d8aafe6856025f2c2b7c70f5e640e03b6bcd46': \"In phishing and pharming, users could be easily tricked into submitting their username/passwords into fraudulent web sites whose appearances look similar as the genuine ones. The traditional blacklist approach for anti-phishing is partially effective due to its partial list of global phishing sites. In this paper, we present a novel anti-phishing approach named Automated Individual White-List (AIWL). AIWL automatically tries to maintain a white-list of user's all familiar Login User Interfaces (LUIs) of web sites. Once a user tries to submit his/her confidential information to an LUI that is not in the white-list, AIWL will alert the user to the possible attack. Next, AIWL can efficiently defend against pharming attacks, because AIWL will alert the user when the legitimate IP is maliciously changed; the legitimate IP addresses, as one of the contents of LUI, are recorded in the white-list and our experiment shows that popular web sites' IP addresses are basically stable. Furthermore, we use Naïve Bayesian classifier to automatically maintain the white-list in AIWL. Finally, we conclude through experiments that AIWL is an efficient automated tool specializing in detecting phishing and pharming.\",\n",
       " '34feeafb5ff7757b67cf5c46da0869ffb9655310': \"Environmental energy is an attractive power source for low power wireless sensor networks. We present Prometheus, a system that intelligently manages energy transfer for perpetual operation without human intervention or servicing. Combining positive attributes of different energy storage elements and leveraging the intelligence of the microprocessor, we introduce an efficient multi-stage energy transfer system that reduces the common limitations of single energy storage systems to achieve near perpetual operation. We present our design choices, tradeoffs, circuit evaluations, performance analysis, and models. We discuss the relationships between system components and identify optimal hardware choices to meet an application's needs. Finally we present our implementation of a real system that uses solar energy to power Berkeley's Telos Mote. Our analysis predicts the system will operate for 43 years under 1% load, 4 years under 10% load, and 1 year under 100% load. Our implementation uses a two stage storage system consisting of supercapacitors (primary buffer) and a lithium rechargeable battery (secondary buffer). The mote has full knowledge of power levels and intelligently manages energy transfer to maximize lifetime.\",\n",
       " '3689220c58f89e9e19cc0df51c0a573884486708': 'AmbiMax is an energy harvesting circuit and a supercapacitor based energy storage system for wireless sensor nodes (WSN). Previous WSNs attempt to harvest energy from various sources, and some also use supercapacitors instead of batteries to address the battery aging problem. However, they either waste much available energy due to impedance mismatch, or they require active digital control that incurs overhead, or they work with only one specific type of source. AmbiMax addresses these problems by first performing maximum power point tracking (MPPT) autonomously, and then charges supercapacitors at maximum efficiency. Furthermore, AmbiMax is modular and enables composition of multiple energy harvesting sources including solar, wind, thermal, and vibration, each with a different optimal size. Experimental results on a real WSN platform, Eco, show that AmbiMax successfully manages multiple power sources simultaneously and autonomously at several times the efficiency of the current state-of-the-art for WSNs',\n",
       " '4833d690f7e0a4020ef48c1a537dbb5b8b9b04c6': 'A low-power low-cost highly efficient maximum power point tracker (MPPT) to be integrated into a photovoltaic (PV) panel is proposed. This can result in a 25% energy enhancement compared to a standard photovoltaic panel, while performing functions like battery voltage regulation and matching of the PV array with the load. Instead of using an externally connected MPPT, it is proposed to use an integrated MPPT converter as part of the PV panel. It is proposed that this integrated MPPT uses a simple controller in order to be cost effective. Furthermore, the converter has to be very efficient, in order to transfer more energy to the load than a directly coupled system. This is achieved by using a simple soft-switched topology. A much higher conversion efficiency at lower cost will then result, making the MPPT an affordable solution for small PV energy systems.',\n",
       " '61c1d66defb225eda47462d1bc393906772c9196': 'The enormous potential for wireless sensor networks to make a positive impact on our society has spawned a great deal of research on the topic, and this research is now producing environment-ready systems. Current technology limits coupled with widely-varying application requirements lead to a diversity of hardware platforms for different portions of the design space. In addition, the unique energy and reliability constraints of a system that must function for months at a time without human intervention mean that demands on sensor network hardware are different from the demands on standard integrated circuits. This paper describes our experiences designing sensor nodes and low level software to control them.\\n In the ZebraNet system we use GPS technology to record fine-grained position data in order to track long term animal migrations [14]. The ZebraNet hardware is composed of a 16-bit TI microcontroller, 4 Mbits of off-chip flash memory, a 900 MHz radio, and a low-power GPS chip. In this paper, we discuss our techniques for devising efficient power supplies for sensor networks, methods of managing the energy consumption of the nodes, and methods of managing the peripheral devices including the radio, flash, and sensors. We conclude by evaluating the design of the ZebraNet nodes and discussing how it can be improved. Our lessons learned in developing this hardware can be useful both in designing future sensor nodes and in using them in real systems.',\n",
       " '576803b930ef44b79028048569e7ea321c1cecb0': 'Due to severe terrorist attacks in recent years, aviation security issues have moved into the focus of politicians as well as the general public. Effective screening of passenger bags using state-of-the-art X-ray screening systems is essential to prevent terrorist attacks. The performance of the screening process depends critically on the security personnel, because they decide whether bags are OK or whether they might contain a prohibited item. Screening X-ray images of passenger bags for dangerous and prohibited items effectively and efficiently is a demanding object recognition task. Effectiveness of computer-based training (CBT) on X-ray detection performance was assessed using computer-based tests and on the job performance measures using threat image projection (TIP). It was found that adaptive CBT is a powerful tool to increase detection performance and efficiency of screeners in X-ray image interpretation. Moreover, the results of training could be generalized to the real life situation as shown in the increased detection performance in TIP not only for trained items, but also for new (untrained) items. These results illustrate that CBT is a very useful tool to increase airport security from a human factors perspective.',\n",
       " '6c1ccc66420136488cf34c1ffe707afefd8b00b9': 'We consider brightness/contrast-invariant and rotation-discriminating template matching that searches an image to analyze A for a query image Q. We propose to use the complex coefficients of the discrete Fourier transform of the radial projections to compute new rotation-invariant local features. These coefficients can be efficiently obtained via FFT. We classify templates in “stable” and “unstable” ones and argue that any local feature-based template matching may fail to find unstable templates. We extract several stable sub-templates of Q and find them in A by comparing the features. The matchings of the sub-templates are combined using the Hough transform. As the features of A are computed only once, the algorithm can find quickly many different sub-templates in A, and it is suitable for: finding many query images in A; multi-scale searching and partial occlusion-robust template matching.',\n",
       " '3370784dacf9df1e54384190dad40b817520ba3a': \"Haswell, Intel's fourth-generation core processor architecture, delivers a range of client parts, a converged core for the client and server, and technologies used across many products. It uses an optimized version of Intel 22-nm process technology. Haswell provides enhancements in power-performance efficiency, power management, form factor and cost, core and uncore microarchitecture, and the core's instruction set.\",\n",
       " '146da74cd886acbd4a593a55f0caacefa99714a6': 'The evolution of Artificial Intelligence has served as the catalyst in the field of technology. We can now develop things which was once just an imagination. One of such creation is the birth of self-driving car. Days have come where one can do their work or even sleep in the car and without even touching the steering wheel, accelerator you will still be able to reach your target destination safely. This paper proposes a working model of self-driving car which is capable of driving from one location to the other or to say on different types of tracks such as curved tracks, straight tracks and straight followed by curved tracks. A camera module is mounted over the top of the car along with Raspberry Pi sends the images from real world to the Convolutional Neural Network which then predicts one of the following directions. i.e. right, left, forward or stop which is then followed by sending a signal from the Arduino to the controller of the remote controlled car and as a result of it the car moves in the desired direction without any human intervention.',\n",
       " '6fd62c67b281956c3f67eb53fafaea83b2f0b4fb': \"Previous neuroimaging studies of spatial perspective taking have tended not to activate the brain's mentalising network. We predicted that a task that requires the use of perspective taking in a communicative context would lead to the activation of mentalising regions. In the current task, participants followed auditory instructions to move objects in a set of shelves. A 2x2 factorial design was employed. In the Director factor, two directors (one female and one male) either stood behind or next to the shelves, or were replaced by symbolic cues. In the Object factor, participants needed to use the cues (position of the directors or symbolic cues) to select one of three possible objects, or only one object could be selected. Mere presence of the Directors was associated with activity in the superior dorsal medial prefrontal cortex (MPFC) and the superior/middle temporal sulci, extending into the extrastriate body area and the posterior superior temporal sulcus (pSTS), regions previously found to be responsive to human bodies and faces respectively. The interaction between the Director and Object factors, which requires participants to take into account the perspective of the director, led to additional recruitment of the superior dorsal MPFC, a region activated when thinking about dissimilar others' mental states, and the middle temporal gyri, extending into the left temporal pole. Our results show that using perspective taking in a communicative context, which requires participants to think not only about what the other person sees but also about his/her intentions, leads to the recruitment of superior dorsal MPFC and parts of the social brain network.\",\n",
       " '30b1447fbfdbd887a9c896a2b0d80177fc17c94e': \"In medical diagnoses and treatments, e.g., the endoscopy, the dosage transition monitoring, it is often desirable to wirelessly track an object that moves through the human GI tract. In this paper, we present a magnetic localization and orientation system for such applications. This system uses a small magnet enclosed in the object to serve as excitation source. It does not require the connection wire and power supply for excitation signal. When the magnet moves, it establishes a static magnetic field around, whose intensity is related to the magnet's position and orientation. With the magnetic sensors, the magnetic intensities in some pre-determined spatial points can be detected, and the magnet's position and orientation parameters can be calculated based on an appropriate algorithm. Here, we propose a real-time tracking system built by Honeywell 3-axis magnetic sensors, HMC1053, as well as the computer sampling circuit. The results show that satisfactory tracking accuracy (average localization error is 3.3 mm) can be achieved using a sensor array with enough number of the 3-axis magnetic sensors\",\n",
       " 'b551feaa696da1ba44c31e081555e50358c6eca9': 'In this work, we present the development of a polymer-based capacitive sensing array. The proposed device is capable of measuring normal and shear forces, and can be easily realized by using micromachining techniques and flexible printed circuit board (FPCB) technologies. The sensing array consists of a polydimethlysiloxane (PDMS) structure and a FPCB. Each shear sensing element comprises four capacitive sensing cells arranged in a 2 × 2 array, and each capacitive sensing cell has two sensing electrodes and a common floating electrode. The sensing electrodes as well as the metal interconnect for signal scanning are implemented on the FPCB, while the floating electrodes are patterned on the PDMS structure. This design can effectively reduce the complexity of the capacitive structures, and thus makes the device highly manufacturable. The characteristics of the devices with different dimensions were measured and discussed. A scanning circuit was also designed and implemented. The measured maximum sensitivity is 1.67%/mN. The minimum resolvable force is 26 mN measured by the scanning circuit. The capacitance distributions induced by normal and shear forces were also successfully captured by the sensing array.',\n",
       " 'bb17e8858b0d3a5eba2bb91f45f4443d3e10b7cd': '',\n",
       " '01cac0a7c2a3240cb77a1e090694a104785f78f5': 'Workflow management systems, a relatively recent technology, are designed to make work more efficient, integrate heterogeneous application systems, and support interorganizational processes in electronic commerce applications. In this paper, we introduce the field of workflow automation, the subject of this special issue of Information Systems Frontiers. In the first part of the paper, we provide basic definitions and frameworks to aid understanding of workflow management technologies. In the remainder of the paper, we discuss technical and management research opportunities in this field and discuss the other contributions to the special issue.',\n",
       " '4cdad9b059b5077fcce00fb8bcb4e381edd353bd': 'This paper proposes a novel scheme that uses robust principal component classifier in intrusion detection problems where the training data may be unsupervised. Assuming that anomalies can be treated as outliers, an intrusion predictive model is constructed from the major and minor principal components of the normal instances. A measure of the difference of an anomaly from the normal instance is the distance in the principal component space. The distance based on the major components that account for 50% of the total variation and the minor components whose eigenvalues less than 0.20 is shown to work well. The experiments with KDD Cup 1999 data demonstrate that the proposed method achieves 98.94% in recall and 97.89% in precision with the false alarm rate 0.92% and outperforms the nearest neighbor method, density-based local outliers (LOF) approach, and the outlier detection algorithm based on Canberra metric.',\n",
       " 'ca20f466791f4b051ef3b8d2bf63789d33c562c9': \"Lately, Twitter has grown to be one of the most favored ways of disseminating information to people around the globe. However, the main challenge faced by the users is how to assess the credibility of information posted through this social network in real time. In this paper, we present a real-time content credibility assessment system named CredFinder, which is capable of measuring the trustworthiness of information through user analysis and content analysis. The proposed system is capable of providing a credibility score for each user's tweets. Hence, it provides users with the opportunity to judge the credibility of information faster. CredFinder consists of two parts: a frontend in the form of an extension to the Chrome browser that collects tweets in real time from a Twitter search or a user-timeline page and a backend that analyzes the collected tweets and assesses their credibility.\",\n",
       " '9923edf7815c720aa0d6d58a28332806ae91b224': \"Based on the avalanche effect of avalanche transistor, a kind of ultra-wideband nanosecond pulse circuit has been designed, whose frequency, pulse width and amplitude are tunable. In this paper, the principle, structure and selection of components' parameters in the circuit are analyzed in detail. The circuit generates periodic negative pulse, whose pulse full width is 890 ps and pulse amplitude is -11.2 V in simulation mode. By setting up circuit for experiment and changing parameters properly, a kind of ultra-wideband pulse with pulse width of 2.131 ns and pulse amplitude of -9.23 V is achieved. With the features such as simple structure, stable and reliable performance and low cost, this pulse generator is applicable to ultra-wideband wireless communication system.\",\n",
       " '33b424698c2b7602dcb579513c34fe20cc3ae669': 'We propose a 50-MS/s two-step flash-ΔΣ time-to-digital converter (TDC) using stable time amplifiers (TAs). The TDC demonstrates low-levels of shaped quantization noise. The system is simulated in 40-nm CMOS and consumes 1.3 mA from a 1.1 V supply. The bandwidth is broadened to Nyquist rate. At frequencies below 25 MHz, the integrated TDC error is as low as 143 fsrms, which is equal to an equivalent TDC resolution of 0.5 ps.',\n",
       " '7fabd0639750563e0fb09df341e0e62ef4d6e1fb': 'The review describes the status of brain-computer or brain-machine interface research. We focus on non-invasive brain-computer interfaces (BCIs) and their clinical utility for direct brain communication in paralysis and motor restoration in stroke. A large gap between the promises of invasive animal and human BCI preparations and the clinical reality characterizes the literature: while intact monkeys learn to execute more or less complex upper limb movements with spike patterns from motor brain regions alone without concomitant peripheral motor activity usually after extensive training, clinical applications in human diseases such as amyotrophic lateral sclerosis and paralysis from stroke or spinal cord lesions show only limited success, with the exception of verbal communication in paralysed and locked-in patients. BCIs based on electroencephalographic potentials or oscillations are ready to undergo large clinical studies and commercial production as an adjunct or a major assisted communication device for paralysed and locked-in patients. However, attempts to train completely locked-in patients with BCI communication after entering the complete locked-in state with no remaining eye movement failed. We propose that a lack of contingencies between goal directed thoughts and intentions may be at the heart of this problem. Experiments with chronically curarized rats support our hypothesis; operant conditioning and voluntary control of autonomic physiological functions turned out to be impossible in this preparation. In addition to assisted communication, BCIs consisting of operant learning of EEG slow cortical potentials and sensorimotor rhythm were demonstrated to be successful in drug resistant focal epilepsy and attention deficit disorder. First studies of non-invasive BCIs using sensorimotor rhythm of the EEG and MEG in restoration of paralysed hand movements in chronic stroke and single cases of high spinal cord lesions show some promise, but need extensive evaluation in well-controlled experiments. Invasive BMIs based on neuronal spike patterns, local field potentials or electrocorticogram may constitute the strategy of choice in severe cases of stroke and spinal cord paralysis. Future directions of BCI research should include the regulation of brain metabolism and blood flow and electrical and magnetic stimulation of the human brain (invasive and non-invasive). A series of studies using BOLD response regulation with functional magnetic resonance imaging (fMRI) and near infrared spectroscopy demonstrated a tight correlation between voluntary changes in brain metabolism and behaviour.',\n",
       " 'a31b795f8defb59889df8f13321e057192d64f73': 'Background\\nImplementation of patient preferences for use of electronic health records for research has been traditionally limited to identifiable data. Tiered e-consent for use of de-identified data has traditionally been deemed unnecessary or impractical for implementation in clinical settings.\\n\\n\\nMethods\\nWe developed a web-based tiered informed consent tool called informed consent for clinical data and bio-sample use for research (iCONCUR) that honors granular patient preferences for use of electronic health record data in research. We piloted this tool in 4 outpatient clinics of an academic medical center.\\n\\n\\nResults\\nOf patients offered access to iCONCUR, 394 agreed to participate in this study, among whom 126 patients accessed the website to modify their records according to data category and data recipient. The majority consented to share most of their data and specimens with researchers. Willingness to share was greater among participants from an Human Immunodeficiency Virus (HIV) clinic than those from internal medicine clinics. The number of items declined was higher for for-profit institution recipients. Overall, participants were most willing to share demographics and body measurements and least willing to share family history and financial data. Participants indicated that having granular choices for data sharing was appropriate, and that they liked being informed about who was using their data for what purposes, as well as about outcomes of the research.\\n\\n\\nConclusion\\nThis study suggests that a tiered electronic informed consent system is a workable solution that respects patient preferences, increases satisfaction, and does not significantly affect participation in research.',\n",
       " '7aa1866adbc2b4758c04d8484e5bf22e4cce9cc9': 'Metamaterials are considered as an option for quasi-optical matching of automotive radar radomes, lowering transmission loss and minimizing reflections. This paper shows a fishnet structure design for the 79 GHz band which is suitable for this type of matching and which exhibits a negative index of refraction. The measured transmission loss is 0.9 dB at 79 GHz. A tolerance study concerning copper plating, substrate permittivity, oblique incidence, and polarization is shown. Quasi-optical measurements were done in the range of 60 – 90 GHz, which agree with simulated results.',\n",
       " 'b95dd9e28f2126aac27da8b0378d3b9487d8b73d': 'When animation of a humanoid figure is to be generated at run-time, instead of by replaying precomposed motion clips, some method is required of specifying the avatar’s movements in a form from which the required motion data can be automatically generated. This form must be of a more abstract nature than raw motion data: ideally, it should be independent of the particular avatar’s proportions, and both writable by hand and suitable for automatic generation from higher-level descriptions of the required actions. We describe here the development and implementation of such a scripting language for the particular area of sign languages of the deaf, called SiGML (Signing Gesture Markup Language), based on the existing HamNoSys notation for sign languages. We conclude by suggesting how this work may be extended to more general animation for interactive virtual reality applications.',\n",
       " '267718d3b9399a5eab90a1b1701e78369696e8fe': \"Preprocessor conditionals are heavily used in C programs since they allow the source code to be configured for different platforms or capabilities. However, preprocessor conditionals, as well as other preprocessor directives, are not part of the C language. They need to be evaluated and removed, and so a single configuration selected, before parsing can take place. Most analysis and program understanding tools run on this preprocessed version of the code so their results are based on a single configuration. This paper describes the approach of CRefactory, a refactoring tool for C programs. A refactoring tool cannot consider only a single configuration: changing the code for one configuration may break the rest of the code. CRefactory analyses the program for all possible configurations simultaneously. CRefactory also preserves preprocessor directives and integrates them in the internal representations. The paper also presents metrics from two case studies to show that CRefactory's program representation is practical.\",\n",
       " '63abfb7d2d35d60a5dc2cc884251f9fee5d46963': \"A powerful way to probe brain function is to assess the relationship between simultaneous changes in activity across different parts of the brain. In recent years, the temporal activity correlation between brain areas has frequently been taken as a measure of their functional connections. Evaluating 'functional connectivity' in this way is particularly popular in the fMRI community, but has also drawn interest among electrophysiologists. Like hemodynamic fluctuations observed with fMRI, electrophysiological signals display significant temporal fluctuations, even in the absence of a stimulus. These neural fluctuations exhibit a correlational structure over a wide range of spatial and temporal scales. Initial evidence suggests that certain aspects of this correlational structure bear a high correspondence to so-called functional networks defined using fMRI. The growing family of methods to study activity covariation, combined with the diverse neural mechanisms that contribute to the spontaneous fluctuations, has somewhat blurred the operational concept of functional connectivity. What is clear is that spontaneous activity is a conspicuous, energy-consuming feature of the brain. Given its prominence and its practical applications for the functional connectivity mapping of brain networks, it is of increasing importance that we understand its neural origins as well as its contribution to normal brain function.\",\n",
       " '1a1f0d0abcbdaa2d487f0a46dba1ca097774012d': 'Backscatter presents an emerging ultralow-power wireless communication paradigm. The ability to offer submilliwatt power consumption makes it a competitive core technology for Internet of Things (IoT) applications. In this article, we provide a tutorial of backscatter communication from the signal processing perspective as well as a survey of the recent research activities in this domain, primarily focusing on bistatic backscatter systems. We also discuss the unique real-world applications empowered by backscatter communication and identify open questions in this domain. We believe this article will shed light on the low-power wireless connectivity design toward building and deploying IoT services in the wild.',\n",
       " '6fd78d20e6f51d872f07cde9350f4d31078ff723': 'Passively compliant legs have been instrumental in the development of dynamically running legged robots. Having properly tuned leg springs is essential for stable, robust and energetically efficient running at high speeds. Recent simulation studies indicate that having variable stiffness legs, as animals do, can significantly improve the speed and stability of these robots in changing environmental conditions. However, to date, the mechanical complexities of designing usefully robust tunable passive compliance into legs has precluded their implementation on practical running robots. This paper describes a new design of a ”structurally controlled variable stiffness” leg for a hexapedal running robot. This new leg improves on previous designs’ performance and enables runtime modification of leg stiffness in a small, lightweight, and rugged package. Modeling and leg test experiments are presented that characterize the improvement in stiffness range, energy storage, and dynamic coupling properties of these legs. We conclude that this variable stiffness leg design is now ready for implementation and testing on a dynamical running robot.',\n",
       " 'd1c4907b1b225f61059915a06a3726706860c71e': 'With over 10 million git repositories, GitHub is becoming one of the most important sources of software artifacts on the Internet. Researchers mine the information stored in GitHub’s event logs to understand how its users employ the site to collaborate on software, but so far there have been no studies describing the quality and properties of the available GitHub data. We document the results of an empirical study aimed at understanding the characteristics of the repositories and users in GitHub; we see how users take advantage of GitHub’s main features and how their activity is tracked on GitHub and related datasets to point out misalignment between the real and mined data. Our results indicate that while GitHub is a rich source of data on software development, mining GitHub for research purposes should take various potential perils into consideration. For example, we show that the majority of the projects are personal and inactive, and that almost 40 % of all pull requests do not appear as merged even though they were. Also, approximately half of GitHub’s registered users do not have public activity, while the activity of GitHub users in repositories is not always easy to pinpoint. We use our identified perils to see if they can pose validity threats; we review selected papers from the MSR 2014 Mining Challenge and see if there are potential impacts to consider. We provide a set of recommendations for software engineering researchers on how to approach the data in GitHub.',\n",
       " '9c1ebae0eea2aa27fed13c71dc98dc0f67dd52a0': 'This paper presents a novel unsupervised segmentation method for 3D medical images. Convolutional neural networks (CNNs) have brought significant advances in image segmentation. However, most of the recent methods rely on supervised learning, which requires large amounts of manually annotated data. Thus, it is challenging for these methods to cope with the growing amount of medical images. This paper proposes a unified approach to unsupervised deep representation learning and clustering for segmentation. Our proposed method consists of two phases. In the first phase, we learn deep feature representations of training patches from a target image using joint unsupervised learning (JULE) that alternately clusters representations generated by a CNN and updates the CNN parameters using cluster labels as supervisory signals. We extend JULE to 3D medical images by utilizing 3D convolutions throughout the CNN architecture. In the second phase, we apply k-means to the deep representations from the trained CNN and then project cluster labels to the target image in order to obtain the fully segmented image. We evaluated our methods on three images of lung cancer specimens scanned with micro-computed tomography (micro-CT). The automatic segmentation of pathological regions in micro-CT could further contribute to the pathological examination process. Hence, we aim to automatically divide each image into the regions of invasive carcinoma, noninvasive carcinoma, and normal tissue. Our experiments show the potential abilities of unsupervised deep representation learning for medical image segmentation.',\n",
       " '27a693acee22752fa66f442b8d52b7f3c83134c7': \"As cloud computing becomes more and more popular, understanding the economics of cloud computing becomes critically important. To maximize the profit, a service provider should understand both service charges and business costs, and how they are determined by the characteristics of the applications and the configuration of a multiserver system. The problem of optimal multiserver configuration for profit maximization in a cloud computing environment is studied. Our pricing model takes such factors into considerations as the amount of a service, the workload of an application environment, the configuration of a multiserver system, the service-level agreement, the satisfaction of a consumer, the quality of a service, the penalty of a low-quality service, the cost of renting, the cost of energy consumption, and a service provider's margin and profit. Our approach is to treat a multiserver system as an M/M/m queuing model, such that our optimization problem can be formulated and solved analytically. Two server speed and power consumption models are considered, namely, the idle-speed model and the constant-speed model. The probability density function of the waiting time of a newly arrived service request is derived. The expected service charge to a service request is calculated. The expected net business gain in one unit of time is obtained. Numerical calculations of the optimal server size and the optimal server speed are demonstrated.\",\n",
       " 'e8217edd7376c26c714757a362724f81f3afbee0': 'This paper provides an overview on the main additive manufacturing/3D printing technologies suitable for many satellite applications and, in particular, radio-frequency components. In fact, nowadays they have become capable of producing complex net-shaped or nearly net-shaped parts in materials that can be directly used as functional parts, including polymers, metals, ceramics, and composites. These technologies represent the solution for low-volume, high-value, and highly complex parts and products.',\n",
       " '738f4d2137fc767b1802963b5e45a2216c27b77c': 'The aim of this paper is to identify the most suitable model for churn prediction based on three different techniques. The paper identifies the variables that affect churn in reverence of customer complaints data and provides a comparative analysis of neural networks, regression trees and regression in their capabilities of predicting customer churn. Keywords—Churn, Decision Trees, Neural Networks, Regression.',\n",
       " 'ae341ad66824e1f30a2675fd50742b97794c8f57': 'Learning from imbalanced data sets, where the number of examples of one (majority) class is much higher than the others, presents an important challenge to the machine learning community. Traditional machine learning algorithms may be biased towards the majority class, thus producing poor predictive accuracy over the minority class. In this paper, we describe a new approach that combines boosting, an ensemble-based learning algorithm, with data generation to improve the predictive power of classifiers against imbalanced data sets consisting of two classes. In the DataBoost-IM method, hard examples from both the majority and minority classes are identified during execution of the boosting algorithm. Subsequently, the hard examples are used to separately generate synthetic examples for the majority and minority classes. The synthetic data are then added to the original training set, and the class distribution and the total weights of the different classes in the new training set are rebalanced. The DataBoost-IM method was evaluated, in terms of the F-measures, G-mean and overall accuracy, against seventeen highly and moderately imbalanced data sets using decision trees as base classifiers. Our results are promising and show that the DataBoost-IM method compares well in comparison with a base classifier, a standard benchmarking boosting algorithm and three advanced boosting-based algorithms for imbalanced data set. Results indicate that our approach does not sacrifice one class in favor of the other, but produces high predictions against both minority and majority classes.',\n",
       " '090a6772a1d69f07bfe7e89f99934294a0dac1b9': '',\n",
       " '0df013671e9e901a9126deb4957e22e3d937b1a5': 'With the goal of reducing computational costs without sacrificing accuracy, we describe two algorithms to find sets of prototypes for nearest neighbor classification. Here, the term “prototypes” refers to the reference instances used in a nearest neighbor computation — the instances with respect to which similarity is assessed in order to assign a class to a new data item. Both algorithms rely on stochastic techniques to search the space of sets of prototypes and are simple to implement. The first is a Monte Carlo sampling algorithm; the second applies random mutation hill climbing. On four datasets we show that only three or four prototypes sufficed to give predictive accuracy equal or superior to a basic nearest neighbor algorithm whose run-time storage costs were approximately 10 to 200 times greater. We briefly investigate how random mutation hill climbing may be applied to select features and prototypes simultaneously. Finally, we explain the performance of the sampling algorithm on these datasets in terms of a statistical measure of the extent of clustering displayed by the target classes.',\n",
       " '32f7aef5c13c715b00b966eaaba5dd2fe35df1a4': 'Undoubtedly, Customer Relationship Management (CRM) has gained its importance through the statement that acquiring a new customer is several times more costly than retaining and selling additional products to existing customers. Consequently, marketing practitioners are currently often focusing on retaining customers for as long as possible. However, recent findings in relationship marketing literature have shown that large differences exist within the group of long-life customers in terms of spending and spending evolution. Therefore, this paper focuses on introducing a measure of a customer’s future spending evolution that might improve relationship marketing decision making. In this study, from a marketing point of view, we focus on predicting whether a newly acquired customer will increase or decrease his/her future spending from initial purchase information. This is essentially a classification task. The main contribution of this study lies in comparing and evaluating several Bayesian network classifiers with statistical and other artificial intelligence techniques for the purpose of classifying customers in the binary classification problem at hand. Certain Bayesian network classifiers have been recently proposed in the artificial',\n",
       " '25519ce6a924f5890180eacfa6e66203048f5dd1': '5 Nowadays computers are in the middle of most economic transactions. 6 These “computer-mediated transactions” generate huge amounts of 7 data, and new tools can be used to manipulate and analyze this data. 8 This essay offers a brief introduction to some of these tools and meth9 ods. 10 Computers are now involved in many economic transactions and can cap11 ture data associated with these transactions, which can then be manipulated 12 and analyzed. Conventional statistical and econometric techniques such as 13 regression often work well but there are issues unique to big data sets that 14 may require different tools. 15 First, the sheer size of the data involved may require more powerful data 16 manipulation tools. Second, we may have more potential predictors than 17 appropriate for estimation, so we need to do some kind of variable selection. 18 Third, large data sets may allow for more flexible relationships than simple 19 ∗Hal Varian is Chief Economist, Google Inc., Mountain View, California, and Emeritus Professor of Economics, University of California, Berkeley, California. Thanks to Jeffrey Oldham, Tom Zhang, Rob On, Pierre Grinspan, Jerry Friedman, Art Owen, Steve Scott, Bo Cowgill, Brock Noland, Daniel Stonehill, Robert Snedegar, Gary King, the editors of this journal for comments on earlier versions of this paper. 1 linear models. Machine learning techniques such as decision trees, support 20 vector machines, neural nets, deep learning and so on may allow for more 21 effective ways to model complex relationships. 22 In this essay I will describe a few of these tools for manipulating and an23 alyzing big data. I believe that these methods have a lot to offer and should 24 be more widely known and used by economists. In fact, my standard advice 25 to graduate students these days is “go to the computer science department 26 and take a class in machine learning.” There have been very fruitful collabo27 rations between computer scientists and statisticians in the last decade or so, 28 and I expect collaborations between computer scientists and econometricians 29 will also be productive in the future. 30 1 Tools to manipulate big data 31 Economists have historically dealt with data that fits in a spreadsheet, but 32 that is changing as new more detailed data becomes available; see Einav 33 and Levin [2013] for several examples and discussion. If you have more 34 than a million or so rows in a spreadsheet, you probably want to store it in a 35 relational database, such as MySQL. Relational databases offer a flexible way 36 to store, manipulate and retrieve data using a Structured Query Language 37 (SQL) which is easy to learn and very useful for dealing with medium-sized 38 data sets. 39 However, if you have several gigabytes of data or several million observa40 tions, standard relational databases become unwieldy. Databases to manage 41 data of this size are generically known as “NoSQL” databases. The term is 42 used rather loosely, but is sometimes interpreted as meaning “not only SQL.” 43 NoSQL databases are more primitive than SQL databases in terms of data 44 manipulation capabilities but can handle larger amounts of data. 45 Due to the rise of computer mediated transactions, many companies have 46 found it necessary to develop systems to process billions of transactions per 47',\n",
       " '634aa5d051512ee4b831e6210a234fb2d9b9d623': 'Intersection management is one of the most challenging problems within the transport system. Traffic light-based methods have been efficient but are not able to deal with the growing mobility and social challenges. On the other hand, the advancements of automation and communications have enabled cooperative intersection management, where road users, infrastructure, and traffic control centers are able to communicate and coordinate the traffic safely and efficiently. Major techniques and solutions for cooperative intersections are surveyed in this paper for both signalized and nonsignalized intersections, whereas focuses are put on the latter. Cooperative methods, including time slots and space reservation, trajectory planning, and virtual traffic lights, are discussed in detail. Vehicle collision warning and avoidance methods are discussed to deal with uncertainties. Concerning vulnerable road users, pedestrian collision avoidance methods are discussed. In addition, an introduction to major projects related to cooperative intersection management is presented. A further discussion of the presented works is given with highlights of future research topics. This paper serves as a comprehensive survey of the field, aiming at stimulating new methods and accelerating the advancement of automated and cooperative intersections.',\n",
       " '593b0a74211460f424d471ab7155a0a05c5fd342': 'This paper presents and analysis the common existing sequential pattern mining algorithms. It presents a classifying study of sequential pattern-mining algorithms into five extensive classes. First, on the basis of Apriori-based algorithm, second on Breadth First Search-based strategy, third on Depth First Search strategy, fourth on sequential closed-pattern algorithm and five on the basis of incremental pattern mining algorithms. At the end, a comparative analysis is done on the basis of important key features supported by various algorithms. This study gives an enhancement in the understanding of the approaches of sequential',\n",
       " 'b5e04a538ecb428c4cfef9784fe1f7d1c193cd1a': 'Substrate integrated waveguide (SIW) cavity resonators are among the emerging group of SIW-based circuit components that are gaining popularity and increasingly employed in integrated microwave and mm-wave circuits. The SIW cavities offer a significantly enhanced performance in comparison to the previously available planar microwave resonators. The high quality factor of the waveguide-based cavity resonators enables designing microwave oscillators with very low phase noise as well as compact high-gain antennas [1–4]. The SIW-cavity-based antennas also show much promise for implementation of low-cost lightweight fully-integrated high-gain antennas that find application in ultra-light communication satellites, low payload spacecrafts, high-frequency radar, and sensors. In this paper, a circular SIW cavity resonator, which is fed by a microstrip and via probe, is presented. The microstrip feed is optimized to achieve a return loss of better than 20dB from both simulation and measurements. A resonance frequency of 16.79GHz and quality factor of 76.3 are determined from vector network analyzer (VNA) measurements. Due to the folded slot on the upper conductor layer, the resonator is an efficient cavity backed antenna. A maximum gain of over 7dB is measured in an anechoic chamber at the resonance frequency of 16.79 GHz.',\n",
       " '35de4258058f02a31cd0a0882b5bcc14d7a06697': 'The process-driven composition of Web services is emerging as a promising approach to integrate business applications within and across organizational boundaries. In this approach, individual Web services are federated into composite Web services whose business logic is expressed as a process model. The tasks of this process model are essentially invocations to functionalities offered by the underlying component services. Usually, several component services are able to execute a given task, although with different levels of pricing and quality. In this paper, we advocate that the selection of component services should be carried out during the execution of a composite service, rather than at design-time. In addition, this selection should consider multiple criteria (e.g., price, duration, reliability), and it should take into account global constraints and preferences set by the user (e.g., budget constraints). Accordingly, the paper proposes a global planning approach to optimally select component services during the execution of a composite service. Service selection is formulated as an optimization problem which can be solved using efficient linear programming methods. Experimental results show that this global planning approach outperforms approaches in which the component services are selected individually for each task in a composite service.',\n",
       " '87d696d7dce4fed554430f100d0f2aaee9f73bc5': 'In the massive online worlds of social media, users frequently rely on organizing themselves around specific topics of interest to find and engage with like-minded people. However, navigating these massive worlds and finding topics of specific interest often proves difficult because the worlds are mostly organized haphazardly, leaving users to find relevant interests by word of mouth or using a basic search feature. Here, we report on a method using the backbone of a network to create a map of the primary topics of interest in any social network. To demonstrate the method, we build an interest map for the social news web site reddit and show how such a map could be used to navigate a social media world. Moreover, we analyze the network properties of the reddit social network and find that it has a scale-free, small-world, and modular community structure, much like other online social networks such as Facebook and Twitter. We suggest that the integration of interest maps into popular social media platforms will assist users in organizing themselves into more specific interest groups, which will help alleviate the overcrowding effect often observed in large online communities. Subjects Network Science and Online Social Networks, Visual Analytics',\n",
       " '1d145b63fd065c562ed2fecb3f34643fc9653b60': \"The rapid growth of investment in information technology (IT) by organizations worldwide has made user acceptance an increasingly critical technology implementation a d management issue. While such acceptance has received fairly extensive attention from previous research, additional efforts are needed to examine or validate existing research results, particularly those involving different technologies, user populations, and/or organizational contexts. In response, this paper reports a research work that examined the applicability of the Technology Acceptance Model (ΤΑΜ) in explaining physicians' decisions to accept telemedicine technology in the health-care context. The technology, the user group, and the organizational context are all new to IT acceptance/adoption research. The study also addressed a pragmatic technology management need resulting from millions of dollars invested by healthcare organizations in developing and implementing telemedicine programs in recent years. The model's overall fit, explanatory power, and the individual causal links that it postulates were evaluated by examining the acceptance of telemedicine technology among physicians practicing at public tertiary hospitals in Hong Kong. Our results suggested that ΤΑΜ was able to provide a reasonable depiction of physicians' intention to use telemedicine technology. Perceived usefulness was found to be a significant determinant ofattitude and intention but perceived ease of use was not. The relatively low R-square of the model suggests both the limitations of the parsimonious model and the need for incorporating additional factors or integrating with other IT acceptance models in order to improve its specificity and explanatory utility in a health-care context. Based on the study findings, implications for user technology acceptance research and telemedicine management are discussed.\",\n",
       " 'd7ab41adebaec9272c2797512a021482a594d040': 'ed descriptions of machines by using a DSL while enjoying the full power of scripting languages (in both Puppet and Chef, you can describe behavior in the Ruby language (a dynamic, general-purpose object-oriented programming language), see http://www.ruby-lang. org/en/). Declarative descriptions of target behavior (i.e., what the system must be). Thus, running the scripts will always lead to the same end result. Management of code in version control. By using a version control system as the leading medium, you do not need to adjust the machines manually (which is not reproducible). Synchronization of environments by using a version control system and automatic provisioning of environments. Continuous integration servers, such as Jenkins, simply have to listen to the path in the version control system to detect changes. Then the configuration management tool (e.g., Puppet) ensures that the corresponding machines apply the behavior that is described in version control. Using tools such as Jenkins (see Chapter 8) and Puppet and Vagrant (see Chapter 9), complete setups, including virtualizations, can be managed automatically. Sharing of scripts (e.g., Puppet manifests). A cross-functional team that includes development and operations can develop this function. Sharing the scripts in the version control system enables all parties, particularly development and operations, to use those scripts to set up their respective environments: test environments (used by development) and production environments (managed by operations). Automation is an essential backbone of DevOps (see Chapter 3 and Chapter 8 for more information on automation). Automation is the use of solutions to reduce the need for human work. Automation can ensure that the software is built the same way each time, that the team sees every change made to the software, and that the software is tested and reviewed in the same way every day so that no defects slip through or are introduced through human error. In software development projects, a high level of automation is a prerequisite for quickly delivering the best quality and for obtaining feedback from stakeholders early and often. Automating aspects of DevOps helps to make parts of the process transparent for the whole team and also helps deploy software to different target environments in the same way. You can best improve what you measure; and to measure something usefully, you need a process that delivers results in a reproducible way. DevOps addresses aspects similar to those tackled by Agile development, but the former focuses on breaking down the walls between developers and operations workers. The challenge is to communicate the benefits of DevOps to both development and operations teams. Both groups may be reluctant to start implementing the shift toward DevOps because their day is already full of activities. So why should they be concerned with the work of others? Why should DEVOPS FOR DEVELOPERS 31 operations want to use unfamiliar tools and adjust their daily routines when their self-made, isolated solutions have worked just fine for years? Because of this resistance, the incentives and commitment provided by upper management are important. Incentives alone are not enough: unified processes and tool chains are also important. Upper management will also resist by questioning the wisdom of implementing DevOps if the concrete benefits are not visible. Better cash flow and improved time to market are hard to measure. Management asks questions that address the core problems of software engineering while ignoring the symptoms: how can the company achieve maximal earnings in a short period of time? How can requirements be made stable and delivered to customers quickly? These results and visions should be measured with metrics that are shared by development and operations. Existing metrics can be further used or replaced by metrics that accurately express business value. One example of an end-to-end metric is the cycle time, which we will discuss in detail in Chapter 3.',\n",
       " 'bca1bf790987bfb8fccf4e158a5c9fab3ab371ac': 'We introduce an extended naive Bayes model for word sense induction (WSI) and apply it to a WSI task. The extended model incorporates the idea the words closer to the target word are more relevant in predicting its sense. The proposed model is very simple yet effective when evaluated on SemEval-2010 WSI data.',\n",
       " '53f3edfeb22de82c7a4b4a02209d296526eee38c': \"Based on the problem that disasters occur frequently all over the world recently. This paper aims to develop dispatching optimization and dynamical routing guidance techniques for emergency vehicles under disaster conditions, so as to reduce emergency response time and avoid further possible deterioration of disaster situation. As to dispatching for emergency vehicles, firstly, classify the casualties into several regions based on the pickup locations, quantity and severity of casualties by an adaptive spectral clustering method, and then work out dispatching strategies for emergency vehicles by k-means clustering method based on the distance among casualties regions, emergency supply stations and hospitals. As to routing guidance for emergency vehicles, centrally dynamic route guidance system based on parallel computing technology is presented to offer safe, reliable and fast routes for emergency vehicles, which are subject to the network's impedance function based on real-time forecasted travel time. Finally, the algorithms presented in this paper are validated based on the platform of ArcGIS by generating casualties randomly in random areas and damaging the simulation network of Changchun city randomly.\",\n",
       " 'ef51ff88c525751e2d09f245a3bedc40cf364961': 'On the Dark Web, several websites inhibit automated scraping attempts by employing CAPTCHAs. Scraping important content from a website is possible if these CAPTCHAs are solved by a web scraper. For this purpose, a Machine Learning tool is used, TensorFlow and an Optical Character Recognition tool, Tesseract to solve simple CAPTCHAs. Two sets of CATPCHAs, which are also used on some Dark Web websites, were generated for testing purposes. Tesseract achieved a success rate of 27.6% and 13.7% for set 1 and 2, respectively. A total of three models were created for TensorFlow. One model per set of CAPTCHAs and one model with the two sets mixed together. TensorFlow achieved a success rate of 94.6%, 99.7%, and 70.1% for the first, second, and mixed set, respectively. The initial investment to train TensorFlow can take up to two days to train for a single type of CAPTCHA, depending on implementation efficiency and hardware. The CAPTCHA images, including the answers, are also a requirement for training TensorFlow. Whereas Tesseract can be used on-demand without need for prior training.',\n",
       " '0d4fca03c4748fcac491809f0f73cde401972e28': 'Business intelligence systems combine operational data with analytical tools to present complex and competitive information to planners and decision makers. The objective is to improve the timeliness and quality of inputs to the decision process. Business Intelligence is used to understand the capabilities available in the firm; the state of the art, trends, and future directions in the markets, the technologies, and the regulatory environment in which the firm competes; and the actions of competitors and the implications of these actions. The emergence of the data warehouse as a repository, advances in data cleansing, increased capabilities of hardware and software, and the emergence of the web architecture all combine to create a richer business intelligence environment than was available previously. Although business intelligence systems are widely used in industry, research about them is limited. This paper, in addition to being a tutorial, proposes a BI framework and potential research topics. The framework highlights the importance of unstructured data and discusses the need to develop BI tools for its acquisition, integration, cleanup, search, analysis, and delivery. In addition, this paper explores a matrix for BI data types (structured vs. unstructured) and data sources (internal and external) to guide research.',\n",
       " '22b22af6c27e6d4348ed9d131ec119ba48d8301e': 'Frameworks and libraries offer reusable and customizable functionality through Application Programming Interfaces (APIs). Correctly using large and sophisticated APIs can represent a challenge due to hidden assumptions and requirements. Numerous approaches have been developed to infer properties of APIs, intended to guide their use by developers. With each approach come new definitions of API properties, new techniques for inferring these properties, and new ways to assess their correctness and usefulness. This paper provides a comprehensive survey of over a decade of research on automated property inference for APIs. Our survey provides a synthesis of this complex technical field along different dimensions of analysis: properties inferred, mining techniques, and empirical results. In particular, we derive a classification and organization of over 60 techniques into five different categories based on the type of API property inferred: unordered usage patterns, sequential usage patterns, behavioral specifications, migration mappings, and general information.',\n",
       " 'e66efb82b1c3982c6451923d73e870e95339c3a6': 'With the development of GPS and the popularity of smart phones and wearable devices, users can easily log their daily trajectories. Prior works have elaborated on mining trajectory patterns from raw trajectories. Trajectory patterns consist of hot regions and the sequential relationships among them, where hot regions refer the spatial regions with a higher density of data points. Note that some hot regions do not have any meaning for users. Moreover, trajectory patterns do not have explicit time information or semantic information. To enrich trajectory patterns, we propose semantic trajectory patterns which are referred to as the moving patterns with spatial, temporal, and semantic attributes. Given a user trajectory, we aim at mining frequent semantic trajectory patterns. Explicitly, we extract the three attributes from a raw trajectory, and convert it into a semantic mobility sequence. Given such a semantic mobility sequence, we propose two algorithms to discover frequent semantic trajectory patterns. The first algorithm, MB (standing for matching-based algorithm), is a naive method to find frequent semantic trajectory patterns. It generates all possible patterns and extracts the occurrence of the patterns from the semantic mobility sequence. The second algorithm, PS (standing for PrefixSpan-based algorithm), is developed to efficiently mine semantic trajectory patterns. Due to the good efficiency of PrefixSpan, algorithm PS will fully utilize the advantage of PrefixSpan. Since the semantic mobility sequence contains three attributes, we need to further transform it into a raw sequence before using algorithm PrefixSpan. Therefore, we propose the SS algorithm (standing for sequence symbolization algorithm) to achieve this purpose. To evaluate our proposed algorithms, we conducted experiments on the real datasets of Google Location History, and the experimental results show the effectiveness and efficiency of our proposed algorithms.',\n",
       " '331cd0d53df0254213557cee2d9f0a2109ba16d8': 'In this paper a modified form of the most efficient resonant LLC series parallel converter configuration is proposed. The proposed system comprises of an additional LC circuit synchronized with the existing resonant tank of LLC configuration (LLC-LC configuration). With the development of power electronics devices, resonant converters have been proved to be more efficient than conventional converters as they employ soft switching technique. Among the three basic configurations of resonant converter, Series Resonant Converter (SRC), Parallel Resonant Converter (PRC) and Series Parallel Resonant Converter (SPRC), the LLC configuration under SPRC is proved to be most efficient providing narrow switching frequency range for wide range of load variation, improved efficiency and providing ZVS capability even under no load condition. The modified LLC configuration i.e., LLC-LC configuration offers better efficiency as well as better output voltage and gain. The efficiency tends to increase with increase in input voltage and hence these are suitable for high input voltage operation. The simulation and analysis has been done for full bridge configuration of the switching circuit and the results are presented',\n",
       " '85fc21452fe92532ec89444055880aadb0eacf4c': 'Epilepsy is common neurological diseases, affecting about 0.6-0.8 % of world population. Epileptic patients suffer from chronic unprovoked seizures, which can result in broad spectrum of debilitating medical and social consequences. Since seizures, in general, occur infrequently and are unpredictable, automated seizure detection systems are recommended to screen for seizures during long-term electroencephalogram (EEG) recordings. In addition, systems for early seizure detection can lead to the development of new types of intervention systems that are designed to control or shorten the duration of seizure events. In this article, we investigate the utility of recurrent neural networks (RNNs) in designing seizure detection and early seizure detection systems. We propose a deep learning framework via the use of Gated Recurrent Unit (GRU) RNNs for seizure detection. We use publicly available data in order to evaluate our method and demonstrate very promising evaluation results with overall accuracy close to 100 %. We also systematically investigate the application of our method for early seizure warning systems. Our method can detect about 98% of seizure events within the first 5 seconds of the overall epileptic seizure duration.',\n",
       " '8f6c14f6743d8f9a8ab0be99a50fb51a123ab62c': 'In the context of document image analysis, image binarization is an important preprocessing step for other document analysis algorithms, but also relevant on its own by improving the readability of images of historical documents. While historical document image binarization is challenging due to common image degradations, such as bleedthrough, faded ink or stains, achieving good binarization performance in a timely manner is a worthwhile goal to facilitate efficient information extraction from historical documents. In this paper, we propose a recurrent neural network based algorithm using Grid Long Short-Term Memory cells for image binarization, as well as a pseudo F-Measure based weighted loss function. We evaluate the binarization and execution performance of our algorithm for different choices of footprint size, scale factor and loss function. Our experiments show a significant trade-off between binarization time and quality for different footprint sizes. However, we see no statistically significant difference when using different scale factors and only limited differences for different loss functions. Lastly, we compare the binarization performance of our approach with the best performing algorithm in the 2016 handwritten document image binarization contest and show that both algorithms perform equally well.',\n",
       " '61dad02743d5333e942677836052b814bef4bad8': 'The rapid development of Internet technologies in recent decades has imposed a heavy information burden on users. This has led to the popularity of recommender systems, which provide advice to users about items they may like to examine. Collaborative Filtering (CF) is the most promising technique in recommender systems, providing personalized recommendations to users based on their previously expressed preferences and those of other similar users. This paper introduces a CF framework based on Fuzzy Association Rules and Multiple-level Similarity (FARAMS). FARAMS extended existing techniques by using fuzzy association rule mining, and takes advantage of product similarities in taxonomies to address data sparseness and nontransitive associations. Experimental results show that FARAMS improves prediction quality, as compared to similar approaches.',\n",
       " '3ddac15bd47bc0745db4297d30be71af43adf0bb': 'Theminimum-degree greedy algorithm, or Greedy for short, is a simple and well-studied method for finding independent sets in graphs. We show that it achieves a performance ratio of (Δ+2)/3 for approximating independent sets in graphs with degree bounded by Δ. The analysis yields a precise characterization of the size of the independent sets found by the algorithm as a function of the independence number, as well as a generalization of Turán’s bound. We also analyze the algorithm when run in combination with a known preprocessing technique, and obtain an improved $$(2\\\\bar d + 3)/5$$ performance ratio on graphs with average degree $$\\\\bar d$$ , improving on the previous best $$(\\\\bar d + 1)/2$$ of Hochbaum. Finally, we present an efficient parallel and distributed algorithm attaining the performance guarantees of Greedy.',\n",
       " 'bb6e6e3251bbb80587bdb5064e24b55d728529b1': '14 The purposes of this article are to position mixed methods research (mixed research is a synonym) as the natural complement to traditional qualitative and quantitative research, to present pragmatism as offering an attractive philosophical partner for mixed methods research, and to provide a framework for designing and conducting mixed methods research. In doing this, we briefly review the paradigm “wars” and incompatibility thesis, we show some commonalities between quantitative and qualitative research, we explain the tenets of pragmatism, we explain the fundamental principle of mixed research and how to apply it, we provide specific sets of designs for the two major types of mixed methods research (mixed-model designs and mixed-method designs), and, finally, we explain mixed methods research as following (recursively) an eight-step process. A key feature of mixed methods research is its methodological pluralism or eclecticism, which frequently results in superior research (compared to monomethod research). Mixed methods research will be successful as more investigators study and help advance its concepts and as they regularly practice it.',\n",
       " 'd8e682b2f33b7c765d879717d68cdf262dba871e': 'Written development communication (e.g. mailing lists, issue trackers) constitutes a precious source of information to build recommenders for software engineers, for example aimed at suggesting experts, or at redocumenting existing source code. In this paper we propose a novel, semi-supervised approach named DECA (Development Emails Content Analyzer) that uses Natural Language Parsing to classify the content of development emails according to their purpose (e.g. feature request, opinion asking, problem discovery, solution proposal, information giving etc), identifying email elements that can be used for specific tasks. A study based on data from Qt and Ubuntu, highlights a high precision (90%) and recall (70%) of DECA in classifying email content, outperforming traditional machine learning strategies. Moreover, we successfully used DECA for re-documenting source code of Eclipse and Lucene, improving the recall, while keeping high precision, of a previous approach based on ad-hoc heuristics.',\n",
       " '4e56ab1afd8a515a0a0b351fbf1b1d08624d0cc2': 'We study the classic problem of choosing a prior distribution for a location parameter β = (β1, . . . , βp) as p grows large. First, we study the standard “global-local shrinkage” approach, based on scale mixtures of normals. Two theorems are presented which characterize certain desirable properties of shrinkage priors for sparse problems. Next, we review some recent results showing how Lévy processes can be used to generate infinite-dimensional versions of standard normal scale-mixture priors, along with new priors that have yet to be seriously studied in the literature. This approach provides an intuitive framework both for generating new regularization penalties and shrinkage rules, and for performing asymptotic analysis on existing models.',\n",
       " '64eb627f5e2048892edbeab44567516af4a43b2e': 'BACKGROUND\\nDespite the popularity of the low-carbohydrate, high-protein, high-fat (Atkins) diet, no randomized, controlled trials have evaluated its efficacy.\\n\\n\\nMETHODS\\nWe conducted a one-year, multicenter, controlled trial involving 63 obese men and women who were randomly assigned to either a low-carbohydrate, high-protein, high-fat diet or a low-calorie, high-carbohydrate, low-fat (conventional) diet. Professional contact was minimal to replicate the approach used by most dieters.\\n\\n\\nRESULTS\\nSubjects on the low-carbohydrate diet had lost more weight than subjects on the conventional diet at 3 months (mean [+/-SD], -6.8+/-5.0 vs. -2.7+/-3.7 percent of body weight; P=0.001) and 6 months (-7.0+/-6.5 vs. -3.2+/-5.6 percent of body weight, P=0.02), but the difference at 12 months was not significant (-4.4+/-6.7 vs. -2.5+/-6.3 percent of body weight, P=0.26). After three months, no significant differences were found between the groups in total or low-density lipoprotein cholesterol concentrations. The increase in high-density lipoprotein cholesterol concentrations and the decrease in triglyceride concentrations were greater among subjects on the low-carbohydrate diet than among those on the conventional diet throughout most of the study. Both diets significantly decreased diastolic blood pressure and the insulin response to an oral glucose load.\\n\\n\\nCONCLUSIONS\\nThe low-carbohydrate diet produced a greater weight loss (absolute difference, approximately 4 percent) than did the conventional diet for the first six months, but the differences were not significant at one year. The low-carbohydrate diet was associated with a greater improvement in some risk factors for coronary heart disease. Adherence was poor and attrition was high in both groups. Longer and larger studies are required to determine the long-term safety and efficacy of low-carbohydrate, high-protein, high-fat diets.',\n",
       " '0dadc024bb2e9cb675165fdc7a13d55f5c732636': \"The assessment of the discrimination ability of a survival analysis model is a problem of considerable theoretical interest and important practical applications. This issue is, however, more complex than evaluating the performance of a linear or logistic regression. Several different measures have been proposed in the biostatistical literature. In this paper we investigate the properties of the overall C index introduced by Harrell as a natural extension of the ROC curve area to survival analysis. We develop the overall C index as a parameter describing the performance of a given model applied to the population under consideration and discuss the statistic used as its sample estimate. We discover a relationship between the overall C and the modified Kendall's tau and construct a confidence interval for our measure based on the asymptotic normality of its estimate. Then we investigate via simulations the length and coverage probability of this interval. Finally, we present a real life example evaluating the performance of a Framingham Heart Study model.\",\n",
       " '9afa9c1c650d915c1b6f56b458ff3759bc26bf09': 'Sleep apnea is a sleep disorder with a high prevalence in the adult male population. Sleep apnea is regarded as an independent risk factor,for cardiovascular sequelae such as ischemic heart attacks and stroke. The diagnosis of sleep apnea requires polysomnographic studies in sleep laboratories with expensive equipment and attending personnel. Sleep apnea can be treated effectively using nasal ventilation therapy (nCPAP). Early recognition and selection of patients with sleep related breathing disorders is an important task. Although it has been suggested that this can be done on the basis of the ECG, careful quantitative studies of the accuracy of such techniques are needed. An annotated database with 70 nighttime ECG recordings has been created to support such studies. The annotations were based on visual scoring of disordered breathing during sleep.',\n",
       " '1d70f0d7bd782c65273bc689b6ada8723e52d7a3': 'Detecting clusters or communities in large real-world graphs such as large social or information networks is a problem of considerable interest. In practice, one typically chooses an objective function that captures the intuition of a network cluster as set of nodes with better internal connectivity than external connectivity, and then one applies approximation algorithms or heuristics to extract sets of nodes that are related to the objective function and that \"look like\" good communities for the application of interest.\\n In this paper, we explore a range of network community detection methods in order to compare them and to understand their relative performance and the systematic biases in the clusters they identify. We evaluate several common objective functions that are used to formalize the notion of a network community, and we examine several different classes of approximation algorithms that aim to optimize such objective functions. In addition, rather than simply fixing an objective and asking for an approximation to the best cluster of any size, we consider a size-resolved version of the optimization problem. Considering community quality as a function of its size provides a much finer lens with which to examine community detection algorithms, since objective functions and approximation algorithms often have non-obvious size-dependent behavior.',\n",
       " '312a2edbec5fae34beaf33faa059d37d04cb7235': 'Uncovering the community structure exhibited by real networks is a crucial step toward an understanding of complex systems that goes beyond the local organization of their constituents. Many algorithms have been proposed so far, but none of them has been subjected to strict tests to evaluate their performance. Most of the sporadic tests performed so far involved small networks with known community structure and/or artificial graphs with a simplified structure, which is very uncommon in real systems. Here we test several methods against a recently introduced class of benchmark graphs, with heterogeneous distributions of degree and community size. The methods are also tested against the benchmark by Girvan and Newman [Proc. Natl. Acad. Sci. U.S.A. 99, 7821 (2002)] and on random graphs. As a result of our analysis, three recent algorithms introduced by Rosvall and Bergstrom [Proc. Natl. Acad. Sci. U.S.A. 104, 7327 (2007); Proc. Natl. Acad. Sci. U.S.A. 105, 1118 (2008)], Blondel [J. Stat. Mech.: Theory Exp. (2008), P10008], and Ronhovde and Nussinov [Phys. Rev. E 80, 016109 (2009)] have an excellent performance, with the additional advantage of low computational complexity, which enables one to analyze large systems.',\n",
       " '3e656e08d2b8d1bf84db56090f4053316b01c10f': 'Many complex networks display a mesoscopic structure with groups of nodes sharing many links with the other nodes in their group and comparatively few with nodes of different groups. This feature is known as community structure and encodes precious information about the organization and the function of the nodes. Many algorithms have been proposed but it is not yet clear how they should be tested. Recently we have proposed a general class of undirected and unweighted benchmark graphs, with heterogeneous distributions of node degree and community size. An increasing attention has been recently devoted to develop algorithms able to consider the direction and the weight of the links, which require suitable benchmark graphs for testing. In this paper we extend the basic ideas behind our previous benchmark to generate directed and weighted networks with built-in community structure. We also consider the possibility that nodes belong to more communities, a feature occurring in real systems, such as social networks. As a practical application, we show how modularity optimization performs on our benchmark.',\n",
       " '56ff48f2b22014d5f59fd2db2b0fc0c651038de1': 'Networks have become a key approach to understanding systems of interacting objects, unifying the study of diverse phenomena including biological organisms and human society. One crucial step when studying the structure and dynamics of networks is to identify communities: groups of related nodes that correspond to functional subunits such as protein complexes or social spheres. Communities in networks often overlap such that nodes simultaneously belong to several groups. Meanwhile, many networks are known to possess hierarchical organization, where communities are recursively grouped into a hierarchical structure. However, the fact that many real networks have communities with pervasive overlap, where each and every node belongs to more than one group, has the consequence that a global hierarchy of nodes cannot capture the relationships between overlapping groups. Here we reinvent communities as groups of links rather than nodes and show that this unorthodox approach successfully reconciles the antagonistic organizing principles of overlapping communities and hierarchy. In contrast to the existing literature, which has entirely focused on grouping nodes, link communities naturally incorporate overlap while revealing hierarchical organization. We find relevant link communities in many networks, including major biological networks such as protein–protein interaction and metabolic networks, and show that a large social network contains hierarchically organized community structures spanning inner-city to regional scales while maintaining pervasive overlap. Our results imply that link communities are fundamental building blocks that reveal overlap and hierarchical organization in networks to be two aspects of the same phenomenon.',\n",
       " '62bb7ce6ae6ed38f0ae4d304d56e8edfba1870d0': 'Given a large-scale linked document collection, such as a collection of blog posts or a research literature archive, there are two fundamental problems that have generated a lot of interest in the research community. One is to identify a set of high-level topics covered by the documents in the collection; the other is to uncover and analyze the social network of the authors of the documents. So far these problems have been viewed as separate problems and considered independently from each other. In this paper we argue that these two problems are in fact inter-dependent and should be addressed together. We develop a Bayesian hierarchical approach that performs topic modeling and author community discovery in one unified framework. The effectiveness of our model is demonstrated on two blog data sets in different domains and one research paper citation data from CiteSeer.',\n",
       " 'b7543053ab5e44a6e0fdfc6f9b9d3451011569b6': \"a r t i c l e i n f o Keywords: Brand logos Brand management Aesthetics Commitment Brand extensions Firm performance This research demonstrates that the positive effects of brand logos on customer brand commitment and firm performance derive not from enabling brand identification, as is currently understood, but primarily from facilitating customer self-identity/expressiveness, representing a brand's functional benefits, and offering aesthetic appeal. This study examines whether brand names or visual symbols as logos are more effective at creating these benefits and whether or not the impact of the three aforementioned brand logo benefits on customer brand commitment and firm performance is contingent on the extent to which a firm leverages its brand (i.e., employs brand extensions to different product categories).\",\n",
       " 'f07fd927971c40261dd7cef1ad6d2360b23fe294': 'We consider the problem of sparse canonical correlation analysis (CCA), i.e., the search for two linear combi nations, one for each multivariate, that yield maximum correlation using a specified number of variables. We propose an efficient numeri cal approximation based on a direct greedy approach which bound s the correlation at each stage. The method is specifically des igned to cope with large data sets and its computational complexit y depends only on the sparsity levels. We analyze the algorith m’s performance through the tradeoff between correlation and parsimony. The results of numerical simulation suggest that a significant portion of the correlation may be captured using a relatively small number of variables. In addition, we exami ne the use of sparse CCA as a regularization method when the number of available samples is small compared to the dimensions of t he multivariates. I. I NTRODUCTION Canonical correlation analysis (CCA), introduced by Harol d Hotelling [1], is a standard technique in multivariate data n lysis for extracting common features from a pair of data sourc es [2], [3]. Each of these data sources generates a random vecto r that we call a multivariate. Unlike classical dimensionali ty reduction methods which address one multivariate, CCA take s into account the statistical relations between samples fro m two spaces of possibly different dimensions and structure. In particular, it searches for two linear combinations, one fo r each multivariate, in order to maximize their correlation. It is used in different disciplines as a stand-alone tool or as a preprocessing step for other statistical methods. Further more, CCA is a generalized framework which includes numerous classical methods in statistics, e.g., Principal Componen t Analysis (PCA), Partial Least Squares (PLS) and Multiple Linear Regression (MLR) [4]. CCA has recently regained attention with the advent of kernel CCA and its application to independent component analysis [5], [6]. The last decade has witnessed a growing interest in the search for sparse representations of signals and sparse numerical methods. Thus, we consider the problem of sparse CCA, i.e., the search for linear combinations with maximal correlation using a small number of variables. The quest for sparsity can be motivated through various reasonings. First is the ability to interpret and visualize the results. A small number of variables allows us to get the “big picture”, while sacrificing some of the small details. Moreover, spars e representations enable the use of computationally efficien t The first two authors contributed equally to this manuscript . This work was supported in part by an AFOSR MURI under Grant FA9550-06-1-0 324. numerical methods, compression techniques, as well as nois e reduction algorithms. The second motivation for sparsity i s regularization and stability. One of the main vulnerabilit ies of CCA is its sensitivity to a small number of observations. Thu s, regularized methods such as ridge CCA [7] must be used. In this context, sparse CCA is a subset selection scheme which allows us to reduce the dimensions of the vectors and obtain a stable solution. To the best of our knowledge the first reference to sparse CCA appeared in [2] where backward and stepwise subset selection were proposed. This discussion was of qualitativ e nature and no specific numerical algorithm was proposed. Recently, increasing demands for multidimensional data pr ocessing and decreasing computational cost has caused the topic to rise to prominence once again [8]–[13]. The main disadvantages with these current solutions is that there is no direct control over the sparsity and it is difficult (and nonintuitive) to select their optimal hyperparameters. In add ition, the computational complexity of most of these methods is too high for practical applications with high dimensional data sets. Sparse CCA has also been implicitly addressed in [9], [14] an d is intimately related to the recent results on sparse PCA [9] , [15]–[17]. Indeed, our proposed solution is an extension of the results in [17] to CCA. The main contribution of this work is twofold. First, we derive CCA algorithms with direct control over the sparsity in each of the multivariates and examine their performance. Our computationally efficient methods are specifically aime d at understanding the relations between two data sets of larg e dimensions. We adopt a forward (or backward) greedy approach which is based on sequentially picking (or dropping) variables. At each stage, we bound the optimal CCA solution and bypass the need to resolve the full problem. Moreover, the computational complexity of the forward greedy method does not depend on the dimensions of the data but only on the sparsity parameters. Numerical simulation results show th at a significant portion of the correlation can be efficiently cap tured using a relatively low number of non-zero coefficients. Our second contribution is investigation of sparse CCA as a regularization method. Using empirical simulations we examin e the use of the different algorithms when the dimensions of the multivariates are larger than (or of the same order of) the number of samples and demonstrate the advantage of sparse CCA. In this context, one of the advantages of the greedy approach is that it generates the full sparsity path i n a single run and allows for efficient parameter tuning using',\n",
       " 'cbf796150ff01714244f09ccc16f16ffc471ffdb': 'Web tables constitute valuable sources of information for various applications, ranging from Web search to Knowledge Base (KB) augmentation. An underlying common requirement is to annotate the rows of Web tables with semantically rich descriptions of entities published in Web KBs. In this paper, we evaluate three unsupervised annotation methods: (a) a lookup-based method which relies on the minimal entity context provided in Web tables to discover correspondences to the KB, (b) a semantic embeddings method that exploits a vectorial representation of the rich entity context in a KB to identify the most relevant subset of entities in the Web table, and (c) an ontology matching method, which exploits schematic and instance information of entities available both in a KB and a Web table. Our experimental evaluation is conducted using two existing benchmark data sets in addition to a new large-scale benchmark created using Wikipedia tables. Our results show that: 1) our novel lookup-based method outperforms state-of-theart lookup-based methods, 2) the semantic embeddings method outperforms lookup-based methods in one benchmark data set, and 3) the lack of a rich schema in Web tables can limit the ability of ontology matching tools in performing high-quality table annotation. As a result, we propose a hybrid method that significantly outperforms individual methods on all the benchmarks.',\n",
       " 'a742d81416a1da97af53e0a9748c16f37fd61b40': 'INTRODUCTION ................................................................................... 571 I. FIRST AMENDMENT BACKGROUND ....................................... 573 II. THE ANALOGOUS HISTORIES OF FILMS AND VIDEO GAMES ....................................................................................... 576 A. Film Controversy and the Formation of the MPAA ................ 576 B. Early Video Game Controversy and the Formation of the ESRB ................................................................................... 580 C. Doom and Columbine ........................................................... 584 D. Jack Thompson and Grand Theft Auto ................................... 586 III. WHY VIDEO GAMES SHOULD NOT BE TREATED DIFFERENTLY THAN FILMS .................................................... 593 A. Violent and Sexual Content in Video Games is Distinguishable from Pornography and Obscenity. .................. 594 B. Violent Game Content is Similar to Violent Film Content. ..... 596 C. Positive Social Aspects of Violent Gaming............................... 597 D. Desensitization Will Lead to a Decrease in Political Outrage. ............................................................................... 604 IV. EXISTING VIDEO GAME JURISPRUDENCE .............................. 605 V. RATINGS AND LABELS AS UNCONSTITUTIONAL CENSORSHIP.............................................................................. 607 CONCLUSION ....................................................................................... 609',\n",
       " '0814694a247a9b6e38dde34ab95067a63f67e458': 'This paper presents a study of the life cycle of news articles posted online. We describe the interplay between website visitation patterns and social media reactions to news content. We show that we can use this hybrid observation method to characterize distinct classes of articles. We also find that social media reactions can help predict future visitation patterns early and accurately. We validate our methods using qualitative analysis as well as quantitative analysis on data from a large international news network, for a set of articles generating more than 3,000,000 visits and 200,000 social media reactions. We show that it is possible to model accurately the overall traffic articles will ultimately receive by observing the first ten to twenty minutes of social media reactions. Achieving the same prediction accuracy with visits alone would require to wait for three hours of data. We also describe significant improvements on the accuracy of the early prediction of shelf-life for news stories.',\n",
       " '275f66e845043217d5c37328b5e71a178302469f': 'Web, HTTP, WWW proxies, caching policies, replacement algorithms, performance Web proxy caches are used to improve performance of the WWW. Since the majority of Web documents are static documents, caching them at WWW proxies reduces both network traffic and response time. One of the keys to better proxy cache performance is an efficient caching policy which keeps in the cache popular documents and replaces rarely used ones. This paper introduces the Greedy-Dual-Size-Frequency caching policy to maximize hit and byte hit rates for WWW proxies. Proposed caching strategy incorporates in a simple way the most important characteristics of the file and its accesses such as file size, file access frequency and recentness of the last access. Greedy-Dual-Size-Frequency is an improvement of Greedy-Dual-Size algorithm – the current champion among the replacement strategies proposed for Web proxy caches.',\n",
       " '1c8ea4d0687eae94871ee1916da4445e08f29076': 'Most text classification approaches model text at the lexical and syntactic level only, lacking domain robustness and explainability. In tasks like sentiment analysis, such approaches can result in limited effectiveness if the texts to be classified consist of a series of arguments. In this paper, we claim that even a shallow model of the argumentation of a text allows for an effective and more robust classification, while providing intuitive explanations of the classification results. Here, we apply this idea to the supervised prediction of sentiment scores for reviews. We combine existing approaches from sentiment analysis with novel features that compare the overall argumentation structure of the given review text to a learned set of common sentiment flow patterns. Our evaluation in two domains demonstrates the benefit of modeling argumentation for text classification in terms of effectiveness and robustness.',\n",
       " 'f698467f1dd781f652c9839379ccc548a9aa4af1': 'This article reviews the now extensive research literature addressing the impact of accountability on a wide range of social judgments and choices. It focuses on 4 issues: (a) What impact do various accountability ground rules have on thoughts, feelings, and action? (b) Under what conditions will accountability attenuate, have no effect on, or amplify cognitive biases? (c) Does accountability alter how people think or merely what people say they think? and (d) What goals do accountable decision makers seek to achieve? In addition, this review explores the broader implications of accountability research. It highlights the utility of treating thought as a process of internalized dialogue; the importance of documenting social and institutional boundary conditions on putative cognitive biases; and the potential to craft empirical answers to such applied problems as how to structure accountability relationships in organizations.',\n",
       " '49afbe880b8bd419605beb84d3382647bf8e50ea': '',\n",
       " 'c1927578a61df3c5a33f6bca9f9bd5c181e1d5ac': 'Wireless communication networks are highly prone to security threats. The major applications of wireless communication networks are in military, business, healthcare, retail, and transportations. These systems use wired, cellular, or adhoc networks. Wireless sensor networks, actuator networks, and vehicular networks have received a great attention in society and industry. In recent years, the Internet of Things (IoT) has received considerable research attention. The IoT is considered as future of the internet. In future, IoT will play a vital role and will change our living styles, standards, as well as business models. The usage of IoT in different applications is expected to rise rapidly in the coming years. The IoT allows billions of devices, peoples, and services to connect with others and exchange information. Due to the increased usage of IoT devices, the IoT networks are prone to various security attacks. The deployment of efficient security and privacy protocols in IoT networks is extremely needed to ensure confidentiality, authentication, access control, and integrity, among others. In this paper, an extensive comprehensive study on security and privacy issues in IoT networks is provided. Keywords—Internet of Things (IoT); security issues in IoT; security; privacy',\n",
       " 'b5f6ee9baa07301bba6e187bd9380686a72866c6': 'Models of Network Growth All networks, whether they are social, technological, or biological, are the result of a growth process. Many of these networks continue to grow for prolonged periods of time, continually modifying their connectivity structure throughout their entire existence. For example, the World Wide Web has grown from a small number of cross-linked documents in the early 1 990s to an estimated 30 billion indexed web pages in 2009.3 The extraordinary growth of the Web continues unabated and has occurred without any top-down design, yet the topology of its hyperlink structure exhibits characteristic statistical patterns (Pastor-Satorras and Vespig\\xad nani, 2004). Other technological networks such as the power grid, global transportation networks, or mobile communication networks continue to grow and evolve, each displaying characteristic patterns of expansion and elaboration. Growth and change in social and organizational',\n",
       " 'd0eab53a6b20bfca924b85fcfb0ee76bfde6d4ef': 'ADS-B is one of many Federal Aviation Administration (FAA) regulated technologies used to monitor air traffic with high precision, while reducing dependencies on dated and costly radar equipment [1]. The FAA hopes to decrease the separation between aircraft, reduce risk of collision as air traffic density increases, save fuel costs, and increase situational awareness of both commercial and general aviation aircraft within United States airspace. Several aviation technology experts have expressed concern over the security of the ADS-B protocol [2] [3]. ADS-B has an open and well known data format, which is broadcast on known frequencies. This means that the protocol is highly susceptible to radio frequency (RF) attacks such as eavesdropping, jamming, and spoofing. Eavesdropping and jamming will be reviewed in Section 3.4. While eavesdropping and jamming attacks are well studied, due to their applicability in many radio technologies, spoofing attacks against ADS-B are particular to this system. As such, the latter is the focus of our research. This paper evaluates so-called Kalman Filtering and Group Validation techniques (described below) in order to assess which would be a better position verification method of ADS-B signals. The parameters for the comparative analysis include both technical feasibility and practical implementation of each position verification technique. The goal is to offer a practical position verification process which could be implemented with limited government funding within the next 10 years.',\n",
       " '4c2fedecddcae64514ad99b7301ad6e04654f10d': 'The purpose of this article is to introduce the readers to the emerging technologies enabled by deep learning and to review the research work conducted in this area that is of direct relevance to signal processing. We also point out, in our view, the future research directions that may attract interests of and require efforts from more signal processing researchers and practitioners in this emerging area for advancing signal and information processing technology and applications.',\n",
       " 'd0c9acb277da76aebf56c021cb02b51cdfbb56b8': 'There have been recent trends of parents in Western countries refusing to vaccinate their children due to numerous reasons and perceived fears. While opposition to vaccines is as old as the vaccines themselves, there has been a recent surge in the opposition to vaccines in general, specifically against the MMR (measles, mumps, and rubella) vaccine, most notably since the rise in prominence of the notorious British ex-physician, Andrew Wakefield,\\xa0and his works. This has caused multiple measles outbreaks in Western countries where the measles virus was previously considered eliminated. This paper evaluates and reviews the origins of the anti-vaccination movement, the reasons behind the recent strengthening of the movement, role of the internet in the spread of anti-vaccination ideas, and the repercussions in terms of public health and safety.',\n",
       " 'e170ca6dad1221f4bb2e4fc3d42a182e23026b80': \"BACKGROUND\\nDuring recent decades, self-regulated learning (SRL) has become a major research field. SRL successfully integrates the cognitive and motivational components of learning. Self-regulation is usually seen as an individual process, with the social aspects of regulation conceptualized as one aspect of the context. However, recent research has begun to investigate whether self-regulation processes are complemented by socially shared regulation processes.\\n\\n\\nAIMS\\nThe presented study investigated what kind of socio-emotional challenges students experience during collaborative learning and whether the students regulate the emotions evoked during these situations. The interplay of the emotion regulation processes between the individual and the group was also studied.\\n\\n\\nSAMPLE\\nThe sample for this study was 63 teacher education students who studied in groups of three to five during three collaborative learning tasks.\\n\\n\\nMETHOD\\nStudents' interpretations of experienced social challenges and their attempts to regulate emotions evoked by these challenges were collected following each task using the Adaptive Instrument for the Regulation of Emotions.\\n\\n\\nRESULTS\\nThe results indicated that students experienced a variety of social challenges. Students also reported the use of shared regulation in addition to self-regulation. Finally, the results suggested that intrinsic group dynamics are derived from both individual and social elements of collaborative situations.\\n\\n\\nCONCLUSION\\nThe findings of the study support the assumption that students can regulate emotions collaboratively as well as individually. The study contributes to our understanding of the social aspects of emotional regulation in collaborative learning contexts.\",\n",
       " '6f8a13a1a7eba8966627775c32ae59dafd91cedc': 'PAPILLON is a technology for designing highly expressive animated eyes for interactive characters, robots and toys. Expressive eyes are essential in any form of face-to-face communication [2] and designing them has been a critical challenge in robotics, as well as in interactive character and toy development.',\n",
       " 'e33b9ee3c575c6a38b873888e796d29f59d98e04': 'As pervasive and vital as they are in human experience, emotions have long remained an enigma to science. This monograph explores recent scientific advances that clarify central controversies in the study of emotion, including the relationship between intellect and emotion, and the historical debate on the source of emotional experience. Particular attention is given to the intriguing body of research illuminating the critical role of ascending input from the body to the brain in the generation and perception of emotions. This discussion culminates in the presentation of a systems-oriented model of emotion in which the brain functions as a complex pattern-matching system, continually processing input from both the external and internal environments. From this perspective it is shown that the heart is a key component of the emotional system, thus providing a physiological basis for the long-acknowledged link between the heart and our emotional life.',\n",
       " '19b7e0786d9e093fdd8c8751dac0c4eb0aea0b74': '',\n",
       " '224f751d4691515b3f8010d12660c70dd62336b8': \"We propose DeepHand to estimate the 3D pose of a hand using depth data from commercial 3D sensors. We discriminatively train convolutional neural networks to output a low dimensional activation feature given a depth map. This activation feature vector is representative of the global or local joint angle parameters of a hand pose. We efficiently identify 'spatial' nearest neighbors to the activation feature, from a database of features corresponding to synthetic depth maps, and store some 'temporal' neighbors from previous frames. Our matrix completion algorithm uses these 'spatio-temporal' activation features and the corresponding known pose parameter values to estimate the unknown pose parameters of the input feature vector. Our database of activation features supplements large viewpoint coverage and our hierarchical estimation of pose parameters is robust to occlusions. We show that our approach compares favorably to state-of-the-art methods while achieving real time performance (≈ 32 FPS) on a standard computer.\",\n",
       " '486eb944129e0d90a7d2ef8d6085fd482c9be6c5': 'I n December 1999 CrossTalk [3], David Cook provided a well-reasoned historical analysis of programming language development and considered the role languages play in the software development process. The article was valuable because it showed that programming language developments are not sufficient to ensure success; however, it would be dangerous to conclude from this that they are not necessary for success. Cook rightly identifies other issues such as requirements capture, specifications, and verification and validation (V&V) that need to be addressed. Perhaps we need to look at programming languages not just in terms of their ability to code some particular design but in the influence the language has on some of these other vital aspects of the development process. The key notion is that of the benefit of a precise language or language subset. If the term subset has set anyone thinking \" oh no, not another coding standard, \" then read on, the topic is much more interesting and useful than that! Language Issues Programming languages have evolved in three main ways. First came improvements in structure; then attempts at improving compile-time error detection through such things as strong typing; and, most significantly , facilities to improve our ability to express abstractions. All of these have shaped the way we think about problem solving. However, programming languages have not evolved in their precision of expression. In fact, they may have actually gotten worse since the meaning of a sample of machine code is exact and unequivocal , whereas the meaning of the constructs of typical modern high-order languages are substantially less certain. The evolution of C into C++ certainly improved its ability to express design abstractions but, if anything, the predictability of the compiled code decreased. These ambiguities arise either from deficiencies in the original language definition or from implementation freedoms given to the compiler writer for ease of implementation or efficiency reasons. None of this may look like a very serious problem. We can still do code walk-throughs and reviews and, after all, we still have to do dynamic testing that should flush out any remaining ambiguities. In fact the evidence is quite strong that it does matter because it creates an environment where we are encouraged to make little attempt to reason about the software we are producing at each stage of its development. Since we typically do not have formal mathematical specifications and we use imprecise …',\n",
       " '15fc05b9da56764192d56036721a6a19239c07fc': 'Abstmct-Designing robots that learn by themselves to perform complex real-world tasks is a still-open challenge for the field of Robotics and Artificial Intelligence. In this paper we present the robot learning problem as a lifelong problem, in which a robot faces a collection of tasks over its entire lifetime. Such a scenario provides the opportunity to gather general-purpose knowledge that transfers across tasks. We illustrate a particular learning mechanism, explanation-based neural network learning, that transfers knowledge between related tasks via neural network action models. The learning approach is illustrated using a mobile robot, equipped with visual, ultrasonic and laser sensors. In less than 10 minutes operation time, the robot is able to learn to navigate to a marked target object in a natural office environment.',\n",
       " '5a3e2899deed746f1513708f1f0f24a25f4a0750': 'We present an algorithm for registration between a large-scale point cloud and a close-proximity scanned point cloud, providing a localization solution that is fully independent of prior information about the initial positions of the two point cloud coordinate systems. The algorithm, denoted LORAX, selects super-points&#x2013;local subsets of points&#x2013;and describes the geometric structure of each with a low-dimensional descriptor. These descriptors are then used to infer potential matching regions for an efficient coarse registration process, followed by a fine-tuning stage. The set of super-points is selected by covering the point clouds with overlapping spheres, and then filtering out those of low-quality or nonsalient regions. The descriptors are computed using state-of-the-art unsupervised machine learning, utilizing the technology of deep neural network based auto-encoders. Abstract This novel framework provides a strong alternative to the common practice of using manually designed key-point descriptors for coarse point cloud registration. Utilizing super-points instead of key-points allows the available geometrical data to be better exploited to find the correct transformation. Encoding local 3D geometric structures using a deep neural network auto-encoder instead of traditional descriptors continues the trend seen in other computer vision applications and indeed leads to superior results. The algorithm is tested on challenging point cloud registration datasets, and its advantages over previous approaches as well as its robustness to density changes, noise, and missing data are shown.',\n",
       " '89a3c09e0a4c54f89a7237be92d3385116030efc': 'Phishing is one of the social engineering attacks and currently hit on mobile devices. Based on security report by Lookout [1], 30% of Lookout users clicking on an unsafe link per year by using mobile device. Few phishing detection techniques have been applied on mobile device. However, review on phishing detection technique on the detection technique redundant is still need. This paper addresses the current trend phishing detection for mobile device and identifies significant criterion to improve phishing detection techniques on mobile device. Thus, existing research on phishing detection technique for computer and mobile device will be compared and analysed. Hence, outcome of the analysis becomes a guideline in proposing generic phishing detection taxonomy for mobile device.',\n",
       " 'c59f39796e0f8e733f44b1cfe374cfe76834dcf8': 'The International Telecommunications Union provides recommendations regarding spectral emission bounds for primary radar systems. These bounds are currently in review and are defined in terms of spectral occupancy, necessary bandwidth, 40dB bandwidth and out-of-band roll-off rates. Here we derive out-of-band domain spectral envelopes, bandwidth formula and roll-off rates, for various Linear FMCW radar waveforms including sawtooth (LFMCW), Quadratic Phase Coded LFMCW, LFM Pulse Train, and Hann amplitude tapered LFMCW.',\n",
       " '585654039c441a15cdda936902f0f1f9b7498a89': 'With capacitive fingerprint readers being increasingly used for access control as well as for smartphone unlock and payments, there is a growing interest among metrology agencies (e.g., the National Institute of Standards and Technology) to develop standard artifacts (targets) and procedures for repeatable evaluation of capacitive readers. We present our design and fabrication procedures to create conductive 3D targets (gold fingers) for capacitive readers. Wearable 3D targets with known feature markings (e.g., fingerprint ridge flow and ridge spacing) are first fabricated using a high-resolution 3D printer. A sputter coating process is subsequently used to deposit a thin layer (~300 nm) of conductive materials (titanium and gold) on 3D printed targets. The wearable gold finger targets are used to evaluate a PIV-certified single-finger capacitive reader as well as small-area capacitive readers embedded in smartphones and access control terminals. In additional, we show that a simple procedure to create 3D printed spoofs with conductive carbon coating is able to successfully spoof a PIV-certified single-finger capacitive reader as well as a capacitive reader embedded in an access control terminal.',\n",
       " '7e0f013e85eff9b089f58d9a3e98605ae1a7ba18': 'Formulation of speech separation as a supervised learning problem has shown considerable promise. In its simplest form, a supervised learning algorithm, typically a deep neural network, is trained to learn a mapping from noisy features to a time-frequency representation of the target of interest. Traditionally, the ideal binary mask (IBM) is used as the target because of its simplicity and large speech intelligibility gains. The supervised learning framework, however, is not restricted to the use of binary targets. In this study, we evaluate and compare separation results by using different training targets, including the IBM, the target binary mask, the ideal ratio mask (IRM), the short-time Fourier transform spectral magnitude and its corresponding mask (FFT-MASK), and the Gammatone frequency power spectrum. Our results in various test conditions reveal that the two ratio mask targets, the IRM and the FFT-MASK, outperform the other targets in terms of objective intelligibility and quality metrics. In addition, we find that masking based targets, in general, are significantly better than spectral envelope based targets. We also present comparisons with recent methods in non-negative matrix factorization and speech enhancement, which show clear performance advantages of supervised speech separation.',\n",
       " 'c1cd441dad61b9d9d294a19a7043adb1582f786b': 'This paper proposes an algorithm for the recognition and separation of speech signals in non-stationary noise, such as another speaker. We present a method to combine hidden Markov models (HMMs) trained for the speech and noise into a factorial HMM to model the mixture signal. Robustness is obtained by separating the speech and noise signals in a feature domain, which discards unnecessary information. We use mel-cepstral coefficients (MFCCs) as features, and estimate the distribution of mixture MFCCs from the distributions of the target speech and noise. A decoding algorithm is proposed for finding the state transition paths and estimating gains for the speech and noise from a mixture signal. Simulations were carried out using speech material where two speakers were mixed at various levels, and even for high noise level (9 dB above the speech level), the method produced relatively good (60% word recognition accuracy) results. Audio demonstrations are available at www.cs.tut.fi/ ̃tuomasv.',\n",
       " 'ecd4bc32bb2717c96f76dd100fcd1255a07bd656': 'Recently, deep learning techniques have been successfully applied to automatic speech recognition tasks -first to phonetic recognition with context-independent deep belief network (DBN) hidden Markov models (HMMs) and later to large vocabulary continuous speech recognition using context-dependent (CD) DBN-HMMs. In this paper, we report our most recent experiments designed to understand the roles of the two main phases of the DBN learning -pre-training and fine tuning -in the recognition performance of a CD-DBN-HMM based large-vocabulary speech recognizer. As expected, we show that pre-training can initialize weights to a point in the space where fine-tuning can be effective and thus is crucial in training deep structured models. However, a moderate increase of the amount of unlabeled pre-training data has an insignificant effect on the final recognition results as long as the original training size is sufficiently large to initialize the DBN weights. On the other hand, with additional labeled training data, the fine-tuning phase of DBN training can significantly improve the recognition accuracy.',\n",
       " '0b3cfbf79d50dae4a16584533227bb728e3522aa': \"Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.\",\n",
       " '0c7d7b4c546e38a4097a97bf1d16a60012916758': 'We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state transducers (using the freely available OpenFst), together with detailed documentation and scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms. Kaldi is released under the Apache License v2.0, which is highly nonrestrictive, making it suitable for a wide community of users.',\n",
       " '9eb67ca57fecc691853636507e2b852de3f56fac': 'Previous studies have shown that semantically meaningful representations of words and text can be acquired through neural embedding models. In particular, paragraph vector (PV) models have shown impressive performance in some natural language processing tasks by estimating a document (topic) level language model. Integrating the PV models with traditional language model approaches to retrieval, however, produces unstable performance and limited improvements. In this paper, we formally discuss three intrinsic problems of the original PV model that restrict its performance in retrieval tasks. We also describe modifications to the model that make it more suitable for the IR task, and show their impact through experiments and case studies. The three issues we address are (1) the unregulated training process of PV is vulnerable to short document over-fitting that produces length bias in the final retrieval model; (2) the corpus-based negative sampling of PV leads to a weighting scheme for words that overly suppresses the importance of frequent words; and (3) the lack of word-context information makes PV unable to capture word substitution relationships.',\n",
       " '6c7e55e3b53029296097ee07ba75b6b6a98b14e5': 'Controller Area Network (CAN) bus in the vehicles is a de facto standard for serial communication to provide an efficient, reliable and economical link between Electronic Control Units (ECU). However, CAN bus does not have enough security features to protect itself from inside or outside attacks. Intrusion Detection System (IDS) is one of the best ways to enhance the vehicle security level. Unlike the traditional IDS for network security, IDS for vehicle requires light-weight detection algorithm because of the limitations of the computing power of electronic devices reside in cars. In this paper, we propose a light-weight intrusion detection algorithm for in-vehicle network based on the analysis of time intervals of CAN messages. We captured CAN messages from the cars made by a famous manufacturer and performed three kinds of message injection attacks. As a result, we find the time interval is a meaningful feature to detect attacks in the CAN traffic. Also, our intrusion detection system detects all of message injection attacks without making false positive errors.',\n",
       " 'b3ff14e4c9b939841dec4d877256e47d12817638': \"While there has been much research on automatically constructing structured Knowledge Bases (KBs), most of it has focused on generating facts to populate a KB. However, a useful KB must go beyond facts. For example, glosses (short natural language definitions) have been found to be very useful in tasks such as Word Sense Disambiguation. However, the important problem of Automatic Gloss Finding, i.e., assigning glosses to entities in an initially gloss-free KB, is relatively unexplored. We address that gap in this paper. In particular, we propose GLOFIN, a hierarchical semi-supervised learning algorithm for this problem which makes effective use of limited amounts of supervision and available ontological constraints. To the best of our knowledge, GLOFIN is the first system for this task. Through extensive experiments on real-world datasets, we demonstrate GLOFIN's effectiveness. It is encouraging to see that GLOFIN outperforms other state-of-the-art SSL algorithms, especially in low supervision settings. We also demonstrate GLOFIN's robustness to noise through experiments on a wide variety of KBs, ranging from user contributed (e.g., Freebase) to automatically constructed (e.g., NELL). To facilitate further research in this area, we have made the datasets and code used in this paper publicly available.\",\n",
       " '6eb662ef35ec514429c5ba533b212a3a512c3517': 'Autonomous micro aerial vehicles (MAVs) will soon play a major role in tasks such as search and rescue, environment monitoring, surveillance, and inspection. They allow us to easily access environments to which no humans or other vehicles can get access. This reduces the risk for both the people and the environment. For the above applications, it is, however, a requirement that the vehicle is able to navigate without using GPS, or without relying on a preexisting map, or without specific assumptions about the environment. This will allow operations in unstructured, unknown, and GPS-denied environments. We present a novel solution for the task of autonomous navigation of a micro helicopter through a completely unknown environment by using solely a single camera and inertial sensors onboard. Many existing solutions suffer from the problem of drift in the xy plane or from the dependency on a clean GPS signal. The novelty in the here-presented approach is to use a monocular simultaneous localization and mapping (SLAM) framework to stabilize the vehicle in six degrees of freedom. This way, we overcome the problem of both the drift and the GPS dependency. The pose estimated by the visual SLAM algorithm is used in a linear optimal controller that allows us to perform all basic maneuvers such as hovering, set point and trajectory following, vertical takeoff, and landing. All calculations including SLAM and controller are running in real time and online while the helicopter is flying. No offline processing or preprocessing is done. We show real experiments that demonstrate that the vehicle can fly autonomously in an unknown and unstructured environment. To the best of our knowledge, the here-presented work describes the first aerial vehicle that uses onboard monocular vision as a main sensor to navigate through an unknown GPS-denied environment and independently of any external artificial aids. C © 2011 Wiley Periodicals, Inc.',\n",
       " '0286bef6d6da7990a2c50aefd5543df2ce481fbb': 'We study the combinatorial pure exploration (CPE) problem in the stochastic multi-armed bandit setting, where a learner explores a set of arms with the objective of identifying the optimal member of a decision class, which is a collection of subsets of arms with certain combinatorial structures such as size-K subsets, matchings, spanning trees or paths, etc. The CPE problem represents a rich class of pure exploration tasks which covers not only many existing models but also novel cases where the object of interest has a nontrivial combinatorial structure. In this paper, we provide a series of results for the general CPE problem. We present general learning algorithms which work for all decision classes that admit offline maximization oracles in both fixed confidence and fixed budget settings. We prove problem-dependent upper bounds of our algorithms. Our analysis exploits the combinatorial structures of the decision classes and introduces a new analytic tool. We also establish a general problem-dependent lower bound for the CPE problem. Our results show that the proposed algorithms achieve the optimal sample complexity (within logarithmic factors) for many decision classes. In addition, applying our results back to the problems of top-K arms identification and multiple bandit best arms identification, we recover the best available upper bounds up to constant factors and partially resolve a conjecture on the lower bounds.',\n",
       " '3f6f45584d7f71e47118bdcd12826995998871d1': 'The Self-Organizing Incremental Neural Network (SOINN) is an unsupervised classifier that is capable of online incremental learning. Studies have been performed not only for improving the SOINN, but also for applying it to various problems. Furthermore, using the SOINN, more intelligent functions are achieved, such as association, reasoning, and so on. In this paper, we show how to use the SOINN software and to apply it to the above problems.',\n",
       " '185f3fcc78ea32ddbad3a5ebdaefa9504bfb3f5e': 'Chinese painting is distinct from other art in that the painting elements are exhibited by complex water-and-ink diffusion and shows gray, white and black visual effect. Rendering such a water-and-ink painting with polychrome style is a challenging problem. In this paper, we propose a novel style transfer method for Chinese painting. We firstly decompose the Chinese painting with adaptive patches based on its structure, and locally colorize the painting. Then, the colorized image is used for guiding the process of texture transfer that is modeled in Markov Random Field (MRF). More precisely, we improve the classic texture transfer algorithm by modifying the compatibility functions for searching the optimal matching, with the chromatism information. The experiment results show that proposed adaptive patches can well preserve the original content while match the example style. Moreover, we present the transfer results with our method and recent style transfer algorithms, in order to make a comparison.',\n",
       " '0b6aab9ce7910938e0d60c0764dc1c09d3219b05': 'This paper investigates the pre-conditions for successful combination of document representations formed from structural markup for the task of known-item search. As this task is very similar to work in meta-search and data fusion, we adapt several hypotheses from those research areas and investigate them in this context. To investigate these hypotheses, we present a mixture-based language model and also examine many of the current meta-search algorithms. We find that compatible output from systems is important for successful combination of document representations. We also demonstrate that combining low performing document representations can improve performance, but not consistently. We find that the techniques best suited for this task are robust to the inclusion of poorly performing document representations. We also explore the role of variance of results across systems and its impact on the performance of fusion, with the surprising result that the correct documents have higher variance across document representations than highly ranking incorrect documents.',\n",
       " 'a57623e6f0de3775513b436510b2d6cd9343dc5f': 'This paper describes a system which uses entity and topic coherence for improved Text Segmentation (TS) accuracy. First, Linear Dirichlet Allocation (LDA) algorithm was used to obtain topics for sentences in the document. We then performed entity mapping across a window in order to discover the transition of entities within sentences. We used the information obtained to support our LDA-based boundary detection for proper boundary adjustment. We report the significance of the entity coherence approach as well as the superiority of our algorithm over existing works.',\n",
       " 'cd472598052666440b8063e7259b35b78a45d757': 'First, the purpose of this study is to examine the impact of situational variables, scarcity and serendipity, on online impulse buying (OIB) in Chinese social commerce (SC) environment. Second, the study further assesses the moderating role of five dimensions of hedonic shopping value. Data were gathered from 671 online shoppers who come from two metropolitan cities of China, Beijing, and Shanghai. Structure equation modeling utilized was generated by AMOS 23 version to test the study hypotheses. The results confirm that situational factors positively influence the online impulse buying among Chinese online shoppers in SC environment. Four dimensions of hedonic shopping value (social shopping, relaxation shopping, adventure shopping and idea shopping) positively moderate the relationship between serendipity and OIB; value shopping is insignificant with moderation effect. The finding is helpful to the online retailers and SC web developers by recommending them to take the scarcity and serendipity in their consideration. These factors have the potential to motivate the consumers to initiate the hedonic shopping aptitude to urge to buy impulsively. Unlike the previous work which remained unsuccessful in incorporating all factors into one study, this study has incorporated irrational and unplanned consumption along with rational and planned one in the same research.',\n",
       " '75225142acc421a15cb1cd5b633f6de5fc036586': 'Sentence similarity calculation plays an important role in text processing-related research. Many unsupervised techniques such as knowledge-based techniques, corpus-based techniques, string similarity based techniques, and graph alignment techniques are available to measure sentence similarity. However, none of these techniques have been experimented with Tamil. In this paper, we present the first-ever system to measure semantic similarity for Tamil short phrases using a hybrid approach that makes use of knowledge-based and corpus-based techniques. We tested this system with 2000 general sentence pairs and 100 mathematical sentence pairs. For the dataset of 2000 sentence pairs, this approach achieved a Mean Squared Error of 0.195 and a Pearson Correlation factor of 0.815. For the 100 mathematical sentence pairs, this approach achieved an 85% of accuracy.',\n",
       " 'b8759b1ea437802d9a1c2a99d22932a960b7beec': 'This paper explores the firewall security and performance relationship for distributed systems. Experiments are conducted to set firewall security into seven different levels and to quantify their performance impacts. These firewall security levels are formulated, designed, implemented, and tested phase by phase under an experimental environment in which all performed tests are evaluated and compared. Based on the test results, the impacts of the various firewall security levels on system performance with respect to transaction time and latency are measured and analyzed. It is interesting to note that the intuitive belief about security to performance, i.e. the more security would result in less performance, does not always hold in the firewall testing. The results reveal that the significant impact from enhanced security on performance could only be observed under some particular scenarios and thus their relationships are not necessarily inversely related. We also discuss the tradeoff between security and performance.',\n",
       " '4df321947a2ac4365584a01d78a780913b171cf5': 'Aspect Based Sentiment Analysis (ABSA) is the task of mining and summarizing opinions from text about specific entities and their aspects. This article describes two datasets for the development and testing of ABSA systems for French which comprise user reviews annotated with relevant entities, aspects and polarity values. The first dataset contains 457 restaurant reviews (2365 sentences) for training and testing ABSA systems, while the second contains 162 museum reviews (655 sentences) dedicated to out-of-domain evaluation. Both datasets were built as part of SemEval-2016 Task 5 “Aspect-Based Sentiment Analysis” where seven different languages were represented, and are publicly available for research purposes. This article provides examples and statistics by annotation type, summarizes the annotation guidelines and discusses their cross-lingual applicability. It also explains how the data was used for evaluation in the SemEval ABSA task and briefly presents the results obtained for French.',\n",
       " 'dd8da9a4a0a5ef1f19ca71caf5eba11192dd2c41': 'Differences in domains of language use between training data and test data have often been reported to result in performance degradation for phrase-based machine translation models. Throughout the past decade or so, a large body of work aimed at exploring domain-adaptation methods to improve system performance in the face of such domain differences. This paper provides a systematic survey of domain-adaptation methods for phrase-based machine-translation systems. The survey starts out with outlining the sources of errors in various components of phrase-based models due to domain change, including lexical selection, reordering and optimization. Subsequently, it outlines the different research lines to domain adaptation in the literature, and surveys the existing work within these research lines, discussing how these approaches differ and how they relate to each other.',\n",
       " '5fbe9d4e616632972e86c31fbb4b1dff4897e59e': 'In the past decade, a number of adaptive hypermedia learning systems have been developed. However, most of these systems tailor presentation content and navigational support solely according to students’ prior knowledge. On the other hand, previous research suggested that cognitive styles significantly affect student learning because they refer to how learners process and organize information. To this end, the study presented in this paper developed an adaptive hypermedia learning system tailored to students’ cognitive styles, with an emphasis on Pask’s Holist–Serialist dimension. How students react to this adaptive hypermedia learning system, including both learning performance and perceptions, was examined in this study. Forty-four undergraduate and postgraduate students participated in the study. The findings indicated that, in general, adapting to cognitive styles improves student learning. The results also showed that the adaptive hypermedia learning system have more effects on students’ perceptions than performance. The implications of these results for the design of adaptive hypermedia learning systems are discussed. 2010 Elsevier Ltd. All rights reserved.',\n",
       " '242b5b545bb17879a73161134bc84d5ba3e3cf35': 'VMware ESX Server is a thin software layer designed to multiplex hardware resources efficiently among virtual machines running unmodified commodity operating systems. This paper introduces several novel ESX Server mechanisms and policies for managing memory. A ballooning technique reclaims the pages considered least valuable by the operating system running in a virtual machine. An idle memory tax achieves efficient memory utilization while maintaining performance isolation guarantees. Content-based page sharing and hot I/O page remapping exploit transparent page remapping to eliminate redundancy and reduce copying overheads. These techniques are combined to efficiently support virtual machine workloads that overcommit memory.',\n",
       " '596785ca2d338ebcdeac1fc29bf5357045574b2b': 'The properties of clouds -- elasticity, pay-per-use, and standardization of the runtime infrastructure -- enable cloud providers and users alike to benefit from economies of scale, faster provisioning times, and reduced runtime costs. However, to achieve these benefits, application architects and developers have to respect the characteristics of the cloud environment.\\n To reduce the complexity of cloud application architectures, we propose a pattern-based approach for cloud application design and development. We defined a pattern format to describe the principles of cloud computing, available cloud offerings, and cloud application architectures. Based on this format we developed an architectural pattern language of cloud-based applications: through interrelation of patterns for cloud offering descriptions and cloud application architectures, developers are guided during the identification of cloud environments and architecture patterns applicable to their problems. We cover the proceeding how we identified patterns in various information sources and existing productively used applications, give an overview of previously discovered patterns, and introduce one new pattern. Further, we propose a framework for the organizations of patterns and the guidance of developers during pattern instantiation.',\n",
       " 'c70ad19c90491e2de8de686b6a49f9bbe44692c0': 'Gaze reflects how humans process visual scenes and is therefore increasingly used in computer vision systems. Previous works demonstrated the potential of gaze for object-centric tasks, such as object localization and recognition, but it remains unclear if gaze can also be beneficial for scene-centric tasks, such as image captioning. We present a new perspective on gaze-assisted image captioning by studying the interplay between human gaze and the attention mechanism of deep neural networks. Using a public large-scale gaze dataset, we first assess the relationship between state-of-the-art object and scene recognition models, bottom-up visual saliency, and human gaze. We then propose a novel split attention model for image captioning. Our model integrates human gaze information into an attention-based long short-term memory architecture, and allows the algorithm to allocate attention selectively to both fixated and non-fixated image regions. Through evaluation on the COCO/SALICON datasets we show that our method improves image captioning performance and that gaze can complement machine attention for semantic scene understanding tasks.',\n",
       " '0d836a0461e9c21fa7a25622115de55b81ceb446': 'This paper proposes a simple estimation of the quality of student oral presentations. It is based on the study and analysis of features extracted from the audio and digital slides of 448 presentations. The main goal of this work is to automatically predict the values assigned by professors to different criteria in a presentation evaluation rubric. Machine Learning methods were used to create several models that classify students in two clusters: high and low performers. The models created from slide features were accurate up to 65%. The most relevant features for the slide-base models were: number of words, images, and tables, and the maximum font size. The audio-based models reached up to 69% of accuracy, with pitch and filled pauses related features being the most significant. The relatively high degrees of accuracy obtained with these very simple features encourage the development of automatic estimation tools for improving presentation skills.',\n",
       " '1627ce7f1429366829df3d49e28b8ecd7f7597b5': 'Community repositories, such as Docker Hub, PyPI, and RubyGems, are bustling marketplaces that distribute software. Even though these repositories use common software signing techniques (e.g., GPG and TLS), attackers can still publish malicious packages after a server compromise. This is mainly because a community repository must have immediate access to signing keys in order to certify the large number of new projects that are registered each day. This work demonstrates that community repositories can offer compromise-resilience and real-time project registration by employing mechanisms that disambiguate trust delegations. This is done through two delegation mechanisms that provide flexibility in the amount of trust assigned to different keys. Using this idea we implement Diplomat, a software update framework that supports security models with different security / usability tradeoffs. By leveraging Diplomat, a community repository can achieve near-perfect compromise-resilience while allowing real-time project registration. For example, when Diplomat is deployed and configured to maximize security on Python’s community repository, less than 1% of users will be at risk even if an attacker controls the repository and is undetected for a month. Diplomat is being integrated by Ruby, CoreOS, Haskell, OCaml, and Python, and has already been deployed by Flynn, LEAP, and Docker.',\n",
       " '69613390ca76bf103791ef251e1568deb5fe91dd': 'Satellite image classification process involves grouping the image pixel values into meaningful categories. Several satellite image classification methods and techniques are available. Satellite image classification methods can be broadly classified into three categories 1) automatic 2) manual and 3) hybrid. All three methods have their own advantages and disadvantages. Majority of the satellite image classification methods fall under first category. Satellite image classification needs selection of appropriate classification method based on the requirements. The current research work is a study on satellite image classification methods and techniques. The research work also compares various researcher&apos;s comparative results on satellite image classification methods.',\n",
       " '456f85fb61fa5f137431e6d12c5fc73cc2ebaced': 'This paper presents a biometric user authentication based on a person’s gait. Unlike most previous gait recognition approaches, which are based on machine vision techniques, in our approach gait patterns are extracted from a physical device attached to the lower leg. From the output of the device accelerations in three directions: vertical, forward-backward, and sideways motion of the lower leg are obtained. A combination of these accelerations is used for authentication. Applying two different methods, histogram similarity and cycle length, equal error rates (EER) of 5% and 9% were achieved, respectively.',\n",
       " '4e116ae01d873ad67fb2ab6da5cb4feeb24bbcb5': \"STUDY DESIGN\\nAuthor experience and literature review.\\n\\n\\nOBJECTIVES\\nTo investigate and discuss decision-making on when to perform a Smith-Petersen osteotomy as opposed to a pedicle subtraction procedure and/or a vertebral column resection.\\n\\n\\nSUMMARY OF BACKGROUND DATA\\nArticles have been published regarding Smith-Petersen osteotomies, pedicle subtraction procedures, and vertebral column resections. Expectations and complications have been reviewed. However, decision-making regarding which of the 3 procedures is most useful for a particular spinal deformity case is not clearly investigated.\\n\\n\\nMETHODS\\nDiscussed in this manuscript is the author's experience and the literature regarding the operative options for a fixed coronal or sagittal deformity.\\n\\n\\nRESULTS\\nThere are roles for Smith-Petersen osteotomy, pedicle subtraction, and vertebral column resection. Each has specific applications and potential complications.\\n\\n\\nCONCLUSION\\nAs the magnitude of resection increases, the ability to correct deformity improves, but also the risk of complication increases. Therein, an understanding of potential applications and complications is helpful.\",\n",
       " '80ca5505a9ba00e91283519a75e01840a15a74bf': 'Key-value (KV) stores have become a backbone of large-scale applications in today’s data centers. Write-optimized data structures like the Log-Structured Merge-tree (LSM-tree) and their variants are widely used in KV storage systems like BigTable and RocksDB. Conventional LSM-tree organizes KV items into multiple, successively larger components, and uses compaction to push KV items from one smaller component to another adjacent larger component until the KV items reach the largest component. Unfortunately, current compaction scheme incurs significant write amplification due to repeated KV item reads and writes, and then results in poor throughput. We propose a new compaction scheme, delayed compaction (dCompaction), that decreases write amplification. dCompaction postpones some compactions and gather them into the following compaction. In this way, it avoids KV item reads and writes during compaction, and consequently improves the throughput of LSM-tree based KV stores. We implement dCompaction on RocksDB, and conduct extensive experiments. Validation using YCSB framework shows that compared with RocksDB dCompaction has about 30% write performance improvements and also comparable read performance.',\n",
       " '131e4a4a40c29737f39e8cb0f4e59864ca1a1b34': \"Current outdoor localization techniques fail to provide the required accuracy for estimating the car's lane. In this paper, we present LaneQuest: a system that leverages the ubiquitous and low-energy inertial sensors available in commodity smart-phones to provide an accurate estimate of the car's current lane. LaneQuest leverages hints from the phone sensors about the surrounding environment to detect the car's lane. For example, a car making a right turn most probably will be in the right-most lane, a car passing by a pothole will be in a specific lane, and the car's angular velocity when driving through a curve reflects its lane. Our investigation shows that there are amble opportunities in the environment, i.e. lane “anchors”, that provide cues about the car's lane. To handle the ambiguous location, sensors noise, and fuzzy lane anchors; LaneQuest employs a novel probabilistic lane estimation algorithm. Furthermore, it uses an unsupervised crowd-sourcing approach to learn the position and lane-span distribution of the different lane-level anchors. Our evaluation results from implementation on different android devices and 260Km driving traces by 13 drivers in different cities shows that LaneQuest can detect the different lane-level anchors with an average precision and recall of more than 90%. This leads to an accurate detection of the exact car's lane position 80% of the time, increasing to 89% of the time to within one lane. This comes with a low-energy footprint, allowing LaneQuest to be implemented on the energy-constrained mobile devices.\",\n",
       " '3cbf0dcbc36a8f70e9b2b2f46b16e5057cbd9a7d': 'Fine-grained sense distinctions are one of the major obstacles to successful Word Sense Disambiguation. In this paper, we present a method for reducing the granularity of the WordNet sense inventory based on the mapping to a manually crafted dictionary encoding sense hierarchies, namely the Oxford Dictionary of English. We assess the quality of the mapping and the induced clustering, and evaluate the performance of coarse WSD systems in the Senseval-3 English all-words task.',\n",
       " '528fa9bb03644ba752fb9491be49b9dd1bce1d52': 'Semantic Textual Similarity (STS) measures the degree of semantic equivalence between two texts. This paper presents the results of the STS pilot task in Semeval. The training data contained 2000 sentence pairs from previously existing paraphrase datasets and machine translation evaluation resources. The test data also comprised 2000 sentences pairs for those datasets, plus two surprise datasets with 400 pairs from a different machine translation evaluation corpus and 750 pairs from a lexical resource mapping exercise. The similarity of pairs of sentences was rated on a 0-5 scale (low to high similarity) by human judges using Amazon Mechanical Turk, with high Pearson correlation scores, around 90%. 35 teams participated in the task, submitting 88 runs. The best results scored a Pearson correlation>80%, well above a simple lexical baseline that only scored a 31% correlation. This pilot task opens an exciting way ahead, although there are still open issues, specially the evaluation metric.',\n",
       " 'd87ceda3042f781c341ac17109d1e94a717f5f60': 'WordNet is perhaps the most important and widely used lexical resource for natural language processing systems up to now. WordNet: An Electronic Lexical Database, edited by Christiane Fellbaum, discusses the design of WordNet from both theoretical and historical perspectives, provides an up-to-date description of the lexical database, and presents a set of applications of WordNet. The book contains a foreword by George Miller, an introduction by Christiane Fellbaum, seven chapters from the Cognitive Sciences Laboratory of Princeton University, where WordNet was produced, and nine chapters contributed by scientists from elsewhere. Miller\\'s foreword offers a fascinating account of the history of WordNet. He discusses the presuppositions of such a lexical database, how the top-level noun categories were determined, and the sources of the words in WordNet. He also writes about the evolution of WordNet from its original incarnation as a dictionary browser to a broad-coverage lexicon, and the involvement of different people during its various stages of development over a decade. It makes very interesting reading for casual and serious users of WordNet and anyone who is grateful for the existence of WordNet. The book is organized in three parts. Part I is about WordNet itself and consists of four chapters: \"Nouns in WordNet\" by George Miller, \"Modifiers in WordNet\" by Katherine Miller, \"A semantic network of English verbs\" by Christiane Fellbaum, and \"Design and implementation of the WordNet lexical database and search software\" by Randee Tengi. These chapters are essentially updated versions of four papers from Miller (1990). Compared with the earlier papers, the chapters in this book focus more on the underlying assumptions and rationales behind the design decisions. The description of the information contained in WordNet, however, is not as detailed as in Miller (1990). The main new additions in these chapters include an explanation of sense grouping in George Miller\\'s chapter, a section about adverbs in Katherine Miller\\'s chapter, observations about autohyponymy (one sense of a word being a hyponym of another sense of the same word) and autoantonymy (one sense of a word being an antonym of another sense of the same word) in Fellbaum\\'s chapter, and Tengi\\'s description of the Grinder, a program that converts the files the lexicographers work with to searchable lexical databases. The three papers in Part II are characterized as \"extensions, enhancements and',\n",
       " '1528def1ddbd2deb261ebb873479f27f48251031': 'This paper presents the results of a set of methods to cluster WordNet word senses. The methods rely on different information sources: confusion matrixes from Senseval-2 Word Sense Disambiguation systems, translation similarities, hand-tagged examples of the target word senses and examples obtained automatically from the web for the target word senses. The clustering results have been evaluated using the coarsegrained word senses provided for the lexical sample in Senseval-2. We have used Cluto, a general clustering environment, in order to test different clustering algorithms. The best results are obtained for the automatically obtained examples, yielding purity values up to 84% on average over 20 nouns.',\n",
       " '2445089d4277ccbec3727fecfe73eaa4cc57e414': 'This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back. We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process. We measured timing and intraand inter-annotator agreement for three types of subjective evaluation. We measured the correlation of automatic evaluation metrics with human judgments. This meta-evaluation reveals surprising facts about the most commonly used methodologies.',\n",
       " '3764e0bfcc4a196eb020d483d8c2f1822206a444': 'Mobile security is an important issue on Android platform. Most malware detection methods based on machine learning models heavily rely on expert knowledge for manual feature engineering, which are still difficult to fully describe malwares. In this paper, we present LSTM-based hierarchical denoise network (HDN), a novel static Android malware detection method which uses LSTM to directly learn from the raw opcode sequences extracted from decompiled Android files. However, most opcode sequences are too long for LSTM to train due to the gradient vanishing problem. Hence, HDN uses a hierarchical structure, whose first-level LSTM parallelly computes on opcode subsequences (we called them method blocks) to learn the dense representations; then the secondlevel LSTM can learn and detect malware through method block sequences. Considering that malicious behavior only appears in partial sequence segments, HDN uses method block denoise module (MBDM) for data denoising by adaptive gradient scaling strategy based on loss cache. We evaluate and compare HDN with the latest mainstream researches on three datasets. The results show that HDN outperforms these Android malware detection methods,and it is able to capture longer sequence features and has better detection efficiency thanN-gram-based malware detection which is similar to our method.',\n",
       " '41bb7d4546fbc95d55342b621f95edad3b06717e': \"Wireless networking protocols are increasingly being designed to exploit a user's measured channel condition; we call such protocols channel-aware. Each user reports the measured channel condition to a manager of wireless resources and a channel-aware protocol uses these reports to determine how resources are allocated to users. In a channel-aware protocol, each user's reported channel condition affects the performance of every other user. The deployment of channel-aware protocols increases the risks posed by false channel-condition feedback. In this paper, we study what happens in the presence of an attacker that falsely reports its channel condition. We perform case studies on channel-aware network protocols to understand how an attack can use false feedback and how much the attack can affect network performance. The results of the case studies show that we need a secure channel condition estimation algorithm to fundamentally defend against the channel-condition misreporting attack. We design such an algorithm and evaluate our algorithm through analysis and simulation. Our evaluation quantifies the effect of our algorithm on system performance as well as the security and the performance of our algorithm.\",\n",
       " 'fe025433b702bf6e946610e0dba77f7dd16ae821': 'For centuries, the conventional approach to lens design has been to grind the surfaces of a uniform material in such a manner as to sculpt the paths that rays of light follow as they transit through the interfaces. Refractive lenses formed by this procedure of bending the surfaces can be of extremely high quality, but are nevertheless limited by geometrical and wave aberrations that are inherent to the manner in which light refracts at the interface between two materials. Conceptually, a more natural--but usually less convenient--approach to lens design would be to vary the refractive index throughout an entire volume of space. In this manner, far greater control can be achieved over the ray trajectories. Here, we demonstrate how powerful emerging techniques in the field of transformation optics can be used to harness the flexibility of gradient index materials for imaging applications. In particular we design and experimentally demonstrate a lens that is broadband (more than a full decade bandwidth), has a field-of-view approaching 180 degrees and zero f-number. Measurements on a metamaterial implementation of the lens illustrate the practicality of transformation optics to achieve a new class of optical devices.',\n",
       " '9b9da80c186d8f6e7fa35747a6543d78e36f17e8': 'We introduce hand movement, orientation, and grasp (HMOG), a set of behavioral features to continuously authenticate smartphone users. HMOG features unobtrusively capture subtle micro-movement and orientation dynamics resulting from how a user grasps, holds, and taps on the smartphone. We evaluated authentication and biometric key generation (BKG) performance of HMOG features on data collected from 100 subjects typing on a virtual keyboard. Data were collected under two conditions: 1) sitting and 2) walking. We achieved authentication equal error rates (EERs) as low as 7.16% (walking) and 10.05% (sitting) when we combined HMOG, tap, and keystroke features. We performed experiments to investigate why HMOG features perform well during walking. Our results suggest that this is due to the ability of HMOG features to capture distinctive body movements caused by walking, in addition to the hand-movement dynamics from taps. With BKG, we achieved the EERs of 15.1% using HMOG combined with taps. In comparison, BKG using tap, key hold, and swipe features had EERs between 25.7% and 34.2%. We also analyzed the energy consumption of HMOG feature extraction and computation. Our analysis shows that HMOG features extracted at a 16-Hz sensor sampling rate incurred a minor overhead of 7.9% without sacrificing authentication accuracy. Two points distinguish our work from current literature: 1) we present the results of a comprehensive evaluation of three types of features (HMOG, keystroke, and tap) and their combinations under the same experimental conditions and 2) we analyze the features from three perspectives (authentication, BKG, and energy consumption on smartphones).',\n",
       " '4e0e664450094cc786898a5e1ef3727135ecfcd8': '1Department of Computer Science, School of Engineering and Applied Science, Institute for Computer Graphics, The George Washington University, 800 22nd Street NW Suite 3400, Washington, DC 20052, USA 2Department of Epidemiology and Biostatistics, Milken Institute School of Public Health, The George Washington University, 800 22nd Street NW Suite 7680, Washington, DC 20052, USA 3Department of Computer Science, School of Engineering and Applied Science, and Department of Pediatrics, School of Medicine and Health Sciences, Institute for Computer Graphics, The George Washington University, 800 22nd Street NW Suite 5830, Washington, DC 20052, USA',\n",
       " 'de0597313056b05fd7dd6b2d5e031cfb96564920': 'Transductive Adversarial Networks (TAN) is a novel domain-adaptation machine learning framework that is designed for learning a conditional probability distribution on unlabelled input data in a target domain, while also only having access to: (1) easily obtained labelled data from a related source domain, which may have a different conditional probability distribution than the target domain, and (2) a marginalised prior distribution on the labels for the target domain. TAN leverages a fully adversarial training procedure and a unique generator/encoder architecture which approximates the transductive combination of the available sourceand target-domain data. A benefit of TAN is that it allows the distance between the sourceand target-domain label-vector marginal probability distributions to be greater than 0 (i.e. different tasks across the source and target domains) whereas other domain-adaptation algorithms require this distance to equal 0 (i.e. a single task across the source and target domains). TAN can, however, still handle the latter case and is a more generalised approach to this case. Another benefit of TAN is that due to being a fully adversarial algorithm, it has the potential to accurately approximate highly complex distributions. Theoretical analysis demonstrates the viability of the TAN framework.',\n",
       " '620c1821f67fd39051fe0863567ac702ce27a72a': 'Abilities of sea animals and efficiency of fish swimming are a few of the impressive solutions of nature. In this paper, design, modeling, simulation and development studies of a robotic dolphin prototype, entirely inspired by the bottlenose dolphin (Tursiops truncatus), are presented. The first section focuses on the design principles and core design features of the prototype. In the second section, modeling and simulation studies which consist of hydrodynamics, kinematics and dynamical analysis of the robotic dolphin are presented. Dynamical simulations of the underwater behavior of the prototype are included in this section. The third section focuses on the general prototype development from mechanical construction to control system structure. Finally in the last section, experimental results obtained through the development of the prototype are discussed.',\n",
       " '7da851a9d67b6c8ee7e985928f91ee577c529f2e': 'Twitter as a new form of social media can potentially contain much useful information, but content analysis on Twitter has not been well studied. In particular, it is not clear whether as an information source Twitter can be simply regarded as a faster news feed that covers mostly the same information as traditional news media. In This paper we empirically compare the content of Twitter with a traditional news medium, New York Times, using unsupervised topic modeling. We use a Twitter-LDA model to discover topics from a representative sample of the entire Twitter. We then use text mining techniques to compare these Twitter topics with topics from New York Times, taking into consideration of topic categories and types. We find that although Twitter and New York Times cover similar categories and types of topics, the distributions of topic categories and types are quite different. Furthermore, there are Twitter-specific topics and NYT-specific topics, and they tend to belong to certain topic categories and types. We also study the relation between the proportions of opinionated tweets and retweets and topic categories and types, and find some interesting dependence. To the best of our knowledge, ours is the first comprehensive empirical comparison between Twitter and traditional news media.',\n",
       " '0b3983a6ad65f7f480d63d5a8d3e9f5c9c57e06a': 'This paper deals with some of the issues that arise in the context of binary rewriting and instrumentation of an operatin g system kernel. OS kernels are very different from ordinary application code in many ways, e.g., they contain a significant amount of hand-written assembly code. Binary rewriting is an attractive approach for processing OS kernel code for several reasons, e.g., it provides a uniform way to handl e heterogeneity in code due to a combination of source code, assembly code and legacy code such as in device drivers. However, because of the many differences between ordinary application code and OS kernel code, binary rewriting techniques that work for application code do not always carry over directly to kernel code. This paper describes some of the issues that arise in this context, and the approaches we have taken to address them. A key goal when developing our system was to deal in a systematic manner with the various peculiarities seen in low-level systems code, and reason about the safety and correctness of code transformation s, without requiring significant deviations from the regular d evelopmental path. For example, a precondition we assumed was that no compiler or linker modifications should be required to use it and the tool should be able to process kernel binaries in the same way as it does ordinary applications.',\n",
       " 'c4cfdcf19705f9095fb60fb2e569a9253a475f11': 'Recognizing how objects interact with each other is a crucial task in visual recognition. If we define the context of the interaction to be the objects involved, then most current methods can be categorized as either: (i) training a single classifier on the combination of the interaction and its context; or (ii) aiming to recognize the interaction independently of its explicit context. Both methods suffer limitations: the former scales poorly with the number of combinations and fails to generalize to unseen combinations, while the latter often leads to poor interaction recognition performance due to the difficulty of designing a contextindependent interaction classifier.,,To mitigate those drawbacks, this paper proposes an alternative, context-aware interaction recognition framework. The key to our method is to explicitly construct an interaction classifier which combines the context, and the interaction. The context is encoded via word2vec into a semantic space, and is used to derive a classification result for the interaction. The proposed method still builds one classifier for one interaction (as per type (ii) above), but the classifier built is adaptive to context via weights which are context dependent. The benefit of using the semantic space is that it naturally leads to zero-shot generalizations in which semantically similar contexts (subject-object pairs) can be recognized as suitable contexts for an interaction, even if they were not observed in the training set. Our method also scales with the number of interaction-context pairs since our model parameters do not increase with the number of interactions. Thus our method avoids the limitation of both approaches. We demonstrate experimentally that the proposed framework leads to improved performance for all investigated interaction representations and datasets.',\n",
       " '39cc0e6c1b85d052c998a1c5949fe51baa96f0c5': 'GPUs are an attractive target for data parallel stencil computations prevalent in scientific computing and image processing applications. Many tiling schemes, such as overlapped tiling and split tiling, have been proposed in past to improve the performance of stencil computations. While effective for 2D stencils, these techniques do not achieve the desired improvements for 3D stencils due to the hardware constraints of GPU.\\n A major challenge in optimizing stencil computations is to effectively utilize all resources available on the GPU. In this paper we develop a tiling strategy that makes better use of resources like shared memory and register file available on the hardware. We present a systematic methodology to reason about which strategy should be employed for a given stencil and also discuss implementation choices that have a significant effect on the achieved performance. Applying these techniques to various 2D and 3D stencils gives a performance improvement of 200-400% over existing tools that target such computations.',\n",
       " 'a98242e420179d2a080069f5a02c6603fb5cfe3d': 'The series battery string or supercapacitor string automatic equalization system based on quasi-resonant switched-capacitor converter is presented in this paper. It realizes the zero-voltage gap between cells and allows maximum energy recovery in a series battery system or supercapacitor system. It not only inherits the advantage of conventional switched-capacitor battery cell balancing system, but also overcomes the drawback of conduction loss, switching loss, and finite voltage difference among battery cells. All switches are MOSFET and controlled by just a pair of complementary signals in synchronous trigger pattern and the resonant tanks operate alternatively between the two states of charging and discharging. Zero-current switching and zero-voltage gap are achieved in this paper. Different resonant tank designs can meet the needs of different balancing time to meet the needs of different energy storage devices. Experimental results indicate that the efficiency of the system is high exceeding 98%. The system is very suitable for balancing used in battery management system.',\n",
       " 'dcaeb29ad3307e2bdab2218416c81cb0c4e548b2': 'We present our work on end-to-end training of acoustic models using the lattice-free maximum mutual information (LF-MMI) objective function in the context of hidden Markov models. By end-to-end training, we mean flat-start training of a single DNN in one stage without using any previously trained models, forced alignments, or building state-tying decision trees. We use full biphones to enable context-dependent modeling without trees, and show that our end-to-end LF-MMI approach can achieve comparable results to regular LF-MMI on well-known large vocabulary tasks. We also compare with other end-to-end methods such as CTC in character-based and lexicon-free settings and show 5 to 25 percent relative reduction in word error rates on different large vocabulary tasks while using significantly smaller models.',\n",
       " '6b4593128ddfcbe006b51de0549596f24e724ff0': \"The proposed work presents a technique for an automated count of cigarette in cigarette packets. Proposed work is based on application of image processing techniques in LabVIEW platform. An objective of the proposed work is to count the number of cigarettes in a packet. National Instrument's Smart camera is used to capture images of cigarette packets moving in packaging line and process the data to fulfill the above objective. The technique was subjected to offline testing on more than 50 number of cigarette packets and the results obtained are found to be satisfactory in all cases.\",\n",
       " '5d7e3fb23f2a14ffdab31f18051c9b8ff573db4e': 'This paper presents the dynamic behaviour of a nonlinear single link inverted pendulum-on-cart system based on Lagrange Equation. The nonlinear model linearization was presented based on Taylor series approximation. LQR, double-PID and simple pole placement control techniques were proposed for upright stabilization and tracking controls of the system. Simulations results for the various control techniques subjected to a unity magnitude pulse input torque with and without disturbance were compared. The performances of the proposed controllers were investigated based on response time specifications and level of disturbance rejection. Thus, the performance of LQR is more reliable and satisfactory. Finally, future work suggestions were made.',\n",
       " '15fc8ce6630616cce1681f049391bdb4e186192b': \"This paper presents a robust method for defect detection in textures, entropy-based automatic selection of the wavelet decomposition level (EADL), based on a wavelet reconstruction scheme, for detecting defects in a wide variety of structural and statistical textures. Two main features are presented. One of the new features is an original use of the normalized absolute function value (NABS) calculated from the wavelet coefficients derived at various different decomposition levels in order to identify textures where the defect can be isolated by eliminating the texture pattern in the first decomposition level. The second is the use of Shannon's entropy, calculated over detail subimages, for automatic selection of the band for image reconstruction, which, unlike other techniques, such as those based on the co-occurrence matrix or on energy calculation, provides a lower decomposition level, thus avoiding excessive degradation of the image, allowing a more accurate defect segmentation. A metric analysis of the results of the proposed method with nine different thresholding algorithms determined that selecting the appropriate thresholding method is important to achieve optimum performance in defect detection. As a consequence, several different thresholding algorithms depending on the type of texture are proposed.\",\n",
       " '29fbadeb12389a2fee4f0739cfc62c0ea9399de1': 'Markets can interact with power systems in ways that can render an otherwise stable market and an otherwise stable power system into an unstable overall system. This unstable system will be characterized not only by fluctuating prices that do not settle to constant values, but, more worrisome, it creates the possibility of inducing slow electromechanical oscillations if left unchecked. This will tend to happen as a result of \"price chasing\" on the part of suppliers that can react (and over-react) to changing system prices. This paper examines the role that futures markets may have on the clearing prices and on altering the volatility and potential instability of real time prices and generator output.',\n",
       " 'ecfc4792299f58390d85753b60ee227b6282ccbc': 'Recent advances in mobile technologies and infrastructures have created the demand for ubiquitous access to enterprise services from mobile handheld devices. Further, with the invention of new interaction devices, the context in which the services are being used becomes an integral part of the activity carried out with the system. Traditional human–computer interface (HCI) theories are now inadequate for developing these context-aware applications, as we believe that the notion of context should be extended to different categories: computing contexts, user contexts, and physical contexts for ubiquitous computing. This demands a new paradigm for system requirements elicitation and design in order to make good use of such extended context information captured from mobile user behavior. Instead of redesigning or adapting existing enterprise services in an ad hoc manner, we introduce a methodology for the elicitation of context-aware adaptation requirements and the matching of context-awareness features to the target context by capability matching. For the implementation of such adaptations, we propose the use of three tiers of views: user interface views, data views, and process views. This approach centers on a novel notion of process views to ubiquitous service adaptation, where mobile users may execute a more concise version or modified procedure of the original process according to their behavior under different contexts. The process view also serves as the key mechanism for integrating user interface views and data views. Based on this model, we analyze the design and implementation issues of some common ubiquitous access situations and show how to adapt them systematically into a context-aware application by considering the requirements of a ubiquitous enterprise information system.',\n",
       " 'edd6b6cd62d4c3b5d288721510e579be62c941d6': 'Generative Adversarial Net is a frontier method of generative models for images, audios and videos. In this paper, we focus on conditional image generation and introduce conditional Feature-Matching Generative Adversarial Net to generate images from category labels. By visualizing state-of-art discriminative conditional generative models, we find these networks do not gain clear semantic concepts. Thus we design the loss function in the light of metric learning to measure semantic distance. The proposed model is evaluated on several well-known datasets. It is shown to be of higher perceptual quality and better diversity then existing generative models.',\n",
       " '7a050d2f0c83a65996e1261b52e6523f24d0bac2': \"A direction-of-arrival (DoA) method that combines the reactance-domain (RD) technique and the ESPRIT algorithm is proposed for use with the 7-element electronically steerable parasitic array radiator (ESPAR) for the estimation of noncoherent sources. Simulations show that the method could resolve up to three incoming signals with an estimation performance that depends on the signal's angle of arrival. Moreover, the method is compared with the Cramer-Rao lower bound (CRB) and the MUSIC asymptotic error variance, both modified for the RD technique. Numerical comparison between this lower bound and the MUSIC algorithm confirmed that the proposed method can achieve the CRB and provide high-precision DoA estimation with a level of performance that is sufficient for many DoA finding applications. The proposed method could be demonstrated by means of experiments on DOA estimation conducted in an anechoic chamber.\",\n",
       " 'f512a4ae0f6b2d8d03c54a6405d2697a74f7256a': 'Network scaling algorithms such as the Pathfinder algorithm are used to prunemany different kinds of networks, including citation networks, randomnetworks, and social networks. However, this algorithm suffers from run time problems for large networks and online processing due to its O(n4) time complexity. In this article, we introduce a new alternative, the MST-Pathfinder algorithm, which will allow us to prune the original network to get its PFNET(∞, n −1) in justO(n2 · logn) time.Theunderlying idea comes from the fact that the union (superposition) of all the Minimum Spanning Trees extracted from a given network is equivalent to the PFNET resulting from the Pathfinder algorithmparameterized by a specific set of values (r = ∞ and q = n −1), those usually considered in many different applications. Although this property is well-known in the literature, it seems that no algorithm based on it has been proposed, up to now, to decrease the high computational cost of the original Pathfinder algorithm.We also present a mathematical proof of the correctness of this new alternative and test its good efficiency in two different case studies: one dedicated to the post-processing of large random graphs, and the other one to a real world case in which medium networks obtained by a cocitation analysis of the scientific domains in different countries are pruned.',\n",
       " 'b49af9c4ab31528d37122455e4caf5fdeefec81a': 'Published research on smart homes and their users is growing exponentially, yet a clear understanding of who these users are and how they might use smart home technologies is missing from a field being overwhelmingly pushed by technology developers. Through a systematic analysis of peer-reviewed literature on smart homes and their users, this paper takes stock of the dominant research themes and the linkages and disconnects between them. Key findings within each of nine themes are analysed, grouped into three: (1) views of the smart home—functional, instrumental, socio-technical; (2) users and the use of the smart home—prospective users, interactions and decisions, using technologies in the home; and (3) challenges for realising the smart home—hardware and software, design, domestication. These themes are integrated into an organising framework for future research that identifies the presence or absence of cross-cutting relationships between different understandings of smart homes and their users. The usefulness of the organising framework is illustrated in relation to two major concerns—privacy and control—that have been narrowly interpreted to date, precluding deeper insights and potential solutions. Future research on smart homes and their users can benefit by exploring and developing cross-cutting relationships between the research themes identified.',\n",
       " 'c4a3da33ac6bcc9acd962f3bbb92d2387a62aed2': 'This research proposed a new mobile application based on Android operating system for identifying Indonesian medicinal plant images based on texture and color features of digital leaf images. In the experiments we used 51 species of Indonesian medicinal plants and each species consists of 48 images, so the total images used in this research are 2,448 images. This research investigates effectiveness of the fusion between the Fuzzy Local Binary Pattern (FLBP) and the Fuzzy Color Histogram (FCH) in order to identify medicinal plants. The FLBP method is used for extracting leaf image texture. The FCH method is used for extracting leaf image color. The fusion of FLBP and FCH is done by using Product Decision Rules (PDR) method. This research used Probabilistic Neural Network (PNN) classifier for classifying medicinal plant species. The experimental results show that the fusion between FLBP and FCH can improve the average accuracy of medicinal plants identification. The accuracy of identification using fusion of FLBP and FCH is 74.51%. This application is very important to help people identifying and finding information about Indonesian medicinal plant.',\n",
       " 'de704a0347322014abc4b3ecc27e86bdc5fac2fd': 'SCIENCE JOURNAL 2015 | JUNE | SCIENCE JOURNAL | 613',\n",
       " 'b648d73edd1a533decd22eec2e7722b96746ceae': 'Selective weed treatment is a critical step in autonomous crop management as related to crop health and yield. However, a key challenge is reliable and accurate weed detection to minimize damage to surrounding plants. In this letter, we present an approach for dense semantic weed classification with multispectral images collected by a micro aerial vehicle (MAV). We use the recently developed encoder–decoder cascaded convolutional neural network, SegNet, that infers dense semantic classes while allowing any number of input image channels and class balancing with our sugar beet and weed datasets. To obtain training datasets, we established an experimental field with varying herbicide levels resulting in field plots containing only either crop or weed, enabling us to use the normalized difference vegetation index as a distinguishable feature for automatic ground truth generation. We train six models with different numbers of input channels and condition (fine tune) it to achieve  $\\\\sim$0.8 F1-score and 0.78 area under the curve classification metrics. For the model deployment, an embedded Graphics Processing Unit (GPU) system (Jetson TX2) is tested for MAV integration. Dataset used in this letter is released to support the community and future work.',\n",
       " '7d712ea3803485467a46b46b71242477560c18f0': 'A novel design concept to enhance the bandwidth of a differential-fed patch antenna using the dual-resonant radiation of a stepped-impedance resonator (SIR) is proposed. The SIR is composed of two distinctive portions: the radiating patch and a pair of open stubs. Initially, based on the transmission line model, the first and second odd-order radiative resonant modes, i.e., TM10 and TM30, of this SIR-typed patch antenna are extensively investigated. It is demonstrated that the frequency ratio between the dual-resonant modes can be fully controlled by the electrical length and the impedance ratios between the open stub and radiating patch. After that, the SIR-typed patch antenna is reshaped with stepped ground plane in order to increase the impedance ratio as highly required for wideband radiation. With this arrangement, these two radiative modes are merged with each other, resulting in a wide impedance bandwidth with a stable radiation pattern under dual-resonant radiation. Finally, the proposed antenna is designed, fabricated, and measured. It is verified in experiment that the impedance bandwidth (|Sdd11| <; -10 dB) of the proposed antenna has gained tremendous increment up to 10% (0.85-0.94 GHz) with two attenuation poles. Most importantly, the antenna has achieved a stable gain varying from 7.4 to 8.5 dB within the whole operating band, while keeping low-cross polarization.',\n",
       " '9f7b1b9d4f7dc2dcf850dcb311ecd309be380226': 'A new microstrip monopolar patch antenna is proposed and analyzed. The antenna has a wide bandwidth and a monopole like radiation pattern. Such antenna is constructed on a circular patch antenna that is shorted concentrically with a set of conductive vias. The antenna is analyzed using a cavity model. The cavity model analysis not only distinguishes each resonating mode and gives a physical insight into each mode of the antenna, but also provides a guideline to design a broadband monopolar patch antenna that utilizes two modes (TM01 and TM02 modes). Both modes provide a monopole like radiation pattern. The proposed antenna has a simple structure with a low profile of 0.024 wavelengths, and yields a wide impedance bandwidth of 18% and a maximum gain of 6 dBi.',\n",
       " '1965a7d9a3eb0727c054fb235b1758c8ffbb8e22': 'Circularly polarized single-layer U-slot microstrip patch antenna has been proposed. The suggested asymmetrical U-slot can generate the two orthogonal modes for circular polarization without chamfering any corner of the probe-fed square patch microstrip antenna. A parametric study has been carried out to investigate the effects caused by different arm lengths of the U-slot. The thickness of the foam substrate is about 8.5% of the wavelength at the operating frequency. The 3 dB axial ratio bandwidth of the antenna is 4%. Both experimental and theoretical results of the antenna have been presented and discussed. Circular polarization, printed antennas, U-slot.',\n",
       " '4f86fdb8312794929b9a11770fba271c5bf886fa': 'A center-fed circular microstrip patch antenna with a coupled annular ring is presented. This antenna has a low profile configuration with a monopole like radiation pattern. Compared to the center-fed circular patch antenna (CPA), the proposed antenna has a large bandwidth and similar radiation pattern. The proposed antenna is fabricated and tested. It resonates at 5.8 GHz, the corresponding impedance bandwidth and gain are 12.8% and 5.7 dBi, respectively. Very good agreement between the measurement and simulation for the return loss and radiation patterns is achieved.',\n",
       " '9462cd1ec2e404b22f76c88b6149d1e84683acb7': 'In this letter, a wideband compact circularly polarized (CP) patch antenna is proposed. This patch antenna consists of a printed meandering probe (M-probe) and truncated patches that excite orthogonal resonant modes to generate a wideband CP operation. The stacked patch is employed to further improve the axial-ratio (AR) bandwidth to fit the 5G Wi-Fi application. The proposed antenna achieves 42.3% impedance bandwidth and 16.8% AR bandwidth, respectively. The average gain within the AR bandwidth is 6.6 dBic with less than 0.5 dB variation. This work demonstrates a bandwidth broadening technique of an M-probe fed CP patch antenna. It is the first study to investigate and exhibit the M-probe could also provide the wideband characteristics in the dielectric loaded patch antenna. The potential applications of the antenna are 5G Wi-Fi and satellite communication systems.',\n",
       " 'c9ed18a4a52503ede1f50691ff77efdf26acedd5': 'This paper deals with a PFC (Power Factor Corrected) Bridgeless Zeta converter based VSI (Voltage Source Inverter) fed BLDC (Brushless DC) motor drive. The speed control is achieved by controlling the voltage at the DC bus of VSI using a single voltage sensor. This facilitates the operation of VSI in fundamental frequency switching mode (Electronic Commutation of BLDC motor) in place of high frequency PWM (Pulse Width Modulation) switching for speed control. This leads to low switching losses in VSI and thus improves the efficiency of the drive. Moreover, a bridgeless configuration is used to reduce the conduction losses of DBR (Diode Bridge Rectifier). The bridgeless Zeta converter working in DCM (Discontinuous Conduction Mode) is used which utilizes a voltage follower approach thus requiring a single voltage sensor for speed control and PFC operation. The proposed drive is designed to operate over a wide range of speed control and under wide variation in supply voltages with high power factor and low harmonic distortion in the supply current at AC mains. An improved power quality is achieved with performance indices satisfying the international PQ (Power Quality) standards such as IEC-61000-3-2.',\n",
       " 'd6002a6cc8b5fc2218754aed970aac91c8d8e7e9': 'In this paper we propose a new method for detecting multiple specific 3D objects in real time. We start from the template-based approach based on the LINE2D/LINEMOD representation introduced recently by Hinterstoisser et al., yet extend it in two ways. First, we propose to learn the templates in a discriminative fashion. We show that this can be done online during the collection of the example images, in just a few milliseconds, and has a big impact on the accuracy of the detector. Second, we propose a scheme based on cascades that speeds up detection. Since detection of an object is fast, new objects can be added with very low cost, making our approach scale well. In our experiments, we easily handle 10-30 3D objects at frame rates above 10fps using a single CPU core. We outperform the state-of-the-art both in terms of speed as well as in terms of accuracy, as validated on 3 different datasets. This holds both when using monocular color images (with LINE2D) and when using RGBD images (with LINEMOD). Moreover, we propose a challenging new dataset made of 12 objects, for future competing methods on monocular color images.',\n",
       " 'aaf0a8925f8564e904abc46b3ad1f9aa2b120cf6': 'Vector processing operations use essential spectral and spatial information to remove noise and localize microarray spots. The proposed fully automated vector technique can be easily implemented in either hardware or software; and incorporated in any existing microarray image analysis and gene expression tool.',\n",
       " '89c59ac45c267a1c12b9d0ed4af88f6d6c619683': 'Many applications that process social data, such as tweets, must extract entities from tweets (e.g., “Obama” and “Hawaii” in “Obama went to Hawaii”), link them to entities in a knowledge base (e.g., Wikipedia), classify tweets into a set of predefined topics, and assign descriptive tags to tweets. Few solutions exist today to solve these problems for social data, and they are limited in important ways. Further, even though several industrial systems such as OpenCalais have been deployed to solve these problems for text data, little if any has been published about them, and it is unclear if any of the systems has been tailored for social media. In this paper we describe in depth an end-to-end industrial system that solves these problems for social data. The system has been developed and used heavily in the past three years, first at Kosmix, a startup, and later at WalmartLabs. We show how our system uses a Wikipedia-based global “real-time” knowledge base that is well suited for social data, how we interleave the tasks in a synergistic fashion, how we generate and use contexts and social signals to improve task accuracy, and how we scale the system to the entire Twitter firehose. We describe experiments that show that our system outperforms current approaches. Finally we describe applications of the system at Kosmix and WalmartLabs, and lessons learned.',\n",
       " '0b632048208c9c6b48b636f9f7ef8a5466325488': 'Driving through dynamically changing traffic scenarios is a highly challenging task for autonomous vehicles, especially on urban roadways. Prediction of surrounding vehicles’ driving behaviors plays a crucial role in autonomous vehicles. Most traditional driving behavior prediction models work only for a specific traffic scenario and cannot be adapted to different scenarios. In addition, priori driving knowledge was never considered sufficiently. This study proposes a novel scenario-adaptive approach to solve these problems. A novel ontology model was developed to model traffic scenarios. Continuous features of driving behavior were learned by Hidden Markov Models (HMMs). Then, a knowledge base was constructed to specify the model adaptation strategies and store priori probabilities based on the scenario’s characteristics. Finally, the target vehicle’s future behavior was predicted considering both a posteriori probabilities and a priori probabilities. The proposed approach was sufficiently evaluated with a real autonomous vehicle. The application scope of traditional models can be extended to a variety of scenarios, while the prediction performance can be improved by the consideration of priori knowledge. For lane-changing behaviors, the prediction time horizon can be extended by up to 56% (0.76 s) on average. Meanwhile, long-term prediction precision can be enhanced by over 26%.',\n",
       " '41d103f751d47f0c140d21c5baa4981b3d4c9a76': 'The personal stories that people write in their Internet weblogs include a substantial amount of information about the causal relationships between everyday events. In this paper we describe our efforts to use millions of these stories for automated commonsense causal reasoning. Casting the commonsense causal reasoning problem as a Choice of Plausible Alternatives, we describe four experiments that compare various statistical and information retrieval approaches to exploit causal information in story corpora. The top performing system in these experiments uses a simple co-occurrence statistic between words in the causal antecedent and consequent, calculated as the Pointwise Mutual Information between words in a corpus of millions of personal stories.',\n",
       " 'c9d1bcdb95aa748940b85508fd7277622f74c0a4': 'Case research has commanded respect in the information systems (IS) discipline for at least a decade. Notwithstanding the relevance and potential value of case studies, this methodological approach was once considered to be one of the least systematic. Toward the end of the 1980s, the issue of whether IS case research was rigorously conducted was first raised. Researchers from our field (e.g., Benbasat et al. 1987; Lee 1989) and from other disciplines (e.g., Eisenhardt 1989; Yin 1994) called for more rigor in case research and, through theirrecommendations, contributed to the advancement of the case study methodology. Considering these contributions, the present study seeks to determine the extent to which the field of IS has advanced in its operational use of case study method. Precisely, it investigates the level of methodological rigor in positivist IS case research conducted over the past decade. To fulfill this objective, we identified and coded 183 case articles from seven major IS journals. Evaluation attributes or criteria considered in the present review focus on three main areas, namely, design issues, data collection, and data analysis. While the level of methodological rigor has experienced modest progress with respect to some specific attributes, the overall assessed rigor is somewhat equivocal and there are still significant areas for improvement. One of the keys is to include better documentation particularly regarding issues related to the data collection and',\n",
       " '30584b8d5bf99e51139e7ca9a8c04637480b73ca': 'In this letter, a compact microstrip low-pass filter (LPF) using T-shaped resonator with wide stopband is presented. The proposed LPF has capability to remove the eighth harmonic and a low insertion loss of 0.12 dB. The bandstop structure using stepped impendence resonator and two open-circuit stubs are used to design a wide stopband with attenuation level better than −20 dB from 3.08 up to 22 GHz. The proposed filter with −3-dB cutoff frequency of 2.68 GHz has been designed, fabricated, and measured. The operating of the LPF is investigated based on equivalent circuit model. Simulation results are verified by measurement results and excellent agreement between them is observed.',\n",
       " 'bb9936dc85acfc794636140f02644f4f29a754c9': 'On-line analytical processing (OLAP) describes an approach to decision support, which aims to extract knowledge from a data warehouse, or more specifically, from data marts. Its main idea is providing navigation through data to non-expert users, so that they are able to interactively generate ad hoc queries without the intervention of IT professionals. This name was introduced in contrast to on-line transactional processing (OLTP), so that it reflected the different requirements and characteristics between these classes of uses. The concept falls in the area of business intelligence.',\n",
       " '566199b865312f259d0cf694d71d6a51462e0fb8': 'With the growing adoption of Role-Based Access Control (RBAC) in commercial security and identity management products, how to facilitate the process of migrating a non-RBAC system to an RBAC system has become a problem with significant business impact. Researchers have proposed to use data mining techniques to discover roles to complement the costly top-down approaches for RBAC system construction. An important problem is how to construct RBAC systems with low complexity. In this article, we define the notion of weighted structural complexity measure and propose a role mining algorithm that mines RBAC systems with low structural complexity. Another key problem that has not been adequately addressed by existing role mining approaches is how to discover roles with semantic meanings. In this article, we study the problem in two primary settings with different information availability. When the only information is user-permission relation, we propose to discover roles whose semantic meaning is based on formal concept lattices. We argue that the theory of formal concept analysis provides a solid theoretical foundation for mining roles from a user-permission relation. When user-attribute information is also available, we propose to create roles that can be explained by expressions of user-attributes. Since an expression of attributes describes a real-world concept, the corresponding role represents a real-world concept as well. Furthermore, the algorithms we propose balance the semantic guarantee of roles with system complexity. Finally, we indicate how to create a hybrid approach combining top-down candidate roles. Our experimental results demonstrate the effectiveness of our approaches.',\n",
       " '0f0133873e0ddf9db8e190ccf44a07249c16ba10': \"For most problems in science and engineering we can obtain data sets that describe the observed system from various perspectives and record the behavior of its individual components. Heterogeneous data sets can be collectively mined by data fusion. Fusion can focus on a specific target relation and exploit directly associated data together with contextual data and data about system's constraints. In the paper we describe a data fusion approach with penalized matrix tri-factorization (DFMF) that simultaneously factorizes data matrices to reveal hidden associations. The approach can directly consider any data that can be expressed in a matrix, including those from feature-based representations, ontologies, associations and networks. We demonstrate the utility of DFMF for gene function prediction task with eleven different data sources and for prediction of pharmacologic actions by fusing six data sources. Our data fusion algorithm compares favorably to alternative data integration approaches and achieves higher accuracy than can be obtained from any single data source alone.\",\n",
       " '34c15e94066894d92b5b0ec6817b8a246c009aaa': 'Cloud services are Internet-based XaaS (X as a Service) services, where X can be hardware, software or applications. As Cloud consumers value QoS (Quality of Service), Cloud providers should make certain service level commitments in order to achieve business success. This paper argues for Cloud service negotiation. It outlines a research roadmap, reviews the state of the art, and reports our work on Cloud service negotiation. Three research problems that we formulate are QoS measurement, QoS negotiation, and QoS enforcement. To address QoS measurement, we pioneer a quality model named CLOUDQUAL for Cloud services. To address QoS negotiation, we propose a tradeoff negotiation approach for Cloud services, which can achieve a higher utility. We also give some ideas to solve QoS enforcement, and balance utility and success rate for QoS negotiation.',\n",
       " '08d6c0f860378a8c56b4ba7f347429970f70e3bd': \"BACKGROUND\\nMany classification methods have been proposed based on magnetic resonance images. Most methods rely on measures such as volume, the cerebral cortical thickness and grey matter density. These measures are susceptible to the performance of registration and limited in representation of anatomical structure. This paper proposes a two-stage local feature fusion method, in which deformable registration is not desired and anatomical information is represented from moderate scale.\\n\\n\\nMETHODS\\nKeypoints are firstly extracted from scale-space to represent anatomical structure. Then, two kinds of local features are calculated around the keypoints, one for correspondence and the other for representation. Scores are assigned for keypoints to quantify their effect in classification. The sum of scores for all effective keypoints is used to determine which group the test subject belongs to.\\n\\n\\nRESULTS\\nWe apply this method to magnetic resonance images of Alzheimer's disease and Parkinson's disease. The advantage of local feature in correspondence and representation contributes to the final classification. With the help of local feature (Scale Invariant Feature Transform, SIFT) in correspondence, the performance becomes better. Local feature (Histogram of Oriented Gradient, HOG) extracted from 16×16 cell block obtains better results compared with 4×4 and 8×8 cell block.\\n\\n\\nDISCUSSION\\nThis paper presents a method which combines the effect of SIFT descriptor in correspondence and the representation ability of HOG descriptor in anatomical structure. This method has the potential in distinguishing patients with brain disease from controls.\",\n",
       " 'e3bb879045c5807a950047d91e65c15e7f087313': 'Large-scale social networks emerged rapidly in recent years. Social networks have become complex networks. The structure of social networks is an important research area and has attracted much scientific interest. Community is an important structure in social networks. In this paper, we propose a community detection algorithm based on influential nodes. First, we introduce how to find influential nodes based on random walk. Then we combine the algorithm with order statistics theory to find community structure. We apply our algorithm in three classical data sets and compare to other algorithms. Our community detection algorithm is proved to be effective in the experiments. Our algorithm also has applications in data mining and recommendations.',\n",
       " '0414c4cc1974e6d3e69d9f2986e5bb9fb1af4701': 'Natural Language Processing is a theoretically motivated range of computational techniques for analysing and representing naturally occurring texts at one or more levels of linguistic analysis for the purpose of achieving human-like language processing for a range of tasks or applications [1]. To perform natural language processing a variety of tools and platform have been developed, in our case we will discuss about NLTK for Python.The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for the Python programming language[2]. It provides easy-to-use interfaces to many corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. In this paper we discuss different approaches for natural language processing using NLTK.',\n",
       " '7beabda9986a546cbf4f6c66de9c9aa7ea92a7ac': 'In this paper, we propose three approaches for Arabic automatic speech recognition. For pronunciation modeling, we propose a pronunciation variant generation with decision tree. For acoustic modeling, we propose the Hybrid approach to adapt the native acoustic model using another native acoustic model. Regarding the language model, we improve the language model using processed text. The experimental results show that the proposed pronunciation model approach has reduction in WER around 1%. The acoustic modeling reduce the WER by 1.2% and the adapted language modeling show reduction in WER by 1.9%.',\n",
       " '2a1c06ea40469064f2419df481a6e6aab7b09cdf': \"A prerequisite to foster proliferation of automated driving is common system acceptance. However, different users groups (novice, enthusiasts) decline automation, which could be, in turn, problematic for a successful market launch. We see a feasible solution in the combination of the advantages of manual (autonomy) and automated (increased safety) driving. Hence, we've developed the Hotzenplotz interface, combining possibility-driven design with psychological user needs. A simulator study (N=30) was carried-out to assess user experience with subjective criteria (Need Scale, PANAS/-X, HEMA, AttrakDiff) and quantitative measures (driving behavior, HR/HRV) in different conditions. Our results confirm that pure AD is significantly less able to satisfy user needs compared to manual driving and make people feeling bored/out of control. In contrast, the Hotzenplotz interface has proven to reduce the negative effects of AD. Our implication is that drivers should be provided with different control options to secure acceptance and avoid deskilling.\",\n",
       " '715250b1a76178cbc057de701524db7b2695234c': 'Nowadays, the performance of processors is primarily bound by a fixed energy budget, the power wall. This forces hardware vendors to optimize processors for specific tasks, which leads to an increasingly heterogeneous hardware landscape. Although efficient algorithms for modern processors such as GPUs are heavily investigated, we also need to prepare the database optimizer to handle computations on heterogeneous processors. GPUs are an interesting base for case studies, because they already offer many difficulties we will face tomorrow. In this paper, we present CoGaDB, a main-memory DBMS with built-in GPU acceleration, which is optimized for OLAP workloads. CoGaDB uses the self-tuning optimizer framework HyPE to build a hardware-oblivious optimizer, which learns cost models for database operators and efficiently distributes a workload on available processors. Furthermore, CoGaDB implements efficient algorithms on CPU and GPU and efficiently supports star joins. We show in this paper, how these novel techniques interact with each other in a single system. Our evaluation shows that CoGaDB quickly adapts to the underlying hardware by increasing the accuracy of its cost models at runtime.',\n",
       " 'e4c98cd116c0b86387354208bc97c1dc3c79d16d': 'Story understanding systems need to be able to perform commonsense reasoning, specifically regarding characters’ goals and their associated actions. Some efforts have been made to form large-scale commonsense knowledge bases, but integrating that knowledge into story understanding systems remains a challenge. We have implemented the Aspire system, an application of large-scale commonsense knowledge to story understanding. Aspire extends Genesis, a rule-based story understanding system, with tens of thousands of goalrelated assertions from the commonsense semantic network ConceptNet. Aspire uses ConceptNet’s knowledge to infer plausible implicit character goals and story causal connections at a scale unprecedented in the space of story understanding. Genesis’s rule-based inference enables precise story analysis, while ConceptNet’s relatively inexact but widely applicable knowledge provides a significant breadth of coverage difficult to achieve solely using rules. Genesis uses Aspire’s inferences to answer questions about stories, and these answers were found to be plausible in a small study. Though we focus on Genesis and ConceptNet, demonstrating the value of supplementing precise reasoning systems with large-scale, scruffy commonsense knowledge is our primary contribution.',\n",
       " '79b4723a010c66c2f3fdcd3bd79dba1c3a3e2d28': 'Negative bias temperature instability has become an important reliability concern for ultra-scaled Silicon IC technology with significant implications for both analog and digital circuit design. In this paper, we construct a comprehensive model for NBTI phenomena within the framework of the standard reaction–diffusion model. We demonstrate how to solve the reaction–diffusion equations in a way that emphasizes the physical aspects of the degradation process and allows easy generalization of the existing work. We also augment this basic reaction–diffusion model by including the temperature and field-dependence of the NBTI phenomena so that reliability projections can be made under arbitrary circuit operating conditions. 2004 Published by Elsevier Ltd.',\n",
       " '8e5ba7d60a4d9f425deeb3a05f3124fe6686b29a': 'Flow refers to a positive, activity-associated, subjective experience under conditions of a perceived fit between skills and task demands. Using functional magnetic resonance perfusion imaging, we investigated the neural correlates of flow in a sample of 27 human subjects. Experimentally, in the flow condition participants worked on mental arithmetic tasks at challenging task difficulty which was automatically and continuously adjusted to individuals\\' skill level. Experimental settings of \"boredom\" and \"overload\" served as comparison conditions. The experience of flow was associated with relative increases in neural activity in the left anterior inferior frontal gyrus (IFG) and the left putamen. Relative decreases in neural activity were observed in the medial prefrontal cortex (MPFC) and the amygdala (AMY). Subjective ratings of the flow experience were significantly associated with changes in neural activity in the IFG, AMY, and, with trend towards significance, in the MPFC. We conclude that neural activity changes in these brain regions reflect psychological processes that map on the characteristic features of flow: coding of increased outcome probability (putamen), deeper sense of cognitive control (IFG), decreased self-referential processing (MPFC), and decreased negative arousal (AMY).',\n",
       " '23e4844f33adaf2e26195ffc2a7514a2e45fd33d': 'Recently, search engines have invested significant effort to answering entity–attribute queries from structured data, but have focused mostly on queries for frequent attributes. In parallel, several research efforts have demonstrated that there is a long tail of attributes, often thousands per class of entities, that are of interest to users. Researchers are beginning to leverage these new collections of attributes to expand the ontologies that power search engines and to recognize entity– attribute queries. Because of the sheer number of potential attributes, such tasks require us to impose some structure on this long and heavy tail of attributes. This paper introduces the problem of organizing the attributes by expressing the compositional structure of their names as a rule-based grammar. These rules offer a compact and rich semantic interpretation of multi-word attributes, while generalizing from the observed attributes to new unseen ones. The paper describes an unsupervised learning method to generate such a grammar automatically from a large set of attribute names. Experiments show that our method can discover a precise grammar over 100,000 attributes of Countries while providing a 40-fold compaction over the attribute names. Furthermore, our grammar enables us to increase the precision of attributes from 47% to more than 90% with only a minimal curation effort. Thus, our approach provides an efficient and scalable way to expand ontologies with attributes of user interest.',\n",
       " '025cdba37d191dc73859c51503e91b0dcf466741': 'Fingerprint image enhancement is an essential preprocessing step in fingerprint recognition applications. In this paper we introduce an approach that extracts simultaneously orientation and frequency of local ridge in the fingerprint image by Gabor wavelet filter bank and use them in Gabor Filtering of image. Furthermore, we describes a robust approach to fingerprint image enhancement, which is based on integration of Gabor Filters and Directional Median Filter(DMF). In fact, Gaussian-distributed noises are reduced effectively by Gabor Filters and impulse noises by DMF. the proposed DMF not only can finish its original tasks, it can also join broken fingerprint ridges, fill out the holes of fingerprint images, smooth irregular ridges as well as remove some annoying small artifacts between ridges. Experimental results show our method to be superior to those described in the literature.',\n",
       " 'cccb533cd14259b92ec7cd71b1d3f679ef251394': 'This paper presents a first approach for a haptic human-machine interface combined with a novel lane-keeping and collision-avoidance assistance system approach, as well as the results of a first exploration study with human test drivers. The assistance system approach is based on a potential field predictive path planning algorithm that incorporates the drivers wishes commanded by the steering wheel angle, the brake pedal or throttle, and the intended maneuver. For the design of the haptic human-machine interface the assistance torque characteristic at the handwheel is shaped and the path planning parameters are held constant. In the exploration, both driving data as well as questionnaires are evaluated. The results show good acceptance for the lane-keeping assistance while the collision avoidance assistance needs to be improved.',\n",
       " 'fbbe5bf055f997cbd1ed1f3b72b1d630771e358e': 'Learning predictors for student retention is very difficult. After reviewing the literature, it is evident that there is considerable room for improvement in the current state of the art. As shown in this paper, improvements are possible if we (a) explore a wide range of learning methods; (b) take care when selecting attributes; (c) assess the efficacy of the learned theory not just by its median performance, but also by the variance in that performance; (d) study the delta of student factors between those who stay and those who are retained. Using these techniques, for the goal of predicting if students will remain for the first three years of an undergraduate degree, the following factors were found to be informative: family background and family’s social-economic status, high school GPA',\n",
       " '03dbbd987ea1fd5307ba5ae2f56d88e4f465b88c': 'An organization makes a new release as new information become available, releases a tailored view for each data request, releases sensitive information and identifying information separately. The availability of related releases sharpens the identification of individuals by a global quasi-identifier consisting of attributes from related releases. Since it is not an option to anonymize previously released data, the current release must be anonymized to ensure that a global quasi-identifier is not effective for identification. In this paper, we study the sequential anonymization problem under this assumption. A key question is how to anonymize the current release so that it cannot be linked to previous releases yet remains useful for its own release purpose. We introduce the lossy join, a negative property in relational database design, as a way to hide the join relationship among releases, and propose a scalable and practical solution.',\n",
       " '3dfce4601c3f413605399267b3314b90dc4b3362': \"ÐToday's globally networked society places great demand on the dissemination and sharing of information. While in the past released information was mostly in tabular and statistical form, many situations call today for the release of specific data (microdata). In order to protect the anonymity of the entities (called respondents) to which information refers, data holders often remove or encrypt explicit identifiers such as names, addresses, and phone numbers. Deidentifying data, however, provides no guarantee of anonymity. Released information often contains other data, such as race, birth date, sex, and ZIP code, that can be linked to publicly available information to reidentify respondents and inferring information that was not intended for disclosure. In this paper we address the problem of releasing microdata while safeguarding the anonymity of the respondents to which the data refer. The approach is based on the definition of k-anonymity. A table provides k-anonymity if attempts to link explicitly identifying information to its content map the information to at least k entities. We illustrate how k-anonymity can be provided without compromising the integrity (or truthfulness) of the information released by using generalization and suppression techniques. We introduce the concept of minimal generalization that captures the property of the release process not to distort the data more than needed to achieve k-anonymity, and present an algorithm for the computation of such a generalization. We also discuss possible preference policies to choose among different minimal\",\n",
       " '5c15b11610d7c3ee8d6d99846c276795c072eec3': 'The proliferation of information on the Internet and access to fast computers with large storage capacities has increased the volume of information collected and disseminated about individuals. The existence os these other data sources makes it much easier to re-identify individuals whose private information is released in data believed to be anonymous. At the same time, increasing demands are made on organizations to release individualized data rather than aggregate statistical information. Even when explicit identi ers, such as name and phone number, are removed or encrypted when releasing individualized data, other characteristic data, which we term quasi-identi ers, can exist which allow the data recipient to re-identify individuals to whom the data refer. In this paper, we provide a computational disclosure technique for releasing information from a private table such that the identity of any individual to whom the released data refer cannot be de nitively recognized. Our approach protects against linking to other data. It is based on the concepts of generalization, by which stored values can be replaced with semantically consistent and truthful but less precise alternatives, and of k-anonymity . A table is said to provide k-anonymity when the contained data do not allow the recipient to associate the released information to a set of individuals smaller than k. We introduce the notions of generalized table and of minimal generalization of a table with respect to a k-anonymity requirement. As an optimization problem, the objective is to minimally distort the data while providing adequate protection. We describe an algorithm that, given a table, e ciently computes a preferred minimal generalization to provide anonymity.',\n",
       " '9492fb8d3b3ce09451fc1df46d5e3c200095f5eb': 'In recent years, privacy-preserving data mining has been studied extensively, because of the wide proliferation of sensitive information on the internet. This paper investigates data mining as a technique for masking data; therefore, termed data mining based privacy protection. This approach incorporates partially the requirement of a targeted data mining task into the process of masking data so that essential structure is preserved in the masked data. The following privacy problem is considered in this paper: a data holder wants to release a version of data for building classification models, but wants to protect against linking the released data to an external source for inferring sensitive information. An iterative bottom-up generalization is adapted from data mining to generalize the data. The generalized data remains useful to classification but becomes difficult to link to other sources. The generalization space is specified by a hierarchical structure of generalizations. A key is identifying the best generalization to climb up the hierarchy at each iteration.',\n",
       " 'b30706600c01e23e11b303842fe548d62bf3ecf8': 'The previous literature of privacy preserving data publication has focused on performing \"one-time\" releases. Specifically, none of the existing solutions supports re-publication of the microdata, after it has been updated with insertions <u>and</u> deletions. This is a serious drawback, because currently a publisher cannot provide researchers with the most recent dataset continuously.\\n This paper remedies the drawback. First, we reveal the characteristics of the re-publication problem that invalidate the conventional approaches leveraging k-anonymity and l-diversity. Based on rigorous theoretical analysis, we develop a new generalization principle m-invariance that effectively limits the risk of privacy disclosure in re-publication. We accompany the principle with an algorithm, which computes privacy-guarded relations that permit retrieval of accurate aggregate information about the original microdata. Our theoretical results are confirmed by extensive experiments with real data.',\n",
       " '43d7f7a090630db3ed0d7fca9d649c8562aeaaa9': \"We report on results of a series of user studies on the perception of four visual variables that are commonly used in the literature to depict uncertainty. To the best of our knowledge, we provide the first formal evaluation of the use of these variables to facilitate an easier reading of uncertainty in visualizations that rely on line graphical primitives. In addition to blur, dashing and grayscale, we investigate the use of `sketchiness' as a visual variable because it conveys visual impreciseness that may be associated with data quality. Inspired by work in non-photorealistic rendering and by the features of hand-drawn lines, we generate line trajectories that resemble hand-drawn strokes of various levels of proficiency-ranging from child to adult strokes-where the amount of perturbations in the line corresponds to the level of uncertainty in the data. Our results show that sketchiness is a viable alternative for the visualization of uncertainty in lines and is as intuitive as blur; although people subjectively prefer dashing style over blur, grayscale and sketchiness. We discuss advantages and limitations of each technique and conclude with design considerations on how to deploy these visual variables to effectively depict various levels of uncertainty for line marks.\",\n",
       " '99aba9948302a8bd2cdf46a809a59e34d1b57d86': 'The United States electricity grid faces significant problems resulting from fundamental design principles that limit its ability to handle the key energy challenges of the 21st century. We propose an innovative electric power architecture, rooted in lessons learned from the Internet and microgrids, which addresses these problems while interfacing gracefully into the current grid to allow for non-disruptive incremental adoption. Such a system, which we term a \"Local\" grid, is controlled by intelligent power switches (IPS), and can consist of loads, energy sources, and energy storage. The desired result of the proposed architecture is to produce a grid network designed for distributed renewable energy, prevalent energy storage, and stable autonomous systems. We will describe organizing principles of such a system that ensure well-behaved operation, such as requirements for communication and energy transfer protocols, regulation and control schemes, and market-based rules of operation.',\n",
       " '30b32f4a6341b5809428df1271bdb707f2418362': 'In this paper, we propose a sequential neural encoder with latent structured description SNELSD for modeling sentences. This model introduces latent chunk-level representations into conventional sequential neural encoders, i.e., recurrent neural networks with long short-term memory LSTM units, to consider the compositionality of languages in semantic modeling. An SNELSD model has a hierarchical structure that includes a detection layer and a description layer. The detection layer predicts the boundaries of latent word chunks in an input sentence and derives a chunk-level vector for each word. The description layer utilizes modified LSTM units to process these chunk-level vectors in a recurrent manner and produces sequential encoding outputs. These output vectors are further concatenated with word vectors or the outputs of a chain LSTM encoder to obtain the final sentence representation. All the model parameters are learned in an end-to-end manner without a dependency on additional text chunking or syntax parsing. A natural language inference task and a sentiment analysis task are adopted to evaluate the performance of our proposed model. The experimental results demonstrate the effectiveness of the proposed SNELSD model on exploring task-dependent chunking patterns during the semantic modeling of sentences. Furthermore, the proposed method achieves better performance than conventional chain LSTMs and tree-structured LSTMs on both tasks.',\n",
       " '36cd10d9afacdc26a019d44ff3d39a8cf3fd4a9a': \"This paper describes the design and the experimental investigation of a gate drive unit with closed-loop control of the collector current slope diC/dt for multichip insulated-gate bipolar transistors (IGBTs). Compared to a pure resistive gate drive, the proposed diC/dt control offers the ability to adjust the collector current slope freely which helps to find an optimized relation between switching losses and secure operation of the freewheeling diode for every type of IGBT. Based on the description of IGBT's switching behavior, the design and the realization of the gate drive are presented. The test setup and the comparison of switching tests with and without the proposed diC/dt control are discussed.\",\n",
       " 'efa2be90d7f48e11da39c3ed3fa14579fd367f9c': \"In this paper, we propose 3DLoc, which performs 3-dimensional localization on the tagged objects by using the RFID tag arrays. 3DLoc deploys three arrays of RFID tags on three mutually orthogonal surfaces of each object. When performing 3D localization, 3DLoc continuously moves the RFID antenna and scans the tagged objects in a 2-dimensional space right in front of the tagged objects. It then estimates the object's 3D position according to the phases from the tag arrays. By referring to the fixed layout of the tag array, we use Angle of Arrival-based schemes to accurately estimate the tagged objects' orientation and 3D coordinates in the 3D space. To suppress the localization errors caused by the multipath effect, we use the linear relationship of the AoA parameters to remove the unexpected outliers from the estimated results. We have implemented a prototype system and evaluated the actual performance in the real complex environment. The experimental results show that 3DLoc achieves the mean accuracy of 10cm in free space and 15.3cm in the multipath environment for the tagged object.\",\n",
       " '3d676791081e7b16a4ead9924fc03bac587d181d': 'In recent years, non-monotonic Inductive Logic Programming has received growing interest. Specifically, several new learning frameworks and algorithms have been introduced for learning under the answer set semantics, allowing the learning of common-sense knowledge involving defaults and exceptions, which are essential aspects of human reasoning. In this paper, we present a noise-tolerant generalisation of the learning from answer sets framework. We evaluate our ILASP3 system, both on synthetic and on real datasets, represented in the new framework. In particular, we show that on many of the datasets ILASP3 achieves a higher accuracy than other ILP systems that have previously been applied to the datasets, including a recently proposed differentiable learning framework.',\n",
       " '7d46c3648f76b5542d8ecd582f01f155e6b248d5': 'Semantics is the meaning of symbols, notations, concepts, functions, and behaviors, as well as their relations that can be deduced onto a set of predefined entities and/or known concepts. Semantic computing is an emerging computational methodology that models and implements computational structures and behaviors at semantic or knowledge level beyond that of symbolic data. In semantic computing, formal semantics can be classified into the categories of to be, to have, and to do semantics. This paper presents a comprehensive survey of formal and cognitive semantics for semantic computing in the fields of computational linguistics, software science, computational intelligence, cognitive computing, and denotational mathematics. A set of novel formal semantics, such as deductive semantics, concept-algebra-based semantics, and visual semantics, is introduced that forms a theoretical and cognitive foundation for semantic computing. Applications of formal semantics in semantic computing are presented in case studies on semantic cognition of natural languages, semantic analyses of computing behaviors, behavioral semantics of human cognitive processes, and visual semantic algebra for image and visual object manipulations.',\n",
       " '927aa1bd7c0e122ae77224e92821f2eaafc96cf9': 'Gender Recognition is important in different commercial and law enforcement applications. In this paper we have proposed a gender recognition system through facial images. We have used a different technique that involves Bandlet Transform instead of previously used Wavelet Transform, which is a multi-resolution technique and more efficiently provides the edges of images, and then mean is combined to create the feature vectors of the images. To classify the images for gender, we have used fuzzy c mean clustering. Experimental results have shown that average 97.1% accuracy have been achieved using this technique when SUMS database was used and 93.3% was achieved when FERET database was used. Keywords----Bandlet, Gender Recognition, Fuzzy C-mean, Multi Resolution.',\n",
       " '86cd8da6c6b35d99b75aaaaf7be55c78a31948eb': \"The advent of 'biological internal fixation' is an important development in the surgical management of fractures. Locked nailing has demonstrated that flexible fixation without precise reduction results in reliable healing. While external fixators are mainly used today to provide temporary fixation in fractures after severe injury, the internal fixator offers flexible fixation, maintaining the advantages of the external fixator but allowing long-term treatment. The internal fixator resembles a plate but functions differently. It is based on pure splinting rather than compression. The resulting flexible stabilisation induces the formation of callus. With the use of locked threaded bolts, the application of the internal fixator foregoes the need of adaptation of the shape of the splint to that of the bone during surgery. Thus, it is possible to apply the internal fixator as a minimally invasive percutaneous osteosynthesis (MIPO). Minimal surgical trauma and flexible fixation allow prompt healing when the blood supply to bone is maintained or can be restored early. The scientific basis of the fixation and function of these new implants has been reviewed. The biomechanical aspects principally address the degree of instability which may be tolerated by fracture healing under different biological conditions. Fractures may heal spontaneously in spite of gross instability while minimal, even non-visible, instability may be deleterious for rigidly fixed small fracture gaps. The theory of strain offers an explanation for the maximum instability which will be tolerated and the minimal degree required for induction of callus formation. The biological aspects of damage to the blood supply, necrosis and temporary porosity explain the importance of avoiding extensive contact of the implant with bone. The phenomenon of bone loss and stress protection has a biological rather than a mechanical explanation. The same mechanism of necrosis-induced internal remodelling may explain the basic process of direct healing.\",\n",
       " '59ae5541e1dc71ff33b4b6e14cbbc5c3d46fc506': \"A wireless sensor network (WSN)consisting of a large number of tiny sensors can be an effective tool for gathering data in diverse kinds of environments. The data collected by each sensor is communicated to the base station, which forwards the data to the end user. Clustering is introduced to WSNs because it has proven to be an effective approach to provide better data aggregation and scalability for large WSNs. Clustering also conserves the limited energy resources of the sensors. This paper synthesises existing clustering algorithms news's and highlights the challenges in clustering.\",\n",
       " 'ce7f81e898f1c78468df2480294806864899549e': 'Direct modulation at 50 Gb/s of 1.3-μm InGaAlAs DFB lasers operating at up to 80°C was experimentally demonstrated by using a ridge-shaped buried heterostructure.',\n",
       " 'c955e319595b8ae051fe18f193490ba56c4f0254': 'Cloud Computing is expected to become the driving force of information technology to revolutionize the future. Presently number of companies is trying to adopt this new technology either as service providers, enablers or vendors. In this way the cloud market is estimated be likely to emerge at a remarkable rate. Under the whole cloud umbrella, PaaS seems to have a relatively small market share. However, it is expected to offer much more as it is compared with its counterparts SaaS and IaaS. This paper is aimed to assess and analyze the future of PaaS technology. Year 2015 named as “the year of PaaS”. It means that PaaS technology has established strong roots and ready to hit the market with better technology services. This research will discuss future PaaS market trends, growth and business competitors. In the current dynamic era, several companies in the market are offering PaaS services. This research will also outline some of the top service providers (proprietary & open source) to discuss their current technology status and present a futuristic look into their services and business strategies. Analysis of the present and future PaaS technology infrastructure will also be a major discussion in this paper.',\n",
       " 'e1a7cf4e4760bb580dd67255fbe872bac33ae28b': 'This is a brief review of recent work on the prospective hybrid CMOS/memristor circuits. Such hybrids combine the flexibility, reliability and high functionality of the CMOS subsystem with very high density of nanoscale thin film resistance switching devices operating on different physical principles. Simulation and initial experimental results demonstrate that performance of CMOS/memristor circuits for several important applications is well beyond scaling limits of conventional VLSI paradigm.',\n",
       " '91d513af1f667f64c9afc55ea1f45b0be7ba08d4': 'Face image quality can be defined as a measure of the utility of a face image to automatic face recognition. In this work, we propose (and compare) two methods for automatic face image quality based on target face quality values from (i) human assessments of face image quality (matcherindependent), and (ii) quality values computed from similarity scores (matcher-dependent). A support vector regression model trained on face features extracted using a deep convolutional neural network (ConvNet) is used to predict the quality of a face image. The proposed methods are evaluated on two unconstrained face image databases, LFW and IJB-A, which both contain facial variations with multiple quality factors. Evaluation of the proposed automatic face image quality measures shows we are able to reduce the FNMR at 1% FMR by at least 13% for two face matchers (a COTS matcher and a ConvNet matcher) by using the proposed face quality to select subsets of face images and video frames for matching templates (i.e., multiple faces per subject) in the IJB-A protocol. To our knowledge, this is the first work to utilize human assessments of face image quality in designing a predictor of unconstrained face quality that is shown to be effective in cross-database evaluation.',\n",
       " '2e3522d8d9c30f34e18161246c2f6dac7a9ae04d': \"While traditional models of language comprehension have focused on the left posterior temporal cortex as the neurological basis for language comprehension, lesion and functional imaging studies indicate the involvement of an extensive network of cortical regions. However, the full extent of this network and the white matter pathways that contribute to it remain to be characterized. In an earlier voxel-based lesion-symptom mapping analysis of data from aphasic patients (Dronkers et al., 2004), several brain regions in the left hemisphere were found to be critical for language comprehension: the left posterior middle temporal gyrus, the anterior part of Brodmann's area 22 in the superior temporal gyrus (anterior STG/BA22), the posterior superior temporal sulcus (STS) extending into Brodmann's area 39 (STS/BA39), the orbital part of the inferior frontal gyrus (BA47), and the middle frontal gyrus (BA46). Here, we investigated the white matter pathways associated with these regions using diffusion tensor imaging from healthy subjects. We also used resting-state functional magnetic resonance imaging data to assess the functional connectivity profiles of these regions. Fiber tractography and functional connectivity analyses indicated that the left MTG, anterior STG/BA22, STS/BA39, and BA47 are part of a richly interconnected network that extends to additional frontal, parietal, and temporal regions in the two hemispheres. The inferior occipito-frontal fasciculus, the arcuate fasciculus, and the middle and inferior longitudinal fasciculi, as well as transcallosal projections via the tapetum were found to be the most prominent white matter pathways bridging the regions important for language comprehension. The left MTG showed a particularly extensive structural and functional connectivity pattern which is consistent with the severity of the impairments associated with MTG lesions and which suggests a central role for this region in language comprehension.\",\n",
       " 'a58fe54d2677fb01533c4dc2d8ef958a2cb921cb': 'Based on the non line-of-sight (NLOS) indoor measurements performed in the corridors on the third floor of the Electronics and Telecommunications Research Institute (ETRI) building in Daejeon city, Republic of Korea, we investigated the distribution of the eigenvalues of HH*, where H denotes a 2×2 multiple-input multiple-output (MIMO) channel matrix. Using the observation that the distribution of the measured eigenvalues matches well with the Gamma distribution, we propose a model of eigenvalues as Gamma distributed random variables that prominently feature both transmitting and receiving correlations. Using the model with positive integer k_i, i=1, 2, which is the shape parameter of a Gamma distribution, we derive the closed-form ergodic capacity of the 2×2 MIMO channel. Validation results show that the proposed model enables the evaluation of the outage and ergodic capacities of the correlated 2×2 MIMO channel in the NLOS indoor corridor environment.',\n",
       " 'd4b29432c3e7bd4e2d06935169c91f781f441160': 'Digit speech recognition is important in many applications such as automatic data entry, PIN entry, voice dialing telephone, automated banking system, etc. This paper presents speaker independent speech recognition system for Malayalam digits. The system employs Mel frequency cepstrum coefficient (MFCC) as feature for signal processing and Hidden Markov model (HMM) for recognition. The system is trained with 21 male and female voices in the age group of 20 to 40 years and there was 98.5% word recognition accuracy (94.8% sentence recognition accuracy) on a test set of continuous digit recognition task.',\n",
       " '517a461a8839733e34c9025154de3d6275543642': 'We present a non-traditional retrieval problem we call subtopic retrieval. The subtopic retrieval problem is concerned with finding documents that cover many different subtopics of a query topic. In such a problem, the utility of a document in a ranking is dependent on other documents in the ranking, violating the assumption of independent relevance which is assumed in most traditional retrieval methods. Subtopic retrieval poses challenges for evaluating performance, as well as for developing effective algorithms. We propose a framework for evaluating subtopic retrieval which generalizes the traditional precision and recall metrics by accounting for intrinsic topic difficulty as well as redundancy in documents. We propose and systematically evaluate several methods for performing subtopic retrieval using statistical language models and a maximal marginal relevance (MMR) ranking strategy. A mixture model combined with query likelihood relevance ranking is shown to modestly outperform a baseline relevance ranking on a data set used in the TREC interactive track.',\n",
       " '6b3abd1a6bf9c9564147cfda946c447955d01804': 'Recently a number of systems have been developed to implement and improve the visual communication over screen-camera links. In this paper we study an opposite problem: how to prevent unauthorized users from videotaping a video played on a screen, such as in a theater, while do not affect the viewing experience of legitimate audiences. We propose and develop a light-weight hardware-free system, called Kaleido, that ensures these properties by taking advantage of the limited disparities between the screen-eye channel and the screen-camera channel. Kaleido does not require any extra hardware and is purely based on re-encoding the original video frame into multiple frames used for displaying. We extensively test our system Kaleido using a variety of smartphone cameras. Our experiments confirm that Kaleido preserves the high-quality screen-eye channel while reducing the secondary screen-camera channel quality significantly.',\n",
       " '528407a9c5b41f81366bbe5cf8058dadcb139fea': 'XOR and XNOR gate plays an important role in digital systems including arithmetic and encryption circuits. This paper proposes a combination of XOR-XNOR gate using 6-transistors for low power applications. Comparison between a best existing XOR-XNOR have been done by simulating the proposed and other design using 65nm CMOS technology in Cadence environment. The simulation results demonstrate the delay, power consumption and power-delay product (PDP) at different supply voltages ranging from 0.6V to 1.2V. The results show that the proposed design has lower power dissipation and has a full voltage swing.',\n",
       " '17580101d22476be5ad1655abb48d967c78a3875': '.................................................................................................................... iv Acknowledgments ...................................................................................................... iv Introduction ............................................................................................................... 1 Labor ..........................................................................................................................2 Gender Division of Labor .................................................................................... 2 Household Labor Availability ............................................................................... 6 Agricultural Labor Markets .................................................................................. 8 Conclusions: Labor and Gender .......................................................................... 9 Land ........................................................................................................................ 10 Access to Land ................................................................................................... 10 Security of Land ................................................................................................ 11 Changing Access to Land ................................................................................... 11 Access to Other Inputs .............................................................................................. 12 Access to Credit ................................................................................................. 13 Access to Fertilizer ............................................................................................. 14 Access to Extension and Information ................................................................. 15 Access to Mechanization .................................................................................... 16 Gender Issues in Access to Inputs: Summary...................................................... 16 Outputs .................................................................................................................... 17 Household Decision-Making .................................................................................... 18 Cooperative Bargaining and Collective Models .................................................. 19 Noncooperative Bargaining Models ................................................................... 19 Conclusions .............................................................................................................. 21 References ................................................................................................................. 23 Annotated Bibliography ............................................................................................ 27',\n",
       " '205917e8e885b3c2c6c31c90f9de7f411a24ec22': \"With ubiquitous adoption of connected sensors, actuators and smart devices are finding inroads into daily life. Internet of Things (IoT) authentication is rapidly transforming from classical cryptographic user-centric knowledge based approaches to device signature based automated methodologies to corroborate identity between claimant and a verifier. Physical Unclonable Function (PUF) based IoT authentication mechanisms are gaining widespread interest as users are required to access IoT devices in real time while also expecting execution of sensitive (even physical) IoT actions immediately. This paper, delineates combination of BlockChain and Sensor based PUF authentication mechanism for solving real-time but non-repudiable access to IoT devices in a Smart Home by utilizing a mining less consensus mechanism for the provision of immutable assurance to users' and IoT devices' transactions i.e. commands, status alerts, actions etc.\",\n",
       " '112f07f90d4395623a51725e90bf9d91b89e559a': 'The effect of violent video games is among the most widely discussed topics in media studies, and for good reason. These games are immensely popular, but many seem morally objectionable. Critics attack them for a number of reasons ranging from their capacity to teach players weapons skills to their ability to directly cause violent actions. This essay shows that many of these criticisms are misguided. Theoretical and empirical arguments against violent video games often suffer from a number of significant shortcomings that make them ineffective. This essay argues that video games are defensible from the perspective of Kantian, Aristotelian, and utilitarian moral theories.',\n",
       " 'cdbb15ee448ee4dc6db16a540a40fbb035d1f4ca': \"Systems for Community Question Answering (CQA) are well-known on the open web (e.g. Stack Overflow or Quora). They have been recently adopted also for use in educational domain (mostly in MOOCs) to mediate communication between students and teachers. As students are only novices in topics they learn about, they may need various scaffoldings to achieve effective question answering. In this work, we focus specifically on automatic recommendation of tags classifying students' questions. We propose a novel method that can automatically analyze a text of a question and suggest appropriate tags to an asker. The method takes specifics of educational domain into consideration by a two-step recommendation process in which tags reflecting course structure are recommended at first and consequently supplemented with additional related tags.\\n Evaluation of the method on data from CS50 MOOC at Stack Exchange platform showed that the proposed method achieved higher performance in comparison with a baseline method (tag recommendation without taking educational specifics into account).\",\n",
       " '598caa7f431930892a78f1083453b1c0ba29e725': 'The ability to use a 2D map to navigate a complex 3D environment is quite remarkable, and even difficult for many humans. Localization and navigation is also an important problem in domains such as robotics, and has recently become a focus of the deep reinforcement learning community. In this paper we teach a reinforcement learning agent to read a map in order to find the shortest way out of a random maze it has never seen before. Our system combines several state-of-theart methods such as A3C and incorporates novel elements such as a recurrent localization cell. Our agent learns to localize itself based on 3D first person images and an approximate orientation angle. The agent generalizes well to bigger mazes, showing that it learned useful localization and naviga-',\n",
       " '13082af1fd6bb9bfe63e73cf007de1655b7f9ae0': 'The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.',\n",
       " '6eb69624732a589e91816cc7722be0b0cdd28767': 'Applications such as those for systems management and intrusion detection employ an automated real-time operation system in which sensor data are collected and processed in real time. Although such a system effectively reduces the need for operation staff, it requires constructing and maintaining correlation rules. Currently, rule construction requires experts to identify problem patterns, a process that is timeconsuming and error-prone. In this paper, we propose reducing this burden by mining historical data that are readily available. Specifically, we first present efficient algorithms to mine three types of important patterns from historical event data: event bursts, periodic patterns, and mutually dependent patterns. We then discuss a framework for efficiently mining events that have multiple attributes. Last, we present Event Correlation Constructor—a tool that validates and extends correlation knowledge.',\n",
       " 'cd866d4510e397dbc18156f8d840d7745943cc1a': '',\n",
       " '0a283fb395343cd26984425306ca24c85b09ccdb': 'In this paper, a Bayesian inference network model for automatic indexing with index terms (descriptors) from a prescribed vocabulary is presented. It requires an indexing dictionary with rules mapping terms of the respective subject field onto descriptors and inverted lists for terms occuring in a set of documents of the subject field and descriptors manually assigned to these documents. The indexing dictionary can be derived automatically from a set of manually indexed documents. An application of the network model is described, followed by an indexing example and some experimental results about the indexing performance of the network model.',\n",
       " '1ac5b0628ff249c388ff5ca934a9ccbec577cbd7': 'One of the most well-studied problems in data mining is mining for association rules in market basket data. Association rules, whose significance is measured via support and confidence, are intended to identify rules of the type, “A customer purchasing item A often also purchases item B.” Motivated by the goal of generalizing beyond market baskets and the association rules used with them, we develop the notion of mining rules that identify correlations (generalizing associations), and we consider both the absence and presence of items as a basis for generating rules. We propose measuring significance of associations via the chi-squared test for correlation from classical statistics. This leads to a measure that is upward closed in the itemset lattice, enabling us to reduce the mining problem to the search for a border between correlated and uncorrelated itemsets in the lattice. We develop pruning strategies and devise an efficient algorithm for the resulting problem. We demonstrate its effectiveness by testing it on census data and finding term dependence in a corpus of text documents, as well as on synthetic data.',\n",
       " '16afeecd0f4dbccdd8e281e0e7e443bd08681da1': 'Cloud-based speech recognition systems enhance Web surfing, transportation, health care, etc. For example, using voice commands helps drivers search the Internet without affecting traffic safety risks. User frustration with network traffic problems can affect the usability of these applications. The performance of these type of applications should be robust in difficult network conditions. We evaluate the performance of several client-server speech recognition applications, under various network conditions. We measure transcription delay and accuracy of each application under different packet loss and jitter values. Results of our study show that performance of client-server speech recognition systems is affected by jitter and packet loss; which commonly occur in WiFi and cellular networks.',\n",
       " '41f3b1aebde4c342211185d2b5e339a60ceff9e2': 'In this article, we develop an analytical model of the relationship between the wafer/pad friction and process configuration. We also provide experimental validation of this model for in situ process monitoring. CMP thus demonstrates that the knowledge and methodologies developed for friction modeling and control can be used to advance the understanding, monitoring, and control of semiconductor manufacturing processes. Meanwhile, relevant issues and challenges in real-time monitoring of CMP are presented as sources of future development.',\n",
       " 'b972f638b7c4ed22e1bcb573520bb232ea88cda5': 'In this paper we explore the theoretical boundaries of planning in a setting where no model of the agent’s actions is given. Instead of an action model, a set of successfully executed plans are given and the task is to generate a plan that is safe, i.e., guaranteed to achieve the goal without failing. To this end, we show how to learn a conservative model of the world in which actions are guaranteed to be applicable. This conservative model is then given to an off-the-shelf classical planner, resulting in a plan that is guaranteed to achieve the goal. However, this reduction from a model-free planning to a model-based planning is not complete: in some cases a plan will not be found even when such exists. We analyze the relation between the number of observed plans and the likelihood that our conservative approach will indeed fail to solve a solvable problem. Our analysis show that the number of trajectories needed scales gracefully.',\n",
       " 'e38b9f339e858c8ac95679737a0852d21c48d89c': \"BACKGROUND\\nThe technological advances that have been made in developing highly functional prostheses are promising for very active patients but we do not yet know whether they cause an increase in biomechanical load along with possibly negative consequences for pressure conditions in the socket. Therefore, this study monitored the socket pressure at specific locations of the stump when using a microprocessor-controlled adaptive prosthetic ankle under different walking conditions.\\n\\n\\nMETHODS\\nTwelve unilateral transtibial amputees between 43 and 59 years of age were provided with the Proprio-Foot (Ossur) and underwent an instrumented 3D gait analysis in level, stair, and incline walking, including synchronous data capturing of socket pressure. Peak pressures and pressure time integrals (PTI) at three different locations were compared for five walking conditions with and without using the device's ankle adaptation mode.\\n\\n\\nFINDINGS\\nHighest peak pressures of 2.4 k Pa/kg were found for incline ascent at the calf muscle as compared to 2.1 k Pa/kg in level walking with large inter-individual variance. In stair ascent a strong correlation was found between maximum knee moment and socket pressure. The most significant pressure changes relative to level walking were seen in ramp descent anteriorly towards the stump end, with PTI values being almost twice as high as those in level walking. Adapting the angle of the prosthesis on stairs and ramps modified the pressure data such that they were closer to those in level walking.\\n\\n\\nINTERPRETATION\\nPressure at the stump depends on the knee moments involved in each walking condition. Adapting the prosthetic ankle angle is a valuable means of modifying joint kinetics and thereby the pressure distribution at the stump. However, large inter-individual differences in local pressures underline the importance of individual socket fitting.\",\n",
       " '6c237c3638eefe1eb39212f801cd857bedc004ee': 'The proliferation of Electronic Health Records (EHRs) challenges data miners to discover potential and previously unknown patterns from a large collection of medical data. One of the tasks that we address in this paper is to reveal previously unknown effects of drugs on laboratory test results. We propose a method that leverages drug information to find a meaningful list of drugs that have an effect on the laboratory result. We formulate the problem as a convex non smooth function and develop a proximal gradient method to optimize it. The model has been evaluated on two important use cases: lowering low-density lipoproteins and glycated hemoglobin test results. The experimental results provide evidence that the proposed method is more accurate than the state-of-the-art method, rediscover drugs that are known to lower the levels of laboratory test results, and most importantly, discover additional potential drugs that may also lower these levels.',\n",
       " 'ebae5af27aafd39358a46c83c1409885773254dd': 'In recent years, the aspect of vehicular ad hoc network (VANET) is becoming an interesting research area; VANET is a mobile ad hoc network considered as a special case of mobile ad hoc network (MANET). Similar to MANET, VANET is characterized as autonomous and self-configured wireless network. However, VANET has very dynamic topology, large and variable network size, and constrained mobility; these characteristics led to the need for efficient routing and resource saving VANET protocols, to fit with different VANET environments. These differences render traditional MANET’s protocols unsuitable for VANET. The aim of this work is to give a survey of the VANETs routing mechanisms, this paper gives an overview of Vehicular ad hoc networks (VANETs) and the existing VANET routing protocols; mainly it focused on vehicle to vehicle (V2V) communication and protocols. The paper also represents the general outlines and goals of VANETs, investigates different routing schemes that have been developed for VANETs, as well as providing classifications of VANET routing protocols (focusing on two classification forms), and gives summarized comparisons between different classes in the context of their methodologies used, strengths, and limitations of each class scheme compared to other classes. Finally, it extracts the current trends and the challenges for efficient routing mechanisms in VANETs.',\n",
       " 'd012519a924e41aa7ff49d9b6be58033bd60fd9c': 'OBJECTIVE\\nTo predict hospital admission at the time of ED triage using patient history in addition to information collected at triage.\\n\\n\\nMETHODS\\nThis retrospective study included all adult ED visits between March 2014 and July 2017 from one academic and two community emergency rooms that resulted in either admission or discharge. A total of 972 variables were extracted per patient visit. Samples were randomly partitioned into training (80%), validation (10%), and test (10%) sets. We trained a series of nine binary classifiers using logistic regression (LR), gradient boosting (XGBoost), and deep neural networks (DNN) on three dataset types: one using only triage information, one using only patient history, and one using the full set of variables. Next, we tested the potential benefit of additional training samples by training models on increasing fractions of our data. Lastly, variables of importance were identified using information gain as a metric to create a low-dimensional model.\\n\\n\\nRESULTS\\nA total of 560,486 patient visits were included in the study, with an overall admission risk of 29.7%. Models trained on triage information yielded a test AUC of 0.87 for LR (95% CI 0.86-0.87), 0.87 for XGBoost (95% CI 0.87-0.88) and 0.87 for DNN (95% CI 0.87-0.88). Models trained on patient history yielded an AUC of 0.86 for LR (95% CI 0.86-0.87), 0.87 for XGBoost (95% CI 0.87-0.87) and 0.87 for DNN (95% CI 0.87-0.88). Models trained on the full set of variables yielded an AUC of 0.91 for LR (95% CI 0.91-0.91), 0.92 for XGBoost (95% CI 0.92-0.93) and 0.92 for DNN (95% CI 0.92-0.92). All algorithms reached maximum performance at 50% of the training set or less. A low-dimensional XGBoost model built on ESI level, outpatient medication counts, demographics, and hospital usage statistics yielded an AUC of 0.91 (95% CI 0.91-0.91).\\n\\n\\nCONCLUSION\\nMachine learning can robustly predict hospital admission using triage information and patient history. The addition of historical information improves predictive performance significantly compared to using triage information alone, highlighting the need to incorporate these variables into prediction models.',\n",
       " '89a523135fc9cb3b0eb6cade2d1eab1b17ea42f4': 'We present a new three-dimensional inventory of the southern San Francisco Bay area faults and use it to calculate stress applied principally by the 1989 M=7.1 Loma Prieta earthquake, and to compare fault seismicity rates before and after 1989. The major high-angle right-lateral faults exhibit a different response to the stress change than do minor oblique (rightlateral/thrust) faults. Seismicity on oblique-slip faults in the southern Santa Clara Valley thrust belt increased where the faults were unclamped. The strong dependence of seismicity change on normal stress change implies a high coefficient of static friction. In contrast, we observe that faults with significant offset (> 50-100 km) behave differently; microseismicity on the Hayward fault diminished where right-lateral shear stress was reduced, and where it was unclamped by the Loma Prieta earthquake. We observe a similar response on the San Andreas fault zone in southern California after the Landers earthquake sequence. Additionally, the offshore San Gregorio fault shows a seismicity rate increase where right-lateral/oblique shear stress was increased by the Loma Prieta earthquake despite also being clamped by it. These responses are consistent with either a low coefficient of static friction or high pore fluid pressures within the fault zones. We can explain the different behavior of the two styles of faults if those with large cumulative offset become impermeable through gouge buildup; coseismically pressurized pore fluids could be trapped and negate imposed normal stress changes, whereas in more limited offset faults fluids could rapidly escape. The difference in behavior between minor and major faults may explain why frictional failure criteria that apply intermediate coefficients of static friction can be effective in describing the broad distributions of aftershocks that follow large earthquakes, since many of these events occur both inside and outside major fault zones.',\n",
       " '5d6959c6d37ed3cc910cd436865a4c2a73284c7c': 'Arterial stiffness index is one of the biomechanical indices of vascular healthiness. These indexes are based on detailed pulse waveform analysis which is presented here. After photoplethysmographyic (PPG) pulse wave measurement, we decompose the pulse waveform for the estimation and determination of arterial elasticity. Firstly, it is electro-optically measured PPG signal and by electromechanical film (EMFi) measured signal that are analyzed and investigated by dividing each wave into five logarithmic normal function components. For both the PPG and EMFi waveform we can find very easily a good fit between the original and overlapped and summed wave components. Each wave component is assumed to resemble certain phenomenon in the arteries and certain indexes can be calculated for example based on the mutual timing of the components. Several studies have demonstrated that these kinds of indexes calculated based on actual biomechanical processed can predict future cardiovascular events. Many dynamic factors, e.g., arterial stiffness, depend on fixed structural features of the arterial wall. For more accurate description, arterial stiffness is estimated based on pulse wave decomposition analysis in the radial measured by EMFi and PPG method and tibial arterial walls measured by PPG method parallelly. Elucidation of the precise relationship between endothelial function and arterial stiffness can be done through biomechanics. However, arterial wall elasticity awaits still further biomechanical studies with clinical relations and the influence of arterial flexibility, resistance and ageing inside of the radial pulse waveform.',\n",
       " 'cb0c85c4eb75016a7098ca0c452e13812b9c95e9': \"Iterated learning describes the process whereby an individual learns their behaviour by exposure to another individual's behaviour, who themselves learnt it in the same way. It can be seen as a key mechanism of cultural evolution. We review various methods for understanding how behaviour is shaped by the iterated learning process: computational agent-based simulations; mathematical modelling; and laboratory experiments in humans and non-human animals. We show how this framework has been used to explain the origins of structure in language, and argue that cultural evolution must be considered alongside biological evolution in explanations of language origins.\",\n",
       " 'dc53c638f58bf3982c5a6ed82002d56c955763c2': 'Finding the most interesting correlations among items is essential for problems in many commercial, medical, and scientific domains. For example, what kinds of items should be recommended with regard to what has been purchased by a customer? How to arrange the store shelf in order to increase sales? How to partition the whole social network into several communities for successful advertising campaigns? Which set of individuals on a social network should we target to convince in order to trigger a large cascade of further adoptions? When conducting correlation analysis, traditional methods have both effectiveness and efficiency problems, which will be addressed in this dissertation. Here, we explore the effectiveness problem in three ways. First, we expand the set of desirable properties and study the property satisfaction for different correlation measures. Second, we study different techniques to adjust original correlation measure, and propose two new correlation measures: the Simplified χ with Continuity Correction and the Simplified χ with Support. Third, we study the upper and lower bounds of different measures and categorize them by the bound differences. Combining with the above three directions, we provide guidelines for users to choose the proper measure according to their situations. With the proper correlation measure, we start to solve the efficiency problem for a large dataset. Here, we propose a fully-correlated itemset (FCI) framework to decouple the correlation measure from the need for efficient search. By wrapping the desired measure in our FCI framework, we take advantage of the desired measure’s superiority in evaluating itemsets, eliminate itemsets with irrelevant items, and achieve good computational performance. In addition, we identify a 1-dimensional monotone property of the upper bound of any good correlation measure, and different 2-dimensional',\n",
       " 'afad9773e74db1927cff4c284dee8afbc4fb849d': 'In this paper, corporate-feed circularly polarized microstrip array antennas are studied. The antenna element is a series-feed slot-coupled structure. Series feeding causes sequential rotation effect at the element level. Antenna elements are then used to form the subarray by applying sequential rotation to their feeding. Arrays having 4, 16, and 64 elements were made. The maximum achieved gains are 15.3, 21, and 25.4 dBic, respectively. All arrays have less than 15 dB return loss and 3 dB axial ratio from 10 to 13 GHz. The patterns are all quite symmetrical.',\n",
       " '5a131856df045cf27a2d5056cea2d2401e2d81b2': 'Social bots are automatic or semi-automatic computer programs that mimic humans and/or human behavior in online social networks. Social bots can attack users (targets) in online social networks to pursue a variety of latent goals, such as to spread information or to influence targets. Without a deep understanding of the nature of such attacks or the susceptibility of users, the potential of social media as an instrument for facilitating discourse or democratic processes is in jeopardy. In this paper, we study data from the Social Bot Challenge 2011 an experiment conducted by the WebEcologyProject during 2011 in which three teams implemented a number of social bots that aimed to influence user behavior on Twitter. Using this data, we aim to develop models to (i) identify susceptible users among a set of targets and (ii) predict users’ level of susceptibility. We explore the predictiveness of three different groups of features (network, behavioral and linguistic features) for these tasks. Our results suggest that susceptible users tend to use Twitter for a conversational purpose and tend to be more open and social since they communicate with many different users, use more social words and show more affection than non-susceptible users.',\n",
       " '618c47bc44c3b6fc27067211214afccce6f7cd2c': 'Motivated by the increasing industry trends towards autonomous driving, vehicles, and transportation we focus on developing a traffic analysis framework for the automatic exploitation of a large pool of available data relative to traffic applications. We propose a cooperative detection and tracking algorithm for the retrieval of vehicle trajectories in video surveillance footage based on deep CNN features that is ultimately used for two separate traffic analysis modalities: (a) vehicle speed estimation based on a state of the art fully automatic camera calibration algorithm and (b) the detection of possibly abnormal events in the scene using robust optical flow descriptors of the detected vehicles and Fisher vector representations of spatiotemporal visual volumes. Finally we measure the performance of our proposed methods in the NVIDIA AI CITY challenge evaluation dataset.',\n",
       " '2230381a078241c4385bb9c20b385e8f0da70b9b': 'We investigate the prospect of using bicoherence features for blind image splicing detection. Image splicing is an essential operation for digital photomontaging, which in turn is a technique for creating image forgery. We examine the properties of bicoherence features on a data set, which contains image blocks of diverse image properties. We then demonstrate the limitation of the baseline bicoherence features for image splicing detection. Our investigation has led to two suggestions for improving the performance of bicoherence features, i.e., estimating the bicoherence features of the authentic counterpart and incorporating features that characterize the variance of the feature performance. The features derived from the suggestions are evaluated with support vector machine (SVM) classification and is shown to improve the image splicing detection accuracy from 62% to about 70%.',\n",
       " '1f0396c08e8485358b3d1e13f451ed12ecfc1a77': 'We build a conversational agent which knowledge base is an online forum for parents of autistic children. We collect about 35,000 threads totalling some 600,000 replies, and label 1% of them for usefulness using Amazon Mechanical Turk. We train a Random Forest Classifier using sent2vec features to label the remaining thread replies. Then, we use word2vec to match user queries conceptually with a thread, and then a reply with a predefined context window.',\n",
       " 'ade7178613e4db90d6a551cb372aebae4c4fa0bf': 'Cyber attacks occur on a near daily basis and are becoming exponentially more common. While some research aims to detect the characteristics of an attack, little focus has been given to patterns of attacks in general. This paper aims to exploit temporal correlations between the number of attacks per day in order to predict future intensity of cyber incidents. Through analysis of attack data collected from Hackmageddon, correlation was found among reported attack volume in consecutive days. This paper presents a forecasting system that aims to predict the number of cyber attacks on a given day based only on a set of historical attack count data. Our system conducts ARIMA time series forecasting on all previously collected incidents to predict the expected number of attacks on a future date. Our tool is able to use only a subset of data relevant to a specific attack method. Prediction models are dynamically updated over time as new data is collected to improve accuracy. Our system outperforms naive forecasting methods by 14.1% when predicting attacks of any type, and up to 21.2% when forecasting attacks of a specific type. Our system also produces a model which more accurately predicts future cyber attack intensity behavior.',\n",
       " 'c906c9b2daddf67ebd949c71fc707d697065c6a0': 'Function recognition in program binaries serves as the foundation for many binary instrumentation and analysis tasks. However, as binaries are usually stripped before distribution, function information is indeed absent in most binaries. By far, identifying functions in stripped binaries remains a challenge. Recent research work proposes to recognize functions in binary code through machine learning techniques. The recognition model, including typical function entry point patterns, is automatically constructed through learning. However, we observed that as previous work only leverages syntax-level features to train the model, binary obfuscation techniques can undermine the pre-learned models in real-world usage scenarios. In this paper, we propose FID, a semantics-based method to recognize functions in stripped binaries. We leverage symbolic execution to generate semantic information and learn the function recognition model through well-performing machine learning techniques.FID extracts semantic information from binary code and, therefore, is effectively adapted to different compilers and optimizations. Moreover, we also demonstrate that FID has high recognition accuracy on binaries transformed by widely-used obfuscation techniques. We evaluate FID with over four thousand test cases. Our evaluation shows that FID is comparable with previous work on normal binaries and it notably outperforms existing tools on obfuscated code.',\n",
       " 'df6b604d1352d4bd81604730f9000d7a29574384': 'Contemporary museums are much more than places devoted to the placement and the exhibition of collections and artworks; indeed, they are nowadays considered as a privileged means for communication and play a central role in making culture accessible to the mass audience. One of the keys to approach the general public is the use of new technologies and novel interaction paradigms. These means, which bring with them an undeniable appeal, allow curators to modulate the cultural proposal by structuring different courses for different user profiles. Immersive Virtual reality (VR) is probably one of the most appealing and potentially effective technologies to serve this purpose; nevertheless, it is still quite uncommon to find immersive installations in museums. Starting from our 10 years’ experience in this topic, and following an in-depth survey about these technologies and their use in cultural contexts, we propose a classification of VR installations, specifically oriented to cultural heritage applications, based on their features in terms of interaction and immersion. On the basis of this classification, aiming to provide a tool for framing VR systems which would hopefully suggest indications related to costs, usability and quality of the sensorial experience, we analyze a series of live examples of which we point out strengths and weak points. We then summarize the current state and the very next future, identifying the major issues that prevent these technologies from being actually widespread, and outline proposals for a more se of pervasive and effective u',\n",
       " '9aade3d26996ce7ef6d657130464504b8d812534': 'In this paper, we present a deep regression approach for face alignment. The deep regressor is a neural network that consists of a global layer and multistage local layers. The global layer estimates the initial face shape from the whole image, while the following local layers iteratively update the shape with local image observations. Combining standard derivations and numerical approximations, we make all layers able to backpropagate error differentials, so that we can apply the standard backpropagation to jointly learn the parameters from all layers. We show that the resulting deep regressor gradually and evenly approaches the true facial landmarks stage by stage, avoiding the tendency that often occurs in the cascaded regression methods and deteriorates the overall performance: yielding early stage regressors with high alignment accuracy gains but later stage regressors with low alignment accuracy gains. Experimental results on standard benchmarks demonstrate that our approach brings significant improvements over previous cascaded regression algorithms.',\n",
       " 'f85ccab7173e543f2bfd4c7a81fb14e147695740': 'We present a robust method to map detected facial Action Units (AUs) to six basic emotions. Automatic AU recognition is prone to errors due to illumination, tracking failures and occlusions. Hence, traditional rule based methods to map AUs to emotions are very sensitive to false positives and misses among the AUs. In our method, a set of chosen AUs are mapped to the six basic emotions using a learned statistical relationship and a suitable matching technique. Relationships between the AUs and emotions are captured as template strings comprising the most discriminative AUs for each emotion. The template strings are computed using a concept called discriminative power. The Longest Common Subsequence (LCS) distance, an approach for approximate string matching, is applied to calculate the closeness of a test string of AUs with the template strings, and hence infer the underlying emotions. LCS is found to be efficient in handling practical issues like erroneous AU detection and helps to reduce false predictions. The proposed method is tested with various databases like CK+, ISL, FACS, JAFFE, MindReading and many real-world video frames. We compare our performance with rule based techniques, and show clear improvement on both benchmark databases and real-world datasets.',\n",
       " '7539293eaadec85917bcfcf4ecc53e7bdd41c227': \"Probabilistic topic models provide an unsupervised method for analyzing unstructured text, which have the potential to be integrated into clinical automatic summarization systems. Clinical documents are accompanied by metadata in a patient's medical history and frequently contains multiword concepts that can be valuable for accurately interpreting the included text. While existing methods have attempted to address these problems individually, we present a unified model for free-text clinical documents that integrates contextual patient- and document-level data, and discovers multi-word concepts. In the proposed model, phrases are represented by chained n-grams and a Dirichlet hyper-parameter is weighted by both document-level and patient-level context. This method and three other Latent Dirichlet allocation models were fit to a large collection of clinical reports. Examples of resulting topics demonstrate the results of the new model and the quality of the representations are evaluated using empirical log likelihood. The proposed model was able to create informative prior probabilities based on patient and document information, and captured phrases that represented various clinical concepts. The representation using the proposed model had a significantly higher empirical log likelihood than the compared methods. Integrating document metadata and capturing phrases in clinical text greatly improves the topic representation of clinical documents. The resulting clinically informative topics may effectively serve as the basis for an automatic summarization system for clinical reports.\",\n",
       " '291c3f4393987f67cded328e984dbae84af643cb': 'OF THESIS FASTER DYNAMIC PROGRAMMING FOR MARKOV DECISION PROCESSES Markov decision processes (MDPs) are a general framework used by Artificial Intelligence (AI) researchers to model decision theoretic planning problems. Solving real world MDPs has been a major and challenging research topic in the AI literature. This paper discusses two main groups of approaches in solving MDPs. The first group of approaches combines the strategies of heuristic search and dynamic programming to expedite the convergence process. The second makes use of graphical structures in MDPs to decrease the effort of classic dynamic programming algorithms. Two new algorithms proposed by the author, MBLAO* and TVI, are described here.',\n",
       " '370063c5491147d88d57bbcd865eb5004484c1eb': 'This article describes and compares four approaches to storing payment keys and executing payment applications on mobile phones via near-field communication at the point of sale. Even though the comparison hinges on security--specifically, how well the keys and payment application are protected against misuse--other criteria such as hardware requirements, availability, management complexity, and performance are also identified and discussed.',\n",
       " '21e480ad39c52d8e770810f8319750a34f8bc091': 'It is traditionally a challenge for home buyers to understand, compare and contrast the investment values of real estates. While a number of estate appraisal methods have been developed to value real property, the performances of these methods have been limited by the traditional data sources for estate appraisal. However, with the development of new ways of collecting estate-related mobile data, there is a potential to leverage geographic dependencies of estates for enhancing estate appraisal. Indeed, the geographic dependencies of the value of an estate can be from the characteristics of its own neighborhood (individual), the values of its nearby estates (peer), and the prosperity of the affiliated latent business area (zone). To this end, in this paper, we propose a geographic method, named ClusRanking, for estate appraisal by leveraging the mutual enforcement of ranking and clustering power. ClusRanking is able to exploit geographic individual, peer, and zone dependencies in a probabilistic ranking model. Specifically, we first extract the geographic utility of estates from geography data, estimate the neighborhood popularity of estates by mining taxicab trajectory data, and model the influence of latent business areas via ClusRanking. Also, we use a linear model to fuse these three influential factors and predict estate investment values. Moreover, we simultaneously consider individual, peer and zone dependencies, and derive an estate-specific ranking likelihood as the objective function. Finally, we conduct a comprehensive evaluation with real-world estate related data, and the experimental results demonstrate the effectiveness of our method.',\n",
       " '044a9cb24e2863c6bcaaf39b7a210fbb11b381e9': \"Users rarely consider running network file systems over slow or wide-area networks, as the performance would be unacceptable and the bandwidth consumption too high. Nonetheless, efficient remote file access would often be desirable over such networks---particularly when high latency makes remote login sessions unresponsive. Rather than run interactive programs such as editors remotely, users could run the programs locally and manipulate remote files through the file system. To do so, however, would require a network file system that consumes less bandwidth than most current file systems.This paper presents LBFS, a network file system designed for low-bandwidth networks. LBFS exploits similarities between files or versions of the same file to save bandwidth. It avoids sending data over the network when the same data can already be found in the server's file system or the client's cache. Using this technique in conjunction with conventional compression and caching, LBFS consumes over an order of magnitude less bandwidth than traditional network file systems on common workloads.\",\n",
       " '964b6997a9c7852deff71d34894dbdc38d34fbdf': 'Delta compression and remote file synchronization techniques are concerned with efficient file transfer over a slow communication link in the case where the receiving party already has a similar file (or files). This problem arises naturally, e.g., when distributing updated versions of software over a network or synchronizing personal files between different accounts and devices. More generally, the problem is becoming increasingly common in many networkbased applications where files and content are widely replicated, frequently modified, and cut and reassembled in different contexts and packagings. In this chapter, we survey techniques, software tools, and applications for delta compression, remote file synchronization, and closely related problems. We first focus on delta compression, where the sender knows all the similar files that are held by the receiver. In the second part, we survey work on the related, but in many ways quite different, problem of remote file synchronization, where the sender does not have a copy of the files held by the receiver. Work supported by NSF CAREER Award NSF CCR-0093400 and by Intel Corporation.',\n",
       " '148ec401da7d5859a9488c0f9a34200de71cc824': 'Caching introduces the overhead and complexity of ensuring consistency, reducing some of its performance benefits. In a distributed system, caching must deal with the additional complications of communication and host failures.\\nLeases are proposed as a time-based mechanism that provides efficient consistent access to cached data in distributed systems. Non-Byzantine failures affect performance, not correctness, with their effect minimized by short leases. An analytic model and an evaluation for file access in the V system show that leases of short duration provide good performance. The impact of leases on performance grows more significant in systems of larger scale and higher processor performance.',\n",
       " '41f1abe566060e53ad93d8cfa8c39ac582256868': 'The state machine approach is a general method for implementing fault-tolerant services in distributed systems. This paper reviews the approach and describes protocols for two different failure models—Byzantine and fail stop. Systems reconfiguration techniques for removing faulty components and integrating repaired components are also discussed.',\n",
       " '46f766c11df69808453e14c900bcb3f4e081fcae': 'In a digital library system, documents are available in digital form and therefore are more easily copied and their copyrights are more easily violated. This is a very serious problem, as it discourages owners of valuable information from sharing it with authorized users. There are two main philosophies for addressing this problem: prevention and detection. The former actually makes unauthorized use of documents difficult or impossible while the latter makes it easier to discover such activity.In this paper we propose a system for registering documents and then detecting copies, either complete copies or partial copies. We describe algorithms for such detection, and metrics required for evaluating detection mechanisms (covering accuracy, efficiency, and security). We also describe a working prototype, called COPS, describe implementation issues, and present experimental results that suggest the proper settings for copy detection parameters.',\n",
       " '6db68f27bcb7c9c001bb0c144c1d0ac5d69a3f3a': 'Copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights. • Users may download and print one copy of any publication from the public portal for the purpose of private study or research. • You may not further distribute the material or use it for any profit-making activity or commercial gain • You may freely distribute the URL identifying the publication in the public portal ? If you believe that this document breaches copyright please contact us providing details, and we will remove access to the work immediately and investigate your claim. May not music be described as the mathematics of the sense, mathematics as music of the reason? The musician feels mathematics, the mathematician thinks music: music the dream, mathematics the working life. Summary The increasing usage of computer-based systems in almost every aspect of our daily life makes more and more dangerous the threat posed by potential attackers , and more and more rewarding a successful attack. Moreover, the complexity of these systems is also increasing, including physical devices, software components and human actors interacting with each other to form so-called socio-technical systems. The importance of socio-technical systems to modern societies requires verifying their security properties formally, while their inherent complexity makes manual analyses impracticable. Graphical models for security offer an unrivalled opportunity to describe socio-technical systems, for they allow to represent different aspects like human behaviour, computation and physical phenomena in an abstract yet uniform manner. Moreover, these models can be assigned a formal semantics, thereby allowing formal verification of their properties. Finally, their appealing graphical notations enable to communicate security concerns in an understandable way also to non-experts, often in charge of the decision making. This dissertation argues that automated techniques can be developed on graphical security models to evaluate qualitative and quantitative security properties of socio-technical systems and to synthesise optimal attack and defence strategies. In support to this claim we develop analysis techniques for widely-used graphical security models such as attack trees and attack-defence trees. Our analyses cope with the optimisation of multiple parameters of an attack and defence scenario. Improving on the literature, in case of conflicting parameters such as probability and cost we compute the set of optimal solutions …',\n",
       " 'e88eec15946dd19bdcf69db882f204386e05ff48': 'Identifying moving objects from a video sequence is a fundamental and critical task in many computer-vision applications. A common approach is to perform background subtraction, which identifies moving objects from the portion of a video frame that differs significantly from a background model. There are many challenges in developing a good background subtraction algorithm. First, it must be robust against changes in illumination. Second, it should avoid detecting non-stationary background objects such as swinging leaves, rain, snow, and shadow cast by moving objects. Finally, its internal background model should react quickly to changes in background such as starting and stopping of vehicles. In this paper, we compare various background subtraction algorithms for detecting moving vehicles and pedestrians in urban traffic video sequences. We consider approaches varying from simple techniques such as frame differencing and adaptive median filtering, to more sophisticated probabilistic modeling techniques. While complicated techniques often produce superior performance, our experiments show that simple techniques such as adaptive median filtering can produce good results with much lower computational complexity.',\n",
       " '2232416778783616736149c870a69beb13cda743': 'In this paper, weinvestigaterecognition of humanfaces in a meetingroom. The major challenges of identifying humanfacesin this environmentincludelow quality of input images,poor illumination,unrestrictedheadposesand continuouslychangingfacial expressionsandocclusion.In order to addresstheseproblemswe proposea novel algorithm, DynamicSpaceWarping (DSW).Thebasic idea of the algorithm is to combinelocal features under certain spatial constraints. We compare DSWwith the eigenface approachondatacollectedfromvariousmeetings.Wehave testedboth front and profile face imagesand imageswith two stagesof occlusion.Theexperimentalresultsindicate thattheDSWapproachoutperformstheeigenfaceapproach in bothcases.',\n",
       " '74c24d7454a2408f766e4d9e507a0e9c3d80312f': 'A smart-card-based user authentication scheme for wireless sensor networks (in short, a SUA-WSN scheme) is designed to restrict access to the sensor data only to users who are in possession of both a smart card and the corresponding password. While a significant number of SUA-WSN schemes have been suggested in recent years, their intended security properties lack formal definitions and proofs in a widely-accepted model. One consequence is that SUA-WSN schemes insecure against various attacks have proliferated. In this paper, we devise a security model for the analysis of SUA-WSN schemes by extending the widely-accepted model of Bellare, Pointcheval and Rogaway (2000). Our model provides formal definitions of authenticated key exchange and user anonymity while capturing side-channel attacks, as well as other common attacks. We also propose a new SUA-WSN scheme based on elliptic curve cryptography (ECC), and prove its security properties in our extended model. To the best of our knowledge, our proposed scheme is the first SUA-WSN scheme that provably achieves both authenticated key exchange and user anonymity. Our scheme is also computationally competitive with other ECC-based (non-provably secure) schemes.',\n",
       " '853860b6472b2c883be4085a3460042fc8b1af3e': 'A key factor contributing to the success of many auto-encoders based deep learning techniques is the implicit consideration of the underlying data manifold in their training criteria. In this paper, we aim to make this consideration more explicit by training auto-encoders completely from the manifold learning perspective. We propose a novel unsupervised manifold learning method termed Laplacian Auto-Encoders (LAEs). Starting from a general regularized function learning framework, LAE regularizes training of autoencoders so that the learned encoding function has the locality-preserving property for data points on the manifold. By exploiting the analog relation between the graph Laplacian and the Laplace–Beltrami operator on the continuous manifold, we derive discrete approximations of the firstand higher-order auto-encoder regularizers that can be applied in practical scenarios, where only data points sampled from the distribution on the manifold are available. Our proposed LAE has potentially better generalization capability, due to its explicit respect of the underlying data manifold. Extensive experiments on benchmark visual classification datasets show that LAE consistently outperforms alternative auto-encoders recently proposed in deep learning literature, especially when training samples are relatively scarce. & 2015 Elsevier B.V. All rights reserved.',\n",
       " '81fd1a1f72963ce16ddebacea82e71dab3d6992a': 'We present an interactive tool for designing physical surfaces made from flexible interlocking quadrilateral elements of a single size and shape. With the element shape fixed, the design task becomes one of finding a discrete structure---i.e., element connectivity and binary orientations---that leads to a desired geometry. In order to address this challenging problem of combinatorial geometry, we propose a forward modeling tool that allows the user to interactively explore the space of feasible designs. Paralleling principles from conventional modeling software, our approach leverages a library of base shapes that can be instantiated, combined, and extended using two fundamental operations: merging and extrusion. In order to assist the user in building the designs, we furthermore propose a method to automatically generate assembly instructions. We demonstrate the versatility of our method by creating a diverse set of digital and physical examples that can serve as personalized lamps or decorative items.',\n",
       " '90a5393b72b85ec21fae9a108ed5dd3e99837701': 'Although computer security technologies are the first line of defence to secure users, their success is dependent on individuals’ behaviour. It is therefore necessary to persuade users to practice good computer security. Our interview analysis of users’ conceptualization of security password guessing attacks, antivirus protection, and mobile online privacy shows that poor understanding of security threats influences users’ motivation and ability to practice safe behaviours. We designed and developed an online interactive comic series called Secure Comics based on instructional design principles to address this problem. An eye-tracking experiment suggests that the graphical and interactive components of the comics direct users’ attention and facilitate comprehension of the information. In our evaluations of Secure Comics, results from several user studies show that the comics improve understanding and motivate positive changes in security management behaviour. We discuss the implication of the findings to better understand the role of instructional design and persuasion in education technology.',\n",
       " '4c9f9f228390d1370e0df91d2565a2559444431d': 'Concurrent programs are prone to various classes of difficult-to-detect faults, of which data races are particularly prevalent. Prior work has attempted to increase the cost-effectiveness of approaches for testing for data races by employing race detection techniques, but to date, no work has considered cost-effective approaches for re-testing for races as programs evolve. In this paper we present SimRT, an automated regression testing framework for use in detecting races introduced by code modifications. SimRT employs a regression test selection technique, focused on sets of program elements related to race detection, to reduce the number of test cases that must be run on a changed program to detect races that occur due to code modifications, and it employs a test case prioritization technique to improve the rate at which such races are detected. Our empirical study of SimRT reveals that it is more efficient and effective for revealing races than other approaches, and that its constituent test selection and prioritization components each contribute to its performance.',\n",
       " '9da15b932df57a8f959471ebc977d620efb18cc1': \"We aim at combining color and shape invariants for indexing and retrieving images. To this end, color models are proposed independent of the object geometry, object pose, and illumination. From these color models, color invariant edges are derived from which shape invariant features are computed. Computational methods are described to combine the color and shape invariants into a unified high-dimensional invariant feature set for discriminatory object retrieval. Experiments have been conducted on a database consisting of 500 images taken from multicolored man-made objects in real world scenes. From the theoretical and experimental results it is concluded that object retrieval based on composite color and shape invariant features provides excellent retrieval accuracy. Object retrieval based on color invariants provides very high retrieval accuracy whereas object retrieval based entirely on shape invariants yields poor discriminative power. Furthermore, the image retrieval scheme is highly robust to partial occlusion, object clutter and a change in the object's pose. Finally, the image retrieval scheme is integrated into the PicToSeek system on-line at http://www.wins.uva.nl/research/isis/PicToSeek/ for searching images on the World Wide Web.\",\n",
       " '3923c0deee252ba10562a4378fc2bbc4885282b3': 'Image forensics aims to detect the manipulation of digital images. Currently, splicing detection, copy-move detection, and image retouching detection are attracting significant attention from researchers. However, image editing techniques develop over time. An emerging image editing technique is colorization, in which grayscale images are colorized with realistic colors. Unfortunately, this technique may also be intentionally applied to certain images to confound object recognition algorithms. To the best of our knowledge, no forensic technique has yet been invented to identify whether an image is colorized. We observed that, compared with natural images, colorized images, which are generated by three state-of-the-art methods, possess statistical differences for the hue and saturation channels. Besides, we also observe statistical inconsistencies in the dark and bright channels, because the colorization process will inevitably affect the dark and bright channel values. Based on our observations, i.e., potential traces in the hue, saturation, dark, and bright channels, we propose two simple yet effective detection methods for fake colorized images: Histogram-based fake colorized image detection and feature encoding-based fake colorized image detection. Experimental results demonstrate that both proposed methods exhibit a decent performance against multiple state-of-the-art colorization approaches.',\n",
       " '63db36cb0b5c8dad17a0c02ab95fde805d585513': 'In the current study, the development and initial validation of the Suitability for Short-Term Cognitive Therapy (SSCT) interview procedure is reported. The SSCT is an interview and rating procedure designed to evaluate the potential appropriateness of patients for short-term cognitive therapy with an interpersonal focus. It consists of a 1-hour, semistructured interview, focused on eliciting information from the patient relevant to nine selection criteria. The procedures involved in the development of this scale are described in detail, and preliminary evidence suggesting that the selection criteria can be rated reliably is presented. In addition, data indicating that scores on the SSCT scale predict the outcome of short-term cognitive therapy on multiple dependent measures, including both therapist and patient perspectives, are reported. It is concluded that the SSCT is a potentially useful scale for identifying patients who may be suitable, or unsuitable, for the type of short-term cognitive therapy administered in the present study.',\n",
       " '2dc18f661b400033abd1086b917c451d3358aef2': 'A major ambition of artificial intelligence lies in translating patient data to successful therapies. Machine learning models face particular challenges in biomedicine, however, including handling of extreme data heterogeneity and lack of mechanistic insight into predictions. Here, we argue for \"visible\" approaches that guide model structure with experimental biology.',\n",
       " '09e941ab733b2c6c26261cb85f00f145d9063b0b': 'SUMMARIST is an attempt to create a robust automated text summarization system, based on the ‘equation’: summarization = topic identification + interpretation + generation. Each of these stages contains several independent modules, many of them trained on large corpora of text. We describe the system’s architecture and provide details of some of its modules.',\n",
       " '03ce7c63eea901962dfae539b3ca6c77d65c5c38': 'The ability to carry out signal processing, classification, recognition, and computation in artificial spiking neural networks (SNNs) is mediated by their synapses. In particular, through activity-dependent alteration of their efficacies, synapses play a fundamental role in learning. The mathematical prescriptions under which synapses modify their weights are termed synaptic plasticity rules. These learning rules can be based on abstract computational neuroscience models or on detailed biophysical ones. As these rules are being proposed and developed by experimental and computational neuroscientists, engineers strive to design and implement them in silicon and en masse in order to employ them in complex real-world applications. In this paper, we describe analog very large-scale integration (VLSI) circuit implementations of multiple synaptic plasticity rules, ranging from phenomenological ones (e.g., based on spike timing, mean firing rates, or both) to biophysically realistic ones (e.g., calcium-dependent models). We discuss the application domains, weaknesses, and strengths of various representative approaches proposed in the literature, and provide insight into the challenges that engineers face when designing and implementing synaptic plasticity rules in VLSI technology for utilizing them in real-world applications.',\n",
       " 'bc9f5d844ea6b989feb989cc6c8fc34f721a6b06': 'I propose a network/search view of international trade in differentiated products. I present evidence that supports the view that proximity and common language/colonial ties are more important for differentiated products than for products traded on organized exchanges in matching international buyers and sellers, and that search barriers to trade are higher for differentiated than for homogeneous products. I also discuss alternative explanations for the findings.',\n",
       " '3973e14770350ed54ba1272aa3e19b4d21f5dad3': \"This paper describes the obstacle detection and tracking algorithms developed for Boss, which is Carnegie Mellon University 's winning entry in the 2007 DARPA Urban Challenge. We describe the tracking subsystem and show how it functions in the context of the larger perception system. The tracking subsystem gives the robot the ability to understand complex scenarios of urban driving to safely operate in the proximity of other vehicles. The tracking system fuses sensor data from more than a dozen sensors with additional information about the environment to generate a coherent situational model. A novel multiple-model approach is used to track the objects based on the quality of the sensor data. Finally, the architecture of the tracking subsystem explicitly abstracts each of the levels of processing. The subsystem can easily be extended by adding new sensors and validation algorithms.\",\n",
       " '6a694487451957937adddbd682d3851fabd45626': 'State-of-the-art question answering (QA) systems employ term-density ranking to retrieve answer passages. Such methods often retrieve incorrect passages as relationships among question terms are not considered. Previous studies attempted to address this problem by matching dependency relations between questions and answers. They used strict matching, which fails when semantically equivalent relationships are phrased differently. We propose fuzzy relation matching based on statistical models. We present two methods for learning relation mapping scores from past QA pairs: one based on mutual information and the other on expectation maximization. Experimental results show that our method significantly outperforms state-of-the-art density-based passage retrieval methods by up to 78% in mean reciprocal rank. Relation matching also brings about a 50% improvement in a system enhanced by query expansion.',\n",
       " 'a3676ecae39afd35b1f7075fc630e28cfbb5a188': 'Virtual machine introspection (VMI) describes the method of monitoring and analyzing the state of a virtual machine from the hypervisor level. This lends itself well to security applications, though the hardware virtualization support from Intel and AMD was not designed with VMI in mind. This results in many challenges for developers of hardware-supported VMI systems. This paper describes the design and implementation of our prototype framework, Nitro, for system call tracing and monitoring. Since Nitro is a purely VMI-based system, it remains isolated from attacks originating within the guest operating system and is not directly visible from within the guest. Nitro is extremely flexible as it supports all three system call mechanisms provided by the Intel x86 architecture and has been proven to work in Windows, Linux, 32-bit, and 64-bit environments. The high performance of our system allows for real-time capturing and dissemination of data without hindering usability. This is supported by extensive testing with various guest operating systems. In addition, Nitro is resistant to circumvention attempts due to a construction called hardware rooting. Finally, Nitro surpasses similar systems in both performance and functionality.',\n",
       " '4b31ec67990a5fa81e7c1cf9fa2dbebcb91ded59': 'In the community of sentiment analysis, supervised learning techniques have been shown to perform very well. When transferred to another domain, however, a supervised sentiment classifier often performs extremely bad. This is so-called domain-transfer problem. In this work, we attempt to attack this problem by making the maximum use of both the old-domain data and the unlabeled new-domain data. To leverage knowledge from the old-domain data, we proposed an effective measure, i.e., Frequently Co-occurring Entropy (FCE), to pick out generalizable features that occur frequently in both domains and have similar occurring probability. To gain knowledge from the newdomain data, we proposed Adapted Naïve Bayes (ANB), a weighted transfer version of Naive Bayes Classifier. The experimental results indicate that proposed approach could improve the performance of base classifier dramatically, and even provide much better performance than the transfer-learning baseline, i.e. the Naïve Bayes Transfer Classifier (NTBC).',\n",
       " '654952a9cc4f3526dda8adf220a50a27a5c91449': \"UNLABELLED\\nDendroPy is a cross-platform library for the Python programming language that provides for object-oriented reading, writing, simulation and manipulation of phylogenetic data, with an emphasis on phylogenetic tree operations. DendroPy uses a splits-hash mapping to perform rapid calculations of tree distances, similarities and shape under various metrics. It contains rich simulation routines to generate trees under a number of different phylogenetic and coalescent models. DendroPy's data simulation and manipulation facilities, in conjunction with its support of a broad range of phylogenetic data formats (NEXUS, Newick, PHYLIP, FASTA, NeXML, etc.), allow it to serve a useful role in various phyloinformatics and phylogeographic pipelines.\\n\\n\\nAVAILABILITY\\nThe stable release of the library is available for download and automated installation through the Python Package Index site (http://pypi.python.org/pypi/DendroPy), while the active development source code repository is available to the public from GitHub (http://github.com/jeetsukumaran/DendroPy).\",\n",
       " '6a69cab99a68869b2f6361c6a3004657e2deeae4': 'We describe a method of mobile robot monocular visual navigation, which uses multiple visual cues to detect and segment the ground plane in the robot’s field of view. Corner points are tracked through an image sequence and grouped into coplanar regions using a method which we call an H-based tracker. The H-based tracker employs planar homographys and is initialised by 5-point planar projective invariants. This allows us to detect ground plane patches and the colour within such patches is subsequently modelled. These patches are grown by colour classification to give a ground plane segmentation, which is then used as an input to a new variant of the artificial potential field algorithm.',\n",
       " '13c48b8c10022b4b2262c5d12f255e21f566cecc': 'In this paper, resonant tank design procedure and practical design considerations are presented for a high performance LLC multi-resonant dc-dc converter in a two-stage smart battery charger for neighborhood electric vehicle applications. The multi-resonant converter has been analyzed and its performance characteristics are presented. It eliminates both low and high frequency current ripple on the battery, thus maximizing battery life without penalizing the volume of the charger. Simulation and experimental results are presented for a prototype unit converting 390 V from the input dc link to an output voltage range of 48 V to 72 V dc at 650 W. The prototype achieves a peak efficiency of 96%.',\n",
       " '27229aff757b797d0cae7bead5a236431b253b91': 'We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identification (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.',\n",
       " '21ff1d20dd7b3e6b1ea02036c0176d200ec5626d': 'We introduce a novel loss max-pooling concept for handling imbalanced training data distributions, applicable as alternative loss layer in the context of deep neural networks for semantic image segmentation. Most real-world semantic segmentation datasets exhibit long tail distributions with few object categories comprising the majority of data and consequently biasing the classifiers towards them. Our method adaptively re-weights the contributions of each pixel based on their observed losses, targeting under-performing classification results as often encountered for under-represented object classes. Our approach goes beyond conventional cost-sensitive learning attempts through adaptive considerations that allow us to indirectly address both, inter- and intra-class imbalances. We provide a theoretical justification of our approach, complementary to experimental analyses on benchmark datasets. In our experiments on the Cityscapes and Pascal VOC 2012 segmentation datasets we find consistently improved results, demonstrating the efficacy of our approach.',\n",
       " '87982ff47c0614cf40204970208312abe943641f': 'Recent years have brought a symbolic growth in the volume of research in Sentiment Analysis, mostly on highly subjective text types like movie or product reviews. The main difference these texts have with news articles is that their target is apparently defined and unique across the text. Thence while dealing with news articles, we performed three subtasks namely identifying the target; separation of good and bad news content from the good and bad sentiment expressed on the target and analysis of clearly marked opinion that is expressed explicitly, not needing interpretation or the use of world knowledge. On concluding these tasks, we present our work on mining opinions about three different Indian political parties during elections in the year 2009. We built a Corpus of 689 opinion-rich instances from three different English dailies namely The Hindu, Times of India and Economic Times extracted from 02/ 01/ 2009 to 05/ 01/ 2009 (MM/ DD/ YY). In which (a) we tested the relative suitability of various sentiment analysis methods (both machine learning and lexical based) and (b) we attempted to separate positive or negative opinion from good or bad news. Evaluation includes comparison of three sentiment analysis methods (two machine learning based and one lexical based) and analyzing the choice of certain words used in political text which influence the Sentiments of public in polls. This preliminary experiment will benefit in predicting and forecasting the winning party in forthcoming Indian elections 2014.',\n",
       " '2538e3eb24d26f31482c479d95d2e26c0e79b990': 'We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.',\n",
       " '317deb87586baa4ee7c7b5dfc603ebed94d1da07': 'We propose a new fast purely discriminative algorithm for natural language parsing, based on a “deep” recurrent convolutional graph transformer network (GTN). Assuming a decomposition of a parse tree into a stack of “levels”, the network predicts a level of the tree taking into account predictions of previous levels. Using only few basic text features which leverage word representations from Collobert and Weston (2008), we show similar performance (in F1 score) to existing pure discriminative parsers and existing “benchmark” parsers (like Collins parser, probabilistic context-free grammars based), with a huge speed advantage.',\n",
       " '0354210007fbe92385acf407549b5cacb41b5835': 'The functional architecture of the object vision pathway in the human brain was investigated using functional magnetic resonance imaging to measure patterns of response in ventral temporal cortex while subjects viewed faces, cats, five categories of man-made objects, and nonsense pictures. A distinct pattern of response was found for each stimulus category. The distinctiveness of the response to a given category was not due simply to the regions that responded maximally to that category, because the category being viewed also could be identified on the basis of the pattern of response when those regions were excluded from the analysis. Patterns of response that discriminated among all categories were found even within cortical regions that responded maximally to only one category. These results indicate that the representations of faces and objects in ventral temporal cortex are widely distributed and overlapping.',\n",
       " '04cc04457e09e17897f9256c86b45b92d70a401f': 'Many data such as social networks, movie preferences or knowledge bases are multi-relational, in that they describe multiple relations between entities. While there is a large body of work focused on modeling these data, modeling these multiple types of relations jointly remains challenging. Further, existing approaches tend to breakdown when the number of these types grows. In this paper, we propose a method for modeling large multi-relational datasets, with possibly thousands of relations. Our model is based on a bilinear structure, which captures various orders of interaction of the data, and also shares sparse latent factors across different relations. We illustrate the performance of our approach on standard tensor-factorization datasets where we attain, or outperform, state-of-the-art results. Finally, a NLP application demonstrates our scalability and the ability of our model to learn efficient and semantically meaningful verb representations.',\n",
       " '052b1d8ce63b07fec3de9dbb583772d860b7c769': 'We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.',\n",
       " '50a6b2b84a9d11ed168ce6380ff17e76136cdfe7': 'In this paper we propose three simple, but significant improvements to the OoCS (Out-of-Core Simplification) algorithm of Lindstrom [20] which increase the quality of approximations and extend the applicability of the algorithm to an even larger class of compute systems.The original OoCS algorithm has memory complexity that depends on the size of the output mesh, but no dependency on the size of the input mesh. That is, it can be used to simplify meshes of arbitrarily large size, but the complexity of the output mesh is limited by the amount of memory available. Our first contribution is a version of OoCS that removes the dependency of having enough memory to hold (even) the simplified mesh. With our new algorithm, the whole process is made essentially independent of the available memory on the host computer. Our new technique uses disk instead of main memory, but it is carefully designed to avoid costly random accesses.Our two other contributions improve the quality of the approximations generated by OoCS. We propose a scheme for preserving surface boundaries which does not use connectivity information, and a scheme for constraining the position of the \"representative vertex\" of a grid cell to an optimal position inside the cell.',\n",
       " '22630a79f1c50603c1356f6ac9dc8524a18d4061': \"In this paper, we propose virtual data center (VDC) as the unit of resource allocation for multiple tenants in the cloud. VDCs are more desirable than physical data centers because the resources allocated to VDCs can be rapidly adjusted as tenants' needs change. To enable the VDC abstraction, we design a data center network virtualization architecture called SecondNet. SecondNet achieves scalability by distributing all the virtual-to-physical mapping, routing, and bandwidth reservation state in server hypervisors. Its port-switching based source routing (PSSR) further makes SecondNet applicable to arbitrary network topologies using commodity servers and switches. SecondNet introduces a centralized VDC allocation algorithm for bandwidth guaranteed virtual to physical mapping. Simulations demonstrate that our VDC allocation achieves high network utilization and low time complexity. Our implementation and experiments show that we can build SecondNet on top of various network topologies, and SecondNet provides bandwidth guarantee and elasticity, as designed.\",\n",
       " '5af83b56353c5fba0518c203d192ffb6375cd986': 'We consider the problem of identifying the patients who are diagnosed with highgrade prostate cancer using the histopathology of tumor in a prostate needle biopsy and are at a very high risk of lethal cancer progression. We hypothesize that the morphology of tumor cell nuclei in digital images from the biopsy can be used to predict tumor aggressiveness and posit the presence of metastasis as a surrogate for disease specific mortality. For this purpose, we apply a compositional multiinstance learning approach which encodes images of nuclei through a convolutional neural network, then predicts the presence of metastasis from sets of encoded nuclei. Through experiments on prostate needle biopsies (PNBX) from a patient cohort with known presence (M1 stage, n = 85) or absence (M0 stage, n = 86) of metastatic disease, we obtained an average area under the receiver operating characteristic curve of 0.71± 0.08 for predicting metastatic cases. These results support our hypothesis that information related to metastatic capacity of prostate cancer cells can be obtained through analysis of nuclei and establish a baseline for future research aimed at predicting the risk of future metastatic disease at a time when it might be preventable.',\n",
       " 'd055b799c521b28bd4d6bf2fc905819d8e88207c': 'Design of a microstrip array antenna to achieve dual circular polarization is proposed in this paper. The proposed antenna is a 2×2 array antenna where each patch element is circularly polarized. The feed network has microstrip lines, cross slot lines and air-bridges. The array antenna can excite both right-hand circular polarization (RHCP) and left-hand circular polarization (LHCP) without using any 90° hybrid circuit or PIN diode. “Both-sided MIC Technology” is used to design the feed network as it provides flexibility to place several types of transmission lines on both sides of the dielectric substrate. The design frequency of the proposed array antenna is 10 GHz. The simulated return loss exhibits an impedance bandwidth of greater than 5% and the 3-dB axial ratio bandwidths for both RHCP and LHCP are approximately 1.39%. The structure and the basic operation along with the simulation results of the proposed dual circularly polarized array antenna are demonstrated in this paper.',\n",
       " '2b72dd0d33e0892436394ef7642c6b517a1c71fd': 'Conveying data uncertainty in visualizations is crucial for preventing viewers from drawing conclusions based on untrustworthy data points. This paper proposes a methodology for efficiently generating density plots of uncertain multivariate data sets that draws viewers to preattentively identify values of high certainty while not calling attention to uncertain values. We demonstrate how to augment scatter plots and parallel coordinates plots to incorporate statistically modeled uncertainty and show how to integrate them with existing multivariate analysis techniques, including outlier detection and interactive brushing. Computing high quality density plots can be expensive for large data sets, so we also describe a probabilistic plotting technique that summarizes the data without requiring explicit density plot computation. These techniques have been useful for identifying brain tumors in multivariate magnetic resonance spectroscopy data and we describe how to extend them to visualize ensemble data sets.',\n",
       " '7d9facefffc720079d837aa421ab79d4856e2c88': 'In this letter, we present a novel gripper, whose design was inspired by chuck clamping devices, for transferring heavy objects and assembling parts precisely in industrial applications. The developed gripper is lightweight (0.9 kg), can manipulate heavy payloads (over 23 kgf), and can automatically align its position and posture via a grasping motion. A fingertip design criterion is presented for the position alignment, while a control strategy is presented for the posture alignment. With one actuator, this gripper realized the above features. This letter describes the mathematical analyses and experiments used to validate these key metrics.',\n",
       " '29f07c86886af63f9bf43d089373ac1f7a95ea0e': 'Demand response is a critical part of renewable integration and energy cost reduction goals across the world. Motivated by the need to reduce costs arising from electricity shortage and renewable energy fluctuations, we propose a novel multiarmed bandit mechanism for demand response (MAB-MDR) which makes monetary offers to strategic consumers who have unknown response characteristics, to incetivize reduction in demand. Our work is inspired by a novel connection we make to crowdsourcing mechanisms. The proposed mechanism incorporates realistic features of the demand response problem including time varying and quadratic cost function. The mechanism marries auctions, that allow users to report their preferences, with online algorithms, that allow distribution companies to learn user-specific parameters. We show that MAB-MDR is dominant strategy incentive compatible, individually rational, and achieves sublinear regret. Such mechanisms can be effectively deployed in smart grids using new information and control architecture innovations and lead to welcome savings in energy costs.',\n",
       " '8e7fdb9d3fc0fef1f82f126072fc675e01ce5873': 'Discussions between data analysts and colleagues or clients with no statistical background are difficult, as the analyst often has to teach and explain their statistical and domain knowledge. We investigate work practices of data analysts who collaborate with non-experts, and report findings regarding types of analysis, collaboration and availability of data. Based on these, we have created a tool to enhance collaboration between data analysts and their clients in the initial stages of the analytical process. Sketching time series data allows analysts to discuss expectations for later analysis. We propose function composition rather than freehand sketching, in order to structure the analyst-client conversation by independently expressing expected features in the data. We evaluate the usability of our prototype through two small studies, and report on user feedback for future iterations.',\n",
       " '0567283bc9affd475eae7cebaae658692a64d5a4': 'The intelligent home environment is a well-established example of the Ambient Intelligence application domain. A variety of sensors and actuators can be used to have the home environment adapt towards changing circumstances and user preferences. However, the complexity of how these intelligent home automation systems operate is often beyond the comprehension of non-technical users, and adding new technology to an existing infrastructure is often a burden. In this paper, we present a home automation framework designed based on smart widgets with a model driven methodology that raises the level of abstraction to configure home automation equipment. It aims to simplify user-level home automation management by mapping high-level home automation concepts onto a low-level composition and configuration of the automation building blocks with a reverse mapping to simplify the integration of new equipment into existing home automation systems. Experiments have shown that the mappings we proposed are sufficient to represent household appliances to the end user in a simple way and that new mappings can easily be added to our framework.',\n",
       " 'a79f43246bed540084ca2d1fcf99a68c69820747': 'Text detection and localization in natural scene images is important for content-based image analysis. This problem is challenging due to the complex background, the non-uniform illumination, the variations of text font, size and line orientation. In this paper, we present a hybrid approach to robustly detect and localize texts in natural scene images. A text region detector is designed to estimate the text existing confidence and scale information in image pyramid, which help segment candidate text components by local binarization. To efficiently filter out the non-text components, a conditional random field (CRF) model considering unary component properties and binary contextual component relationships with supervised parameter learning is proposed. Finally, text components are grouped into text lines/words with a learning-based energy minimization method. Since all the three stages are learning-based, there are very few parameters requiring manual tuning. Experimental results evaluated on the ICDAR 2005 competition dataset show that our approach yields higher precision and recall performance compared with state-of-the-art methods. We also evaluated our approach on a multilingual image dataset with promising results.',\n",
       " '1b9de2d1e74fbe49bf852fa495f63c31bb038a31': 'The advent of Oculus Rift indicates the start of a booming era of virtual reality. In order to increase the immersive feeling of interaction with the virtual world, haptic devices allow us to touch and manipulate virtual objects in an intuitive way. In this paper, we introduce a portable and low-cost haptic glove that provides both force and tactile feedback using a direct-control pneumatic concept. To produce force feedback, two inlet ports of a double acting pneumatic cylinder are opened and closed via solenoid DC valves through Pulse-width modulation (PWM) technique. For tactile feedback, an air bladder is actuated using a diaphragm pump via PWM operated solenoid valve. Experiments on a single finger prototype validated that the glove can provide force and tactile feedback with sufficient moving range of the finger joints. The maximum continuous force is 9 Newton and the response time is less than 400ms. The glove is light weighted and easy to be mounted on the index finger. The proposed glove could be potentially used for virtual reality grasping scenarios and for teleoperation of a robotic hand for handling hazardous objects.',\n",
       " 'c6504fbbfcf32854e0bd35eb70539cafbecf332f': 'HTTP Adaptive Streaming (HAS) is an adaptive bitrate streaming technique which is able to adapt to the network conditions using conventional HTTP web servers. An HAS player periodically requests pre-encoded video chunks by sending an HTTP GET message. When the downloading a video chunk is finished, the player estimates the network bandwidth by calculating the goodput and adjusts the video quality based on its estimates. However, the bandwidth estimation in application layer is pretty inaccurate due to its architectural limitation. We show that inaccurate bandwidth estimation in rate adaptation may incur serious rate oscillations which is poor quality-of-experience for users. In this paper, we propose a buffer-based rate adaptation scheme which eliminates the bandwidth estimation step in rate adaptation to provide a smooth playback of HTTP-based streaming. We evaluate the performance of the HAS player implemented in the ns-3 network simulator. Our simulation results show that the proposed scheme significantly improves the stability by replacing bandwidth estimation with buffer occupancy estimation.',\n",
       " '03dc771ebf5b7bc3ccf8c4689d918924da524fe4': 'Physically plausible illumination at real-time framerates is often achieved using approximations. One popular example is ambient occlusion (AO), for which very simple and efficient implementations are used extensively in production. Recent methods approximate AO between nearby geometry in screen space (SSAO). The key observation described in this paper is, that screen-space occlusion methods can be used to compute many more types of effects than just occlusion, such as directional shadows and indirect color bleeding. The proposed generalization has only a small overhead compared to classic SSAO, approximates direct and one-bounce light transport in screen space, can be combined with other methods that simulate transport for macro structures and is visually equivalent to SSAO in the worst case without introducing new artifacts. Since our method works in screen space, it does not depend on the geometric complexity. Plausible directional occlusion and indirect lighting effects can be displayed for large and fully dynamic scenes at real-time frame rates.',\n",
       " '1b656883bed80fdec1d109ae04873540720610fa': 'In this article, we describe the development and validation of a short (10 item) but comprehensive self-report measure of childhood narcissism. The Childhood Narcissism Scale (CNS) is a 1-dimensional measure of stable individual differences in childhood narcissism with strong internal consistency reliability (Studies 1-4). The CNS is virtually unrelated to conventional measures of self-esteem but is positively related to self-appraised superiority, social evaluative concern and self-esteem contingency, agentic interpersonal goals, and emotional extremity (Study 5). Furthermore, the CNS is negatively related to empathic concern and positively related to aggression following ego threat (Study 6). These results suggest that childhood narcissism has similar psychological and interpersonal correlates as adult narcissism. The CNS provides researchers a convenient tool for measuring narcissism in children and young adolescents with strong preliminary psychometric characteristics.',\n",
       " '92c1f538613ff4923a8fa3407a16bed4aed361ac': 'Exploring data requires a fast feedback loop from the analyst to the system, with a latency below about 10 seconds because of human cognitive limitations. When data becomes large or analysis becomes complex, sequential computations can no longer be completed in a few seconds and data exploration is severely hampered. This article describes a novel computation paradigm called Progressive Computation for Data Analysis or more concisely Progressive Analytics, that brings at the programming language level a low-latency guarantee by performing computations in a progressive fashion. Moving this progressive computation at the language level relieves the programmer of exploratory data analysis systems from implementing the whole analytics pipeline in a progressive way from scratch, streamlining the implementation of scalable exploratory data analysis systems. This article describes the new paradigm through a prototype implementation called ProgressiVis, and explains the requirements it implies through examples.',\n",
       " 'bb6508fb4457f09b5e146254220247bc4ea7b71c': 'Human sensing enables intelligent vehicles to provide driver-adaptive support by classifying perceived workload into multiple levels. Objective of this study is to classify driver workload associated with traffic complexity into five levels. We conducted driving experiments in systematically varied traffic complexity levels in a simulator. We recorded driver physiological signals including electrocardiography, electrodermal activity, and electroencephalography. In addition, we integrated driver performance and subjective workload measures. Deep learning based models outperform statistical machine learning methods when dealing with dynamic time-series data with variable sequence lengths. We show that our long short-term memory based recurrent neural network model can classify driver perceived-workload into five classes with an accuracy of 74.5%. Since perceived workload differ between individual drivers for the same traffic situation, our results further highlight the significance of including driver characteristics such as driving style and workload sensitivity to achieve higher classification accuracy.',\n",
       " 'c8e7c2a9201eb6217e62266c9d8c061b5394866e': 'Models for predicting the risk of cardiovascular (CV) events based on individual patient characteristics are important tools for managing patient care. Most current and commonly used risk prediction models have been built from carefully selected epidemiological cohorts. However, the homogeneity and limited size of such cohorts restrict the predictive power and generalizability of these risk models to other populations. Electronic health data (EHD) from large health care systems provide access to data on large, heterogeneous, and contemporaneous patient populations. The unique features and challenges of EHD, including missing risk factor information, non-linear relationships between risk factors and CV event outcomes, and differing effects from different patient subgroups, demand novel machine learning approaches to risk model development. In this paper, we present a machine learning approach based on Bayesian networks trained on EHD to predict the probability of having a CV event within 5\\xa0years. In such data, event status may be unknown for some individuals, as the event time is right-censored due to disenrollment and incomplete follow-up. Since many traditional data mining methods are not well-suited for such data, we describe how to modify both modeling and assessment techniques to account for censored observation times. We show that our approach can lead to better predictive performance than the Cox proportional hazards model (i.e., a regression-based approach commonly used for censored, time-to-event data) or a Bayesian network with ad hoc approaches to right-censoring. Our techniques are motivated by and illustrated on data from a large US Midwestern health care system.',\n",
       " 'fc5a530ea80a3295d0872b85c3991a4d81336a61': 'Currently, Voice Activated Virtual Assistants and Artificial Intelligence technologies are not just about performance or the functionalities they can carry out, it is also about the associated personality. This empirical multi-country study explores the personality perceptions of current VAVA users regarding these technologies. Since this is a rather unexplored territory for research, this study has identified two well-established personality evaluation methodologies, Aaker’s traits approach and Jung’s archetypes, to investigate current perceived personality and future desired personality of the four main Voice Activated Virtual Assistants: Siri, Google Assistant, Cortana and Alexa. Following are a summary of results by each methodology, and an analysis of the commonalities found between the two methodologies.',\n",
       " 'f64d18d4bad30ea544aa828eacfa83208f2b7815': 'Profile-driven personalization based on socio-demographics is currently regarded as the most convenient base for successful personalized advertising. However, signs point to the dormant power of context recognition: Advertising systems that can adapt to the situational context of a consumer will rapidly gain importance. While technologies that can sense the environment are increasingly advanced, questions such as what to sense and how to adapt to a consumer’s context are largely unanswered. In this chapter, we analyze the purchase context of a retail outlet and conceptualize it such that adaptive pervasive advertising applications really deliver on their potential: showing the right message at the right time to the right recipient. full version published as: Bauer, Christine & Spiekermann, Sarah (2011). Conceptualizing Context for Pervasive Advertising. In Müller, Jörg, Alt, Florian, & Michelis, Daniel (Eds.), Pervasive Advertising (pp. 159-183). London: Springer.',\n",
       " '2e92ddcf2e7a9d6c27875ec442637e13753f21a2': 'The connection mechanism between neighboring modules is the most critical subsystem of each module in a modular robot. Here, we describe a strong, lightweight, and solid-state connection method based on heating a low melting point alloy to form reversible soldered connections. No external manipulation is required for forming or breaking connections between adjacent connectors, making this method suitable for reconfigurable systems such as self-reconfiguring modular robots. Energy is only consumed when switching connectivity, and the ability to transfer power and signal through the connector is inherent to the method. Soldering connectors have no moving parts, are orders of magnitude lighter than other connectors, and are readily mass manufacturable. The mechanical strength of the connector is measured as 173 N, which is enough to support many robot modules, and hundreds of connection cycles are performed before failure.',\n",
       " '459fbc416eb9a55920645c741b1e4cce95f39786': 'In this paper, we analyze the numerics of common algorithms for training Generative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train.',\n",
       " '40c8e3894314581d0241162374602d68a6d1f38c': 'A growing body of empirical work measuring different types of cultural traits has shown that culture matters for a variety of economic outcomes. This paper focuses on one specific aspect of the relevance of culture: its relationship to institutions. We review work with a theoretical, empirical, and historical bent to assess the presence of a two-way causal effect between culture and institutions. 1 We thank Benjamin Friedman and Andrei Shleifer for useful conversations and Janet Currie, Steven Durlauf, and six anonymous referees for excellent comments.',\n",
       " '6b894324281bd4b0251549c0e40802d6ca3d0b8f': 'Electrical energy storage is one of the most critical needs of 21st century society. Applications that depend on electrical energy storage include portable electronics, electric vehicles, and devices for renewable energy storage from solar and wind. Lithium-ion (Li-ion) batteries have the highest energy density among the rechargeable battery chemistries. As a result, Li-ion batteries have proven successful in the portable electronics market and will play a significant role in large-scale energy storage. Over the past two decades, Li-ion batteries based on insertion cathodes have reached a cathode capacity of ∼250 mA h g(-1) and an energy density of ∼800 W h kg(-1), which do not meet the requirement of ∼500 km between charges for all-electric vehicles. With a goal of increasing energy density, researchers are pursuing alternative cathode materials such as sulfur and O2 that can offer capacities that exceed those of conventional insertion cathodes, such as LiCoO2 and LiMn2O4, by an order of magnitude (>1500 mA h g(-1)). Sulfur, one of the most abundant elements on earth, is an electrochemically active material that can accept up to two electrons per atom at ∼2.1 V vs Li/Li(+). As a result, sulfur cathode materials have a high theoretical capacity of 1675 mA h g(-1), and lithium-sulfur (Li-S) batteries have a theoretical energy density of ∼2600 W h kg(-1). Unlike conventional insertion cathode materials, sulfur undergoes a series of compositional and structural changes during cycling, which involve soluble polysulfides and insoluble sulfides. As a result, researchers have struggled with the maintenance of a stable electrode structure, full utilization of the active material, and sufficient cycle life with good system efficiency. Although researchers have made significant progress on rechargeable Li-S batteries in the last decade, these cycle life and efficiency problems prevent their use in commercial cells. To overcome these persistent problems, researchers will need new sulfur composite cathodes with favorable properties and performance and new Li-S cell configurations. In this Account, we first focus on the development of novel composite cathode materials including sulfur-carbon and sulfur-polymer composites, describing the design principles, structure and properties, and electrochemical performances of these new materials. We then cover new cell configurations with carbon interlayers and Li/dissolved polysulfide cells, emphasizing the potential of these approaches to advance capacity retention and system efficiency. Finally, we provide a brief survey of efficient electrolytes. The Account summarizes improvements that could bring Li-S technology closer to mass commercialization.',\n",
       " 'e8691980eeb827b10cdfb4cc402b3f43f020bc6a': 'In this paper we propose to solve the problem of Visual Question Answering by using a novel segmentation guided attention based network which we call SegAttendNet. We use image segmentation maps, generated by a Fully Convolutional Deep Neural Network to refine our attention maps and use these refined attention maps to make the model focus on the relevant parts of the image to answer a question. The refined attention maps are used by the LSTM network to learn to produce the answer. We presently train our model on the visual7W dataset and do a category wise evaluation of the 7 question categories. We achieve state of the art results on this dataset and beat the previous benchmark on this dataset by a 1.5% margin improving the question answering accuracy from 54.1% to 55.6% and demonstrate improvements in each of the question categories. We also visualize our generated attention maps and note their improvement over the attention maps generated by the previous best approach.',\n",
       " '07f3f736d90125cb2b04e7408782af411c67dd5a': 'Semantic matching is of central importance to many natural language tasks [2, 28]. A successful matching algorithm needs to adequately model the internal structures of language objects and the interaction between them. As a step toward this goal, we propose convolutional neural network models for matching two sentences, by adapting the convolutional strategy in vision and speech. The proposed models not only nicely represent the hierarchical structures of sentences with their layerby-layer composition and pooling, but also capture the rich matching patterns at different levels. Our models are rather generic, requiring no prior knowledge on language, and can hence be applied to matching tasks of different nature and in different languages. The empirical study on a variety of matching tasks demonstrates the efficacy of the proposed model on a variety of matching tasks and its superiority to competitor models.',\n",
       " '0af737eae02032e66e035dfed7f853ccb095d6f5': 'How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS), paraphrase identification (PI) and textual entailment (TE). Most prior work (i) deals with one individual task by fine-tuning a specific system; (ii) models each sentence’s representation separately, rarely considering the impact of the other sentence; or (iii) relies fully on manually designed, task-specific linguistic features. This work presents a general Attention Based Convolutional Neural Network (ABCNN) for modeling a pair of sentences. We make three contributions. (i) The ABCNN can be applied to a wide variety of tasks that require modeling of sentence pairs. (ii) We propose three attention schemes that integrate mutual influence between sentences into CNNs; thus, the representation of each sentence takes into consideration its counterpart. These interdependent sentence pair representations are more powerful than isolated sentence representations. (iii) ABCNNs achieve state-of-the-art performance on AS, PI and TE tasks. We release code at: https://github.com/yinwenpeng/Answer_Selection.',\n",
       " '1c059493904b2244d2280b8b4c0c7d3ca115be73': \"Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations.\\n We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.\",\n",
       " '468b9055950c428b17f0bf2ff63fe48a6cb6c998': 'Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.',\n",
       " '81eb0a1ea90a6f6d5e7f14cb3397a4ee0f77824a': 'Community-based Question Answering (CQA) has become popular in knowledge sharing sites since it allows users to get answers to complex, detailed, and personal questions directly from other users. Large archives of historical questions and associated answers have been accumulated. Retrieving relevant historical answers that best match a question is an essential component of a CQA service. Most state of the art approaches are based on bag-of-words models, which have been proven successful in a range of text matching tasks, but are insufficient for capturing the important word sequence information in short text matching. In this paper, a new architecture is proposed to more effectively model the complicated matching relations between questions and answers. It utilises a similarity matrix which contains both lexical and sequential information. Afterwards the information is put into a deep architecture to find potentially suitable answers. The experimental study shows its potential in improving matching accuracy of question and answer.',\n",
       " '81ff60a35e57e150875cfdde735fe69d19e9fdc4': 'Recent research in attention has involved three networks of anatomical areas that carry out the functions of orienting, alerting and executive control (including conflict monitoring). There have been extensive cognitive and neuroimaging studies of these networks in adults. We developed an integrated Attention Network Test (ANT) to measure the efficiency of the three networks with adults. We have now adapted this test to study the development of these networks during childhood. The test is a child-friendly version of the flanker task with alerting and orienting cues. We studied the development of the attentional networks in a cross-sectional experiment with four age groups ranging from 6 through 9 (Experiment 1). In a second experiment, we compared children (age 10 years) and adult performance in both child and adults versions of the ANT. Reaction time and accuracy improved at each age interval and positive values were found for the average efficiency of each of the networks. Alertness showed evidence of change up to and beyond age 10, while conflict scores appear stable after age seven and orienting scores do not change in the age range studied. A final experiment with forty 7-year-old children suggested that children like adults showed independence between the three networks under some conditions.',\n",
       " '6c1cabe3f5980cbc50d290c2ed60b9aca624eab8': 'INTRODUCTION\\nMathematical models allow us to extrapolate from current information about the state and progress of an outbreak, to predict the future and, most importantly, to quantify the uncertainty in these predictions. Here, we illustrate these principles in relation to the current H1N1 epidemic.\\n\\n\\nSOURCES OF DATA\\nMany sources of data are used in mathematical modelling, with some forms of model requiring vastly more data than others. However, a good estimation of the number of cases is vitally important.\\n\\n\\nAREAS OF AGREEMENT\\nMathematical models, and the statistical tools that underpin them, are now a fundamental element in planning control and mitigation measures against any future epidemic of an infectious disease. Well-parameterized mathematical models allow us to test a variety of possible control strategies in computer simulations before applying them in reality.\\n\\n\\nAREAS OF CONTROVERSY\\nThe interaction between modellers and public-health practitioners and the level of detail needed for models to be of use.\\n\\n\\nGROWING POINTS\\nThe need for stronger statistical links between models and data.\\n\\n\\nAREAS TIMELY FOR DEVELOPING RESEARCH\\nGreater appreciation by the medical community of the uses and limitations of models and a greater appreciation by modellers of the constraints on public-health resources.',\n",
       " '5e22c4362df3b0accbe04517c41848a2b229efd1': 'A system for predicting the results of football matches that beats the bookmakers’ odds is presented. The predictions for the matches are based on previous results of the teams involved.',\n",
       " 'de93c4f886bdf55bfc1bcaefad648d5996ed3302': 'This chapter examines the state of modern intrusion detection, with a particular emphasis on the emerging approach of data mining. The discussion paralleIs two important aspects of intrusion detection: general detection strategy (misuse detection versus anomaly detection) and data source (individual hosts versus network trafik). Misuse detection attempts to match known patterns of intrusion , while anomaly detection searches for deviations from normal behavior . Between the two approaches, only anomaly detection has the ability to detect unknown attacks. A particularly promising approach to anomaly detection combines association mining with other forms of machine learning such as classification. Moreover, the data source that an intrusion detection system employs significantly impacts the types of attacks it can detect. There is a tradeoff in the level of detailed information available verD. Barbará et al. (ed .), Applications of Data Mining in Computer Security © Kluwer Academic Publishers 2002 s',\n",
       " 'df25eaf576f55c09bb460d67134646fcb422b2ac': 'We consider the problem of data augmentation, i.e., generating artificial samples to extend a given corpus of training data. Specifically, we propose attributed-guided augmentation (AGA) which learns a mapping that allows to synthesize data such that an attribute of a synthesized sample is at a desired value or strength. This is particularly interesting in situations where little data with no attribute annotation is available for learning, but we have access to a large external corpus of heavily annotated samples. While prior works primarily augment in the space of images, we propose to perform augmentation in feature space instead. We implement our approach as a deep encoder-decoder architecture that learns the synthesis function in an end-to-end manner. We demonstrate the utility of our approach on the problems of (1) one-shot object recognition in a transfer-learning setting where we have no prior knowledge of the new classes, as well as (2) object-based one-shot scene recognition. As external data, we leverage 3D depth and pose information from the SUN RGB-D dataset. Our experiments show that attribute-guided augmentation of high-level CNN features considerably improves one-shot recognition performance on both problems.',\n",
       " 'b52abc5f401a6dec62d650f5a2a500f469b9a7c0': 'Problem: The demand of beverages in PET bottles is constantly increasing. In this context, environmental, technological and regulatory aspects set a stronger focus on recycling. Generally, the reuse of recycled material from post-consumer PET bottles in bottle-to-bottle applications is seen as least environmentally harmful. However, closedloop systems are not widely implemented in Europe. Previous research mainly focuses on open-loop recycling systems and generally lacks discussion about the current German and Swedish systems and their challenges. Furthermore, previous studies lack theoretical and practical enhancements for bottle-to-bottle recycling from a managerial perspective. Purpose: The purpose of this study is to compare the PET bottle recycling systems in Germany and Sweden, analyse the main barriers and develop enhancements for closedloop systems. Method: This qualitative study employs a case study strategy about the two cases of Germany and Sweden. In total, 14 semi-structured interviews are conducted with respondents from different industry sectors within the PET bottle recycling systems. The empirical data is categorised and then analysed by pattern matching with the developed theoretical framework. Conclusion: Due to the theoretical and practical commitment to closed-loop recycling, the Swedish PET bottle recycling system outperforms the Germany system. In Germany, bottle-to-bottle recycling is currently performed on a smaller scale without a unified system. The main barriers for bottle-to-bottle recycling are distinguished into (1) quality and material factors, (2) regulatory and legal factors, (3) economic and market factors and (4) factors influenced by consumers. The enhancements for the systems are (1) quality and material factors, (2) regulatory and legal factors, (3) recollection factors and (4) expanding factors. Lastly, the authors provide further recommendations, which are (1) a recycling content symbol on bottle labels, (2) a council for bottle quality in Germany, (3) a quality seal for the holistic systems, (4) a reduction of transportation in Sweden and (5) an increase of consumer awareness on PET bottle consumption.',\n",
       " '9e00005045a23f3f6b2c9fca094930f8ce42f9f6': '',\n",
       " '2ec2f8cd6cf1a393acbc7881b8c81a78269cf5f7': 'We construct multi-modal concept representations by concatenating a skip-gram linguistic representation vector with a visual concept representation vector computed using the feature extraction layers of a deep convolutional neural network (CNN) trained on a large labeled object recognition dataset. This transfer learning approach brings a clear performance gain over features based on the traditional bag-of-visual-word approach. Experimental results are reported on the WordSim353 and MEN semantic relatedness evaluation tasks. We use visual features computed using either ImageNet or ESP Game images.',\n",
       " '94d1a665c0c7fbd017c9f3c50d35992e1c0c1ed0': 'Aphelenchoides fuchsi sp. n. is described and illustrated from bark and wood samples of a weakened Mondell pine in Kermanshah Province, western Iran. The new species has body length of 332 to 400 µm (females) and 365 to 395 µm (males). Lip region set off from body contour. The cuticle is weakly annulated, and there are four lines in the lateral field. The stylet is 8 to 10 μm long and has small basal swellings. The excretory pore is located ca one body diam. posterior to metacorpus valve or 51 to 62 μm from the head. The postuterine sac well developed (60-90 µm). Spicules are relatively short (15-16 μm in dorsal limb) with apex and rostrum rounded, well developed, and the end of the dorsal limb clearly curved ventrad like a hook. The male tail has usual three pairs of caudal papillae (2+2+2) and a well-developed mucro. The female tail is conical, terminating in a complicated step-like projection, usually with many tiny nodular protuberances. The new species belongs to the Group 2 sensu Shahina, category of Aphelenchoides species. Phylogenetic analysis based on small subunit (SSU) and partial large subunit (LSU) sequences of rRNA supported the morphological results.',\n",
       " '4d40a715a51bcca554915ecc5d88005fd56dc1e5': 'In recent years, numerous large-scale seawater desalination plants have been built in water-stressed countries to augment available water resources, and construction of new desalination plants is expected to increase in the near future. Despite major advancements in desalination technologies, seawater desalination is still more energy intensive compared to conventional technologies for the treatment of fresh water. There are also concerns about the potential environmental impacts of large-scale seawater desalination plants. Here, we review the possible reductions in energy demand by state-of-the-art seawater desalination technologies, the potential role of advanced materials and innovative technologies in improving performance, and the sustainability of desalination as a technological solution to global water shortages.',\n",
       " '6180482e02eb79eca6fd2e9b1ee9111d749d5ca2': \"THIS paper presents the development of a bidirectional fabric-based soft pneumatic actuator requiring low fluid pressurization for actuation, which is incorporated into a soft robotic gripper to demonstrate its utility. The bidirectional soft fabric-based actuator is able to provide both flexion and extension. Fabrication of the fabric actuators is simple as compared to the steps involved in traditional silicone-based approach. In addition, the fabric actuators are able to generate comparably larger vertical grip resistive force at lower operating pressure than elastomeric actuators and 3D-printed actuators, being able to generate resistive grip force up to 20N at 120 kPa. Five of the bidirectional soft fabric-based actuators are deployed within a five-fingered soft robotic gripper, complete with five casings and a base. It is capable of grasping a variety of objects with maximum width or diameter closer to its bending curvature. A cutting task involved bimanual manipulation was demonstrated successfully with the gripper. To incorporate intelligent control for such a task, a soft force made completely of compliant material was attached to the gripper, which allows determination of whether the cutting task is completed. To the authors' knowledge, this work is the first study which incorporates two soft robotic grippers for bimanual manipulation with one of the grippers sensorized to provide closed loop control.\",\n",
       " '3f0924241a7deba2b40b0c1ea57a2e3d10c57ae0': 'This second edition of Dr. Grove\\'s book (the original was published in 2008) could arguably be considered a new work. At just under 1,000 pages (including the 11 appendices on the DVD), the second edition is 80% longer than the original. Frankly, the word \"book\" hardly seems adequate, considering the wide range of topics covered. \"Mini-encyclopedia\" seems more appropriate. The hardcover portion of the book comprises 18 chapters, and the DVD includes the aforementioned appendices plus 20 fully worked examples, 125 problems or exercises (with answers), and MATLAB routines for the simulation of many of the algorithms discussed in the main text. Here is a brief overview of the contents: ▸ Chapters 1–3: an overview of the diversity of positioning techniques and navigation systems; fundamentals of coordinate frames, kinematics and earth models; introduction to Kaiman filtering ▸ Chapters 4–6: inertial sensors, inertial navigation, and lower-cost dead reckoning systems ▸ Chapters 7–12: principles of radio positioning, short-, medium-, and long-range radio navigation, as well as extensive coverage of global navigation satellite systems (GNSS) ▸ Chapter 13: environmental feature matching. ▸ Chapters 14–16: various integration topics, including inertial navigation system (INS)/GNSS integration, alignment, zero-velocity updates, and multisensor integration ▸ Chapter 17: fault detection. ▸ Chapter 18: applications and trends. In summary, this book is an excellent reference (with numerous nuggets of wisdom) that should be readily handy on the shelf of every practicing navigation engineer. In the hands of an experienced instructor, the book will also serve students as a great textbook. However, the lack of examples integrated in the main text makes it difficult for the book to serve as a self-study guide for those that are new to the field.',\n",
       " 'b0e7d36c94935fadf3c514903e4340eaa415e4ee': 'For the Internet of Things to finally become a reality, obstacles on different levels need to be overcome. This is especially true for the upcoming challenge of leaving the domain of technical experts and scientists. Devices need to connect to the Internet and be able to offer services. They have to announce and describe these services in machine-understandable ways so that user-facing systems are able to find and utilize them. They have to learn about their physical surroundings, so that they can serve sensing or acting purposes without explicit configuration or programming. Finally, it must be possible to include IoT devices in complex systems that combine local and remote data, from different sources, in novel and surprising ways. We show how all of that is possible today. Our solution uses open standards and state-of-the art protocols to achieve this. It is based on 6LowPAN and CoAP for the communications part, semantic web technologies for meaningful data exchange, autonomous sensor correlation to learn about the environment, and software built around the Linked Data principles to be open for novel and unforeseen applications.',\n",
       " 'a8e656fe16825c47a41df9b28e0c97d4bc8fa58f': 'This article provides a historical overview of educational computing research at MIT from the mid-1960s to the present day, focusing on physical interfaces. It discusses some of the results of this research: electronic toys that help children develop advanced modes of thinking through free-form play. In this historical context, the article then describes and discusses the author’s own research into tangible programming, culminating in the development of the Tangible Programming Bricks system—a platform for creating microworlds for children to explore computation and scientific thinking.',\n",
       " 'f83a207712fd4cf41aded79e9e6c4345ba879128': 'The next generation of AI applications will continuously interact with the environment and learn from these interactions. These applications impose new and demanding systems requirements, both in terms of performance and flexibility. In this paper, we consider these requirements and present Ray—a distributed system to address them. Ray implements a unified interface that can express both task-parallel and actor-based computations, supported by a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler and a distributed and fault-tolerant store to manage the system’s control state. In our experiments, we demonstrate scaling beyond 1.8 million tasks per second and better performance than existing specialized systems for several challenging reinforcement learning applications.',\n",
       " 'aa2213a9f39736f80ccc54b9096e414682afa082': 'Relying on abrupt phase discontinuities, metasurfaces characterized by a transversely inhomogeneous surface impedance profile have been recently explored as an ultrathin platform to generate arbitrary wave fronts over subwavelength thicknesses. Here, we outline fundamental limitations of passive gradient metasurfaces in molding the impinging wave and show that local phase compensation is essentially insufficient to realize arbitrary wave manipulation, but full-wave designs should be considered. These findings represent a critical step towards realistic and highly efficient conformal wave manipulation beyond the scope of ray optics, enabling unprecedented nanoscale light molding.',\n",
       " '8641be8daff5b24e98a0d68138a61456853aef82': 'Self-adaptive systems have the ability to adapt their behavior to dynamic operating conditions. In reaction to changes in the environment, these systems determine the appropriate corrective actions based in part on information about which action will have the best impact on the system. Existing models used to describe the impact of adaptations are either unable to capture the underlying uncertainty and variability of such dynamic environments, or are not compositional and described at a level of abstraction too low to scale in terms of specification effort required for non-trivial systems. In this paper, we address these shortcomings by describing an approach to the specification of impact models based on architectural system descriptions, which at the same time allows us to represent both variability and uncertainty in the outcome of adaptations, hence improving the selection of the best corrective action. The core of our approach is a language equipped with a formal semantics defined in terms of Discrete Time Markov Chains that enables us to describe both the impact of adaptation tactics, as well as the assumptions about the environment. To validate our approach, we show how employing our language can improve the accuracy of predictions used for decision-making in the Rainbow framework for architecture-based self-adaptation.',\n",
       " 'a65e815895bed510c0549957ce6baa129c909813': 'We propose an unsupervised approach to learning non-concatenative morphology, which we apply to induce a lexicon of Arabic roots and pattern templates. The approach is based on the idea that roots and patterns may be revealed through mutually recursive scoring based on hypothesized pattern and root frequencies. After a further iterative refinement stage, morphological analysis with the induced lexicon achieves a root identification accuracy of over 94%. Our approach differs from previous work on unsupervised learning of Arabic morphology in that it is applicable to naturally-written, unvowelled text.',\n",
       " '5da41b7d7b1963cd1e86d99b4d9b86ad6d7a227a': 'This letter presents a Wilkinson power divider operating at a frequency and its first harmonic with unequal power divider ratio. To obtain the unequal property, four groups of 1/6 wavelength transmission lines with different characteristic impedances are needed to match all ports. Theoretically, closed-form equations for the design are derived based on transmission line theory. Experimental results have indicated that all the features of this novel power divider can be fulfilled at f 0 and 2f 0 simultaneously.',\n",
       " '6cd700af0b7953345d831c129a5a4e0d927bfa19': 'Controlling a virtual vehicle is a sensory-motor activity with a specific rendering methodology that depends on the hardware technology and the software in use. We propose a method that computes haptic feedback for the steering wheel. It is best suited for low-cost, fixed-base driving simulators but can be ported to any driving simulator platform. The goal of our method is twofold. 1) It provides an efficient yet simple algorithm to model the steering mechanism using a quadri-polar representation. 2) This model is used to compute the haptic feedback on top of which a tunable haptic augmentation is adjusted to overcome the lack of presence and the unavoidable simulation loop latencies. This algorithm helps the driver to laterally control the virtual vehicle. We also discuss the experimental results that demonstrate the usefulness of our haptic feedback method.',\n",
       " '3f4e71d715fce70c89e4503d747aad11fcac8a43': 'This case study examines three different digital innovation projects within Auto Inc -- a large European automaker. By using the competing values framework as a theoretical lens we explore how dynamic capabilities occur in a firm trying to meet increasing demands in originating and innovating from digitalization. In this digitalization process, our study indicates that established socio-technical congruences are being challenged. More so, we pinpoint the need for organizations to find ways to embrace new experimental learning processes in the era of digitalization. While such a change requires long-term commitment and vision, this study presents three informal enablers for such experimental processes these enablers are timing, persistence, and contacts.',\n",
       " '215b4c25ad34557644b1a177bd5aeac8b2e66bc6': 'Encrypted databases, a popular approach to protecting data from compromised database management systems (DBMS\\'s), use abstract threat models that capture neither realistic databases, nor realistic attack scenarios. In particular, the \"snapshot attacker\" model used to support the security claims for many encrypted databases does not reflect the information about past queries available in any snapshot attack on an actual DBMS.\\n We demonstrate how this gap between theory and reality causes encrypted databases to fail to achieve their \"provable security\" guarantees.',\n",
       " '84cf1178a7526355f323ce0442458de3b3744358': 'In this paper we propose a parallel high performance FFT algorithm based on a multi-dimensional formulation. We use this to solve a commonly encountered FFT based kernel on a distributed memory parallel machine, the IBM scalable parallel system, SP1. The kernel requires a forward FFT computation of an input sequence, multiplication of the transformed data by a coefficient array, and finally an inverse FFT computation of the resultant data. We show that the multidimensional formulation helps in reducing the communication costs and also improves the single node performance by effectively utilizing the memory system of the node. We implemented this kernel on the IBM SP1 and observed a performance of 1.25 GFLOPS on a 64-node machine.',\n",
       " '1f7594d3be7f5c32e117bc669ed898dd0af88aa3': 'A dual-band textile antenna for multiple-input-multiple-output (MIMO) applications, based on substrate-integrated waveguide (SIW) technology, is designed. The fundamental SIW cavity mode is designed to resonate at 2.4 GHz. Meanwhile, the second and third modes are modified and combined by careful placement of a via within the cavity to enable wideband coverage in the 5-GHz WLAN band. The simple antenna topology can be fabricated fully using textiles in a planar form, ensuring reliability and comfort. Numerical and experimental results indicate satisfactory antenna performance when worn on body in terms of impedance bandwidth, radiation efficiency, and specific absorption ratio (SAR). In order to validate its potential for MIMO applications, two elements of the proposed SIW antenna are arranged in six configurations to study the performance in terms of mutual coupling and envelope correlation. It is observed that the placement of the shorted edges of the two elements adjacent to each other produces the lowest mutual coupling and consequently the best envelope correlation.',\n",
       " 'a2204b1ae6109db076a2b3c8d0db8cf390008812': 'Using prospective data from the Dunedin Multidisciplinary Health and Development Study birth cohort, the authors found that adolescents with low self-esteem had poorer mental and physical health, worse economic prospects, and higher levels of criminal behavior during adulthood, compared with adolescents with high self-esteem. The long-term consequences of self-esteem could not be explained by adolescent depression, gender, or socioeconomic status. Moreover, the findings held when the outcome variables were assessed using objective measures and informant reports; therefore, the findings cannot be explained by shared method variance in self-report data. The findings suggest that low self-esteem during adolescence predicts negative real-world consequences during adulthood.',\n",
       " '02bb762c3bd1b3d1ad788340d8e9cdc3d85f33e1': 'We describe a family of caching protocols for distrib-uted networks that can be used to decrease or eliminate the occurrence of hot spots in the network. Our protocols are particularly designed for use with very large networks such as the Internet, where delays caused by hot spots can be severe, and where it is not feasible for every server to have complete information about the current state of the entire network. The protocols are easy to implement using existing network protocols such as TCP/fF’,and require very little overhead. The protocols work with local control, make efficient use of existing resources, and scale gracefully as the network grows. Our caching protocols are based on a special kind of hashing that we call consistent hashing. Roughly speaking, a consistent hash function is one which changes nr.inimaflyas the range of the function changes. Through the development of good consistent hash functions, we are able to develop caching protocols which do not require users to have a current or even consistent view of the network. We believe that consistent hash functions may eventually prove to be useful in other applications such as distributed name servers and/or quorum systems.',\n",
       " '155ca30ef360d66af571eee47c7f60f300e154db': 'Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandability, Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety.',\n",
       " '2a0d27ae5c82d81b4553ea44e81eb986be5fd126': 'The Paxos algorithm, when presented in plain English, is very simple.',\n",
       " '3593269a4bf87a7d0f7aba639a50bc74cb288fb1': 'In this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash-coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency.\\nThe new methods are intended to reduce the amount of space required to contain the hash-coded information from that associated with conventional methods. The reduction in space is accomplished by exploiting the possibility that a small fraction of errors of commission may be tolerable in some applications, in particular, applications in which a large amount of data is involved and a core resident hash area is consequently not feasible using conventional methods.\\nIn such applications, it is envisaged that overall performance could be improved by using a smaller core resident hash area in conjunction with the new methods and, when necessary, by using some secondary and perhaps time-consuming test to “catch” the small fraction of errors associated with the new methods. An example is discussed which illustrates possible areas of application for the new methods.\\nAnalysis of the paradigm problem demonstrates that allowing a small number of test messages to be falsely identified as members of the given set will permit a much smaller hash area to be used without increasing reject time.',\n",
       " '691564e0f19d5f62597adc0720d0e51ddbce9b89': 'A key performance measure for the World Wide Web is the speed with which content is served to users. As traffic on the Web increases, users are faced with increasing delays and failures in data delivery. Web caching is one of the key strategies that has been explored to improve performance. An important issue in many caching systems is how to decide what is cached where at any given time. Solutions have included multicast queries and directory schemes. In this paper, we offer a new Web caching strategy based on consistent hashing. Consistent hashing provides an alternative to multicast and directory schemes, and has several other advantages in load balancing and fault tolerance. Its performance was analyzed theoretically in previous work; in this paper we describe the implementation of a consistent-hashing-based system and experiments that support our thesis that it can provide performance improvements. \\uf8e9 1999 Published by Elsevier Science B.V. All rights reserved.',\n",
       " '215ac9b23a9a89ad7c8f22b5f9a9ad737204d820': 'Recent studies in the literature have shown that syntax remains a significant barrier to novice computer science students in the field. While this syntax barrier is known to exist, whether and how it varies across programming languages has not been carefully investigated. For this article, we conducted four empirical studies on programming language syntax as part of a larger analysis into the, so called, programming language wars. We first present two surveys conducted with students on the intuitiveness of syntax, which we used to garner formative clues on what words and symbols might be easy for novices to understand. We followed up with two studies on the accuracy rates of novices using a total of six programming languages: Ruby, Java, Perl, Python, Randomo, and Quorum. Randomo was designed by randomly choosing some keywords from the ASCII table (a metaphorical placebo). To our surprise, we found that languages using a more traditional C-style syntax (both Perl and Java) did not afford accuracy rates significantly higher than a language with randomly generated keywords, but that languages which deviate (Quorum, Python, and Ruby) did. These results, including the specifics of syntax that are particularly problematic for novices, may help teachers of introductory programming courses in choosing appropriate first languages and in helping students to overcome the challenges they face with syntax.',\n",
       " 'e4edc414773e709e8eb3eddd77b519637f26f9a5': 'For the past 5 years, the ILSVRC competition and the ImageNet dataset have attracted a lot of interest from the Computer Vision community, allowing for state-of-the-art accuracy to grow tremendously. This should be credited to the use of deep artificial neural network designs. As these became more complex, the storage, bandwidth, and compute requirements increased. This means that with a non-distributed approach, even when using the most high-density server available, the training process may take weeks, making it prohibitive. Furthermore, as datasets grow, the representation learning potential of deep networks grows as well by using more complex models. This synchronicity triggers a sharp increase in the computational requirements and motivates us to explore the scaling behaviour on petaflop scale supercomputers. In this paper we will describe the challenges and novel solutions needed in order to train ResNet50 in this large scale environment. We demonstrate above 90% scaling efficiency and a training time of 28 minutes using up to 104K x86 cores. This is supported by software tools from Intel’s ecosystem. Moreover, we show that with regular 90 120 epoch train runs we can achieve a top-1 accuracy as high as 77% for the unmodified ResNet-50 topology. We also introduce the novel Collapsed Ensemble (CE) technique that allows us to obtain a 77.5% top-1 accuracy, similar to that of a ResNet-152, while training a unmodified ResNet-50 topology for the same fixed training budget. All ResNet-50 models as well as the scripts needed to replicate them will be posted shortly. Keywords—deep learning, scaling, convergence, large minibatch, ensembles.',\n",
       " '154d62d97d43243d73352b969b2335caaa6c2b37': 'Evolutionary Learning proceeds by evolving a population of classifiers, from which it generally returns (with some notable exceptions) the single best-of-run classifier as final result. In the meanwhile, Ensemble Learning, one of the most efficient approaches in supervised Machine Learning for the last decade, proceeds by building a population of diverse classifiers. Ensemble Learning with Evolutionary Computation thus receives increasing attention. The Evolutionary Ensemble Learning (EEL) approach presented in this paper features two contributions. First, a new fitness function, inspired by co-evolution and enforcing the classifier diversity, is presented. Further, a new selection criterion based on the classification margin is proposed. This criterion is used to extract the classifier ensemble from the final population only (Off-EEL) or incrementally along evolution (On-EEL). Experiments on a set of benchmark problems show that Off-EEL outperforms single-hypothesis evolutionary learning and state-of-art Boosting and generates smaller classifier ensembles.',\n",
       " '3146fabd5631a7d1387327918b184103d06c2211': 'Person-independent and pose-invariant estimation of eye-gaze is important for situation analysis and for automated video annotation. We propose a fast cascade regression based method that first estimates the location of a dense set of markers and their visibility, then reconstructs face shape by fitting a part-based 3D model. Next, the reconstructed 3D shape is used to estimate a canonical view of the eyes for 3D gaze estimation. The model operates in a feature space that naturally encodes local ordinal properties of pixel intensities leading to photometric invariant estimation of gaze. To evaluate the algorithm in comparison with alternative approaches, three publicly-available databases were used, Boston University Head Tracking, Multi-View Gaze and CAVE Gaze datasets. Precision for head pose and gaze averaged 4 degrees or less for pitch, yaw, and roll. The algorithm outperformed alternative methods in both datasets.',\n",
       " '39773ed3c249a731224b77783a1c1e5f353d5429': 'We investigate sequence machine learning techniques on raw radio signal time-series data. By applying deep recurrent neural networks we learn to discriminate between several application layer traffic types on top of a constant envelope modulation without using an expert demodulation algorithm. We show that complex protocol sequences can be learned and used for both classification and generation tasks using this approach. Keywords—Machine Learning, Software Radio, Protocol Recognition, Recurrent Neural Networks, LSTM, Protocol Learning, Traffic Classification, Cognitive Radio, Deep Learning',\n",
       " 'fb7f39d7d24b30df7b177bca2732ff8c3ade0bc0': 'In sport scenarios like football or basketball, we often deal with central views where only the central circle and some additional primitives like the central line and the central point or a touch line are visible. In this paper we first characterize, from a mathematical point of view, the set of homographies that project a given ellipse into the unit circle, next, using some extra minimal additional information like the knowledge of the position in the image of the central line and central point or a touch line we show a method to fully determine the plane homography. We present some experiments in sport scenarios to show the ability of the proposed method to properly recover the plane homography.',\n",
       " '591b52d24eb95f5ec3622b814bc91ac872acda9e': 'Multilayer connectionist models of memory based on the encoder model using the backpropagation learning rule are evaluated. The models are applied to standard recognition memory procedures in which items are studied sequentially and then tested for retention. Sequential learning in these models leads to 2 major problems. First, well-learned information is forgotten rapidly as new information is learned. Second, discrimination between studied items and new items either decreases or is nonmonotonic as a function of learning. To address these problems, manipulations of the network within the multilayer model and several variants of the multilayer model were examined, including a model with prelearned memory and a context model, but none solved the problems. The problems discussed provide limitations on connectionist models applied to human memory and in tasks where information to be learned is not all available during learning.',\n",
       " '639f02d25eab3794e35b757ef64c6815a8929f84': \"A self-boost charge pump topology is presented for a floating high-side gate drive power supply that features high voltage and current capabilities for use in integrated power electronic modules (IPEMs). The transformerless topology uses a small capacitor to transfer energy to the high-side switch from a single power supply referred to the negative rail. Unlike conventional bootstrap power supplies, no switching of the main phase-leg switches is required to provide power continuously to the high-side gate drive, even if the high-side switch is permanently on. Additional advantages include low parts-count and simple control requirements. A piecewise linear model of the self-boost charge pump is derived and the circuit's operating characteristics are analyzed. Simulation and experimental results are provided to verify the desired operation of the new charge pump circuit. Guidelines are provided to assist with circuit component selection in new applications.\",\n",
       " 'a4bf5c295f0bf4f7f8d5c1e702b62018cca9bc58': 'The purpose of the present study was to examine the relationship between childhood and adolescent physical and sexual abuse before the age of 18 and psychosocial functioning in mid-adolescence (age 15) and early adulthood (age 21) in a representative community sample of young adults. Subjects were 375 participants in an ongoing 17-years longitudinal study. At age 21, nearly 11% reported physical or sexual abuse before age 18. Psychiatric disorders based on DSM-III-R criteria were assessed utilizing the NIMH Diagnostic Interview Schedule, Revised Version (DIS-III-R). Approximately 80% of the abused young adults met DSM-III-R criteria for at least one psychiatric disorder at age 21. Compared to their nonabused counterparts, abused subjects demonstrated significant impairments in functioning both at ages 15 and at 21, including more depressive symptomatology, anxiety, psychiatric disorders, emotional-behavioral problems, suicidal ideation, and suicide attempts. While abused individuals were functioning significantly more poorly overall at ages 15 and 21 than their nonabused peers, gender differences and distinct patterns of impaired functioning emerged. These deficits underscore the need for early intervention and prevention strategies to forestall or minimize the serious consequences of child abuse.',\n",
       " '16e39000918a58e0755dc42abed368b2215c2aed': 'This paper elaborates on the design, implementation and performance evaluation of a prototype Radio Resource Management (RRM) framework for TV white spaces (TVWS) exploitation, under an auction-based approach. The proposed RRM framework is applied in a centralised Cognitive Radio (CR) network architecture, where exploitation of the available TVWS by Secondary Systems is orchestrated via a Spectrum Broker. Efficient RRM framework performance, as a matter of maximum-possible resources utilization and benefit of Spectrum Broker, is achieved by proposing and evaluating an auction-based algorithm. This auction-based algorithm considers both frequency and time domain during TVWS allocation process which was defined as an optimization problem, where maximum payoff of Spectrum Broker is the optimization goal. Experimental tests that were carried-out under controlled conditions environment, verified the validity of the proposed framework, besides identifying fields for further research.',\n",
       " 'd6619b3c0523f0a12168fbce750edeee7b6b8a53': 'Microwaves have been widely used for the modern communication systems, which have advantages in high bit rate transmission and the easiness of compact circuit and antenna design. Gallium Nitride (GaN), featured with high breakdown and high saturation velocity, is one of the promising material for high power and high frequency devices, and a kW-class output power has already been achieved [1]. We have developed the high power and high efficiency GaN HEMTs [2–5], targeting the amplifier for the base transceiver station (BTS). This presentation summarizes our recent works, focusing on the developments for the efficiency boosting and the robustness in high power RF operation.',\n",
       " 'd2f210e3f34d65e3ae66b60e98d9c3a740b3c52a': 'Graph coloring register allocation tries to minimize the total cost of spilled live ranges of variables. Live-range splitting and coalescing are often performed before the coloring to further reduce the total cost. Coalescing of split live ranges, called sub-ranges, can decrease the total cost by lowering the interference degrees of their common interference neighbors. However, it can also increase the total cost because the coalesced sub-ranges can become uncolorable. In this paper, we propose coloring-based coalescing, which first performs trial coloring and next coalesces all copyrelated sub-ranges that were assigned the same color. The coalesced graph is then colored again with the graph coloring register allocation. The rationale is that coalescing of differently colored sub-ranges could result in spilling because there are some interference neighbors that prevent them from being assigned the same color. Experiments on Java programs show that the combination of live-range splitting and coloring-based coalescing reduces the static spill cost by more than 6% on average, comparing to the baseline coloring without splitting. In contrast, well-known iterated and optimistic coalescing algorithms, when combined with splitting, increase the cost by more than 20%. Coloring-based coalescing improves the execution time by up to 15% and 3% on average, while the existing algorithms improve by up to 12% and 1% on average.',\n",
       " '4bbd31803e900aebcdb984523ef3770de3641981': 'Computational Thinking represents a terminology that embraces the complex set of reasoning processes that are held for problem stating and solving through a computational tool. The ability of systematizing problems and solve them by these means is currently being considered a skill to be developed by all students, together with Language, Mathematics and Sciences. Considering that Computer Science has many of its roots on Mathematics, it is reasonable to ponder if and how Mathematics learning can be influenced by offering activities related to Computational Thinking to students. In this sense, this article presents a Systematic Literature Review on reported evidences of Mathematics learning in activities aimed at developing Computational Thinking skills. Forty-two articles which presented didactic activities together with an experimental design to evaluate learning outcomes published from 2006 to 2017 were analyzed. The majority of identified activities used a software tool or hardware device for their development. In these papers, a wide variety of mathematical topics has been being developed, with some emphasis on Planar Geometry and Algebra. Conversion of models and solutions between different semiotic representations is a high level cognitive skill that is most frequently associated to educational outcomes. This review indicated that more recent articles present a higher level of rigor in methodological procedures to assess learning effects. However, joint analysis of evidences from more than one data source is still not frequently used as a validation procedure.',\n",
       " 'e9b87d8ba83281d5ea01e9b9fab14c73b0ae75eb': 'Neuroimagery findings have shown similar cerebral networks associated with imagination and execution of a movement. On the other hand, neuropsychological studies of parietal-lesioned patients suggest that these networks may be at least partly distinct. In the present study, normal subjects were asked to either imagine or execute auditory-cued hand movements. Compared with rest, imagination and execution showed overlapping networks, including bilateral premotor and parietal areas, basal ganglia and cerebellum. However, direct comparison between the two experimental conditions showed that specific cortico-subcortical areas were more engaged in mental simulation, including bilateral premotor, prefrontal, supplementary motor and left posterior parietal areas, and the caudate nuclei. These results suggest that a specific neuronal substrate is involved in the processing of hand motor representations.',\n",
       " '8a718fccc947750580851f10698de1f41f5991f4': 'Cognition arises as a result of coordinated processing among distributed brain regions and disruptions to communication within these neural networks can result in cognitive dysfunction. Cortical disconnection may thus contribute to the declines in some aspects of cognitive functioning observed in healthy aging. Diffusion tensor imaging (DTI) is ideally suited for the study of cortical disconnection as it provides indices of structural integrity within interconnected neural networks. The current review summarizes results of previous DTI aging research with the aim of identifying consistent patterns of age-related differences in white matter integrity, and of relationships between measures of white matter integrity and behavioral performance as a function of adult age. We outline a number of future directions that will broaden our current understanding of these brain-behavior relationships in aging. Specifically, future research should aim to (1) investigate multiple models of age-brain-behavior relationships; (2) determine the tract-specificity versus global effect of aging on white matter integrity; (3) assess the relative contribution of normal variation in white matter integrity versus white matter lesions to age-related differences in cognition; (4) improve the definition of specific aspects of cognitive functioning related to age-related differences in white matter integrity using information processing tasks; and (5) combine multiple imaging modalities (e.g., resting-state and task-related functional magnetic resonance imaging; fMRI) with DTI to clarify the role of cerebral white matter integrity in cognitive aging.',\n",
       " 'd53432934fa78151e7b75c95093c9b0be94b4b9a': 'A new paradigm of the evolving computational intelligence systems (ECIS) is introduced in a generic framework of the knowledge and data integration (KDI). This generalization of the recent advances in the development of evolving fuzzy and neuro-fuzzy models and the more analytical angle of consideration through the prism of knowledge evolution as opposed to the usually used datacentred approach marks the novelty of the present paper. ECIS constitutes a suitable paradigm for adaptive modeling of continuous dynamic processes and tracing the evolution of knowledge. The elements of evolution, such as inheritance and structure development are related to the knowledge and data pattern dynamics and are considered in the context of an individual system/model. Another novelty of this paper consists of the comparison at a conceptual level between the concept of models and knowledge captured by these models evolution and the well known paradigm of evolutionary computation. Although ECIS differs from the concept of evolutionary (genetic) computing, both paradigms heavily borrow from the same source – nature and human evolution. As the origin of knowledge, humans are the best model of an evolving intelligent system. Instead of considering the evolution of population of spices or genes as the evolutionary computation algorithms does the ECIS concentrate on the evolution of a single intelligent system. The aim is to develop the intelligence/knowledge of this system through an evolution using inheritance and modification, upgrade and reduction. This approach is also suitable for the integration of new data and existing models into new models that can be incrementally adapted to future incoming data. This powerful new concept has been recently introduced by the authors in a series of parallel works and is still under intensive development. It forms the conceptual basis for the development of the truly intelligent systems. Another specific of this paper includes bringing together the two working examples of ECIS, namely ECOS and EFS. The ideas are supported by illustrative examples (a synthetic non-linear function for the ECOS case and a benchmark problem of house price modelling from UCI repository for the case of EFS).',\n",
       " '7bdec3d91d8b649f892a779da78428986d8c5e3b': 'As more and more college classrooms utilize online platforms to facilitate teaching and learning activities, analyzing student online behaviors becomes increasingly important for instructors to effectively monitor and manage student progress and performance. In this paper, we present CCVis, a visual analytics tool for analyzing the course clickstream data and exploring student online learning behaviors. Targeting a large college introductory course with over two thousand student enrollments, our goal is to investigate student behavior patterns and discover the possible relationships between student clickstream behaviors and their course performance. We employ higher-order network and structural identity classification to enable visual analytics of behavior patterns from the massive clickstream data. CCVis includes four coordinated views (the behavior pattern, behavior breakdown, clickstream comparative, and grade distribution views) for user interaction and exploration. We demonstrate the effectiveness of CCVis through case studies along with an ad-hoc expert evaluation. Finally, we discuss the limitation and extension of this work.',\n",
       " '104829c56a7f1236a887a6993959dd52aebd86f5': 'The interconnectedness of different actors in the global freight transportation industry has rendered such a system as a large complex system where different sub-systems are interrelated. On such a system, policy-related- exploratory analyses which have predictive capacity are difficult to perform. Although there are many global simulation models for various large complex systems, there is unfortunately very little research aimed to develop a global freight transportation model. In this paper, we present a multi-level framework to develop an integrated model of the global freight transportation system. We employ a system view to incorporate different relevant sub-systems and categorize them in different levels. The fourstep model of freight transport is used as the basic foundation of the framework proposed. In addition to that, we also present the computational framework which adheres to the high level modeling framework to provide a conceptualization of the discrete-event simulation model which will be developed.',\n",
       " 'c22366074e3b243f2caaeb2f78a2c8d56072905e': 'A longitudinally-slotted ridge waveguide antenna array with a compact transverse dimension is presented. To broaden the bandwidth of the array, it is separated into two subarrays fed by a novel compact convex waveguide divider. A 16-element uniform linear array at X-band was fabricated and measured to verify the validity of the design. The measured bandwidth of S11les-15 dB is 14.9% and the measured cross- polarization level is less than -36 dB over the entire bandwidth. This array can be combined with the edge-slotted waveguide array to build a two-dimensional dual-polarization antenna array for the synthetic aperture radar (SAR) application',\n",
       " '09c5b100f289a3993d91a66116e35ee95e99acc0': 't—This paper describes a new method for the segmentation and extraction of cardiac MRI s. Our method is based on the novel use of a 2D bank. By convolving the tagged input image with ilters, the tagging lines are automatically enhanced ted out. We design the Gabor filter bank based on age’s spatial and frequency characteristics. The is a combination of each filter’s response in the bank. We demonstrate that compared to bandpass ds such as HARP, this method results in robust and mentation of the tagging lines.',\n",
       " '41e4eb8fbb335ae70026f4216069f33f8f9bbe53': \"Stepparent-child relationship quality is linked to stepfamily stability and children's well-being. Yet, the literature offers an incomplete understanding of factors that promote high-quality stepparent-child relationships, especially among socio-demographically diverse stepfamilies. In this study, we explore the association between stepfather involvement and stepfather-child relationship quality among a racially diverse and predominately low-income sample of stepfamilies with preadolescent children. Using a subsample of 467 mother-stepfather families from year 9 of the Fragile Families and Child Wellbeing Study, results indicate that stepfather involvement is positively associated with stepfather-child relationship quality. This association is statistically indistinguishable across racial groups, although the association is stronger among children in cohabiting stepfamilies compared to children in married stepfamilies.\",\n",
       " '45063cf2e0116e700da5ca2863c8bb82ad4d64c2': 'Comparing graph databases with traditional, e.g., relational databases, some important database features are often missing there. Particularly, a graph database schema including integrity constraints is not explicitly defined, also a conceptual modelling is not used at all. It is hard to check a consistency of the graph database, because almost no integrity constraints are defined. In the paper, we discuss these issues and present current possibilities and challenges in graph database modelling. Also a conceptual level of a graph database design is considered. We propose a sufficient conceptual model and show its relationship to a graph database model. We focus also on integrity constraints modelling functional dependencies between entity types, which reminds modelling functional dependencies known from relational databases and extend them to conditional functional dependencies.',\n",
       " '6733017c5a01b698cc07b57fa9c9b9207b85cfbc': 'In neuroscience, all kinds of computation models were designed to answer the open question of how sensory stimuli are encoded by neurons and conversely, how sensory stimuli can be decoded from neuronal activities. Especially, functional Magnetic Resonance Imaging (fMRI) studies have made many great achievements with the rapid development of the deep network computation. However, comparing with the goal of decoding orientation, position and object category from activities in visual cortex, accurate reconstruction of image stimuli from human fMRI is a still challenging work. In this paper, the capsule network (CapsNet) architecture based visual reconstruction (CNAVR) method is developed to reconstruct image stimuli. The capsule means containing a group of neurons to perform the better organization of feature structure and representation, inspired by the structure of cortical mini column including several hundred neurons in primates. The high-level capsule features in the CapsNet includes diverse features of image stimuli such as semantic class, orientation, location and so on. We used these features to bridge between human fMRI and image stimuli. We firstly employed the CapsNet to train the nonlinear mapping from image stimuli to high-level capsule features, and from highlevel capsule features to image stimuli again in an end-to-end manner. After estimating the serviceability of each voxel by encoding performance to accomplish the selecting of voxels, we secondly trained the nonlinear mapping from dimension-decreasing fMRI data to high-level capsule features. Finally, we can predict the high-level capsule features with fMRI data, and reconstruct image stimuli with the CapsNet. We evaluated the proposed CNAVR method on the dataset of handwritten digital images, and exceeded about 10% than the accuracy of all existing state-of-the-art methods on the structural similarity index (SSIM).',\n",
       " 'f8be08195b1a7e9e45028eee4844ea2482170a3e': 'The diverse microbial community that inhabits the human gut has an extensive metabolic repertoire that is distinct from, but complements the activity of mammalian enzymes in the liver and gut mucosa and includes functions essential for host digestion. As such, the gut microbiota is a key factor in shaping the biochemical profile of the diet and, therefore, its impact on host health and disease. The important role that the gut microbiota appears to play in human metabolism and health has stimulated research into the identification of specific microorganisms involved in different processes, and the elucidation of metabolic pathways, particularly those associated with metabolism of dietary components and some host-generated substances. In the first part of the review, we discuss the main gut microorganisms, particularly bacteria, and microbial pathways associated with the metabolism of dietary carbohydrates (to short chain fatty acids and gases), proteins, plant polyphenols, bile acids, and vitamins. The second part of the review focuses on the methodologies, existing and novel, that can be employed to explore gut microbial pathways of metabolism. These include mathematical models, omics techniques, isolated microbes, and enzyme assays.',\n",
       " '7ec5f9694bc3d061b376256320eacb8ec3566b77': 'Systems for inducing concept descriptions from examples are valuable tools for assisting in the task of knowledge acquisition for expert systems. This paper presents a description and empirical evaluation of a new induction system, CN2, designed for the efficient induction of simple, comprehensible production rules in domains where problems of poor description language and/or noise may be present. Implementations of the CN2, ID3, and AQ algorithms are compared on three medical classification tasks.',\n",
       " '0d57ba12a6d958e178d83be4c84513f7e42b24e5': 'Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ∼90% scaling efficiency when moving from 8 to 256 GPUs. This system enables us to train visual recognition models on internetscale data with high efficiency.',\n",
       " '22ba26e56fc3e68f2e6a96c60d27d5f721ea00e9': 'Parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks. Following recent work that strongly suggests that most of the critical points encountered when training such networks are saddle points, we find how considering the presence of negative eigenvalues of the Hessian could help us design better suited adaptive learning rate schemes. We show that the popular Jacobi preconditioner has undesirable behavior in the presence of both positive and negative curvature, and present theoretical and empirical evidence that the socalled equilibration preconditioner is comparatively better suited to non-convex problems. We introduce a novel adaptive learning rate scheme, called ESGD, based on the equilibration preconditioner. Our experiments show that ESGD performs as well or better than RMSProp in terms of convergence speed, always clearly improving over plain stochastic gradient descent.',\n",
       " '27da8d31b23f15a8d4feefe0f309dfaad745f8b0': 'Despite their massivesize, successful deep artificial neural networkscan exhibit a remarkably small differencebetween training and test performance. Conventional wisdom attributessmall generalization error either to propertiesof themodel family, or to the regularization techniquesused during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experimentsestablish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simpledepth two neural networksalready haveperfect finitesampleexpressivity assoon as thenumber of parameters exceeds thenumber of datapointsas it usually does in practice. We interpret our experimental findingsby comparison with traditional models.',\n",
       " '8e0eacf11a22b9705a262e908f17b1704fd21fa7': 'We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech—two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system [26]. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.',\n",
       " 'bcdce6325b61255c545b100ef51ec7efa4cced68': 'Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.',\n",
       " '907149ace088dad97fe6a6cadfd0c9260bb75795': 'Introduction Emotion and its physical expression are an integral part of social interaction, informing others about how we are feeling and affecting social outcomes (Vosk, Forehand, and Figueroa 1983). Studies on the physical expression of emotion can be traced back to the 19th century with Darwin’s seminal book “The Expression of the Emotions in Man and Animals” that reveals the key role of facial expressions and body movement in communicating status and emotion (Darwin 1872).',\n",
       " 'eaa6537b640e744216c8ec1272f6db5bbc53e0fe': 'An important aspect of collision avoidance and driver assistance systems, as well as autonomous vehicles, is the tracking of vehicle taillights and the detection of alert signals (turns and brakes). In this paper, we present the design and implementation of a robust and computationally lightweight algorithm for a real-time vision system, capable of detecting and tracking vehicle taillights, recognizing common alert signals using a vehicle-mounted embedded smart camera, and counting the cars passing on both sides of the vehicle. The system is low-power and processes scenes entirely on the microprocessor of an embedded smart camera. In contrast to most existing work that addresses either daytime or nighttime detection, the presented system provides the ability to track vehicle taillights and detect alert signals regardless of lighting conditions. The mobile vision system has been tested in actual traffic scenes and the results obtained demonstrate the performance and the lightweight nature of the algorithm.',\n",
       " 'dd18d4a30cb1f516b62950db44f73589f8083c3e': 'During the past two decades, an important focus of pain research has been the study of chronic pain mechanisms, particularly the processes that lead to the abnormal sensitivity — spontaneous pain and hyperalgesia — that is associated with these states. For some time it has been recognized that inflammatory mediators released from immune cells can contribute to these persistent pain states. However, it has only recently become clear that immune cell products might have a crucial role not just in inflammatory pain, but also in neuropathic pain caused by damage to peripheral nerves or to the CNS.',\n",
       " '5592c7e0225c956419a9a315718a87190b33f4c2': 'Binary weight convolutional neural networks (BCNNs) can achieve near state-of-the-art classification accuracy and have far less computation complexity compared with traditional CNNs using high-precision weights. Due to their binary weights, BCNNs are well suited for vision-based Internet-of-Things systems being sensitive to power consumption. BCNNs make it possible to achieve very high throughput with moderate power dissipation. In this paper, an energy-efficient architecture for BCNNs is proposed. It fully exploits the binary weights and other hardware-friendly characteristics of BCNNs. A judicious processing schedule is proposed so that off-chip I/O access is minimized and activations are maximally reused. To significantly reduce the critical path delay, we introduce optimized compressor trees and approximate binary multipliers with two novel compensation schemes. The latter is able to save significant hardware resource, and almost no computation accuracy is compromised. Taking advantage of error resiliency of BCNNs, an innovative approximate adder is developed, which significantly reduces the silicon area and data path delay. Thorough error analysis and extensive experimental results on several data sets show that the approximate adders in the data path cause negligible accuracy loss. Moreover, algorithmic transformations for certain layers of BCNNs and a memory-efficient quantization scheme are incorporated to further reduce the energy cost and on-chip storage requirement. Finally, the proposed BCNN hardware architecture is implemented with the SMIC 130-nm technology. The postlayout results demonstrate that our design can achieve an energy efficiency over 2.0TOp/s/W when scaled to 65 nm, which is more than two times better than the prior art.',\n",
       " '55ea7bb4e75608115b50b78f2fea6443d36d60cc': \"BACKGROUND\\nThe study attempts to develop an ordinal logistic regression (OLR) model to identify the determinants of child malnutrition instead of developing traditional binary logistic regression (BLR) model using the data of Bangladesh Demographic and Health Survey 2004.\\n\\n\\nMETHODS\\nBased on weight-for-age anthropometric index (Z-score) child nutrition status is categorized into three groups-severely undernourished (< -3.0), moderately undernourished (-3.0 to -2.01) and nourished (≥-2.0). Since nutrition status is ordinal, an OLR model-proportional odds model (POM) can be developed instead of two separate BLR models to find predictors of both malnutrition and severe malnutrition if the proportional odds assumption satisfies. The assumption is satisfied with low p-value (0.144) due to violation of the assumption for one co-variate. So partial proportional odds model (PPOM) and two BLR models have also been developed to check the applicability of the OLR model. Graphical test has also been adopted for checking the proportional odds assumption.\\n\\n\\nRESULTS\\nAll the models determine that age of child, birth interval, mothers' education, maternal nutrition, household wealth status, child feeding index, and incidence of fever, ARI & diarrhoea were the significant predictors of child malnutrition; however, results of PPOM were more precise than those of other models.\\n\\n\\nCONCLUSION\\nThese findings clearly justify that OLR models (POM and PPOM) are appropriate to find predictors of malnutrition instead of BLR models.\",\n",
       " '32f6c0b6f801da365ed39f50a4966cf241bb905e': 'The Centers for Disease Control and Prevention (CDC) in the United States has declared insufficient sleep a \"public health problem.\" Indeed, according to a recent CDC study, more than a third of American adults are not getting enough sleep on a regular basis. However, insufficient sleep is not exclusively a US problem, and equally concerns other industrialised countries such as the United Kingdom, Japan, Germany, or Canada. According to some evidence, the proportion of people sleeping less than the recommended hours of sleep is rising and associated with lifestyle factors related to a modern 24/7 society, such as psychosocial stress, alcohol consumption, smoking, lack of physical activity and excessive electronic media use, among others. This is alarming as insufficient sleep has been found to be associated with a range of negative health and social outcomes, including success at school and in the labour market. Over the last few decades, for example, there has been growing evidence suggesting a strong association between short sleep duration and elevated mortality risks. Given the potential adverse effects of insufficient sleep on health, well-being and productivity, the consequences of sleep-deprivation have far-reaching economic consequences. Hence, in order to raise awareness of the scale of insufficient sleep as a public-health issue, comparative quantitative figures need to be provided for policy- and decision-makers, as well as recommendations and potential solutions that can help tackling the problem.',\n",
       " '506277ae84149b82d215f76bc4f7135400f65b1d': 'We present a video-based gesture dataset and a methodology for annotating video-based gesture datasets. Our dataset consists of user-defined gestures generated by 18 participants from a previous investigation of gesture memorability. We design and use a crowd-sourced classification task to annotate the videos. The results are made available through a web-based visualization that allows researchers and designers to explore the dataset. Finally, we perform an additional descriptive analysis and quantitative modeling exercise that provide additional insights into the results of the original study. To facilitate the use of the presented methodology by other researchers we share the data, the source of the human intelligence tasks for crowdsourcing, a new taxonomy that integrates previous work, and the source code of the visualization tool.',\n",
       " 'aa6da71c3099cd394b9af663cfadce1ef77cb37b': 'In the process of selecting commercial off-the-shelf (COTS) products, it is inevitable to encounter mismatches between COTS products and system requirements. Mismatches occur when COTS attributes do not exactly match our requirements. Many of these mismatches are resolved after selecting a COTS product in order to improve its fitness with the requirements. This paper proposes a decision support approach that aims at addressing COTS mismatches during and after the selection process. Our approach can be integrated with existing COTS selection methods at two stages: (I) When evaluating COTS candidates: our approach is used to estimate the anticipated fitness of the candidates if their mismatches are resolved. This helps to base our COTS selection decisions on the fitness that the COTS candidates will eventually have if selected. (2) After selecting a COTS product: the approach suggests alternative plans for resolving the most appropriate mismatches using suitable actions, such that the most important risk, technical, and resource constraints are met. A case study from the e-services domain is used to illustrate the method and to discuss its added value',\n",
       " '58bd0411bce7df96c44aa3579136eff873b56ac5': 'Earth observation through remote sensing images allows the accurate characterization and identification of materials on the surface from space and airborne platforms. Multiple and heterogeneous image sources can be available for the same geographical region: multispectral, hyperspectral, radar, multitemporal, and multiangular images can today be acquired over a given scene. These sources can be combined/fused to improve classification of the materials on the surface. Even if this type of systems is generally accurate, the field is about to face new challenges: the upcoming constellations of satellite sensors will acquire large amounts of images of different spatial, spectral, angular, and temporal resolutions. In this scenario, multimodal image fusion stands out as the appropriate framework to address these problems. In this paper, we provide a taxonomical view of the field and review the current methodologies for multimodal classification of remote sensing images. We also highlight the most recent advances, which exploit synergies with machine learning and signal processing: sparse methods, kernel-based fusion, Markov modeling, and manifold alignment. Then, we illustrate the different approaches in seven challenging remote sensing applications: 1) multiresolution fusion for multispectral image classification; 2) image downscaling as a form of multitemporal image fusion and multidimensional interpolation among sensors of different spatial, spectral, and temporal resolutions; 3) multiangular image classification; 4) multisensor image fusion exploiting physically-based feature extractions; 5) multitemporal image classification of land covers in incomplete, inconsistent, and vague image sources; 6) spatiospectral multisensor fusion of optical and radar images for change detection; and 7) cross-sensor adaptation of classifiers. The adoption of these techniques in operational settings will help to monitor our planet from space in the very near future.',\n",
       " '9b69889c7d762c04a2d13b112d0b37e4f719ca34': 'Advancing perovskite solar cell technologies toward their theoretical power conversion efficiency (PCE) requires delicate control over the carrier dynamics throughout the entire device. By controlling the formation of the perovskite layer and careful choices of other materials, we suppressed carrier recombination in the absorber, facilitated carrier injection into the carrier transport layers, and maintained good carrier extraction at the electrodes. When measured via reverse bias scan, cell PCE is typically boosted to 16.6% on average, with the highest efficiency of ~19.3% in a planar geometry without antireflective coating. The fabrication of our perovskite solar cells was conducted in air and from solution at low temperatures, which should simplify manufacturing of large-area perovskite devices that are inexpensive and perform at high levels.',\n",
       " '159f32e0d91ef919e94d9b6f1ef13ce9be62155c': 'Text embedding has gained a lot of interests in text classification area. This paper investigates the popular neural document embedding method Paragraph Vector as a source of evidence in document ranking. We focus on the effects of combining knowledge-based with knowledge-free document embeddings for text classification task. We concatenate these two representations so that the classification can be done more accurately. The results of our experiments show that this approach achieves better performances on a popular dataset.',\n",
       " '8db81373f22957d430dddcbdaebcbc559842f0d8': \"A range of applications, from predicting the spread of human and electronic viruses to city planning and resource management in mobile communications, depend on our ability to foresee the whereabouts and mobility of individuals, raising a fundamental question: To what degree is human behavior predictable? Here we explore the limits of predictability in human dynamics by studying the mobility patterns of anonymized mobile phone users. By measuring the entropy of each individual's trajectory, we find a 93% potential predictability in user mobility across the whole user base. Despite the significant differences in the travel patterns, we find a remarkable lack of variability in predictability, which is largely independent of the distance users cover on a regular basis.\",\n",
       " '2bbe9735b81e0978125dad005656503fca567902': \"Kernel rootkits are formidable threats to computer systems. They are stealthy and can have unrestricted access to system resources. This paper presents NumChecker, a new virtual machine (VM) monitor based framework to detect and identify control-flow modifying kernel rootkits in a guest VM. NumChecker detects and identifies malicious modifications to a system call in the guest VM by measuring the number of certain hardware events that occur during the system call's execution. To automatically count these events, NumChecker leverages the hardware performance counters (HPCs), which exist in modern processors. By using HPCs, the checking cost is significantly reduced and the tamper-resistance is enhanced. We implement a prototype of NumChecker on Linux with the kernel-based VM. An HPC-based two-phase kernel rootkit detection and identification technique is presented and evaluated on a number of real-world kernel rootkits. The results demonstrate its practicality and effectiveness.\",\n",
       " 'e7317fd7bd4f31e70351ca801f41d0040558ad83': 'Artificial bee colony algorithm (ABC), which is inspired by the foraging behavior of honey bee swarm, is a biological-inspired optimization. It shows more effective than genetic algorithm (GA), particle swarm optimization (PSO) and ant colony optimization (ACO). However, ABC is good at exploration but poor at exploitation, and its convergence speed is also an issue in some cases. For these insufficiencies, we propose an improved ABC algorithm called I-ABC. In I-ABC, the best-so-far solution, inertia weight and acceleration coefficients are introduced to modify the search process. Inertia weight and acceleration coefficients are defined as functions of the fitness. In addition, to further balance search processes, the modification forms of the employed bees and the onlooker ones are different in the second acceleration coefficient. Experiments show that, for most functions, the I-ABC has a faster convergence speed and ptimization better performances than each of ABC and the gbest-guided ABC (GABC). But I-ABC could not still substantially achieve the best solution for all optimization problems. In a few cases, it could not find better results than ABC or GABC. In order to inherit the bright sides of ABC, GABC and I-ABC, a high-efficiency hybrid ABC algorithm, which is called PS-ABC, is proposed. PS-ABC owns the abilities of prediction and selection. Results show that PS-ABC has a faster convergence speed like I-ABC and better search ability ods f than other relevant meth',\n",
       " '7401611a24f86dffb5b0cd39cf11ee55a4edb32b': 'We present a comparative evaluation of a large number of anomaly detection techniques on a variety of publicly available as well as artificially generated data sets. Many of these are existing techniques while some are slight variants and/or adaptations of traditional anomaly detection techniques to sequence data.',\n",
       " 'd7988bb266bc6653efa4b83dda102e1fc464c1f8': 'Planar and rigid wafer-based electronics are intrinsically incompatible with curvilinear and deformable organisms. Recent development of organic and inorganic flexible and stretchable electronics enabled sensing, stimulation, and actuation of/for soft biological and artificial entities. This review summarizes the enabling technologies of soft sensors and actuators, as well as power sources based on flexible and stretchable electronics. Examples include artificial electronic skins, wearable biosensors and stimulators, electronics-enabled programmable soft actuators, and mechanically compliant power sources. Their potential applications in soft robotics are illustrated in the framework of a five-step human–robot interaction loop. Outlooks of future directions and challenges are provided at the end.',\n",
       " 'a3d638ab304d3ef3862d37987c3a258a24339e05': 'CycleGAN [Zhu et al., 2017] is one recent successful approach to learn a transformation between two image distributions. In a series of experiments, we demonstrate an intriguing property of the model: CycleGAN learns to “hide” information about a source image into the images it generates in a nearly imperceptible, highfrequency signal. This trick ensures that the generator can recover the original sample and thus satisfy the cyclic consistency requirement, while the generated image remains realistic. We connect this phenomenon with adversarial attacks by viewing CycleGAN’s training procedure as training a generator of adversarial examples and demonstrate that the cyclic consistency loss causes CycleGAN to be especially vulnerable to adversarial attacks.',\n",
       " '5b54b6aa8288a1e9713293cec0178e8f3db3de2d': 'In order to simplify the manufacturing process of variable reluctance (VR) resolvers for hybrid electric vehicle/electric vehicle (HEV/EV) applications, a novel VR resolver with nonoverlapping tooth-coil windings is proposed in this paper. A comparison of the winding configurations is first carried out between the existing and the proposed designs, followed by the description of the operating principle. Furthermore, the influence of actual application conditions is investigated by finite-element (FE) analyses, including operating speed and assembling eccentricity. In addition, identical stator and windings of the novel design can be employed in three resolvers of different rotor saliencies. The voltage difference among the three rotor combinations, as well as the detecting accuracy, is further investigated. Finally, prototypes are fabricated and tested to verify the analyses.',\n",
       " '355f9782e9667c19144e137761a7d44977c7a5c2': 'This study examines depression-related chatter on Twitter to glean insight into social networking about mental health. We assessed themes of a random sample (n=2,000) of depression-related tweets (sent 4-11 to 5-4-14). Tweets were coded for expression of DSM-5 symptoms for Major Depressive Disorder (MDD). Supportive or helpful tweets about depression was the most common theme (n=787, 40%), closely followed by disclosing feelings of depression (n=625; 32%). Two-thirds of tweets revealed one or more symptoms for the diagnosis of MDD and/or communicated thoughts or ideas that were consistent with struggles with depression after accounting for tweets that mentioned depression trivially. Health professionals can use our findings to tailor and target prevention and awareness messages to those Twitter users in need.',\n",
       " '69393d1fe9d68b7aeb5dd57741be392d18385e13': 'The Task Force on Organizational Memory presented a report at the Hawaii International Conference for System Sciences in January 1998. The report included perspectives on knowledge-oriented research, conceptual models for organizational memory, and research methodologies for researchers considering work in organizational memory. This paper builds on the ideas originally presented in the 1998 report by examining research presented at HICSS in the general areas of knowledge management, organizational memory and organizational learning in the five years since the original task force report.',\n",
       " 'c171faac12e0cf24e615a902e584a3444fcd8857': '',\n",
       " '5a14949bcc06c0ae9eecd29b381ffce22e1e75b2': \"T he articles in this issue ofDATA BASE were chosen b y Anthony G . Hopwood, who is a professor of accounting and financial reporting at the London Graduate Schoo l of Business Studies . The articles contain important ideas , Professor Hopwood wrote, of significance to all intereste d in information systems, be they practitioners or academics . The authors, with their professional affiliations at th e time, were Chris Argyris, Graduate School of Education , Harvard University; Bo Hedberg and Sten Jonsson, Department of Business Administration, University o f Gothenburg; J . Frisco den Hertog, N . V. Philips' Gloeilampenfabrieken, The Netherlands, and Michael J . Earl, Oxford Centre for Management Studies . The articles appeared originally in Accounting, Organizations and Society, a publication of which Professor Hopwood is editor-in-chief. AOS exists to monitor emergin g developments and to actively encourage new approaches and perspectives .\",\n",
       " 'ae4bb38eaa8fecfddbc9afefa33188ba3cc2282b': 'In this paper, we examine the problem of missing data in high-dimensional datasets by taking into consideration the Missing Completely at Random and Missing at Random mechanisms, as well as the Arbitrary missing pattern. Additionally, this paper employs a methodology based on Deep Learning and Swarm Intelligence algorithms in order to provide reliable estimates for missing data. The deep learning technique is used to extract features from the input data via an unsupervised learning approach by modeling the data distribution based on the input. This deep learning technique is then used as part of the objective function for the swarm intelligence technique in order to estimate the missing data after a supervised fine-tuning phase by minimizing an error function based on the interrelationship and correlation between features in the dataset. The investigated methodology in this paper therefore has longer running times, however, the promising potential outcomes justify the trade-off. Also, basic knowledge of statistics is presumed.',\n",
       " '349119a443223a45dabcda844ac41e37bd1abc77': 'Effective processing of extremely large volumes of spatial data has led to many organizations employing distributed processing frameworks. Apache Spark is one such open-source framework that is enjoying widespread adoption. Within this data space, it is important to note that most of the observational data (i.e., data collected by sensors, either moving or stationary) has a temporal component, or timestamp. In order to perform advanced analytics and gain insights, the temporal component becomes equally important as the spatial and attribute components. In this paper, we detail several variants of a spatial join operation that addresses both spatial, temporal, and attribute-based joins. Our spatial join technique differs from other approaches in that it combines spatial, temporal, and attribute predicates in the join operator.\\n In addition, our spatio-temporal join algorithm and implementation differs from others in that it runs in commercial off-the-shelf (COTS) application. The users of this functionality are assumed to be GIS analysts with little if any knowledge of the implementation details of spatio-temporal joins or distributed processing. They are comfortable using simple tools that do not provide the ability to tweak the configuration of the',\n",
       " '0161e4348a7079e9c37434c5af47f6372d4b412d': 'We propose a method to identify and localize object classes in images. Instead of operating at the pixel level, we advocate the use of superpixels as the basic unit of a class segmentation or pixel localization scheme. To this end, we construct a classifier on the histogram of local features found in each superpixel. We regularize this classifier by aggregating histograms in the neighborhood of each superpixel and then refine our results further by using the classifier in a conditional random field operating on the superpixel graph. Our proposed method exceeds the previously published state-of-the-art on two challenging datasets: Graz-02 and the PASCAL VOC 2007 Segmentation Challenge.',\n",
       " '02227c94dd41fe0b439e050d377b0beb5d427cda': 'Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.',\n",
       " '081651b38ff7533550a3adfc1c00da333a8fe86c': 'Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.',\n",
       " '17facd6efab9d3be8b1681bb2c1c677b2cb02628': 'Transfer learning is established as an effective technology in computer vision for leveraging rich labeled data in the source domain to build an accurate classifier for the target domain. However, most prior methods have not simultaneously reduced the difference in both the marginal distribution and conditional distribution between domains. In this paper, we put forward a novel transfer learning approach, referred to as Joint Distribution Adaptation (JDA). Specifically, JDA aims to jointly adapt both the marginal distribution and conditional distribution in a principled dimensionality reduction procedure, and construct new feature representation that is effective and robust for substantial distribution difference. Extensive experiments verify that JDA can significantly outperform several state-of-the-art methods on four types of cross-domain image classification problems.',\n",
       " '1c734a14c2325cb76783ca0431862c7f04a69268': 'Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias on a standard benchmark. Fine-tuning deep models in a new domain can require a significant amount of data, which for many applications is simply not available. We propose a new CNN architecture which introduces an adaptation layer and an additional domain confusion loss, to learn a representation that is both semantically meaningful and domain invariant. We additionally show that a domain confusion metric can be used for model selection to determine the dimension of an adaptation layer and the best position for the layer in the CNN architecture. Our proposed adaptation method offers empirical performance which exceeds previously published results on a standard benchmark visual domain adaptation task.',\n",
       " '1e21b925b65303ef0299af65e018ec1e1b9b8d60': 'We study the ecological use of analogies in AI. Specifically, we address the problem of transferring a sample in one domain to an analog sample in another domain. Given two related domains, S and T, we would like to learn a generative function G that maps an input sample from S to the domain T, such that the output of a given representation function f, which accepts inputs in either domains, would remain unchanged. Other than f, the training data is unsupervised and consist of a set of samples from each domain, without any mapping between them. The Domain Transfer Network (DTN) we present employs a compound loss function that includes a multiclass GAN loss, an f preserving component, and a regularizing component that encourages G to map samples from T to themselves. We apply our method to visual domains including digits and face images and demonstrate its ability to generate convincing novel images of previously unseen entities, while preserving their identity.',\n",
       " '6918fcbf5c5a86a7ffaf5650080505b95cd6d424': 'In this paper the difference between hierarchical organization and selforganization is investigated. Organization is defined as a structure with a function. But how does the structure affect the function? I will start to examine this by doing two simulations. The idea is to have a given network of agents, which influence their neighbors. How the result differs in three different types of networks, is then explored. In the first simulation, agents try to align with their neighbors. The second simulation is inspired by the ecosystem. Agents take certain products from their neighbors, and transform them into products their neighbors can use.',\n",
       " '891d443dc003ed5f8762373395aacfa9ff895fd4': 'MOVING OBJECT DETECTION, TRACKING AND CLASSIFICATION FOR SMART VIDEO SURVEILLANCE Yiğithan Dedeoğlu M.S. in Computer Engineering Supervisor: Assist. Prof. Dr. Uğur Güdükbay August, 2004 Video surveillance has long been in use to monitor security sensitive areas such as banks, department stores, highways, crowded public places and borders. The advance in computing power, availability of large-capacity storage devices and high speed network infrastructure paved the way for cheaper, multi sensor video surveillance systems. Traditionally, the video outputs are processed online by human operators and are usually saved to tapes for later use only after a forensic event. The increase in the number of cameras in ordinary surveillance systems overloaded both the human operators and the storage devices with high volumes of data and made it infeasible to ensure proper monitoring of sensitive areas for long times. In order to filter out redundant information generated by an array of cameras, and increase the response time to forensic events, assisting the human operators with identification of important events in video by the use of “smart” video surveillance systems has become a critical requirement. The making of video surveillance systems “smart” requires fast, reliable and robust algorithms for moving object detection, classification, tracking and activity analysis. In this thesis, a smart visual surveillance system with real-time moving object detection, classification and tracking capabilities is presented. The system operates on both color and gray scale video imagery from a stationary camera. It can handle object detection in indoor and outdoor environments and under changing illumination conditions. The classification algorithm makes use of the shape of the detected objects and temporal tracking results to successfully categorize objects into pre-defined classes like human, human group and vehicle. The system is also able to detect the natural phenomenon fire in various scenes reliably. The proposed tracking algorithm successfully tracks video objects even in full occlusion cases. In addition to these, some important needs of a robust iii',\n",
       " '38a08fbe5eabbd68db495fa38f4ee506d82095d4': 'Influence Maximization aims to find the top-$(K)$ influential individuals to maximize the influence spread within a social network, which remains an important yet challenging problem. Proven to be NP-hard, the influence maximization problem attracts tremendous studies. Though there exist basic greedy algorithms which may provide good approximation to optimal result, they mainly suffer from low computational efficiency and excessively long execution time, limiting the application to large-scale social networks. In this paper, we present IMGPU, a novel framework to accelerate the influence maximization by leveraging the parallel processing capability of graphics processing unit (GPU). We first improve the existing greedy algorithms and design a bottom-up traversal algorithm with GPU implementation, which contains inherent parallelism. To best fit the proposed influence maximization algorithm with the GPU architecture, we further develop an adaptive K-level combination method to maximize the parallelism and reorganize the influence graph to minimize the potential divergence. We carry out comprehensive experiments with both real-world and sythetic social network traces and demonstrate that with IMGPU framework, we are able to outperform the state-of-the-art influence maximization algorithm up to a factor of 60, and show potential to scale up to extraordinarily large-scale networks.',\n",
       " '1459a6fc833e60ce0f43fe0fc9a48f8f74db77cc': 'We analyze stochastic algorithms for optimizing nonconvex, nonsmooth finite-sum problems, where the nonsmooth part is convex. Surprisingly, unlike the smooth case, our knowledge of this fundamental problem is very limited. For example, it is not known whether the proximal stochastic gradient method with constant minibatch converges to a stationary point. To tackle this issue, we develop fast stochastic algorithms that provably converge to a stationary point for constant minibatches. Furthermore, using a variant of these algorithms, we obtain provably faster convergence than batch proximal gradient descent. Our results are based on the recent variance reduction techniques for convex optimization but with a novel analysis for handling nonconvex and nonsmooth functions. We also prove global linear convergence rate for an interesting subclass of nonsmooth nonconvex functions, which subsumes several recent works.',\n",
       " '19229afbce15d62bcf8d3afe84a2d47a0b6f1939': 'Participatory design has become increasingly engaged in public spheres and everyday life and is no longer solely concerned with the workplace. This is not only a shift from work oriented productive activities to leisure and pleasurable engagements, but also a new milieu for production and innovation and entails a reorientation from \"democracy at work\" to \"democratic innovation\". What democratic innovation entails is currently defined by management and innovation research, which claims that innovation has been democratized through easy access to production tools and lead-users as the new experts driving innovation. We sketch an alternative \"democratizing innovation\" practice more in line with the original visions of participatory design based on our experience of running Malmö Living Labs - an open innovation milieu where new constellations, issues and ideas evolve from bottom-up long-term collaborations amongst diverse stakeholders. Two cases and controversial matters of concern are discussed. The fruitfulness of the concepts \"Things\" (as opposed to objects), \"infrastructuring\" (as opposed to projects) and \"agonistic public spaces\" (as opposed to consensual decision-making) are explored in relation to participatory innovation practices and democracy.',\n",
       " '853331d5c2e4a5c29ff578c012bff7fec7ebd7bc': 'This paper proposes a Myo device that has electromyography (EMG) sensors for detecting electrical activities from different parts of the forearm muscles; it also has a gyroscope and an accelerometer. EMG sensors detect and provide very clear and important data from muscles compared with other types of sensors. The Myo armband sends data from EMG, gyroscope, and accelerometer sensors to a computer via Bluetooth and uses these data to control a virtual robotic arm which was built in Unity 3D. Virtual robotic arms based on EMG, gyroscope, and accelerometer sensors have different features. A robotic arm based on EMG is controlled by using the tension and relaxation of muscles. Consequently, a virtual robotic arm based on EMG is preferred for a hand amputee to a virtual robotic arm based on a gyroscope and an accelerometer',\n",
       " '21786e6ca30849f750656277573ee11fa4d469c5': 'The purpose of this study was to evaluate the physical demands of English Football Association (FA) Premier League soccer of three different positional classifications (defender, midfielder and striker). Computerised time-motion video-analysis using the Bloomfield Movement Classification was undertaken on the purposeful movement (PM) performed by 55 players. Recognition of PM had a good inter-tester reliability strength of agreement (κ= 0.7277). Players spent 40.6 ± 10.0% of the match performing PM. Position had a significant influence on %PM time spent sprinting, running, shuffling, skipping and standing still (p < 0.05). However, position had no significant influence on the %PM time spent performing movement at low, medium, high or very high intensities (p > 0.05). Players spent 48.7 ± 9.2% of PM time moving in a directly forward direction, 20.6 ± 6.8% not moving in any direction and the remainder of PM time moving backward, lateral, diagonal and arced directions. The players performed the equivalent of 726 ± 203 turns during the match; 609 ± 193 of these being of 0° to 90° to the left or right. Players were involved in the equivalent of 111 ± 77 on the ball movement activities per match with no significant differences between the positions for total involvement in on the ball activity (p > 0.05). This study has provided an indication of the different physical demands of different playing positions in FA Premier League match-play through assessment of movements performed by players. Key pointsPlayers spent ~40% of the match performing Pur-poseful Movement (PM).Position had a significant influence on %PM time spent performing each motion class except walking and jogging. Players performed >700 turns in PM, most of these being of 0°-90°.Strikers performed most high to very high intensity activity and most contact situations.Defenders also spent a significantly greater %PM time moving backwards than the other two posi-tions.Different positions could benefit from more specific conditioning programs.',\n",
       " '76737d93659b31d5a6ce07a4e9e5107bc0c39adf': 'Induction of neuroprotective heat-shock proteins via pharmacological Hsp90 inhibitors is currently being investigated as a potential treatment for neurodegenerative diseases. Two major hurdles for therapeutic use of Hsp90 inhibitors are systemic toxicity and limited central nervous system permeability. We demonstrate here that chronic treatment with a proprietary Hsp90 inhibitor compound (OS47720) not only elicits a heat-shock-like response but also offers synaptic protection in symptomatic Tg2576 mice, a model of Alzheimer’s disease, without noticeable systemic toxicity. Despite a short half-life of OS47720 in mouse brain, a single intraperitoneal injection induces rapid and long-lasting (>3 days) nuclear activation of the heat-shock factor, HSF1. Mechanistic study indicates that the remedial effects of OS47720 depend upon HSF1 activation and the subsequent HSF1-mediated transcriptional events on synaptic genes. Taken together, this work reveals a novel role of HSF1 in synaptic function and memory, which likely occurs through modulation of the synaptic transcriptome.',\n",
       " 'd65c2cbc0980d0840b88b569516ae9c277d9d200': 'Financial fraud is an ever growing menace with far consequences in the financial industry. Data mining had played an imperative role in the detection of credit card fraud in online transactions. Credit card fraud detection, which is a data mining problem, becomes challenging due to two major reasons — first, the profiles of normal and fraudulent behaviours change constantly and secondly, credit card fraud data sets are highly skewed. The performance of fraud detection in credit card transactions is greatly affected by the sampling approach on dataset, selection of variables and detection technique(s) used. This paper investigates the performance of naïve bayes, k-nearest neighbor and logistic regression on highly skewed credit card fraud data. Dataset of credit card transactions is sourced from European cardholders containing 284,807 transactions. A hybrid technique of under-sampling and oversampling is carried out on the skewed data. The three techniques are applied on the raw and preprocessed data. The work is implemented in Python. The performance of the techniques is evaluated based on accuracy, sensitivity, specificity, precision, Matthews correlation coefficient and balanced classification rate. The results shows of optimal accuracy for naïve bayes, k-nearest neighbor and logistic regression classifiers are 97.92%, 97.69% and 54.86% respectively. The comparative results show that k-nearest neighbour performs better than naïve bayes and logistic regression techniques.',\n",
       " 'fc4bd8f4db91bbb4053b8174544f79bf67b96b3b': 'This work investigated two different machine learning techniques: Cascade Learning and Deep Learning, to find out which algorithm performs better to detect the number plate of vehicles registered in Bangladesh. To do this, we created a dataset of about 1000 images collected from a security camera of Independent University, Bangladesh. Each image in the dataset were then labelled manually by selecting the Region of Interest (ROI). In the Cascade Learning approach, a sliding window technique was used to detect objects. Then a cascade classifier was employed to determine if the window contained object of interest or not. In the Deep Learning approach, CIFAR-10 dataset was used to pre-train a 15-layer Convolutional Neural Network (CNN). Using this pretrained CNN, a Regions with CNN (R-CNN) was then trained using our dataset. We found that the Deep Learning approach (maximum accuracy 99.60% using 566 training images) outperforms the detector constructed using Cascade classifiers (maximum accuracy 59.52% using 566 positive and 1022 negative training images) for 252 test images.',\n",
       " '049c15a106015b287fec6fc3e8178d4c3f4adf67': 'In this paper, a novel Bayesian image restoration method based on a combination of priors is presented. It is well known that the Total Variation (TV) image prior preserves edge structures while imposing smoothness on the solutions. However, it tends to oversmooth textured areas. To alleviate this problem we propose to combine the TV and the Poisson Singular Integral (PSI) models, which, as we will show, preserves the image textures. The PSI prior depends on a parameter that controls the shape of the filter. A study on the behavior of the filter as a function of this parameter is presented. Our restoration model utilizes a bound for the TV image model based on the majorization–minimization principle, and performs maximum a posteriori Bayesian inference. In order to assess the performance of the proposed approach, in the experimental section we compare it with other restoration methods. & 2013 Elsevier B.V. All rights reserved.',\n",
       " 'ebf35073e122782f685a0d6c231622412f28a53b': 'The last decade has seen an astronomical shift from imaging with DSLR and point-and-shoot cameras to imaging with smartphone cameras. Due to the small aperture and sensor size, smartphone images have notably more noise than their DSLR counterparts. While denoising for smartphone images is an active research area, the research community currently lacks a denoising image dataset representative of real noisy images from smartphone cameras with high-quality ground truth. We address this issue in this paper with the following contributions. We propose a systematic procedure for estimating ground truth for noisy images that can be used to benchmark denoising performance for smartphone cameras. Using this procedure, we have captured a dataset - the Smartphone Image Denoising Dataset (SIDD) - of ~30,000 noisy images from 10 scenes under different lighting conditions using five representative smartphone cameras and generated their ground truth images. We used this dataset to benchmark a number of denoising algorithms. We show that CNN-based methods perform better when trained on our high-quality dataset than when trained using alternative strategies, such as low-ISO images used as a proxy for ground truth data.',\n",
       " '156e7730b8ba8a08ec97eb6c2eaaf2124ed0ce6e': 'Benjamini and Hochberg suggest that the false discovery rate may be the appropriate error rate to control in many applied multiple testing problems. A simple procedure was given there as an FDR controlling procedure for independent test statistics and was shown to be much more powerful than comparable procedures which control the traditional familywise error rate. We prove that this same procedure also controls the false discovery rate when the test statistics have positive regression dependency on each of the test statistics corresponding to the true null hypotheses. This condition for positive dependency is general enough to cover many problems of practical interest, including the comparisons of many treatments with a single control, multivariate normal test statistics with positive correlation matrix and multivariate t. Furthermore, the test statistics may be discrete, and the tested hypotheses composite without posing special difficulties. For all other forms of dependency, a simple conservative modification of the procedure controls the false discovery rate. Thus the range of problems for which a procedure with proven FDR control can be offered is greatly increased.',\n",
       " '7f47767d338eb39664844c94833b52ae73d964ef': 'Inspired by the adequacy of convolutional neural networks in implicit extraction of visual features and the efficiency of Long Short-Term Memory Recurrent Neural Networks in dealing with long-range temporal dependencies, we propose a Convolutional Long Short-Term Memory Recurrent Neural Network (CNNLSTM) for the problem of dynamic gesture recognition. The model is able to successfully learn gestures varying in duration and complexity and proves to be a significant base for further development. Finally, the new gesture command TsironiGR-dataset for human-robot interaction is presented for the evaluation of CNNLSTM.',\n",
       " 'a9b533329845d5d1a31c3ff2821ce9865c440158': \"The mirror neuron system (MNS) has been proposed to play an important role in social cognition by providing a neural mechanism by which others' actions, intentions, and emotions can be understood. Here functional magnetic resonance imaging was used to directly examine the relationship between MNS activity and two distinct indicators of social functioning in typically-developing children (aged 10.1 years+/-7 months): empathy and interpersonal competence. Reliable activity in pars opercularis, the frontal component of the MNS, was elicited by observation and imitation of emotional expressions. Importantly, activity in this region (as well as in the anterior insula and amygdala) was significantly and positively correlated with established behavioral measures indexing children's empathic behavior (during both imitation and observation) and interpersonal skills (during imitation only). These findings suggest that simulation mechanisms and the MNS may indeed be relevant to social functioning in everyday life during typical human development.\",\n",
       " '64887b38c382e331cd2b045f7a7edf05f17586a8': \"We recruited twins systematically from the Australian Twin Registry and assessed their sexual orientation and 2 related traits: childhood gender nonconformity and continuous gender identity. Men and women differed in their distributions of sexual orientation, with women more likely to have slight-to-moderate degrees of homosexual attraction, and men more likely to have high degrees of homosexual attraction. Twin concordances for nonheterosexual orientation were lower than in prior studies. Univariate analyses showed that familial factors were important for all traits, but were less successful in distinguishing genetic from shared environmental influences. Only childhood gender nonconformity was significantly heritable for both men and women. Multivariate analyses suggested that the causal architecture differed between men and women, and, for women, provided significant evidence for the importance of genetic factors to the traits' covariation.\",\n",
       " '4f2d62eaf7559b91b97bab3076fcd5f306da57f2': 'This paper presents a novel and efficient texture-based method for modeling the background and detecting moving objects from a video sequence. Each pixel is modeled as a group of adaptive local binary pattern histograms that are calculated over a circular region around the pixel. The approach provides us with many advantages compared to the state-of-the-art. Experimental results clearly justify our model.',\n",
       " '23ddae93514a47b56dcbeed80e67fab62e8b5ec9': 'In distributed systems shared by multiple tenants, effective resource management is an important pre-requisite to providing quality of service guarantees. Many systems deployed today lack performance isolation and experience contention, slowdown, and even outages caused by aggressive workloads or by improperly throttled maintenance tasks such as data replication. In this work we present Retro, a resource management framework for shared distributed systems. Retro monitors per-tenant resource usage both within and across distributed systems, and exposes this information to centralized resource management policies through a high-level API. A policy can shape the resources consumed by a tenant using Retro’s control points, which enforce sharing and ratelimiting decisions. We demonstrate Retro through three policies providing bottleneck resource fairness, dominant resource fairness, and latency guarantees to high-priority tenants, and evaluate the system across five distributed systems: HBase, Yarn, MapReduce, HDFS, and Zookeeper. Our evaluation shows that Retro has low overhead, and achieves the policies’ goals, accurately detecting contended resources, throttling tenants responsible for slowdown and overload, and fairly distributing the remaining cluster capacity.',\n",
       " '8f81d1854da5f6254780f00966d0c00d174b9881': 'The proportion of the aging population is rapidly increasing around the world, which will cause stress on society and healthcare systems. In recent years, advances in technology have created new opportunities for automatic activities of daily living (ADL) monitoring to improve the quality of life and provide adequate medical service for the elderly. Such automatic ADL monitoring requires reliable ADL information on a fine-grained level, especially for the status of interaction between body gestures and the environment in the real-world. In this work, we propose a significant change spotting mechanism for periodic human motion segmentation during cleaning task performance. A novel approach is proposed based on the search for a significant change of gestures, which can manage critical technical issues in activity recognition, such as continuous data segmentation, individual variance, and category ambiguity. Three typical machine learning classification algorithms are utilized for the identification of the significant change candidate, including a Support Vector Machine (SVM), k-Nearest Neighbors (kNN), and Naive Bayesian (NB) algorithm. Overall, the proposed approach achieves 96.41% in the F1-score by using the SVM classifier. The results show that the proposed approach can fulfill the requirement of fine-grained human motion segmentation for automatic ADL monitoring.',\n",
       " '6c8d5d5eee5967958a2e03a84bcc00f1f81f4d9e': 'High-throughput sequencing has made it theoretically possible to obtain high-quality de novo assembled genome sequences but in practice DNA extracts are often contaminated with sequences from other organisms. Currently, there are few existing methods for rigorously decontaminating eukaryotic assemblies. Those that do exist filter sequences based on nucleotide similarity to contaminants and risk eliminating sequences from the target organism. We introduce a novel application of an established machine learning method, a decision tree, that can rigorously classify sequences. The major strength of the decision tree is that it can take any measured feature as input and does not require a priori identification of significant descriptors. We use the decision tree to classify de novo assembled sequences and compare the method to published protocols. A decision tree performs better than existing methods when classifying sequences in eukaryotic de novo assemblies. It is efficient, readily implemented, and accurately identifies target and contaminant sequences. Importantly, a decision tree can be used to classify sequences according to measured descriptors and has potentially many uses in distilling biological datasets.',\n",
       " '6588070d6578bc8a9d1f284f766340587501d620': 'A variety of real world applications fit into the broad definition of time series classification. Using traditional machine learning approaches such as treating the time series sequences as high dimensional vectors have faced the well known \"curse of dimensionality\" problem. Recently, the field of time series classification has seen success by using preprocessing steps that discretize the time series using a Symbolic Aggregate ApproXimation technique (SAX) and using recurring subsequences (\"motifs\") as features.\\n In this paper we explore a feature construction algorithm based on genetic programming that uses SAX-generated motifs as the building blocks for the construction of more complex features. The research shows that the constructed complex features improve the classification accuracy in a statistically significant manner for many applications.',\n",
       " '6c201c1ded432c98178f1d35410b8958decc884a': 'The constraint of energy consumption is a serious problem in wireless sensor networks (WSNs). In this regard, many solutions for this problem have been proposed in recent years. In one line of research, scholars suggest data driven approaches to help conserve energy by reducing the amount of required communication in the network. This paper is an attempt in this area and proposes that sensors be powered on intermittently. A neural network will then simulate sensors’ data during their idle periods. The success of this method relies heavily on a high correlation between the points making a time series of sensed data. To demonstrate the effectiveness of the idea, we conduct a number of experiments. In doing so, we train a NAR network against various datasets of sensed humidity and temperature in different environments. By testing on actual data, it is shown that the predictions by the device greatly obviate the need for sensed data during sensors’ idle periods and save over 65 percent of energy.',\n",
       " '2c17972edee8cd41f344009dc939cf51260f425a': 'The hypertensive and normotensive Appropriate Blood Pressure Control in Diabetes (ABCD) studies were prospective, randomized, interventional clinical trials with 5 years of follow-up that examined the role of intensive versus standard blood pressure control in a total of 950 patients with type 2 diabetes mellitus. In the hypertensive ABCD study, a significant decrease in mortality was detected in the intensive blood pressure control group when compared with the standard blood pressure control group. There was also a marked reduction in the incidence of myocardial infarction when patients were randomly assigned to initial antihypertensive therapy with angiotensin-converting-enzyme inhibition rather than calcium channel blockade. The results of the normotensive ABCD study included associations between intensive blood pressure control and significant slowing of the progression of nephropathy (as assessed by urinary albumin excretion) and retinopathy, and fewer strokes. In both the hypertensive and normotensive studies, mean renal function (as assessed by 24 h creatinine clearance) remained stable during 5 years of either intensive or standard blood pressure intervention in patients with normoalbuminuria (<30 mg/24 h) or microalbuminuria (30–300 mg/24 h) at baseline. By contrast, the rate of creatinine clearance in patients with overt diabetic nephropathy (>300 mg/24 h; albuminuria) at baseline decreased by an average of 5 ml/min/year in spite of either intensive or standard blood pressure control. Analysis of the results of 5 years of follow-up revealed a highly significant correlation of all-cause and cardiovascular mortality with left ventricular mass and severity of albuminuria.',\n",
       " '4e4c9d1d7893795d386fa6e62385faa6c5eff814': 'Context-aware recommendation (CAR) can lead to significant improvements in the relevance of the recommended items by modeling the nuanced ways in which context influences preferences. The dominant approach in context-aware recommendation has been the multidimensional latent factors approach in which users, items, and context variables are represented as latent features in low-dimensional space. An interaction between a user, item, and a context variable is typically modeled as some linear combination of their latent features. However, given the many possible types of interactions between user, items and contextual variables, it may seem unrealistic to restrict the interactions among them to linearity.\\n To address this limitation, we develop a novel and powerful non-linear probabilistic algorithm for context-aware recommendation using Gaussian processes. The method which we call Gaussian Process Factorization Machines (GPFM) is applicable to both the explicit feedback setting (e.g. numerical ratings as in the Netflix dataset) and the implicit feedback setting (i.e. purchases, clicks). We derive stochastic gradient descent optimization to allow scalability of the model. We test GPFM on five different benchmark contextual datasets. Experimental results demonstrate that GPFM outperforms state-of-the-art context-aware recommendation methods.',\n",
       " 'e083df3577b3231d16678aaf7a020767bdc9c3a0': 'This paper presents two models of complex-valued neurons (CVNs) for real-valued classification problems, incorporating two newly-proposed activation functions, and presents their abilities as well as differences between them on benchmark problems. In both models, each real-valued input is encoded into a phase between 0 and \\uf070 of a complex number of unity magnitude, and multiplied by a complex-valued weight. The weighted sum of inputs is fed to an activation function. Activation functions of both models map complex values into real values, and their role is to divide the net-input (weighted sum) space into multiple regions representing the classes of input patterns. The gradient-based learning rule is derived for each of the activation functions. Ability of such CVNs are discussed and tested with two-class problems, such as two and three input Boolean problems, and symmetry detection in binary sequences. We exhibit here that both the models can form proper boundaries for these linear and nonlinear problems. For solving n-class problems, a complex-valued neural network (CVNN) consisting of n CVNs is also considered in this paper. We tested such single-layered CVNNs on several real world benchmark problems. The results show that the classification and generalization abilities of single-layered CVNNs are comparable to the conventional real-valued neural networks (RVNNs) having one hidden layer. Moreover, convergence of CVNNs is much faster than that of RVNNs in most of the cases.',\n",
       " '0b8651737442ec30052724a68e85fefc4c941970': 'Networked systems still suffer from poor firewall configuration and monitoring. VisualFirewall seeks to aid in the configuration of firewalls and monitoring of networks by providing four simultaneous views that display varying levels of detail and time-scales as well as correctly visualizing firewall reactions to individual packets. The four implemented views, real-time traffic, visual signature, statistics, and IDS alarm, provide the levels of detail and temporality that system administrators need to properly monitor their systems in a passive or an active manner. We have visualized several attacks, and we feel that even individuals unfamiliar with networking concepts can quickly distinguish between benign and malignant traffic patterns with a minimal amount of introduction.',\n",
       " '9fe265de1cfab7a97b5efd81d7d42b386b15f2b9': 'The massive amount of alarm data generated from intrusion detection systems is cumbersome for network system administrators to analyze. Often, important details are overlooked and it is difficult to get an overall picture of what is occurring in the network by manually traversing textual alarm logs. We have designed a novel visualization to address this problem by showing alarm activity within a network. Alarm data is presented in an overview where system administrators can get a general sense of network activity and easily detect anomalies. They then have the option of zooming and drilling down for details. The information is presented with local network IP (Internet Protocol) addresses plotted over multiple yaxes to represent the location of alarms. Time on the x-axis is used to show the pattern of the alarms and variations in color encode the severity and amount of alarms. Based on our system administrator requirements study, this graphical layout addresses what system administrators need to see, is faster and easier than analyzing text logs, and uses visualization techniques to effectively scale and display the data. With this design, we have built a tool that effectively uses operational alarm log data generated on the Georgia Tech campus network. The motivation and background of our design is presented along with examples that illustrate its usefulness. CR Categories: C.2.0 [Computer-Communication Networks]: General—Security and Protection C.2.3 [ComputerCommunication Networks]: Network Operations—Network Monitoring H.5.2 [Information Systems]: Information Interfaces and Presentation—User Interfaces',\n",
       " '0c90a3d183dc5f467c692fb7cbf60303729c8078': \"It is a well-known problem that intrusion detection systems overload their human operators by triggering thousands of alarms per day. This paper presents a new approach for handling intrusion detection alarms more efficiently. Central to this approach is the notion that each alarm occurs for a reason, which is referred to as the alarm's root causes. This paper observes that a few dozens of rather persistent root causes generally account for over 90% of the alarms that an intrusion detection system triggers. Therefore, we argue that alarms should be handled by identifying and removing the most predominant and persistent root causes. To make this paradigm practicable, we propose a novel alarm-clustering method that supports the human analyst in identifying root causes. We present experiments with real-world intrusion detection alarms to show how alarm clustering helped us identify root causes. Moreover, we show that the alarm load decreases quite substantially if the identified root causes are eliminated so that they can no longer trigger alarms in the future.\",\n",
       " '3c1f860a678f3f6f33f2cbfdfa7dfc7119a57a00': 'This paper describes an aggregation and correlation algorithm used in the design and implementation of an intrusion-detection console built on top of the Tivoli Enterprise Console (TEC). The aggregation and correlation algorithm aims at acquiring intrusion-detection alerts and relating them together to expose a more condensed view of the security issues raised by intrusion-detection systems.',\n",
       " '844f1a88efc648b5c604c0a098b5c49f3fea4139': 'System administration has become an increasingly important function, with the fundamental task being the inspection of computer log-files. It is not, however, easy to perform such tasks for two reasons. One is the high recognition load of log contents due to the massive amount of textual data. It is a tedious, time-consuming and often error-prone task to read through them. The other problem is the difficulty in extracting unusual messages from the log. If an administrator does not have the knowledge or experience, he or she cannot readily recognize unusual log messages. To help address these issues, we have developed a highly interactive visual log browser called ‘‘MieLog.’’ MieLog uses two techniques for manual log inspection tasks: information visualization and statistical analysis. Information visualization is helpful in reducing the recognition load because it provides an alternative method of interpreting textual information without reading. Statistical analysis enables the extraction of unusual log messages without domain specific knowledge. We will give three examples that illustrate the ability of the MieLog system to isolate unusual messages more easily than before.',\n",
       " '7f481f1a5fac3a49ee8b2f1bfa7e5f2f8eda3085': 'Deployment of convolutional neural networks (ConvNets) in always-on Internet of Everything (IoE) edge devices is severely constrained by the high memory energy consumption of hardware ConvNet implementations. Leveraging the error resilience of ConvNets by accepting bit errors at reduced voltages presents a viable option for energy savings, but few implementations utilize this due to the limited quantitative understanding of how bit errors affect performance. This paper demonstrates the efficacy of SRAM voltage scaling in a 9-layer CIFAR-10 binarized ConvNet processor, achieving memory energy savings of 3.12× with minimal accuracy degradation (∼99% of nominal). Additionally, we quantify the effect of bit error accumulation in a multi-layer network and show that further energy savings are possible by splitting weight and activation voltages. Finally, we compare the measured error rates for the CIFAR-10 binarized ConvNet against MNIST networks to demonstrate the difference in bit error requirements across varying complexity in network topologies and classification tasks.',\n",
       " '2aa5aaddbb367e477ba3bed67ce780f30e055279': 'We present a model for intrinsic decomposition of RGB-D images. Our approach analyzes a single RGB-D image and estimates albedo and shading fields that explain the input. To disambiguate the problem, our model estimates a number of components that jointly account for the reconstructed shading. By decomposing the shading field, we can build in assumptions about image formation that help distinguish reflectance variation from shading. These assumptions are expressed as simple nonlocal regularizers. We evaluate the model on real-world images and on a challenging synthetic dataset. The experimental results demonstrate that the presented approach outperforms prior models for intrinsic decomposition of RGB-D images.',\n",
       " '504054b182fc4d028c430e74a51b2d6ac2c43f64': 'Biological brains can adapt and learn from past experience. In neuroevolution, i.e. evolving artificial neural networks (ANNs), one way that agents controlled by ANNs can evolve the ability to adapt is by encoding local learning rules. However, a significant problem with most such approaches is that local learning rules for every connection in the network must be discovered separately. This paper aims to show that learning rules can be effectively indirectly encoded by extending the Hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) method. Adaptive HyperNEAT is introduced to allow not only patterns of weights across the connectivity of an ANN to be generated by a function of its geometry, but also patterns of arbitrary learning rules. Several such adaptive models with different levels of generality are explored and compared. The long-term promise of the new approach is to evolve large-scale adaptive ANNs, which is a major goal for neuroevolution.',\n",
       " 'fa92af436a7d04fcccc025fdedde4039d19df42f': 'Breast cancer is composed of multiple subtypes with distinct morphologies and clinical implications. The advent of microarrays has led to a new paradigm in deciphering breast cancer heterogeneity, based on which the intrinsic subtyping system using prognostic multigene classifiers was developed. Subtypes identified using different gene panels, though overlap to a great extent, do not completely converge, and the avail of new information and perspectives has led to the emergence of novel subtypes, which complicate our understanding towards breast tumor heterogeneity. This review explores and summarizes the existing intrinsic subtypes, patient clinical features and management, commercial signature panels, as well as various information used for tumor classification. Two trends are pointed out in the end on breast cancer subtyping, i.e., either diverging to more refined groups or converging to the major subtypes. This review improves our understandings towards breast cancer intrinsic classification, current status on clinical application, and future trends.',\n",
       " 'fc1f88e48ab29a1fa21f1e7d73f47c270353de59': 'An offset algorithm is important to the contour-parallel tool path generation process. Usually, it is necessary to offset with islands. In this paper a new offset algorithm for a 2D point-sequence curve (PS-curve) with multiple islands is presented. The algorithm consists of three sub-processes, the islands bridging process, the raw offset curve generation and the global invalid loops removal. The input of the algorithm is a set of PS-curves, in which one of them is the outer profile and the others are islands. The bridging process bridges all the islands to the outer profile with the Delaunay triangulation method, forming a single linked PS-curve.With the fact that local problems are caused by intersections of adjacent bisectors, the concept of stuck circle is proposed. Based on stuck circle, local problems are fixed by updating the original profile with the proposed basic rule and append rule, so that a raw offset curve can be generated. The last process first reports all the self-intersections on the raw offset PS-curve, and then a procedure called tree analysis puts all the self-intersections into a tree. All the points between the nodes in even depth and its immediate children are collected using the collecting rule. The collected points form the valid loops, which is the output of the proposed algorithm. Each sub-process can be complete in near linear time, so the whole algorithm has a near linear time complexity. This can be proved by the examples tested in the paper. © 2012 Elsevier Ltd. All rights reserved.',\n",
       " '27266a1dd3854e4effe41b9a3c0e569d33004a33': \"Open Information Extraction (Open IE) has gained increasing research interest in recent years. The first step in Open IE is to extract raw subject--predicate--object triples from the data. These raw triples are rarely usable per se, and need additional post-processing. To that end, we proposed the use of Boolean Tucker tensor decomposition to simultaneously find the entity and relation synonyms and the facts connecting them from the raw triples. Our method represents the synonym sets and facts using (sparse) binary matrices and tensor that can be efficiently stored and manipulated. We consider the presentation of the problem as a Boolean tensor decomposition as one of this paper's main contributions. To study the validity of this approach, we use a recent algorithm for scalable Boolean Tucker decomposition. We validate the results with empirical evaluation on a new semi-synthetic data set, generated to faithfully reproduce real-world data features, as well as with real-world data from existing Open IE extractor. We show that our method obtains high precision while the low recall can easily be remedied by considering the original data together with the decomposition.\",\n",
       " '7dd434b3799a6c8c346a1d7ee77d37980a4ef5b9': 'Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and semantics, e.g., computer programs and molecular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where the syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational autoencoder (SD-VAE) by introducing stochastic lazy attributes. This approach converts the offline SDT check into on-the-fly generated guidance for constraining the decoder. Comparing to the state-of-the-art methods, our approach enforces constraints on the output space so that the output will be not only syntactically valid, but also semantically reasonable. We evaluate the proposed model with applications in programming language and molecules, including reconstruction and program/molecule optimization. The results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models, which is significantly better than current state-of-the-art approaches.',\n",
       " 'f43c1aee5382bb6fe9c92c54767218d954016e0c': 'Integrative Co-occurrence matrices are introduced as novel features for color texture classi\"cation. The extended Co-occurrence notation allows the comparison between integrative and parallel color texture concepts. The information pro\"t of the new matrices is shown quantitatively using the Kolmogorov distance and by extensive classi\"cation experiments on two datasets. Applying them to the RGB and the LUV color space the combined color and intensity textures are studied and the existence of intensity independent pure color patterns is demonstrated. The results are compared with two baselines: gray-scale texture analysis and color histogram analysis. The novel features improve the classi\"cation results up to 20% and 32% for the \"rst and second baseline, respectively. ? 2003 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.',\n",
       " 'eb596f7a723c955387ef577ba6bf8817cd3ebdc1': 'While hybrid capacitive-resistive D/A Converter (DAC) has been known for many years, its potential for energy-efficient operation is sometimes overlooked. This paper investigates the utilization of hybrid DACs in successive-approximation register A/D converters. To improve energy efficiency of SAR ADCs, a new hybrid DAC is introduced. In an exemplar 10-bit 100-MS/s ADC, simulation results show that the energy efficiency and chip area (of passive devices) can be improved by more than an order of magnitude.',\n",
       " '4c790c71219f6be248a3d426347bf7c4e3a0a6c4': \"OBJECTIVES\\nTo develop a 10-minute cognitive screening tool (Montreal Cognitive Assessment, MoCA) to assist first-line physicians in detection of mild cognitive impairment (MCI), a clinical state that often progresses to dementia.\\n\\n\\nDESIGN\\nValidation study.\\n\\n\\nSETTING\\nA community clinic and an academic center.\\n\\n\\nPARTICIPANTS\\nNinety-four patients meeting MCI clinical criteria supported by psychometric measures, 93 patients with mild Alzheimer's disease (AD) (Mini-Mental State Examination (MMSE) score > or =17), and 90 healthy elderly controls (NC).\\n\\n\\nMEASUREMENTS\\nThe MoCA and MMSE were administered to all participants, and sensitivity and specificity of both measures were assessed for detection of MCI and mild AD.\\n\\n\\nRESULTS\\nUsing a cutoff score 26, the MMSE had a sensitivity of 18% to detect MCI, whereas the MoCA detected 90% of MCI subjects. In the mild AD group, the MMSE had a sensitivity of 78%, whereas the MoCA detected 100%. Specificity was excellent for both MMSE and MoCA (100% and 87%, respectively).\\n\\n\\nCONCLUSION\\nMCI as an entity is evolving and somewhat controversial. The MoCA is a brief cognitive screening tool with high sensitivity and specificity for detecting MCI as currently conceptualized in patients performing in the normal range on the MMSE.\",\n",
       " '92bb6791f0e8bc2dd0874f09930b6a7ff990827d': 'A total engine failure poses a major threat to passengers as well as the aircraft and requires a fast decision by the pilot. We develop an assistant system to support the pilot in this decision process. An aircraft is able to glide a certain distance without thrust power by converting the potential energy of the altitude into distance. The objective of our work is to calculate an approach route which allows the aircraft to reach a suitable landing field at an appropriate altitude. This is a non-trivial problem because of many free parameters like wind direction and wind velocity. Our solution computes an approach route with two tangents and two co-rotating circular segments. For this purpose, valid approach routes can be calculated for many general cases. The method has a constant complexity and can dispense with iterative approaches. The route is calculated entirely in the wind frame which avoids complex calculations with trochoids. The central idea is to take the wind into account by moving the target towards the wind direction.',\n",
       " '72663b8c32a4ee99da679366868ca4de863c3ba4': 'In this paper, we propose a novel method, called \"dynamic cascade\", for training an efficient face detector on massive data sets. There are three key contributions. The first is a new cascade algorithm called \"dynamic cascade \", which can train cascade classifiers on massive data sets and only requires a small number of training parameters. The second is the introduction of a new kind of weak classifier, called \"Bayesian stump\", for training boost classifiers. It produces more stable boost classifiers with fewer features. Moreover, we propose a strategy for using our dynamic cascade algorithm with multiple sets of features to further improve the detection performance without significant increase in the detector\\'s computational cost. Experimental results show that all the new techniques effectively improve the detection performance. Finally, we provide the first large standard data set for face detection, so that future researches on the topic can be compared on the same training and testing set.',\n",
       " '43dcd8b78857cbfe58ae684c44dd57c8c72368c3': 'Learning management systems (LMS) are commonly used in e-learning but provide little, or in most cases, no adaptivity. However, courses which adapt to the individual needs of students make learning easier for them and lead to a positive effect in learning. In this paper, we introduce a concept for providing adaptivity based on learning styles in LMS. In order to show the effectiveness of our approach, Moodle was extended by an add-on and an experiment with 437 students was performed. From the analysis of the students’ performance and behaviour in the course, we found out that students who learned from a course that matches their learning styles spent significantly less time in the course and achieved in average the same marks than students who got a course that either mismatched with their learning styles or included all available learning objects. Therefore, providing adaptive courses in LMS according to the proposed concept can be seen as effective in supporting students in learning.',\n",
       " '34001fa75ba229639dc251fb1714a6bc2dfb76b3': \"Many procedures in SAs/STAT~ can be used to perform 10' gistic regression analysis: CATMOD, GENMOD,LOGISTIC, al)d PROBIT. Each procedure has special features that make. ~ useful for gertain applications. For most applica· tions, PROC LOGISTIC is the preferred choice,. It fits binary response or proportional odds models, provides various model·selection methods to identify important prognostic variables from a large number of candidate variables, and computes regression diagnostic statistics. This tutorial dis· cusses some of the problems users encountered when they used the LOGISTIC procedure.\",\n",
       " '4f58366300e6031ece7b770d3cc7ecdd019ca440': 'Buildings are responsible for 40% of global energy use and contribute towards 30% of the total CO2 emissions. The drive to reduce energy use and associated greenhouse gas emissions from buildings has acted as a catalyst in the development of advanced computational methods for energy efficient design, management and control of buildings and systems. Heating, ventilation and air-conditioning (HVAC) systems are the major source of energy consumption in buildings and ideal candidates for substantial reductions in energy demand. Significant advances have been made in the past decades on the application of computational intelligence (CI) techniques for HVAC design, control, management, optimization, and fault detection and diagnosis. This article presents a comprehensive and critical review on the theory and applications of CI techniques for prediction, optimization, control and diagnosis of HVAC systems. The analysis of trends reveals that the minimisation of energy consumption was the key optimization objective in the reviewed research, closely followed by the optimization of thermal comfort, indoor air quality and occupant preferences. Hardcoded Matlab program was the most widely used simulation tool, followed by TRNSYS, EnergyPlus, DOE-2, HVACSim+ and ESP-r. Metaheuristic algorithms were the preferred CI method for solving HVAC related problems and in particular genetic algorithms were applied in most of the studies. Despite the low number of studies focussing on multi-agent systems (MAS), as compared to the other CI techniques, interest in the technique is increasing due to their ability of dividing and conquering an HVAC optimization problem with enhanced overall performance. The paper also identifies prospective future advancements and research directions.',\n",
       " '6c4917342e7c81c09b28a4cd4e7575b4e9b176bf': 'Physical problems offer scope for macro level parallelization of solution by their essential structure. For parallelization of electrical network simulation, the most natural structure based method is that of multiport decomposition. In this paper this method is used for the simulation of electrical networks consisting of resistances, voltage and current sources using a distributed cluster of weakly coupled processors. At the two levels in which equations are solved in this method the authors have used sparse LU for both levels in the first scheme and sparse LU in the inner level and conjugate gradient in the outer level in the second scheme. Results are presented for planar networks, for the cases where the numbers of slave processors are 1 and 2, and for circuit sizes up to 8.2 million nodes and 16.4 million edges using 8 slave processors. The authors use a cluster of Pentium IV processors linked through a 10/100MBPS Ethernet switch',\n",
       " '2d6d67ec52505e890f777900e9de4e5fa827ebd7': 'We present methods for online linear optimization that take advantage of benign (as opposed to worst-case) sequences. Specifically if the sequence encountered by the learner is described well by a known “predictable process”, the algorithms presented enjoy tighter bounds as compared to the typical worst case bounds. Additionally, the methods achieve the usual worst-case regret bounds if the sequence is not benign. Our approach can be seen as a way of adding prior knowledge about the sequence within the paradigm of online learning. The setting is shown to encompass partial and side information. Variance and path-length bounds [11, 9] can be seen as particular examples of online learning with simple predictable sequences. We further extend our methods and results to include competing with a set of possible predictable processes (models), that is “learning” the predictable process itself concurrently with using it to obtain better regret guarantees. We show that such model selection is possible under various assumptions on the available feedback. Our results suggest a promising direction of further research with potential applications to stock market and time series prediction.',\n",
       " '3eb54c38421009eac93c667b303afdd1e5544fce': 'An efficient algorithm is developed that identifies all independencies implied by the topology of a Baye\\xad sian network. Its correctness and maximality stems from the soundness and completeness of d\\xad separation with respect to probability theory. The al\\xad gorithm runs in time 0 (IE I ) where E is the number of edges in the network.',\n",
       " '70666e2ecb5fd35af9adeccae0e2ef765cf149fe': 'In this paper, we propose a robust image hash algorithm by using the invariance of the image histogram shape to geometric deformations. Robustness and uniqueness of the proposed hash function are investigated in detail by representing the histogram shape as the relative relations in the number of pixels among groups of two different bins. It is found from extensive testing that the histogram-based hash function has a satisfactory performance to various geometric deformations, and is also robust to most common signal processing operations thanks to the use of Gaussian kernel low-pass filter in the preprocessing phase.',\n",
       " '4daa0a41e8d3049cc40fb9804f22318d5302abc1': 'This paper aims to examine the comprehensive social perception of autism spectrum disorders (ASDs) within the United States today. In order to study the broad public view of those with ASDs, this study investigates the evolution of the syndrome in both sociological and scientific realms. By drawing on the scientific progression of the syndrome and the mixture of this research with concurrent social issues and media representations, this study infers why such a significant amount of stigmatization has become attached to those with ASDs and how these stigmatizations have varied throughout history. After studying this evolving social perception of ASDs in the United States, the writer details suggestions for the betterment of this awareness, including boosted and specified research efforts, increased collaboration within those experts in autism, and positive visibility of those with ASDs and their families. Overall, the writer suggests that public awareness has increased and thus negative stigmatization has decreased in recent years; however, there remains much to be done to increase general social understanding of ASDs. “Autism is about having a pure heart and being very sensitive... It is about finding a way to survive in an overwhelming, confusing world... It is about developing differently, in a different pace and with different leaps.” -Trisha Van Berkel The identification of autism, in both sociological and scientific terms, has experienced a drastic evolution since its original definition in the early 20th century. From its original designation by Leo Kanner (1943), public understanding of autism spectrum disorders (ASDs) has been shrouded in mystery and misperception. The basic core features of all ASDs include problems with basic socialization and communication, strange intonation and facial expressions, and intense preoccupations or repetitive behaviors; however, one important aspect of what makes autism so complex is the wide variation in expression of the disorder (Lord, 2011). When comparing individuals with the same autism diagnosis, one will undoubtedly encounter many different personalities, strengths and weaknesses. This wide variability between individuals diagnosed with autism, along with the lack of basic understanding of the general public, accounts for a significant amount of social stigma in our society today. Social stigma stemming from this lack of knowledge has been reported in varying degrees since the original formation of the diagnosis. Studies conducted over the past two centuries have shown perceived negative stigma from the view of both the autistic individual and the family or caretakers',\n",
       " '87a1273dea59e3748372c5ea69488d70e9125046': 'This paper presents a method forfinding and tracking road lanes. The method extracts and tracks lane boundaries for vision-guided vehicle navigation by combining the hough transform and the “active line model ( A M ) ” . The hough transform can extract vanishing points of the road, which can be used as a good estimation of the vehicle heading. For the curved road, however, the estimation may be too crude to be used for navigation. Therefore, the hough transform is used to obtain an initial position estimation of the lane boundaries on the road. The line snake ALM then improves the initial approximation to an accurate configuration of the lane boundaries. Once the line snake is initialized in the first image, it tracks the road lanes using the external and internal forces computed from images and a proposed boundary refinement technique. Specifically, an image region is divided into a few subregions along the vertical direction. The hough transform is then performed for each sub-region and candidate lines of road lanes in each sub-region are extracted. Among candidate lines, a most prominent line is found by the ALM that minimizes a defined snake energy. The external energy of ALM is a normalized sum of image gradients along the line. The internal deformation energy ensures the continuity of two neighboring lines by using the angledifference between two adjacent lines and the distance between the two lines. A search method based on the dynamic programming reduces the computational cost. The proposed method gives a uniJied framework for detecting, refining and tracking the road lane. Experimental results using images of a real road scene are presented.',\n",
       " '8045f9b48d3e848861620f86a4f7add0ad919556': 'Non-contact imaging devices such as digital cameras and overhead scanners can convert hardcopy books to digital images without cutting them to individual pages. However, the captured images have distinct distortions. A book dewarping system is proposed to remove the perspective and geometric distortions automatically from single images. A book boundary model is extracted, and a 3D book surface is reconstructed. And then the horizontal and vertical metrics of each column are restored from it. Experimental results show the good dewarping and speed performance. Since no additional equipments and no restrictions to specific book layouts or contents are needed, the proposed system is very practical in real applications.',\n",
       " 'e7d3bd1df77c30b8db6a9a9c83692aa54d21e12a': 'The concept of thesaurus has evolved from a list of conceptually interrelated words to today\\'s controlled vocabularies, where terms form complex structures through semantic relationships. This term comes from the Latin and has turn been derived from the Greek \"θησαυρός\", which means treasury according to the Spanish Royal Academy, in whose dictionary it is also defined as: \\'name given by its authors to certain dictionaries, catalogues and anthologies\\'. The increase in scientific communication and productivity made it essential to develop keyword indexing systems. At that time, Howerton spoke of controlled lists to refer to concepts that were heuristically or intuitively related. According to Roberts (1984), Mooers was the first to relate thesauri to information retrieval systems; Taube established the foundations of post-coordination, while Luhn dealt, at a basic level, with the creation of thesauri using automatic techniques. Brownson (1957) was the first to use the term to refer to the issue of translating concepts and their relationships expressed in documents into a more precise language free of ambiguities in order to facilitate information retrieval. The ASTIA Thesaurus was published in the early 1960s (Currás, 2005), already bearing the characteristics of today\\'s thesauri and taking on the need for a tool to administer a controlled vocabulary in terms of indexing, thereby giving rise to the concept of documentary language.',\n",
       " '8c1351ff77f34a5a6e11be81a995e3faf862931b': 'This paper proposes a new dimensionality reduction algorithm named branching embedding (BE). It converts a dendrogram to a two-dimensional scatter plot, and visualizes the inherent structures of the original high-dimensional data. Since the conversion part is not computationally demanding, the BE algorithm would be beneficial for the case where hierarchical clustering is already performed. Numerical experiments revealed that the outputs of the algorithm moderately preserve the original hierarchical structures.',\n",
       " '1a68d1fb23fd11649c111a3c374f2c3d3ef4c09e': 'Moments before the launch of every space vehicle, engineering discipline specialists must make a critical go/no-go decision. The cost of a false positive, allowing a launch in spite of a fault, or a false negative, stopping a potentially successful launch, can be measured in the tens of millions of dollars, not including the cost in morale and other more intangible detriments. The Aerospace Corporation is responsible for providing engineering assessments critical to the go/no-go decision for every Department of Defense (DoD) launch vehicle. These assessments are made by constantly monitoring streaming telemetry data in the hours before launch. For this demonstration, we will introduce VizTree, a novel time-series visualization tool to aid the Aerospace analysts who must make these engineering assessments. VizTree was developed at the University of California, Riverside and is unique in that the same tool is used for mining archival data and monitoring incoming live telemetry. Unlike other time series visualization tools, VizTree can scale to very large databases, giving it the potential to be a generally useful data mining and database tool.',\n",
       " 'f1aa9bbf85aa1a0b3255b38032c08781f491f4d3': 'We propose a novel nLDMOS structure and a design concept in BCD technology with the best-in-class performance. The drift profile is optimized and the multi-oxide in the drift region is adopted to approach the RESURF limit of on-resistance vs. BVdss characteristic (i.e., 36V DMOS has a Ron_sp of 20mohm-mm2 with a BVdss of 50V; 45V DMOS has a Ron_sp of 28mohm-mm2 with a BVdss of 65V). Moreover, this modification requires merely three extra masks in the Non-Epitaxy LV process to achieve this improvement. Therefore it is not only a high performance but also a low cost solution.',\n",
       " '3f203c85493bce692da54648ef3f015db956a924': 'In this paper, we present a new approach to integrate in a 0.35 μm BCD technology, low Ron LDMOS power transistors with highly competitive Specific Resistance figure of merit (Rsp, defined as Ron*Area). The LDMOS are fully isolated in order to support applications which may bias the source/drain electrodes below the substrate potential, which is critical for devices used in high-current, high-frequency switching applications. The new devices are suitable for high-efficiency DC-DC converter products with operating voltage of up to 30V, such as mobile PMICs. For maximum performance, two different extended-drain LDMOS structures have been developed to cover the entire operating voltage range: for 16V and below, a planar-gate structure is used and for 20V and above, a non-planar “offset-LOCOS” gate is used for 20V and above.',\n",
       " 'ad3324136e70d546e2744bda4e2b14e6b5311c55': \"Toshiba's 5th generation BiCD/CD-0.13 is a new process platform for analog power applications based on 0.13µm CMOS technology. The process platform has six varieties of rated voltage, 5V, 6V, 18V, 25V, 40V, and 60V. 5 to 18V CD-0.13 process use P-type silicon substrate. 25 to 60V BiCD-0.13 process use N-Epi wafer with N+/P+ buried layer on P type silicon substrate. Each LDMOS recode ultra-low on-resistance compared with that of previous papers, and we will realize the highest performance analog power IC's using this technology.\",\n",
       " 'd0a5e8fd8ca4537e6c0b76e90ae32f86de3cb0fc': 'This paper presents BCD process integrating 7V to 70V power devices on 0.13um CMOS platform for various power management applications. BJT, Zener diode and Schottky diode are available and non-volatile memory is embedded as well. LDMOS shows best-in-class specific Ron (R<sub>SP</sub>) vs. BV<sub>DSS</sub> characteristics (i.e., 70V NMOS has R<sub>SP</sub> of 69mΩ-mm<sup>2</sup> with BV<sub>DSS</sub> of 89V). Modular process scheme is used for flexibility to various requirements of applications.',\n",
       " 'dd73a3d6682f3e1e70fd7b4d04971128fad5f27b': 'Advanced 0.16 μm BCD technology platform offering dense logic transistors (1.8 V-5 V CMOS) and high performance analog features has been developed. Thanks to dedicated field plate optimization, body and drain engineering, state of the art power devices (8 V to 42 V rated) have been obtained ensuring large Safe Operating Areas with best RONXAREA-BVDSS tradeoff.',\n",
       " 'b3e04836a8f1a1efda32d15296ff9435ab8afd86': 'This paper presents HeNet, a hierarchical ensemble neural network, applied to classify hardware-generated control flow traces for malware detection. Deep learning-based malware detection has so far focused on analyzing executable files and runtime API calls. Static code analysis approaches face challenges due to obfuscated code and adversarial perturbations. Behavioral data collected during execution is more difficult to obfuscate but recent research has shown successful attacks against API call based malware classifiers. We investigate control flow based characterization of a program execution to build robust deep learning malware classifiers. HeNet consists of a low-level behavior model and a toplevel ensemble model. The low-level model is a per-application behavior model, trained via transfer learning on a time-series of images generated from control flow trace of an execution. We use Intel R © Processor Trace enabled processor for low overhead execution tracing and design a lightweight image conversion and segmentation of the control flow trace. The top-level ensemble model aggregates the behavior classification of all the trace segments and detects an attack. The use of hardware trace adds portability to our system and the use of deep learning eliminates the manual effort of feature engineering. We evaluate HeNet against real-world exploitations of PDF readers. HeNet achieves 100% accuracy and 0% false positive on test set, and higher classification accuracy compared to classical machine learning algorithms.',\n",
       " 'a8540ff90bc1cf9eb54a2ba1ad4125e726d1980d': 'Despite remarkable advances in image synthesis research, existing works often fail in manipulating images under the context of large geometric transformations. Synthesizing person images conditioned on arbitrary poses is one of the most representative examples where the generation quality largely relies on the capability of identifying and modeling arbitrary transformations on different body parts. Current generative models are often built on local convolutions and overlook the key challenges (e.g. heavy occlusions, different views or dramatic appearance changes) when distinct geometric changes happen for each part, caused by arbitrary pose manipulations. This paper aims to resolve these challenges induced by geometric variability and spatial displacements via a new Soft-Gated Warping Generative Adversarial Network (Warping-GAN), which is composed of two stages: 1) it first synthesizes a target part segmentation map given a target pose, which depicts the region-level spatial layouts for guiding image synthesis with higher-level structure constraints; 2) the Warping-GAN equipped with a soft-gated warping-block learns feature-level mapping to render textures from the original image into the generated segmentation map. Warping-GAN is capable of controlling different transformation degrees given distinct target poses. Moreover, the proposed warping-block is lightweight and flexible enough to be injected into any networks. Human perceptual studies and quantitative evaluations demonstrate the superiority of our WarpingGAN that significantly outperforms all existing methods on two large datasets.',\n",
       " '19c953d98323e6f56a26e9d4b6b46f5809793fe4': 'This paper describes the vehicle guidance system of an autonomous vehicle. This system is part of the control structure of the vehicle and consists of a path generator, a motion planning algorithm and a sensor fusion module. A stereo vision sensor and a DGPS sensor are used as position sensors. The trajectory for the vehicle motion is generated in a rst step by using only information from a digital map. Object-detecting sensors such as the stereo vision sensor, three laserscanner and a radar sensor observe the vehicle environment and report detected objects to the sensor fusion module. This information is used to dynamically update the planned vehicle trajectory to the nal vehicle motion.',\n",
       " '43a050a0279c86baf842371c73b68b674061b579': 'The use of flying platforms such as unmanned aerial vehicles (UAVs), popularly known as drones, is rapidly growing in a wide range of wireless networking applications. In particular, with their inherent attributes such as mobility, flexibility, and adaptive altitude, UAVs admit several key potential applications in wireless systems. On the one hand, UAVs can be used as aerial base stations to enhance coverage, capacity, reliability, and energy efficiency of wireless networks. For instance, UAVs can be deployed to complement existing cellular systems by providing additional capacity to hotspot areas as well as to provide network coverage in emergency and public safety situations. On the other hand, UAVs can operate as flying mobile terminals within the cellular networks. Such cellular-connected UAVs can enable a wide range of key applications expanding from real-time video streaming to item delivery. Despite the several benefits and practical applications of using UAVs as aerial wireless devices, one must address many technical challenges. In this paper, a comprehensive tutorial on the potential benefits and applications of UAVs in wireless communications is presented. Moreover, the important challenges and the fundamental tradeoffs in UAV-enabled wireless networks are thoroughly investigated. In particular, the key UAV challenges such as three-dimensional (3D) deployment, performance analysis, air-to-ground channel modeling, and energy efficiency are explored along with representative results. Then, fundamental open problems and potential research directions pertaining to wireless communications and networking with UAVs are introduced. To cope with the open research problems, various analytical frameworks and mathematical tools such as optimization theory, machine learning, stochastic geometry, transport theory, and game theory are described. The use of such tools for addressing unique UAV problems is also presented. In a nutshell, this tutorial provides key guidelines on how to analyze, optimize, and design UAV-based wireless communication systems.',\n",
       " 'c139770aba13d62bf2f5a3a579da98a28148fb09': 'Analysis of static data is one of the best studied research areas. However, data changes over time. These changes may reveal patterns or groups of similar values, properties, and entities. We study changes in large, publicly available data repositories by modelling them as time series and clustering these series by their similarity. In order to perform change exploration on real-world data we use the publicly available revision data of Wikipedia Infoboxes and weekly snapshots of IMDB. The changes to the data are captured as events, which we call change records. In order to extract temporal behavior we count changes in time periods and propose a general transformation framework that aggregates groups of changes to numerical time series of different resolutions. We use these time series to study different application scenarios of unsupervised clustering. Our explorative results show that changes made to collaboratively edited data sources can help find characteristic behavior, distinguish entities or properties and provide insight into the respective domains.',\n",
       " '2f4bdb8f54ae1a7666c7be8e6b461e3c717a20cf': 'OBJECTIVES\\nSurvival of directly placed composite to restore worn teeth has been reported in studies with small sample sizes, short observation periods and different materials. This study aimed to estimate survival for a hybrid composite placed by one clinician up to 8-years follow-up.\\n\\n\\nMETHODS\\nAll patients were referred and recruited for a prospective observational cohort study. One composite was used: Spectrum(®) (DentsplyDeTrey). Most restorations were placed on the maxillary anterior teeth using a Dahl approach.\\n\\n\\nRESULTS\\nA total of 1010 direct composites were placed in 164 patients. Mean follow-up time was 33.8 months (s.d. 27.7). 71 of 1010 restorations failed during follow-up. The estimated failure rate in the first year was 5.4% (95% CI 3.7-7.0%). Time to failure was significantly greater in older subjects (p=0.005) and when a lack of posterior support was present (p=0.003). Bruxism and an increase in the occlusal vertical dimension were not associated with failure. The proportion of failures was greater in patients with a Class 3 or edge-to-edge incisal relationship than in Class 1 and Class 2 cases but this was not statistically significant. More failures occurred in the lower arch (9.6%) compared to the upper arch (6%) with the largest number of composites having been placed on the maxillary incisors (n=519).\\n\\n\\nCONCLUSION\\nThe worn dentition presents a restorative challenge but composite is an appropriate restorative material.\\n\\n\\nCLINICAL SIGNIFICANCE\\nThis study shows that posterior occlusal support is necessary to optimise survival.',\n",
       " 'b1ad2a08dc6b9c598f2ac101b22c2de8cd23e47f': 'This study explores the dimensionality of organizational justice and provides evidence of construct validity for a new justice measure. Items for this measure were generated by strictly following the seminal works in the justice literature. The measure was then validated in 2 separate studies. Study 1 occurred in a university setting, and Study 2 occurred in a field setting using employees in an automobile parts manufacturing company. Confirmatory factor analyses supported a 4-factor structure to the measure, with distributive, procedural, interpersonal, and informational justice as distinct dimensions. This solution fit the data significantly better than a 2- or 3-factor solution using larger interactional or procedural dimensions. Structural equation modeling also demonstrated predictive validity for the justice dimensions on important outcomes, including leader evaluation, rule compliance, commitment, and helping behavior.',\n",
       " 'b0fa515948682246559cebd1190cec39e87334c0': 'This article describes a new synthesis technology called reconstructive phrase modeling (RPM). A goal of RPM is to combine the realistic sound quality of sampling with the performance interaction of functional synthesis. Great importance is placed on capturing the dynamics of note transitions-slurs, legato, bow changes, etc. Expressive results are achieved with conventional keyboard controllers. Mastery of special performance techniques is not needed. RPM is an analysis-synthesis system that is related to two important trends in computer music research. The first is a form of additive synthesis in which sounds are represented as a sum of time-varying harmonics plus noise elements. RPM creates expressive performances by searching a database of idiomatic instrumental phrases and combining modified fragments of these phrases to form a new expressive performance. This approach is related to another research trend called concatenative synthesis',\n",
       " '2b550251323d541dd5d3f72ab68073e05cd485c5': 'While many synthetic aperture radar (SAR) image formation techniques exist, two of the most intuitive methods for implementation by SAR novices are the matched filter and backprojection algorithms. The matched filter and (non-optimized) backprojection algorithms are undeniably computationally complex. However, the backprojection algorithm may be successfully employed for many SAR research endeavors not involving considerably large data sets and not requiring time-critical image formation. Execution of both image reconstruction algorithms in MATLAB is explicitly addressed. In particular, a manipulation of the backprojection imaging equations is supplied to show how common MATLAB functions, ifft and interp1, may be used for straight-forward SAR image formation. In addition, limits for scene size and pixel spacing are derived to aid in the selection of an appropriate imaging grid to avoid aliasing. Example SAR images generated though use of the backprojection algorithm are provided given four publicly available SAR datasets. Finally, MATLAB code for SAR image reconstruction using the matched filter and backprojection algorithms is provided.',\n",
       " '0392a3743157a87d0bdfe53dffa416a9a6b93dcc': 'Machine learning (ML) over distributed multiparty data is required for a variety of domains. Existing approaches, such as federated learning, collect the outputs computed by a group of devices at a central aggregator and run iterative algorithms to train a globally shared model. Unfortunately, such approaches are susceptible to a variety of attacks, including model poisoning, which is made substantially worse in the presence of sybils. In this paper we first evaluate the vulnerability of federated learning to sybil-based poisoning attacks. We then describe FoolsGold, a novel defense to this problem that identifies poisoning sybils based on the diversity of client updates in the distributed learning process. Unlike prior work, our system does not bound the expected number of attackers, requires no auxiliary information outside of the learning process, and makes fewer assumptions about clients',\n",
       " '786f730302115d19f5ea14c49332bba77bde75c2': 'In protein subcellular localization prediction, a predominant scenario is that the number of available features is much larger than the number of data samples. Among the large number of features, many of them may contain redundant or irrelevant information, causing the prediction systems suffer from overfitting. To address this problem, this paper proposes a dimensionality-reduction method that applies random projection (RP) to construct an ensemble multi-label classifier for predicting protein subcellular localization. Specifically, the frequencies of occurrences of gene-ontology terms are used as feature vectors, which are projected onto lower-dimensional spaces by random projection matrices whose elements conform to a distribution with zero mean and unit variance. The transformed low-dimensional vectors are classified by an ensemble of one-vs-rest multi-label support vector machine (SVM) classifiers, each corresponding to one of the RP matrices. The scores obtained from the ensemble are then fused for making the final decision. Experimental results on two recent datasets suggest that the proposed method can reduce the dimensions by six folds and remarkably improve the classification performance.',\n",
       " 'ce83f8c2227ce387d7491f247d97f45e534fac24': 'This paper presents a novel control method for multimodule photovoltaic microinverter (MI). The proposed MI employs a two-stage topology with active-clamped current-fed push–pull converter cascaded with a full-bridge inverter. This system can operate in grid-connected mode to feed power to the grid with a programmable power factor. This system can also operate in line-interactive mode, i.e., share load power without feeding power to the grid. In the event of grid power failure, the MI can operate in a standalone mode to supply uninterruptible power to the load. This paper presents a multiloop control scheme with power programmable capability for achieving the above multiple functions. In addition, the proposed control scheme embedded a multimodule parallel capability that multiple MI modules can be paralleled to enlarge the capacity with autonomous control in all operation modes. Finally, three 250-W MI modules are adopted to demonstrate the effectiveness of the proposed control method in simulations as well as experiments.',\n",
       " 'fe0e8bd42fe96f67d73756a582d1d394b326eaac': 'A pulse-width-modulation (PWM) scheme that uses the converter switching frequency to minimize unwanted load current harmonics is described. This results in the reduction of the number of switch communications per cycle. The method is suitable for high-performance variable-speed AC-motor drives that require high-output switching frequencies for near-silent operation. It is also applicable, without change, to voltage or current-source inverters and to two and four-quadrant three-phase PWM rectifiers. Key predicted results have been verified experimentally on a 5 kVA inverter breadboard.<<ETX>>',\n",
       " 'b2cc59430df4ff20e34c48d122ccb47b45b96f83': 'Military operations are turning to more complex and advanced automation technologies for minimum risk and maximum efficiency.A critical piece to this strategy is unmanned aerial vehicles. Unmanned aerial vehicles require the intelligence to safely maneuver along a path to an intended target and avoiding obstacles such as other aircrafts or enemy threats. This paper presents a unique three-dimensional path planning problem formulation and solution approach using particle swarm optimization. The problem formulation was designed with three objectives: 1) minimize risk owing to enemy threats, 2) minimize fuel consumption incurred by deviating from the original path, and 3) fly over defined reconnaissance targets. The initial design point is defined as the original path of the unmanned aerial vehicles. Using particle swarm optimization, alternate paths are generated using B-spline curves, optimized based on the three defined objectives. The resulting paths can be optimized with a preference toward maximum safety, minimum fuel consumption, or target reconnaissance. This method has been implemented in a virtual environment where the generated alternate paths can be visualized interactively to better facilitate the decision-making process. The problem formulation and solution implementation is described along with the results from several simulated scenarios demonstrating the effectiveness of the method.',\n",
       " '7a48f36e567b3beda7866a25e37e4a3cc668d987': 'Complex biological systems have been successfully modeled by biochemical and genetic interaction networks, typically gathered from high-throughput (HTP) data. These networks can be used to infer functional relationships between genes or proteins. Using the intuition that the topological role of a gene in a network relates to its biological function, local or diffusionbased “guilt-by-association” and graph-theoretic methods have had success in inferring gene functions. Here we seek to improve function prediction by integrating diffusion-based methods with a novel dimensionality reduction technique to overcome the incomplete and noisy nature of network data. In this paper, we introduce diffusion component analysis (DCA), a framework that plugs in a diffusion model and learns a low-dimensional vector representation of each node to encode the topological properties of a network. As a proof of concept, we demonstrate DCA’s substantial improvement over state-of-the-art diffusion-based approaches in predicting protein function from molecular interaction networks. Moreover, our DCA framework can integrate multiple networks from heterogeneous sources, consisting of genomic information, biochemical experiments and other resources, to even further improve function prediction. Yet another layer of performance gain is achieved by integrating the DCA framework with support vector machines that take our node vector representations as features. Overall, our DCA framework provides a novel representation of nodes in a network that can be used as a plug-in architecture to other machine learning algorithms to decipher topological properties of and obtain novel insights into interactomes. This paper was selected for oral presentation at RECOMB 2015 and an abstract is published in the conference proceedings. ar X iv :1 50 4. 02 71 9v 1 [ qbi o. M N ] 1 0 A pr 2 01 5',\n",
       " '45f3c92cfe87fa376dfe184cead765ff251b9b30': 'In this paper we present some experiments using a deep learning model for speech denoising. We propose a very lightweight procedure that can predict clean speech spectra when presented with noisy speech inputs, and we show how various parameter choices impact the quality of the denoised signal. Through our experiments we conclude that such a structure can perform better than some comparable single-channel approaches and that it is able to generalize well across various speakers, noise types and signal-to-noise ratios.',\n",
       " '881a4de26b49a08608ae056b7688d64396bd62cd': 'Drug-drug interactions (DDIs) constitute an important concern in drug development and postmarketing pharmacovigilance. They are considered the cause of many adverse drug effects exposing patients to higher risks and increasing public health system costs. Methods to follow-up and discover possible DDIs causing harm to the population are a primary aim of drug safety researchers. Here, we review different methodologies and recent advances using data mining to detect DDIs with impact on patients. We focus on data mining of different pharmacovigilance sources, such as the US Food and Drug Administration Adverse Event Reporting System and electronic health records from medical institutions, as well as on the diverse data mining studies that use narrative text available in the scientific biomedical literature and social media. We pay attention to the strengths but also further explain challenges related to these methods. Data mining has important applications in the analysis of DDIs showing the impact of the interactions as a cause of adverse effects, extracting interactions to create knowledge data sets and gold standards and in the discovery of novel and dangerous DDIs.',\n",
       " '06a267a3b7a696c57d857c3ff1cf0e7711ff0dee': \"In this paper, we present a fast min-sum algorithm for decoding LDPC codes over GF(q). Our algorithm is different from the one presented by David Declercq and Marc Fossorier (2005) only at the way of speeding up the horizontal scan in the min-sum algorithm. The Declercq and Fossorier's algorithm speeds up the computation by reducing the number of configurations, while our algorithm uses the dynamic programming instead. Compared with the configuration reduction algorithm, the dynamic programming one is simpler at the design stage because it has less parameters to tune. Furthermore, it does not have the performance degradation problem caused by the configuration reduction because it searches the whole configuration space efficiently through dynamic programming. Both algorithms have the same level of complexity and use simple operations which are suitable for hardware implementations\",\n",
       " 'a6355b13e74d2a11aba4bad75464bf721d7b61d4': '—In the last decades, the researchers of the human arm prosthesis are using different types of machine learning algorithms. This review article firstly gives a brief explanation about type of machine learning methods. Secondly, some recent applications of myoelectric control of human arm prosthesis by using machine learning algorithms are compared. This study presents two different comparisons based on feature extraction methods which are time series modeling and wavelet transform of EMG signal. Finally, of characterization of EMG for of human arm prosthesis have been and discussed.',\n",
       " '892c22460a1ef1da7d10d1cf007ff46c6c080f18': \"Vibrotactile actuation is mainly used to deliver buzzing sensations. But if vibrotactile actuation is tightly coupled to users' actions, it can be used to create much richer haptic experiences. It is not well understood, however, how this coupling should be done or which vibrotactile parameters create which experiences. To investigate how actuation parameters relate to haptic experiences, we built a physical slider with minimal native friction, a vibrotactile actuator and an integrated position sensor. By vibrating the slider as it is moved, we create an experience of texture between the sliding element and its track. We conducted a magnitude estimation experiment to map how granularity, amplitude and timbre relate to the experiences of roughness, adhesiveness, sharpness and bumpiness. We found that amplitude influences the strength of the perceived texture, while variations in granularity and timbre create distinct experiences. Our study underlines the importance of action in haptic perception and suggests strategies for deploying such tightly coupled feedback in everyday devices.\",\n",
       " 'df5c0ae24bdf598a9fe8e85facf476f4903bf8aa': 'We present an analysis of taxi flows in Manhattan (NYC) using a variety of data mining approaches. The methods presented here can aid in development of representative and accurate models of large-scale traffic flows with applications to many areas, including outlier detection and characterization.',\n",
       " '265a32d3e5a55140389df0a0b666ac5c2dfaa0bd': 'Learning from sparse and delayed reward is a central issue in reinforcement learning. In this paper, to tackle reward sparseness problem of task oriented dialogue management, we propose a curriculum based approach on the number of slots of user goals. This curriculum makes it possible to learn dialogue management for sets of user goals with large number of slots. We also propose a dialogue policy based on progressive neural networks whose modules with parameters are appended with previous parameters fixed as the curriculum proceeds, and this policy improves performances over the one with single set of parameters.',\n",
       " '3c77afb5f21b4256f289371590fa539e074cc3aa': 'Information graphics, or infographics, are visual representations of information, data or knowledge. Understanding of infographics in documents is a relatively new research problem, which becomes more challenging when infographics appear as raster images. This paper describes technical details and practical applications of the system we built for recognizing and understanding imaged infographics located in document pages. To recognize infographics in raster form, both graphical symbol extraction and text recognition need to be performed. The two kinds of information are then auto-associated to capture and store the semantic information carried by the infographics. Two practical applications of the system are introduced in this paper, including supplement to traditional optical character recognition (OCR) system and providing enriched information for question answering (QA). To test the performance of our system, we conducted experiments using a collection of downloaded and scanned infographic images. Another set of scanned document pages from the University of Washington document image database were used to demonstrate how the system output can be used by other applications. The results obtained confirm the practical value of the system.',\n",
       " '0c85afa692a692da6e444b1098e59d11f0b07b83': 'Coupling design of rim-driven integrated motor propulsor for vessels and underwater vehicle is presented in this paper. The main characteristic of integrated motor propulsor is that the motor is integrated in the duct of propulsor. So the coupling design of motor and propulsor is the key to the overall design. Considering the influence of the motor and duct size, the propeller was designed, and the CFD Method was used to analyze the hydrodynamic performance of the propulsor. Based on the air-gap magnetic field of permanent magnet motor and the equivalent magnetic circuit, the integrated motor electromagnetic model was proposed, and the finite element method was used to analyze the motor electromagnetic field. Finally, the simulation of the integrated motor starting process with the load of propulsor torque was carried out, and the results meets the design specifications.',\n",
       " 'cae3bc55809a531a933bf6071550eeb3a2632f55': 'Keyphrases can provide highly condensed and valuable information that allows users to quickly acquire the main ideas. Most previous studies realize the automatic keyphrase extraction through dividing the source text into multiple chunks and then rank and select the most suitable ones. These approaches ignore the deep semantics behind the text and could not predict the keyphrases not appearing in the source text. A sequence to sequence model to generate keyphrases from vocabulary could solve the issues above. However, traditional sequence to sequence model based on recurrent neural network(RNN) suffers from low efficiency problem. We propose an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations can be completely parallelized over all elements so as to better exploit the GPU hardware. Our use of gated linear units alleviates gradient propagation and we equip each decoder layer with a separate attention model. Moreover, we incorporate a copying mechanism to handle out-of-vocabulary phrases. In experiments, we evaluate our model on six datasets, and our proposed model is demonstrated to outperform state-of-the-art baseline models consistently and significantly, both on extracting the keyphrases existing in the source text and generating the absent keyphrases based on the sematic meaning of the text.',\n",
       " '280f9cc6ee7679d02a7b8b58d08173628057f3ea': 'Classic news summarization plays an important role with the exponential document growth on the Web. Many approaches are proposed to generate summaries but seldom simultaneously consider evolutionary characteristics of news plus to traditional summary elements. Therefore, we present a novel framework for the web mining problem named Evolutionary Timeline Summarization (ETS). Given the massive collection of time-stamped web documents related to a general news query, ETS aims to return the evolution trajectory along the timeline, consisting of individual but correlated summaries of each date, emphasizing relevance, coverage, coherence and cross-date diversity. ETS greatly facilitates fast news browsing and knowledge comprehension and hence is a necessity. We formally formulate the task as an optimization problem via iterative substitution from a set of sentences to a subset of sentences that satisfies the above requirements, balancing coherence/diversity measurement and local/global summary quality. The optimized substitution is iteratively conducted by incorporating several constraints until convergence. We develop experimental systems to evaluate on 6 instinctively different datasets which amount to 10251 documents. Performance comparisons between different system-generated timelines and manually created ones by human editors demonstrate the effectiveness of our proposed framework in terms of ROUGE metrics.',\n",
       " '39c230241d51b1435472115aaa8c62b94ab9927d': 'This paper addresses the task of constructing a timeline of events mentioned in a given text. To accomplish that, we present a novel representation of the temporal structure of a news article based on time intervals. We then present an algorithmic approach that jointly optimizes the temporal structure by coupling local classifiers that predict associations and temporal relations between pairs of temporal entities with global constraints. Moreover, we present ways to leverage knowledge provided by event coreference to further improve the system performance. Overall, our experiments show that the joint inference model significantly outperformed the local classifiers by 9.2% of relative improvement in F1. The experiments also suggest that good event coreference could make remarkable contribution to a robust event timeline construction system.',\n",
       " '6d823b8098ec2b13fb5dcbb02bb55d7030d37d5a': 'We present an overview of TARSQI, a modular system for automatic temporal annotation that adds time expressions, events and temporal relations to news texts.',\n",
       " '0229c0eb39efed90db6691469daf0bb7244cf649': 'New Event Detection is a challenging task that still offers scope for great improvement after years of effort. In this paper we show how performance on New Event Detection (NED) can be improved by the use of text classification techniques as well as by using named entities in a new way. We explore modifications to the document representation in a vector space-based NED system. We also show that addressing named entities preferentially is useful only in certain situations. A combination of all the above results in a multi-stage NED system that performs much better than baseline single-stage NED systems.',\n",
       " '19e8aaa1021f829c8ff0378158d9b69699ea4f83': 'We discuss technology to help a person monitor changes in news coverage over time. We define temporal summaries of news stories as extracting a single sentence from each event within a news topic, where the stories are presented one at a time and sentences from a story must be ranked before the next story can be considered. We explain a method for evaluation, and describe an evaluation corpus that we have built. We also propose several methods for constructing temporal summaries and evaluate their effectiveness in comparison to degenerate cases. We show that simple approaches are effective, but that the problem is far from solved.',\n",
       " 'a75b289951207f627cdba8580a65cf18f9188d62': 'A 537W saturation output power (Psat) asymmetric Doherty amplifier for 2.6GHz band was successfully developed. The main and peak amplifiers were implemented with Psat of 210W and 320W GaN HEMTs. The newly developed 320W GaN HEMT consists of a single GaN die, both input and output partial match networks and a compact package. Its output matching network was tuned to inverse class F and a single-ended 320W GaN HEMT achieved higher than 61.8% drain efficiency from 2.4GHz to 2.7GHz. The 210W and 320W GaN HEMTs asymmetric Doherty amplifier exhibited 57.3dBm (537W) Psat and 48% drain efficiency with −50.6dBc ACLR at 50.3dBm (107W) average output power using a 4-carrier W-CDMA signal and commercially available digital pre-distortion system. These excellent performances show the good suitability for 2.6GHz band basestations.',\n",
       " '90d0b469521883bf24d673457f080343b97902fb': 'The fast-paced developments and technological breakthroughs in the semiconductor manufacturing industry elevates the importance of optimum utilization of resources. The newer 300-mm wafers fabs place a high level of emphasis on increasing yield and reducing cycle times. Automated material handling systems are importanttools that help us achieve these objectives. In addition, due to the increased weight and size of 300-mm wafers, an automated material handling system isa must for a 300-mm manufacturing facility. This paper discusses various approaches for automated materials handling in semiconductor manufacturing industries.',\n",
       " '66baa370deca40f0928554a62cd8b0e4dd5985d3': 'TGF-beta and BMP receptor kinases activate Smad transcription factors by C-terminal phosphorylation. We have identified a subsequent agonist-induced phosphorylation that plays a central dual role in Smad transcriptional activation and turnover. As receptor-activated Smads form transcriptional complexes, they are phosphorylated at an interdomain linker region by CDK8 and CDK9, which are components of transcriptional mediator and elongation complexes. These phosphorylations promote Smad transcriptional action, which in the case of Smad1 is mediated by the recruitment of YAP to the phosphorylated linker sites. An effector of the highly conserved Hippo organ size control pathway, YAP supports Smad1-dependent transcription and is required for BMP suppression of neural differentiation of mouse embryonic stem cells. The phosphorylated linker is ultimately recognized by specific ubiquitin ligases, leading to proteasome-mediated turnover of activated Smad proteins. Thus, nuclear CDK8/9 drive a cycle of Smad utilization and disposal that is an integral part of canonical BMP and TGF-beta pathways.',\n",
       " 'c9c30fe890c7b77b13d78c116cd80e046ae737b6': 'Graphene has attracted increasing attention for potential applications in biotechnology due to its excellent electronic property and biocompatibility. Here we use both Gram-positive Staphylococcus aureus (S. aureus) and Gram-negative Escherichia coli (E. coli) to investigate the antibacterial actions of large-area monolayer graphene film on conductor Cu, semiconductor Ge and insulator SiO2. The results show that the graphene films on Cu and Ge can surprisingly inhibit the growth of both bacteria, especially the former. However, the proliferation of both bacteria cannot be significantly restricted by the graphene film on SiO2. The morphology of S. aureus and E. coli on graphene films further confirms that the direct contact of both bacteria with graphene on Cu and Ge can cause membrane damage and destroy membrane integrity, while no evident membrane destruction is induced by graphene on SiO2. From the viewpoint of charge transfer, a plausible mechanism is proposed here to explain this phenomenon. This study may provide new insights for the better understanding of antibacterial actions of graphene film and for the better designing of graphene-based antibiotics or other biomedical applications.',\n",
       " '87e9ed98e6b7d5c7bf3837f62f3af9d182224f3b': 'Purpose – The purpose of this paper is to explore the mediating role of psychological empowerment and the moderating role of self-construal (independent and interdependent) on the relationship between transformational leadership and employees’ innovative work behavior (IWB). Design/methodology/approach – A total of 639 followers and 87 leaders filled out questionnaires from cross-industry sample of five most innovative companies of China. Structural equation modeling was used to analyze the relations. Findings – Results revealed that psychological empowerment mediated the relationship between transformational leadership and IWB. The research established that transformational leadership positively influences IWB which includes idea generation as well as idea implementation. The results also showed that the relationship between transformational leadership and IWB was stronger among employees with a higher interdependent self-construal and a lower independent self-construal. Originality/value – This study adds to IWB literature by empirically testing the moderating role of self-construal and the mediating role of psychological empowerment on transformational leadership-IWB link.',\n",
       " '6ccd19770991f35f7f4a9b0af62e3ff771536ae4': 'Named Entity Recognition (NER) is a task which helps in finding out Persons name, Location names, Brand names, Abbreviations, Date, Time etc and classifies the m into predefined different categories. NER plays a major role in various Natural Language Processing (NLP) fields like Information Extraction, Machine Translations and Question Answering. This paper describes the problems of NER in the context of Urdu Language and provides relevan t solutions. The system is developed to tag thirteen different Named Entities (NE), twelve NE proposed by IJCNLP-08 and Izaafats. We have used the Rule Based approach and developed the various rules to extract the Named Entities in the given Urdu text.',\n",
       " '4d26eb642175bcd01f0f67e55d73735bcfb13bab': 'This paper describes a 3.6-Gb/s 27-mW transceiver for chip-to-chip applications. A voltage-mode transmitter is proposed that equalizes the channel while maintaining impedance matching. A comparator is proposed that achieves sampling bandwidth control and offset compensation. A novel timing recovery circuit controls the phase by mismatching the current in the charge pump. The architecture maintains high signal integrity while each port consumes only 7.5 mW/Gb/s. The entire design occupies 0.2 mm/sup 2/ in a 0.18-/spl mu/m 1.8-V CMOS technology.',\n",
       " '4faa414044d97b8deb45c37b78f59f18e6886bc7': 'Knowledge can often be represented using entities connected by relations. For example, the fact that tennis ball is round can be represented as “TennisBall HasShape Round”, where a “TennisBall” is one entity, “HasShape” is a relation and “Round” is another entity. A knowledge base is a way to store such structured information, a knowledge base stores triples of the “an entity-relation-an entity” form, and a real world knowledge base often has millions or billions of such triples. There are several well-known knowledge bases including FreeBase [1], WordNet [2], YAGO [3], etc. They are important in fields like reasoning and question answering; for instance if one asks “what is the shape of a tennis ball”, we can search the knowledge base for the triple as “TennisBall HasShape Round” and output “round” as the answer.',\n",
       " '9981e27f01960526ea68227c7f8120e0c3ffe87f': 'Abstract: This paper generalises the golden section optimal search method to higher dimensional optimisation problem. The method is applicable to a strict quasi-convex function of N-variables over an N-dimensional hyper rectangle. An algorithm is proposed in N-dimension. The algorithm is illustrated graphically in two dimensions and verified through several test functions in higher dimension using MATLAB.',\n",
       " 'ac06b09e232fe9fd8c3fabef71e1fd7f6f752a7b': 'Network forensics is basically used to detect attackers activity and to analyze their behavior. Data collection is the major task of network forensics and honeypots are used in network forensics to collect useful data. Honeypot is an exciting new technology with enormous potential for security communities. This review paper is based upon the introduction to honeypots, their importance in network security, types of honeypots, their advantages disadvantages and legal issues related with them. Research Paper also discuss about the shortcomings of intrusion detection system in a network security and how honeypots improve the security architecture of the organizational network. Furthermore, paper reveals about the different kind of honeypots their level of interaction and risks associated with them. Keywords—honeyd; honeypots; nmap; network forensics',\n",
       " '5eea7073acfa8b946204ff681aca192571a1d6c2': \"We present PANDA, an open-source tool that has been purpose-built to support whole system reverse engineering. It is built upon the QEMU whole system emulator, and so analyses have access to all code executing in the guest and all data. PANDA adds the ability to record and replay executions, enabling iterative, deep, whole system analyses. Further, the replay log files are compact and shareable, allowing for repeatable experiments. A nine billion instruction boot of FreeBSD, e.g., is represented by only a few hundred MB. PANDA leverages QEMU's support of thirteen different CPU architectures to make analyses of those diverse instruction sets possible within the LLVM IR. In this way, PANDA can have a single dynamic taint analysis, for example, that precisely supports many CPUs. PANDA analyses are written in a simple plugin architecture which includes a mechanism to share functionality between plugins, increasing analysis code re-use and simplifying complex analysis development. We demonstrate PANDA's effectiveness via a number of use cases, including enabling an old but legitimately purchased game to run despite a lost CD key, in-depth diagnosis of an Internet Explorer crash, and uncovering the censorship activities and mechanisms of an IM client.\",\n",
       " '34776d39fec2da8b10de1c2633bec5968341509f': 'We demonstrate an active attack on the WEP protocol that is able to recover a 104-bit WEP key using less than 40.000 frames with a success probability of 50%. In order to succeed in 95% of all cases, 85.000 packets are needed. The IV of these packets can be randomly chosen. This is an improvement in the number of required frames by more than an order of magnitude over the best known key-recovery attacks for WEP. On a IEEE 802.11g network, the number of frames required can be obtained by re-injection in less than a minute. The required computational effort is approximately 2 RC4 key setups, which on current desktop and laptop CPUs is neglegible.',\n",
       " '16b187a157ad1599bf785912ac7974e38198be7a': 'Studies at the Wadsworth Center over the past 14 years have shown that people with or without motor disabilities can learn to control the amplitude of mu or beta rhythms in electroencephalographic (EEG) activity recorded from the scalp over sensorimotor cortex and can use that control to move a cursor on a computer screen in one or two dimensions. This EEG-based brain-computer interface (BCI) could provide a new augmentative communication technology for those who are totally paralyzed or have other severe motor impairments. Present research focuses on improving the speed and accuracy of BCI communication.',\n",
       " '1ada7e038d9dd3030cdbc7b0fc1eb041c1e4fb6b': 'We introduce a new approach to unsupervised estimation of feature-rich semantic role labeling models. Our model consists of two components: (1) an encoding component: a semantic role labeling model which predicts roles given a rich set of syntactic and lexical features; (2) a reconstruction component: a tensor factorization model which relies on roles to predict argument fillers. When the components are estimated jointly to minimize errors in argument reconstruction, the induced roles largely correspond to roles defined in annotated resources. Our method performs on par with most accurate role induction methods on English and German, even though, unlike these previous approaches, we do not incorporate any prior linguistic knowledge about the languages.',\n",
       " 'e4d3d316d29c6612593be1c9ce4736e3928213ed': 'This paper presents a new method for extracting unique features of items based on their textual reviews. The method is built of two similar iterations of applying a weighting scheme and then clustering the resultant set of vectors. In the first iteration, restaurants of similar food genres are grouped together into clusters. The second iteration reduces the importance of common terms in each such cluster, and highlights those that are unique to each specific restaurant. Clustering the restaurants again, now according to their unique features, reveals very interesting connections between the restaurants.',\n",
       " '3981416d7c8f784db8d9bfff2216fb50af711dbe': 'The paper proposes a new technique for deadlock control for a class of generalized Petri net (PN) with S4PR net, from the concept of control siphon. One important property of PN is to design structure, in terms of siphons, in order to the characterization of the deadlock prevent, and analytic structure of the synchronization subsystem which is needed to control place in a system. An efficient siphon method provides a solution to calculating minimal siphons, which is lead to constructing an optimal supervisor, ensuring that the system live state is preserved. The experimental results show the illustration of the proposed methods by using an S4PR net with an example is represented in FMS. The simulation result is also provided as a Petri nets the conception as well as the integration with MATLAB.',\n",
       " '483a5429b8f027bb75a03943c3150846377e44f6': 'Software-defined networking (SDN) has generated tremendous interest from both academia and industry. SDN aims at simplifying network management while enabling researchers to experiment with network protocols on deployed networks. This article is a distillation of the state of the art of SDN in the context of wireless networks. We present an overview of the major design trends and highlight key differences between them.',\n",
       " '75ebe1e0ae9d42732e31948e2e9c03d680235c39': 'We investigate the problem of automatically labelling appearances of characters in TV or film material. This is tremendously challenging due to the huge variation in imaged appearance of each character and the weakness and ambiguity of available annotation. However, we demonstrate that high precision can be achieved by combining multiple sources of information, both visual and textual. The principal novelties that we introduce are: (i) automatic generation of time stamped character annotation by aligning subtitles and transcripts; (ii) strengthening the supervisory information by identifying when characters are speaking; (iii) using complementary cues of face matching and clothing matching to propose common annotations for face tracks. Results are presented on episodes of the TV series “Buffy the Vampire Slayer”.',\n",
       " 'd85a7bef0adb2651353d6aa74d6e6a2e12c3d841': 'The current Internet of Things (IoT) ecosystem consists of non-interoperable products and services. The Web of Things (WoT) advances the IoT by allowing consumers to interact with the IoT ecosystem through the open and standard web technologies. But the Web alone does not solve the interoperability issues. It is widely acknowledged that Semantic Web Technologies hold the potential of achieving data and platform interoperability in both the IoT and WoT landscapes. In this context, the paper attempts to review and analyze the current state of ontology-based software tools for semantic interoperability.',\n",
       " 'e4acbc3656424766e39a6fbb0ae758d90554111e': \"Social media is an increasingly important part of modern life. We investigate the use of and usability of Twitter by blind users, via a combination of surveys of blind Twitter users, large-scale analysis of tweets from and Twitter profiles of blind and sighted users, and analysis of tweets containing embedded imagery. While Twitter has traditionally been thought of as the most accessible social media platform for blind users, Twitter's increasing integration of image content and users' diverse uses for images have presented emergent accessibility challenges. Our findings illuminate the importance of the ability to use social media for people who are blind, while also highlighting the many challenges such media currently present this user base, including difficulty in creating profiles, in awareness of available features and settings, in controlling revelations of one's disability status, and in dealing with the increasing pervasiveness of image-based content. We propose changes that Twitter and other social platforms should make to promote fuller access to users with visual impairments.\",\n",
       " '1b3c1759c16fe3f0ea57140bb23489fbf104616b': 'Elevation beamforming and Full Dimension MIMO (FD-MIMO) has been an active area of research and standardization in 3GPP LTE-Advanced. In an FD-MIMO system, a base station with 2-dimensional (2D) active array supports multi-user joint elevation and azimuth beamforming (a.k.a. 3D beamfoming), which results in much higher cell capacity compared to conventional systems. Recent study has shown that with these new FD-MIMO technologies, we can achieve promising 3-5× gain in both cell capacity as well as cell-edge throughput. In this paper, we will provide a brief summary of recent 3GPP activities, including the recently completed 3D channel model, ongoing study on FD-MIMO scenarios, antenna/RF (radio frequency) transceiver architectures, as well as potential network performance benefits. In addition, we also discuss some methods for reducing CSI (channel state information) feedback overhead and ensuring efficient operation of large size FD-MIMO for both TDD and FDD systems.',\n",
       " '77a78f27356d502425ad232bf5cc554b73b38897': 'Efficient and scalable live-streaming overlay construction has become a hot topic recently. In order to improve the performance metrics, such as startup delay, source-to-end delay, and playback continuity, most previous studies focused on intra-overlay optimization. Such approaches have drawbacks including low resource utilization, high startup and source-to-end delay, and unreasonable resource assignment in global P2P networks. Anysee is a peer-to-peer live streaming system and adopts an inter-overlay optimization scheme, in which resources can join multiple overlays, so as to (1) improve global resource utilization and distribute traffic to all physical links evenly; (2) assign resources based on their locality and delay; (3) guarantee streaming service quality by using the nearest peers, even when such peers might belong to different overlays; and (4) balance the load among the group members. We compare the performance of our design with existing approaches based on comprehensive trace driven simulations. Results show that AnySee outperforms previous schemes in resource utilization and the QoS of streaming services. AnySee has been implemented as an Internet based live streaming system, and was successfully released in the summer of 2004 in CERNET of China. Over 60,000 users enjoy massive entertainment programs, including TV programs, movies, and academic conferences. Statistics prove that this design is scalable and robust, and we believe that the wide deployment of AnySee will soon benefit many more Internet users.',\n",
       " 'e7dbd9ba29c59c68a9dae9f40dfc4040476c4624': 'The authors report results from 5 experiments that describe the influence of emotional states on trust. They found that incidental emotions significantly influence trust in unrelated settings. Happiness and gratitude--emotions with positive valence--increase trust, and anger--an emotion with negative valence--decreases trust. Specifically, they found that emotions characterized by other-person control (anger and gratitude) and weak control appraisals (happiness) influence trust significantly more than emotions characterized by personal control (pride and guilt) or situational control (sadness). These findings suggest that emotions are more likely to be misattributed when the appraisals of the emotion are consistent with the judgment task than when the appraisals of the emotion are inconsistent with the judgment task. Emotions do not influence trust when individuals are aware of the source of their emotions or when individuals are very familiar with the trustee.',\n",
       " 'd6f7c761fa64754d7d93601a4802da27b5858f8b': 'Our goal is to classify 3D models directly using convolutional neural network. Most of existing approaches rely on a set of human-engineered features. We use 3D convolutional neural network to let the network learn the features over 3D space to minimize classification error. We trained and tested over ShapeNet dataset with data augmentation by applying random transformations. We made various visual analysis to find out what the network has learned. We extended our work to extract additional information such as pose of the 3D model.',\n",
       " '00f51b60ef3929097ada76a16ff71badc2277165': 'Empirical software engineering research needs research guidelines to improve the research and reporting processes. We propose a preliminary set of research guidelines aimed at stimulating discussion among software researchers. They are based on a review of research guidelines developed for medical researchers and on our own experience in doing and reviewing software engineering research. The guidelines are intended to assist researchers, reviewers and meta-analysts in designing, conducting and evaluating empirical studies. Editorial boards of software engineering journals may wish to use our recommendations as a basis for developing guidelines for reviewers and for framing policies for dealing with the design, data collection and analysis and reporting of empirical studies.',\n",
       " 'a023f6d6c383f4a3839036f07b1ea0aa04da9cbb': '0950-5849/$ see front matter 2010 Elsevier B.V. A doi:10.1016/j.infsof.2010.12.001 ⇑ Address: School of Computing, Blekinge Institute 372 25, Sweden. E-mail addresses: kai.petersen@bth.se, kai.petersen URLs: http://www.bth.se/besq, http://www.ericsso Context: Software productivity measurement is essential in order to control and improve the performance of software development. For example, by identifying role models (e.g. projects, individuals, tasks) when comparing productivity data. The prediction is of relevance to determine whether corrective actions are needed, and to discover which alternative improvement action would yield the best results. Objective: In this study we identify studies for software productivity prediction and measurement. Based on the identified studies we first create a classification scheme and map the studies into the scheme (systematic map). Thereafter, a detailed analysis and synthesis of the studies is conducted. Method: As a research method for systematically identifying and aggregating the evidence of productivity measurement and prediction approaches systematic mapping and systematic review have been used. Results: In total 38 studies have been identified, resulting in a classification scheme for empirical research on software productivity. The mapping allowed to identify the rigor of the evidence with respect to the different productivity approaches. In the detailed analysis the results were tabulated and synthesized to provide recommendations to practitioners. Conclusion: Risks with simple ratio-based measurement approaches were shown. In response to the problems data envelopment analysis seems to be a strong approach to capture multivariate productivity measures, and allows to identify reference projects to which inefficient projects should be compared. Regarding simulation no general prediction model can be identified. Simulation and statistical process control are promising methods for software productivity prediction. Overall, further evidence is needed to make stronger claims and recommendations. In particular, the discussion of validity threats should become standard, and models need to be compared with each other. 2010 Elsevier B.V. All rights reserved.',\n",
       " 'b58b3a1dd84fe44f91510df00905a1ed33c1525c': 'Since the late seventies, efforts to catalog factors that influences productivity, as well as actions to improve it, has been a huge concern for both academy and software development industry. Despite numerous studies, software organizations still do not know which the most significant factors are and what to do with it. Several studies present the factors in a very superficial way, some others address only the related factors or there are those that describe only a single factor. Actions to deal with the factors are spread and frequently were not mapped. Through a literature review, this paper presents a consolidated view of the main factors that have affected productivity over the years, and the strategies to deal with these factors nowadays. This research aims to support software development industry on the selection of their strategies to improve productivity by maximizing the positive factors and minimizing or avoiding the impact of the negative ones.',\n",
       " 'd7b9dde9a7d304b378079049a0c2af40454a13bb': 'Agile software development practices such as eXtreme Programming (XP) and SCRUM have increasingly been adopted to respond to the challenges of volatile business environments, where the markets and technologies evolve rapidly and present the unexpected. In spite of the encouraging results so far, little is known about how agile practices affect communication. This article presents the results from a study which examined the impact of XP and SCRUM practices on communication within software development teams and within the focal organization. The research was carried out as a case study in F-Secure where two agile software development projects were compared from the communication perspective. The goal of the study is to increase the understanding of communication in the context of agile software development: internally among the developers and project leaders and in the interface between the development team and stakeholders (i.e. customers, testers, other development teams). The study shows that agile practices improve both informal and formal communication. However, it further indicates that, in larger development situations involving multiple external stakeholders, a mismatch of adequate communication mechanisms can sometimes even hinder the communication. The study highlights the fact that hurdles and improvements in the communication process can both affect the feature requirements and task subtask dependencies as described in coordination theory. While the use of SCRUM and some XP practices facilitate team and organizational communication of the dependencies between product features and working tasks, the use of agile practices requires that the team and organization use also additional plan-driven practices to ensure the efficiency of external communication between all the actors of software development.',\n",
       " '0ee47ca8e90f3dd2107b6791c0da42357c56f5bc': \"T he rise and fall of the dot-com-driven Internet economy shouldn't distract us from seeing that the business environment continues to change at a dramatically increasing pace. To thrive in this turbulent environment, we must confront the business need for relentless innovation and forge the future workforce culture. Agile software development approaches such as Extreme Programming , Crystal methods, Lean Development, Scrum, Adaptive Software Development (ASD), and others view change from a perspective that mirrors today's turbulent business and technology environment. In a recent study of more than 200 software development projects, QSM Associates' Michael Mah reported that the researchers couldn't find nearly half of the projects' original plans to measure against. Why? Conforming to plan was no longer the primary goal; instead, satisfying customers—at the time of delivery , not at project initiation—took precedence. In many projects we review, major changes in the requirements, scope, and technology that are outside the development team's control often occur within a project's life span. Accepting that Barry Boehm's life cycle cost differentials theory—the cost of change grows through the software's development life cycle—remains valid, the question today is not how to stop change early in a project but how to better handle inevitable changes throughout its life cycle. Traditional approaches assumed that if we just tried hard enough, we could anticipate the complete set of requirements early and reduce cost by eliminating change. Today, eliminating change early means being unresponsive to business con-ditions—in other words, business failure. Similarly, traditional process manage-ment—by continuous measurement, error identification, and process refine-ments—strove to drive variations out of processes. This approach assumes that variations are the result of errors. Today, while process problems certainly cause some errors, external environmental changes cause critical variations. Because we cannot eliminate these changes, driving down the cost of responding to them is the only viable strategy. Rather than eliminating rework, the new strategy is to reduce its cost. However, in not just accommodating change, but embracing it, we also must be careful to retain quality. Expectations have grown over the years. The market demands and expects innovative, high-quality software that meets its needs— and soon. Agile methods are a response to this expectation. Their strategy is to reduce the cost of change throughout a project. Extreme Programming (XP), for example, calls for the software development team to • produce the first delivery in weeks, to achieve an early win and rapid …\",\n",
       " '12f3f9a5bbcaa3ab09eff325bd4554924ac1356d': 'The fundamental issues in ICT Governance (ICTG) implementation for Malaysian Public Sector (MPS) is how ICT be applied to support improvements in productivity, management effectiveness and the quality of services offered to its citizens. Our main concern is to develop and adopt a common definition and framework to illustrate how ICTG can be used to better align ICT with government’s operations and strategic focus. In particular, we want to identify and categorize factors that drive a successful ICTG process. This paper presents the results of an exploratory study to identify, validate and refine such Critical Success Factors (CSFs) and confirmed seven CSFs and nineteen sub-factors as influential factors that fit MPS after further validated and refined. The Delphi method applied in validation and refining process before being endorsed as appropriate for MPS. The identified CSFs reflect the focus areas that need to be considered strategically to strengthen ICT Governance implementation and ensure business success. Keywords—IT Governance, Critical Success Factors.',\n",
       " '427a03a0746f398340e8a7f95d56316dacd8d70c': 'The Streptococcus Invasion Locus (Sil) was first described in Streptococcus pyogenes and Streptococcus pneumoniae, where it has been implicated in virulence. The two-component peptide signaling system consists of the SilA response regulator and SilB histidine kinase along with the SilCR signaling peptide and SilD/E export/processing proteins. The presence of an associated bacteriocin region suggests this system may play a role in competitive interactions with other microbes. Comparative analysis of 42 Streptococcus Anginosus/Milleri Group (SAG) genomes reveals this to be a hot spot for genomic variability. A cluster of bacteriocin/immunity genes is found adjacent to the sil system in most SAG isolates (typically 6-10 per strain). In addition, there were two distinct SilCR peptides identified in this group, denoted here as SilCRSAG-A and SilCRSAG-B, with corresponding alleles in silB. Our analysis of the 42 sil loci showed that SilCRSAG-A is only found in Streptococcus intermedius while all three species can carry SilCRSAG-B. In S. intermedius B196, a putative SilA operator is located upstream of bacteriocin gene clusters, implicating the sil system in regulation of microbe-microbe interactions at mucosal surfaces where the group resides. We demonstrate that S. intermedius B196 responds to its cognate SilCRSAG-A, and, less effectively, to SilCRSAG-B released by other Anginosus group members, to produce putative bacteriocins and inhibit the growth of a sensitive strain of S. constellatus.',\n",
       " '7ba0aa88b813b8e7e47d5999cf4802bb9b30b86a': 'Filling the gap between natural language expressions and ontology concepts or properties is the new trend in Semantic Web. Ontology lexicalization introduces a new layer of lexical information for ontology properties and concepts. We propose a method based on unsupervised learning for the extraction of the potential lexical expressions of DBpedia propertiesfrom Wikipedia text corpus. It is a resource-driven approach that comprises three main steps. The first step consists of the extraction of DBpedia triples for the aimed property followed by the extraction of Wikipedia articles describing the resources from these triples. In the second step, sentences mostly related to the property are extracted from the articles and they are analyzed with a Semantic Role Labeler resulting in a set of SRL annotated trees. In the last step, clusters of expressions are built using spectral clustering based on the distances between the SRL trees. The clusters with the least variance are considered to be relevant for the lexical expressions of the property.',\n",
       " '2d633db75b177aad6045c0469ba0696b905f314f': 'Building a text corpus suitable to be used in corpus-based speech synthesis is a time-consuming process that usually requires some human intervention to select the desired phonetic content and the necessary variety of prosodic contexts. If an emotional text-to-speech (TTS) system is desired, the complexity of the corpus generation process increases. This paper presents a study aiming to validate or reject the use of a semantically neutral text corpus for the recording of both neutral and emotional (acted) speech. The use of this kind of texts would eliminate the need to include semantically emotional texts into the corpus. The study has been performed for Basque language. It has been made by performing subjective and objective comparisons between the prosodic characteristics of recorded emotional speech using both semantically neutral and emotional texts. At the same time, the performed experiments allow for an evaluation of the capability of prosody to carry emotional information in Basque language. Prosody manipulation is the most common processing tool used in concatenative TTS. Experiments of automatic recognition of the emotions considered in this paper (the \"Big Six emotions\") show that prosody is an important emotional indicator, but cannot be the only manipulated parameter in an emotional TTS system-at least not for all the emotions. Resynthesis experiments transferring prosody from emotional to neutral speech have also been performed. They corroborate the results and support the use of a neutral-semantic-content text in databases for emotional speech synthesis',\n",
       " '516f412a76911a13c9128aac827b52b27b98fad9': \"Sybil accounts are fake identities created to unfairly increase the power or resources of a single user. Researchers have long known about the existence of Sybil accounts in online communities such as file-sharing systems, but have not been able to perform large scale measurements to detect them or measure their activities. In this paper, we describe our efforts to detect, characterize and understand Sybil account activity in the Renren online social network (OSN). We use ground truth provided by Renren Inc. to build measurement based Sybil account detectors, and deploy them on Renren to detect over 100,000 Sybil accounts. We study these Sybil accounts, as well as an additional 560,000 Sybil accounts caught by Renren, and analyze their link creation behavior. Most interestingly, we find that contrary to prior conjecture, Sybil accounts in OSNs do not form tight-knit communities. Instead, they integrate into the social graph just like normal users. Using link creation timestamps, we verify that the large majority of links between Sybil accounts are created accidentally, unbeknownst to the attacker. Overall, only a very small portion of Sybil accounts are connected to other Sybils with social links. Our study shows that existing Sybil defenses are unlikely to succeed in today's OSNs, and we must design new techniques to effectively detect and defend against Sybil attacks.\",\n",
       " '6847de10b11501b26f6d35405d1b6436ef17c0b4': 'Search engine is the popular term for an information retrieval (IR) system. Typically, search engine can be based on full-text indexing. Changing the presentation from the text data to multimedia data types make an information retrieval process more complex such as a retrieval of image or sounds in large databases. This paper introduces the use of language and text independent speech as input queries in a large sound database by using Speaker identification algorithm. The method consists of 2 main processing first steps, we separate vocal and non-vocal identification after that vocal be used to speaker identification for audio query by speaker voice. For the speaker identification and audio query by process, we estimate the similarity of the example signal and the samples in the queried database by calculating the Euclidian distance between the Mel frequency cepstral coefficients (MFCC) and Energy spectrum of acoustic features. The simulations show that the good performance with a sustainable computational cost and obtained the average accuracy rate more than 90%.',\n",
       " '26433d86b9c215b5a6871c70197ff4081d63054a': 'Multimodal biometrics has recently attracted substantial interest for its high performance in biometric recognition system. In this paper we introduce multimodal biometrics for face and palmprint images using fusion techniques at the feature level. Gabor based image processing is utilized to extract discriminant features, while principal component analysis (PCA) and linear discriminant analysis (LDA) are used to reduce the dimension of each modality. The output features of LDA are serially combined and classified by a Euclidean distance classifier. The experimental results based on ORL face and Poly-U palmprint databases proved that this fusion technique is able to increase biometric recognition rates compared to that produced by single modal biometrics.',\n",
       " '4ca6307c991f8f4c28ebdd45a08239dfb9da1c0c': 'A popular series of style transfer methods apply a style to a content image by controlling mean and covariance of values in early layers of a feature stack. This is insufficient for transferring styles that have strong structure across spatial scales like, e.g., textures where dots lie on long curves. This paper demonstrates that controlling inter-layer correlations yields visible improvements in style transfer methods. We achieve this control by computing cross-layer, rather than within-layer, gram matrices. We find that (a) cross-layer gram matrices are sufficient to control within-layer statistics. Inter-layer correlations improves style transfer and texture synthesis. The paper shows numerous examples on ”hard” real style transfer problems (e.g. long scale and hierarchical patterns); (b) a fast approximate style transfer method can control cross-layer gram matrices; (c) we demonstrate that multiplicative, rather than additive style and content loss, results in very good style transfer. Multiplicative loss produces a visible emphasis on boundaries, and means that one hyper-parameter can be eliminated. 1 ar X iv :1 80 1. 01 93 3v 1 [ cs .C V ] 5 J an 2 01 8',\n",
       " '58c87d2d678aab8bccd5cb20d04bc867682b07f2': 'The INTERSPEECH 2017 Computational Paralinguistics Challenge addresses three different problems for the first time in research competition under well-defined conditions: In the Addressee sub-challenge, it has to be determined whether speech produced by an adult is directed towards another adult or towards a child; in the Cold sub-challenge, speech under cold has to be told apart from ‘healthy’ speech; and in the Snoring subchallenge, four different types of snoring have to be classified. In this paper, we describe these sub-challenges, their conditions, and the baseline feature extraction and classifiers, which include data-learnt feature representations by end-to-end learning with convolutional and recurrent neural networks, and bag-of-audiowords for the first time in the challenge series.',\n",
       " '1ae9b720b3b3e497ef6a2a3e97079c0acb8570f1': 'Entities (e.g., users, services) have to authenticate themselves to service providers (SPs) in order to use their services. An entity provides personally identifiable information (PII) that uniquely identifies it to an SP. In the traditional application-centric Identity Management (IDM) model, each application keeps trace of identities of the entities that use it. In cloud computing, entities may have multiple accounts associated with different SPs, or one SP. Sharing PIIs of the same entity across services along with associated attributes can lead to mapping of PIIs to the entity. We propose an entity-centric approach for IDM in the cloud. The approach is based on: (1) active bundles—each including a payload of PII, privacy policies and a virtual machine that enforces the policies and uses a set of protection mechanisms to protect themselves, (2) anonymous identification to mediate interactions between the entity and cloud services using entity’s privacy policies. The main characteristics of the approach are: it is independent of third party, gives minimum information to the SP and provides ability to use identity data on untrusted hosts.',\n",
       " '626a38a32e2255e5bef98880ebbddf6994840e9e': 'Local binary pattern (LBP) is widely adopted for efficient image feature description and simplicity. To describe the color images, it is required to combine the LBPs from each channel of the image. The traditional way of binary combination is to simply concatenate the LBPs from each channel, but it increases the dimensionality of the pattern. In order to cope with this problem, this paper proposes a novel method for image description with multichannel decoded LBPs. We introduce adder- and decoder-based two schemas for the combination of the LBPs from more than one channel. Image retrieval experiments are performed to observe the effectiveness of the proposed approaches and compared with the existing ways of multichannel techniques. The experiments are performed over 12 benchmark natural scene and color texture image databases, such as Corel-1k, MIT-VisTex, USPTex, Colored Brodatz, and so on. It is observed that the introduced multichannel adder- and decoder-based LBPs significantly improve the retrieval performance over each database and outperform the other multichannel-based approaches in terms of the average retrieval precision and average retrieval rate.',\n",
       " '57a809faecdeb6c97160be4cab0d0b2f42ed3c6f': 'We investigate how to organize a large collection of geotagged photos, working with a dataset of about 35 million images collected from Flickr. Our approach combines content analysis based on text tags and image data with structural analysis based on geospatial data. We use the spatial distribution of where people take photos to define a relational structure between the photos that are taken at popular places. We then study the interplay between this structure and the content, using classification methods for predicting such locations from visual, textual and temporal features of the photos. We find that visual and temporal features improve the ability to estimate the location of a photo, compared to using just textual features. We illustrate using these techniques to organize a large photo collection, while also revealing various interesting properties about popular cities and landmarks at a global scale.',\n",
       " '98997f81cc77f53ee84e9b6df1edc253f9f9d5f9': \"We study the problem of personalized, interactive tag recommendation for Flickr: While a user enters/selects new tags for a particular picture, the system suggests related tags to her, based on the tags that she or other people have used in the past along with (some of) the tags already entered. The suggested tags are dynamically updated with every additional tag entered/selected. We describe a new algorithm, called Hybrid, which can be applied to this problem, and show that it outperforms previous algorithms. It has only a single tunable parameter, which we found to be very robust.\\n Apart from this new algorithm and its detailed analysis, our main contributions are (i) a clean methodology which leads to conservative performance estimates, (ii) showing how classical classification algorithms can be applied to this problem, (iii) introducing a new cost measure, which captures the effort of the whole tagging process, (iv) clearly identifying, when purely local schemes (using only a user's tagging history) can or cannot be improved by global schemes (using everybody's tagging history).\",\n",
       " '424f92289a632f85f6ba9a611614d145c7d3393a': 'Agile methodology such as Scrum, Extreme Programming (XP), Feature Driven Development (FDD) and the Dynamic System Development Method (DSDM) have gained enough recognition as efficient development process by delivering software fast even under the time constrains. However, like other agile methods DSDM has been criticized because of unavailability of security element in its four phases. In order to have a deeper look into the matter and discover more about the reality, we conducted a literature review. Our findings highlight that, in its current form, the DSDM does not support developing secure software. Although, there are a few researches on this topic about Scrum, XP and FDD but, based on our findings, there is no research on developing secure software using DSDM. Thus, in our future work we intend to propose enhanced DSDM that will cater the security aspects in software development.',\n",
       " 'cbd8a90e809151b684e73fb3e31c2731874570c4': \"AIM\\nTo present the findings of a literature review regarding nurses' intention to leave their employment or the profession.\\n\\n\\nBACKGROUND\\nThe nursing shortage is a problem that is being experienced worldwide. It is a problem that, left unresolved, could have a serious impact on the provision of quality health care. Understanding the reasons why nurses leave their employment or the profession is imperative if efforts to increase retention are to be successful.\\n\\n\\nEVALUATION\\nElectronic databases were systematically searched to identify English research reports about nurses' intention to leave their employment or the profession. Key results concerning the issue were extracted and synthesized.\\n\\n\\nKEY ISSUES\\nThe diversified measurement instruments, samples and levels of intention to leave caused difficulties in the attempt to compare or synthesize findings. The factors influencing nurses' intention to leave were identified and categorized into organizational and individual factors.\\n\\n\\nCONCLUSIONS\\nThe reasons that trigger nurses' intention to leave are complex and are influenced by organizational and individual factors. Further studies should be conducted to investigate how external factors such as job opportunities correlate with nurses' intention to leave.\\n\\n\\nIMPLICATIONS FOR NURSING MANAGEMENT\\nThe review provides insight that can be useful in designing and implementing strategies to maintain a sustainable workforce in nursing.\",\n",
       " '3df4b372489e734beeb70b320666354e53eb23c4': 'BACKGROUND/AIM\\nThe importance of play in the social development of children is undisputed. Even though children with attention-deficit hyperactivity disorder (ADHD) experience serious social problems, there is limited research on their play. By integrating literature on ADHD with literature on play, we can postulate how play is influenced by the characteristics of ADHD. These postulations enabled us to propose a theoretical model (proposed model) to depict the interactive process between the characteristics of ADHD and factors that promote play. This paper presents the revised model and principles for intervention based on the results of a study investigating the play of children with ADHD (reported elsewhere).\\n\\n\\nMETHODS\\nWe tested the proposed model in a study comparing two groups of children (n = 350) between the ages of 5 and 11 years. One group consisted of children diagnosed with ADHD (n = 112) paired with playmates (n = 112) who were typically developing; the control group consisted of typically developing children paired with typically developing playmates (n = 126). The Test of Playfulness was administered, and the model was revised in line with the findings.\\n\\n\\nRESULTS AND CONCLUSIONS\\nThe findings suggest difficulties in the social play and lack of interpersonal empathy in the play of children with ADHD. We draw on the revised model to propose preliminary principles for play-based interventions for children with ADHD. The principles emphasise the importance of capturing the motivation of children with ADHD, counteracting the effects of lack of interpersonal empathy, and considerations for including playmates in the intervention process.',\n",
       " 'a864fbfd34426c98b2832a3c2aa9fbc7df8bb910': \"This prospective study with 464 older adolescents (14 to 19 years at Time 1; 16 to 21 years at Time 2) tested the structural paths of influence through which perceived self-efficacy for affect regulation operates in concert with perceived behavioral efficacy in governing diverse spheres of psychosocial functioning. Self-efficacy to regulate positive and negative affect is accompanied by high efficacy to manage one's academic development, to resist social pressures for antisocial activities, and to engage oneself with empathy in others' emotional experiences. Perceived self-efficacy for affect regulation essentially operated mediationally through the latter behavioral forms of self-efficacy rather than directly on prosocial behavior, delinquent conduct, and depression. Perceived empathic self-efficacy functioned as a generalized contributor to psychosocial functioning. It was accompanied by prosocial behavior and low involvement in delinquency but increased vulnerability to depression in adolescent females.\",\n",
       " 'f4b98dbd75c87a86a8bf0d7e09e3ebbb63d14954': 'Making no claim of being exhaustive, a review of the most popular MFCC (Mel Frequency Cepstral Coefficients) implementations is made. These differ mainly in the particular approximation of the nonlinear pitch perception of human, the filter bank design, and the compression of the filter bank output. Then, a comparative evaluation of the presented implementations is performed on the task of text-independent speaker verification, by means of the well-known 2001 NIST SRE (speaker recognition evaluation) one-speaker detection database.',\n",
       " '7b143616c637734d6c89f28723e2ceb7aabc0389': 'Whilst affective responses to various forms and genres of multimedia content have been well researched, precious few studies have investigated the combined impact that multimedia system parameters and human factors have on affect. Consequently, in this paper we explore the role that two primordial dimensions of human factors personality and culture in conjunction with system factors frame rate, resolution, and bit rate have on user affect and enjoyment of multimedia presentations. To this end, a two-site, cross-cultural study was undertaken, the results of which produced three predictve models. Personality and Culture traits were shown statistically to represent 5.6% of the variance in positive affect, 13.6% in negative affect and 9.3% in enjoyment. The correlation between affect and enjoyment, was significant. Predictive modeling incorporating human factors showed about 8%, 7% and 9% improvement in predicting positive affect, negative affect and enjoyment respectively when compared to models trained only on system factors. Results and analysis indicate the significant role played by human factors in influencing affect that users experience while watching multimedia.',\n",
       " '906dc85636c056408f13f0d24b6d6f92ffb63113': 'The need for a representation language for ontology design patterns has long been recognized. However, the body of literature on the topic is still rather small and does not sufficiently reflect the diverse requirements on such a language. Herein, we propose a simple but useful and extendable approach which is fully compatible with the Web Ontology Language and should be easy to adopt by the community.',\n",
       " 'f3da8e33c90dc19a33d91a1b6b2ec4430f3b0315': 'Information from the different senses is seamlessly integrated by the brain in order to modify our behaviors and enrich our perceptions. It is only through the appropriate binding and integration of information from the different senses that a meaningful and accurate perceptual gestalt can be generated. Although a great deal is known about how such cross-modal interactions influence behavior and perception in the adult, there is little knowledge as to the impact of aging on these multisensory processes. In the current study, we examined the speed of discrimination responses of aged and young individuals to the presentation of visual, auditory or combined visual-auditory stimuli. Although the presentation of multisensory stimuli speeded response times in both groups, the performance gain was significantly greater in the aged. Most strikingly, multisensory stimuli restored response times in the aged to those seen in young subjects to the faster of the two unisensory stimuli (i.e., visual). The current results suggest that despite the decline in sensory processing that accompanies aging, the use of multiple sensory channels may represent an effective compensatory strategy to overcome these unisensory deficits.',\n",
       " '404574efdb5193dc6b69ffcfbf2190212ebfa43f': \"How do addictive drugs hijack the brain's reward system? This review speculates how normal, physiological reward processes may be affected by addictive drugs. Addictive drugs affect acute responses and plasticity in dopamine neurons and postsynaptic structures. These effects reduce reward discrimination, increase the effects of reward prediction error signals, and enhance neuronal responses to reward-predicting stimuli, which may contribute to compulsion. Addictive drugs steepen neuronal temporal reward discounting and create temporal myopia that impairs the control of drug taking. Tonically enhanced dopamine levels may disturb working memory mechanisms necessary for assessing background rewards and thus may generate inaccurate neuronal reward predictions. Drug-induced working memory deficits may impair neuronal risk signaling, promote risky behaviors, and facilitate preaddictive drug use. Malfunctioning adaptive reward coding may lead to overvaluation of drug rewards. Many of these malfunctions may result in inadequate neuronal decision mechanisms and lead to choices biased toward drug rewards.\",\n",
       " '64290c658d2f1c47ad4fd8757a87ac6c9a708f89': '\" This article uses an ecological approach to analyze factors in the effectiveness of work teams--small groups of interdependent individuals who share responsibility for outcomes for their organizations. Applications include advice and involvement, as in quality control circles and committees; production and service, as in assembly groups and sales teams; projects and development, as in engineering and research groups; and action and negotiation, as in sports teams and combat units. An analytic framework depicts team effectiveness as interdependent with organizational context, boundaries, and team development. Key context factors include (a) organizational culture, (b) technology and task design, (c) mission clarity, (d) autonomy, (e) rewards, ( f ) performance feedback, (g) training/consultation, and (h) physical environment. Team boundaries may mediate the impact of organizational context on team development. Current research leaves unanswered questions but suggests that effectiveness depends on organizational context and boundaries as much as on internal processes. Issues are raised for research and practice. The terms work team and work group appear often in today\\'s discussions of organizations. Some experts claim that to be effective modern firms need to use small teams for an increasing variety of jobs. For instance, in an article subtitled \"The Team as Hero,\" Reich (1987) wrote, If we are to compete in today\\'s world, we must begin to celebrate collective entrepreneurship, endeavors in which the whole of the effort is greater than the sum of individual contributions. We need to honor our teams more, our aggressive leaders and maverick geniuses less. (p. 78) Work teams occupy a pivotal role in what has been described as a management transformation (Walton, 1985), paradigm shift (Ketehum, 1984), and corporate renaissance (Kanter, 1983). In this management revolution, Peters (1988) advised that organizations use \"multi-function teams for all development activities\" (p. 210) and \"organize every function into tento thirty-person, largely self-managing teams\" (p. 296). Tornatzky (1986) pointed to new technologies that allow small work groups to take responsibility for whole products. Hackman (1986) predicted that, \"organizations in the future will rely heavily on member self-management\" (p. 90). Building blocks of such organizations are self-regulating work teams. But University of Tennessee University of Wisconsin--Eau Claire University o f Tennessee far from being revolutionary, work groups are traditional; \"the problem before us is not to invent more tools, but to use the ones we have\" (Kanter, 1983, p. 64). In this article, we explore applications of work teams and propose an analytic framework for team effectiveness. Work teams are defined as interdependent collections of individuals who share responsibility for specific outcomes for their organizations. In what follows, we first identify applications of work teams and then offer a framework for analyzing team effectiveness. Its facets make up topics of subsequent sections: organizational context, boundaries, and team development. We close with issues for research and practice. A p p l i c a t i o n s o f W o r k T e a m s Two watershed events called attention to the benefits of applying work teams beyond sports and mih\\'tary settings: the Hawthorne studies (Homans, 1950) and European experiments with autonomous work groups (Kelly, 1982). Enthusiasm has alternated with disenchantment (Bramel & Friend, 1987), but the 1980s have brought a resurgence of interest. Unfortunately, we have little evidence on how widely work teams are used or whether their use is expanding. Pasmore, Francis, Haldeman, and Shani (1982) reported that introduction of autonomous work groups was the most common intervention in 134 experiments in manufacturing firms. Production teams number among four broad categories of work team applications: (a) advice and involvement, (b) production and service, (c) projects and development, and (d) action and negotiation. Advice and Involvement Decision-making committees traditional in management now are expanding to first-line employees. Quality control (QC) circles and employee involvement groups have been common in the 1980s, often as vehicles for employee participation ( Cole, 1982 ). Perhaps several hundred thousand U.S. employees belong to QC circles (Ledford, Lawler, & Mohrman, 1988), usually first-line manufacturing employees who meet to identify opportunities for improvement. Some make and carry out proposals, but most have restricted scopes of activity and little working time, perhaps a few hours each month (Thompson, 1982). Employee involvement groups operate similarly, exploring ways to improve customer service (Peterfreund, 1982). 120 February 1990 • American Psychologist Copyright 1990 by the American Psyc2aological A~mciafion, Inc. 0003-066X/90/$00.75 Vol. 45, No. 2, 120-133 QC circles and employee involvement groups at times may have been implemented poorly (Shea, 1986), but they have been used extensively in some companies',\n",
       " '16deaf3986d996a7bf5f6188d39607c2e406a1f8': 'Clustering streaming data requires algorithms which are capable of updating clustering results for the incoming data. As data is constantly arriving, time for processing is limited. Clustering has to be performed in a single pass over the incoming data and within the possibly varying inter-arrival times of the stream. Likewise, memory is limited, making it impossible to store all data. For clustering, we are faced with the challenge of maintaining a current result that can be presented to the user at any given time. In this work, we propose a parameter free algorithm that automatically adapts to the speed of the data stream. It makes best use of the time available under the current constraints to provide a clustering of the objects seen up to that point. Our approach incorporates the age of the objects to reflect the greater importance of more recent data. Moreover, we are capable of detecting concept drift, novelty and outliers in the stream. For efficient and effective handling, we introduce the ClusTree, a compact and self-adaptive index structure for maintaining stream summaries. Our experiments show that our approach is capable of handling a multitude of different stream characteristics for accurate and scalable anytime stream clustering.',\n",
       " '31cc80ffb56d7f82dcc44e78fbdea95bffe5028e': 'We present a study of depth-disparity calibration for augmented reality applications using binocular optical see-through displays. Two techniques were proposed and compared. The \"paired-eyes\" technique leverages the Panum\\'s fusional area to help viewer find alignment between the virtual and physical objects. The \"separate-eyes\" technique eliminates the need of binocular fusion and involves using both eyes sequentially to check the virtual-physical object alignment on retinal images. We conducted a user study to measure the calibration results and assess the subjective experience of users with the proposed techniques.',\n",
       " '0c4867f11c9758014d591381d8b397a1d38b04a7': 'The first € price and the £ and $ price are net prices, subject to local VAT. Prices indicated with * include VAT for books; the €(D) includes 7% for Germany, the €(A) includes 10% for Austria. Prices indicated with ** include VAT for electronic products; 19% for Germany, 20% for Austria. All prices exclusive of carriage charges. Prices and other details are subject to change without notice. All errors and omissions excepted. C. Bishop Pattern Recognition and Machine Learning',\n",
       " '04c5268d7a4e3819344825e72167332240a69717': 'In this paper we introduce a template-based method for recognizing human actions called action MACH. Our approach is based on a maximum average correlation height (MACH) filter. A common limitation of template-based methods is their inability to generate a single template using a collection of examples. MACH is capable of capturing intra-class variability by synthesizing a single Action MACH filter for a given action class. We generalize the traditional MACH filter to video (3D spatiotemporal volume), and vector valued data. By analyzing the response of the filter in the frequency domain, we avoid the high computational cost commonly incurred in template-based approaches. Vector valued data is analyzed using the Clifford Fourier transform, a generalization of the Fourier transform intended for both scalar and vector-valued data. Finally, we perform an extensive set of experiments and compare our method with some of the most recent approaches in the field by using publicly available datasets, and two new annotated human action datasets which include actions performed in classic feature films and sports broadcast television.',\n",
       " '139a860b94e9b89a0d6c85f500674fe239e87099': 'We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application.',\n",
       " '1c01e44df70d6fde616de1ef90e485b23a3ea549': 'We introduce a new class of upper bounds on the log partition function of a Markov random field (MRF). This quantity plays an important role in various contexts, including approximating marginal distributions, parameter estimation, combinatorial enumeration, statistical decision theory, and large-deviations bounds. Our derivation is based on concepts from convex duality and information geometry: in particular, it exploits mixtures of distributions in the exponential domain, and the Legendre mapping between exponential and mean parameters. In the special case of convex combinations of tree-structured distributions, we obtain a family of variational problems, similar to the Bethe variational problem, but distinguished by the following desirable properties: i) they are convex, and have a unique global optimum; and ii) the optimum gives an upper bound on the log partition function. This optimum is defined by stationary conditions very similar to those defining fixed points of the sum-product algorithm, or more generally, any local optimum of the Bethe variational problem. As with sum-product fixed points, the elements of the optimizing argument can be used as approximations to the marginals of the original model. The analysis extends naturally to convex combinations of hypertree-structured distributions, thereby establishing links to Kikuchi approximations and variants.',\n",
       " '39a6cc80b1590bcb2927a9d4c6c8f22d7480fbdd': 'In this paper we introduce a 3-dimensional (3D) SIFT descriptor for video or 3D imagery such as MRI data. We also show how this new descriptor is able to better represent the 3D nature of video data in the application of action recognition. This paper will show how 3D SIFT is able to outperform previously used description methods in an elegant and efficient manner. We use a bag of words approach to represent videos, and present a method to discover relationships between spatio-temporal words in order to better describe the video data.',\n",
       " 'efaf07d40b9c5837639bed129794efc00f02e4c3': 'This paper presents work on using continuous representations for authorship attribution. In contrast to previous work, which uses discrete feature representations, our model learns continuous representations for n-gram features via a neural network jointly with the classification layer. Experimental results demonstrate that the proposed model outperforms the state-of-the-art on two datasets, while producing comparable results on the remaining two.',\n",
       " '6683426ca06560523fc7461152d4dd3b84a07854': 'Social tags are user-generated keywords associated with some resource on the Web. In the case of music, social tags have become an important component of “Web 2.0” recommender systems, allowing users to generate playlists based on use-dependent terms such as chill or jogging that have been applied to particular songs. In this paper, we propose a method for predicting these social tags directly from MP3 files. Using a set of 360 classifiers trained using the online ensemble learning algorithm FilterBoost, we map audio features onto social tags collected from the Web. The resulting automatic tags (or autotags) furnish information about music that is otherwise untagged or poorly tagged, allowing for insertion of previously unheard music into a social recommender. This avoids the “cold-start problem” common in such systems. Autotags can also be used to smooth the tag space from which similarities and recommendations are made by providing a set of comparable baseline tags for all tracks in a recommender system. Because the words we learn are the same as those used by people who label their music collections, it is easy to integrate our predictions into existing similarity and prediction methods based on web data.',\n",
       " 'd86e51d6e1215d792a9d00995d367b6161fc33e7': 'In the real world, a learning system could receive an input that looks nothing like anything it has seen during training, and this can lead to unpredictable behaviour. We thus need to know whether any given input belongs to the population distribution of the training data to prevent unpredictable behaviour in deployed systems. A recent surge of interest on this problem has led to the development of sophisticated techniques in the deep learning literature. However, due to the absence of a standardized problem formulation or an exhaustive evaluation, it is not evident if we can rely on these methods in practice. What makes this problem different from a typical supervised learning setting is that we cannot model the diversity of out-of-distribution samples in practice. The distribution of outliers used in training may not be the same as the distribution of outliers encountered in the application. Therefore, classical approaches that learn inliers vs. outliers with only two datasets can yield optimistic results. We introduce OD-test, a three-dataset evaluation scheme as a practical and more reliable strategy to assess progress on this problem. The OD-test benchmark provides a straightforward means of comparison for methods that address the out-of-distribution sample detection problem. We present an exhaustive evaluation of a broad set of methods from related areas on image classification tasks. Furthermore, we show that for realistic applications of high-dimensional images, the existing methods have low accuracy. Our analysis reveals areas of strength and weakness of each method.',\n",
       " '158d62f4e3363495148cf16c7b800daab7765760': 'We study statistical risk minimization problems under a privacy model in which the data is kept confidential even from the learner. In this local privacy framework, we establish sharp upper and lower bounds on the convergence rates of statistical estimation procedures. As a consequence, we exhibit a precise tradeoff between the amount of privacy the data preserves and the utility, as measured by convergence rate, of any statistical estimator or learning procedure.',\n",
       " '77071790f7e3a2ab7fb8b2a7e8d0a10e0debc5c1': \"This article presents the development of an autonomous motion planning algorithm for a soft planar grasping manipulator capable of grasp-and-place operations by encapsulation with uncertainty in the position and shape of the object. The end effector of the soft manipulator is fabricated in one piece without weakening seams using lost-wax casting instead of the commonly used multilayer lamination process. The soft manipulation system can grasp randomly positioned objects within its reachable envelope and move them to a desired location without human intervention. The autonomous planning system leverages the compliance and continuum bending of the soft grasping manipulator to achieve repeatable grasps in the presence of uncertainty. A suite of experiments is presented that demonstrates the system's capabilities.\",\n",
       " '3ce29949228103340391dbb57e38dd68d58e9b9e': 'OBJECTIVES\\nTo establish values of fetal left brachiocephalic vein (LBCV) dimensions during normal pregnancy and determine whether routine assessment of the LBCV may help in identifying fetuses with congenital abnormalities of this vessel.\\n\\n\\nMETHODS\\nFetal LBCV was assessed prospectively during ultrasound examinations in 431 normal singleton pregnancies. The visualization rate of the transverse view of the upper fetal chest at the level of drainage of the LBCV into the superior vena cava (SVC) by two-dimensional (2D) and 2D plus color Doppler ultrasound was evaluated. Reference ranges of LBCV diameter during non-complicated pregnancies were established. Interobserver and intraobserver measurement variability was analyzed. In addition, a retrospective review of the hospital medical records of 91 pregnancies with fetuses diagnosed with LBCV abnormalities was performed.\\n\\n\\nRESULTS\\nSonographic assessment of the fetal LBCV was consistently achieved in the second and third trimesters and in some fetuses in the first trimester of pregnancy. In normal fetuses LBCV diameter increased significantly throughout pregnancy, with a mean value of 0.7 mm at 11 weeks and 4.9 mm at term. Dilation of the fetal LBCV was noted in five cases of intracranial arteriovenous malformation and six cases of supracardiac type total anomalous pulmonary venous connection. Abnormal course of the LBCV was noted in 12 fetuses. In 63 fetuses with a persistent left SVC and a right SVC the LBCV was absent.\\n\\n\\nCONCLUSION\\nThis is the first study describing an effective sonographic approach for the assessment of fetal LBCV dimensions during pregnancy. The normative data may provide an additional means of detecting rare anomalies of systemic and pulmonary veins during pregnancy.',\n",
       " '9960994d81af14ae49a684883ea376a6bef41b2d': '-This paper is a study on the importance of drawing ( both formal drafting and informal sketching) during the process of mechanical design. Five hypotheses, focused on the t~es of drawings, their necessity in mechanical problem solving, and their relation to the external representation medium, are presented and supported. Support is through referenced studies in other domains and the results of protocol studies performed on five mechanical designers. Videotapes of all the marks-on-paper made by designers in representative sections of the design process were studied in detail for their type and purpose. The resulting data is supportive of the hypotheses. These results also give requirements for future computer aided design tools and graphics education, and goals for further studies. I. I N T R O D U C T I O N The goal of this paper is to study the importance of drawing (both formal drafting and informal sketching) during the process of mechanical design. This goal can be extended to state that we intend to show the necessio\" of drawing during all the developmental stages of a mechanical design. Through the information presented here, the requirements for future computer aided design tools, graphics education, and further studies will be developed. All mechanical engineers are taught drafting. Thus, most engineers are skilled at making and interpreting these formal mechanical drawings. These drawings are representations of a final design (the end product of the design process) and they are intended to archive the completed design and communicate it to other designers and manufacturing personnel. Additionally, engineers are notorious for not being able to think without making \"\\'back-of-the-envelope\" sketches of rough ideas. Sometimes these informal sketches serve to communicate a concept to a colleague, but more often they just help the idea take shape on paper. It is in considering how these sketches help an idea take form that gives a hint that drawing\\'s role in engineering is more than just to archive a concept or to communicate with others. Understanding the use of both drafting and sketching in design is important to help formulate the future development of Computer Aided Design or Drafting (CAD) systems. As CAD evolves and becomes more \"\\'intelligent,\\'\" the question of what attributes these systems must have becomes more important. In the past, CAD system attributes have primarily been driven from developments in the computer industry, it is only through understanding drawing\\'s importance in the design process that these systems can be based on design needs. Additionally, the pressures of CAD tool development, faculty time demands, and course expenses cause academic institutions to reevaluate the content of their \"graphics\\'\" courses. Understanding drawing\\'s importance in the design process helps establish what skills need to be taught to engineers during their training. This paper is organized by first, in Section 2, clarifying the types of drawings used in mechanical design. The hypotheses to be addressed in this paper are given in Section 3. A discussion of research on the understanding of visual imagery to be used as a basis for arguments in support of the hypotheses is in Section 4. In Section 5 is a discussion of the results of data taken on how mechanical engineers use drawings during design. Lastly, in Section 6, is a discussion of how well the hypotheses have been supported and the implications of our findings on CAD development, educational requirements, and future research directions. 2. TYPES OF D R A W I N G S USED IN DESIGN Engineers make many types of marks-on-paper. In research, to be described in Section 5, we have broken down these marks into two main groupings: support notation and graphic representations. Support notation includes textual notes, lists, dimensions (including leaders and arrows), and calculations. Graphic representations include drawings of objects and their functions, and plots and charts. Mechanical design graphic representations are often scale drawings made with mechanical instruments or CAD computer systems. These drawings, made in accordance with a set of widely accepted rules, are defined as having been drafted. Sketches, on the other hand, are defined as \"free-hand\" drawings. They are usually not to scale and may use shorthand notations to represent both objects and their function. A differentiation must be made between the act of graphic representation and the medium on which it occurs. The medium, whether it be paper and pencil, a computer stylus on a tablet, chalk on a blackboard, or other medium may put interface restrictions on the representation. The following discussions are concerned with what is being represented, not with how the representation is made. However, the discussions point to the medium\\'s restriction on representation and the need for improved interfaces. Another aspect of drawings to be considered is the level of abstraction of the information to be represented. During the design process, the design is refined',\n",
       " '2cbc1789ba0a5df8069948aa2dfbd080d8184fc9': 'This paper introduces our initial investigation on the problem of providing a semi-autonomous robot collaborator with anticipative capabilities to predict human actions. Anticipative robot behavior is a desired characteristic of robot collaborators that lead to fluid, proactive interactions. We are particularly interested in improving reactive methods that rely on human action recognition to activate the corresponding robot action. Action recognition invariably causes delay in the robot’s response, and the goal of our method is to eliminate this delay by predicting the next human action. Prediction is achieved by using a lookup table containing variations of assembly sequences, previously demonstrated by different users. The method uses the nearest neighbor sequence in the table that matches the actual sequence of human actions. At the movement level, our method uses a probabilistic representation of interaction primitives to generate robot trajectories. The method is demonstrated using a 7 degree-offreedom lightweight arm equipped with a 5-finger hand on an assembly task consisting of 17 steps.',\n",
       " '6217d2c64b6f843b2078dd0cf4fdb8ab15f06d43': 'The question of Jewish ancestry has been the subject of controversy for over two centuries and has yet to be resolved. The \"Rhineland hypothesis\" depicts Eastern European Jews as a \"population isolate\" that emerged from a small group of German Jews who migrated eastward and expanded rapidly. Alternatively, the \"Khazarian hypothesis\" suggests that Eastern European Jews descended from the Khazars, an amalgam of Turkic clans that settled the Caucasus in the early centuries CE and converted to Judaism in the 8th century. Mesopotamian and Greco-Roman Jews continuously reinforced the Judaized empire until the 13th century. Following the collapse of their empire, the Judeo-Khazars fled to Eastern Europe. The rise of European Jewry is therefore explained by the contribution of the Judeo-Khazars. Thus far, however, the Khazars\\' contribution has been estimated only empirically, as the absence of genome-wide data from Caucasus populations precluded testing the Khazarian hypothesis. Recent sequencing of modern Caucasus populations prompted us to revisit the Khazarian hypothesis and compare it with the Rhineland hypothesis. We applied a wide range of population genetic analyses to compare these two hypotheses. Our findings support the Khazarian hypothesis and portray the European Jewish genome as a mosaic of Near Eastern-Caucasus, European, and Semitic ancestries, thereby consolidating previous contradictory reports of Jewish ancestry. We further describe a major difference among Caucasus populations explained by the early presence of Judeans in the Southern and Central Caucasus. Our results have important implications for the demographic forces that shaped the genetic diversity in the Caucasus and for medical studies.',\n",
       " '8433bee637213243749bc3ef8bdbd61d9d3a0f3e': 'Falls can cause serious traumas such as brain injuries and bone fractures, especially among elderly people. Fear of falling might reduce physical activities resulting in declining social interactions and eventually causing depression. To lessen the effects of a fall, timely delivery of medical treatment can play a vital role. In a similar scenario, an IoT-based wearable system can pave the most promising way to mitigate serious consequences of a fall while providing the convenience of usage. However, to deliver sufficient degree of monitoring and reliability, wearable devices working at the core of fall detection systems are required to work for a prolonged period of time. In this work, we focus on energy efficiency of a wearable sensor node in an Internet-of-Things (IoT) based fall detection system. We propose the design of a tiny, lightweight, flexible and energy efficient wearable device. We investigate different parameters (e.g. sampling rate, communication bus interface, transmission protocol, and transmission rate) impacting on energy consumption of the wearable device. In addition, we provide a comprehensive analysis of energy consumption of the wearable in different configurations and operating conditions. Furthermore, we provide hints (hardware and software) for system designers implementing the optimal wearable device for IoT-based fall detection systems in terms of energy efficiency and high quality of service. The results clearly indicate that the proposed sensor node is novel and energy efficient. In a critical condition, the wearable device can be used continuously for 76 h with a 1000 mAh li-ion battery.',\n",
       " '27b7e8f3b11dfe12318f8ff10f1d4a60e144a646': 'Toxins from animal venoms are small peptides that recognize specific molecular targets in the brains of prey or predators. Next generation sequencing has uncovered thousands of diverse toxin sequences, but the functions of these peptides are poorly understood. Here we demonstrate that the use of machine learning techniques on sequence-derived features enables high accuracy in the task of predicting a toxin’s functionality using only its amino acid sequence. Comparison of the performance of several learning algorithms in this prediction task demonstrates that both physiochemical properties of the amino acid residues in a sequence as well as noncontiguous sequence motifs can be used independently to model the sequence dependence of venom function. We rationalize the observed model performance using unsupervised learning and make broad predictions about the distribution of toxin functions in the venome. Keywords—Bioinformatics, machine learning, protein function prediction, venomics.',\n",
       " 'bd8d9e1b3a192fcd045c7a3389920ac98097e774': \"Recent algorithmic progression has brought competitive classification accuracy despite constraining neural networks to binary weights (+1/-1). These findings show remarkable optimization opportunities to eliminate the need for computationally-intensive multiplications, reducing memory access and storage. In this paper, we present ParaPIM architecture, which transforms current Spin Orbit Torque Magnetic Random Access Memory (SOT-MRAM) sub-arrays to massively parallel computational units capable of running inferences for Binary-Weight Deep Neural Networks (BWNNs). ParaPIM's in-situ computing architecture can be leveraged to greatly reduce energy consumption dealing with convolutional layers, accelerate BWNNs inference, eliminate unnecessary off-chip accesses and provide ultra-high internal bandwidth. The device-to-architecture co-simulation results indicate ~4x higher energy efficiency and 7.3x speedup over recent processing-in-DRAM acceleration, or roughly 5x higher energy-efficiency and 20.5x speedup over recent ASIC approaches, while maintaining inference accuracy comparable to baseline designs.\",\n",
       " '7902e4fb3e30e085c0b88ea84c611be2b601f0d7': 'Code clones are common in software. When applying similar edits to clones, developers often find it difficult to examine the runtime behavior of clones. The problem is exacerbated when some clones are tested, while their counterparts are not. To reuse tests for similar but not identical clones, Grafter transplants one clone to its counterpart by (1) identifying variations in identifier names, types, and method call targets, (2) resolving compilation errors caused by such variations through code transformation, and (3) inserting stub code to transfer input data and intermediate output values for examination. To help developers examine behavioral differences between clones, Grafter supports fine-grained differential testing at both the test outcome level and the intermediate program state level. In our evaluation on three open source projects, Grafter successfully reuses tests in 94% of clone pairs without inducing build errors, demonstrating its automated code transplantation capability. To examine the robustness of G RAFTER, we systematically inject faults using a mutation testing tool, Major, and detect behavioral differences induced by seeded faults. Compared with a static cloning bug finder, Grafter detects 31% more mutants using the test-level comparison and almost 2X more using the state-level comparison. This result indicates that Grafter should effectively complement static cloning bug finders.',\n",
       " '0c8ce51f3384208518c328bd0306507079102d55': '1077-3142/$ see front matter 2011 Elsevier Inc. A doi:10.1016/j.cviu.2010.12.011 ⇑ Corresponding author. E-mail address: dmac@cs.toronto.edu (D. Macrini) The recognition of 3-D objects from their silhouettes demands a shape representation which is stable with respect to minor changes in viewpoint and articulation. This can be achieved by parsing a silhouette into parts and relationships that do not change across similar object views. Medial descriptions, such as skeletons and shock graphs, provide part-based decompositions but suffer from instabilities. As a result, similar shapes may be represented by dissimilar part sets. We propose a novel shape parsing approach which is based on identifying and regularizing the ligature structure of a medial axis, leading to a bone graph, a medial abstraction which captures a more stable notion of an object’s parts. Our experiments show that it offers improved recognition and pose estimation performance in the presence of within-class deformation over the shock graph. 2011 Elsevier Inc. All rights reserved.',\n",
       " '697bbd2f32b0eeb10783d87503d37e1e56ec5e2e': 'We introduce a method for simulating the inelastic deformation of thin shells: we model plasticity and fracture of curved, deformable objects such as light bulbs, egg-shells and bowls. Our novel approach uses triangle meshes yet evolves fracture lines unrestricted to mesh edges. We present a novel measure of bending strain expressed in terms of surface invariants such as lengths and angles. We also demonstrate simple techniques to improve the robustness of standard timestepping as well as collisionresponse algorithms.',\n",
       " '98431da7222ee3fe12d277facf5ca1561c56d4f3': 'Nonparametric density gradient estimation using a generalized kernel approach is investigated. Conditions on the kernel functions are derived to guarantee asymptotic unbiasedness, consistency, and uniform consistenby of the estimates. The results are generalized to obtain a simple mean-shift estimate that can be extended in a k-nearestneighbor approach. Applications of gradient estimation to pattern recognition are presented using clustering and intrinsic dimensionality problems, with the ultimate goal of providing further understanding of these problems in terms of density gradients.',\n",
       " '721e64bfd3158a77c55d59dd6415570594a72e9c': 'This study characterizes the NVIDIA Jetson TK1 and TX1 Platforms, both built on a NVIDIA Tegra System on Chip and combining a quad-core ARM CPU and an NVIDIA GPU. Their heterogeneous nature, as well as their wide operating frequency range, make it hard for application developers to reason about performance and determine which optimizations are worth pursuing. This paper attempts to inform developers’ choices by characterizing the platforms’ performance using Roofline models obtained through an empirical measurement-based approach as well as through a case study of a heterogeneous application (matrix multiplication). Our results highlight a difference of more than an order of magnitude in compute performance between the CPU and GPU on both platforms. Given that the CPU and GPU share the same memory bus, their Roofline models’ balance points are also more than an order of magnitude apart. We also explore the impact of frequency scaling: build CPU and GPU Roofline profiles and characterize both platforms’ balance point variation, power consumption, and performance per watt as frequency is scaled. The characterization we provide can be used in two main ways. First, given an application, it can inform the choice and number of processing elements to use (i.e., CPU/GPU and number of cores) as well as the optimizations likely to lead to high performance gains. Secondly, this characterization indicates that developers can use frequency scaling to tune the Jetson Platform to suit the requirements of their applications. Third, given a required power/performance budget, application developers can identify the appropriate parameters to use to tune the Jetson platforms to their specific workload requirements. We expect that this optimization approach can lead to overall gains in performance and/or power efficiency without requiring application changes.',\n",
       " '3972dc2d2306c48135b6dfa587a5433d0b75b1cd': 'We present an attention-based model for end-to-end handwriting recognition. Our system does not require any segmentation of the input paragraph. The model is inspired by the differentiable attention models presented recently for speech recognition, image captioning or translation. The main difference is the implementation of covert and overt attention with a multi-dimensional LSTM network. Our principal contribution towards handwriting recognition lies in the automatic transcription without a prior segmentation into lines, which was critical in previous approaches. Moreover, the system is able to learn the reading order, enabling it to handle bidirectional scripts such as Arabic. We carried out experiments on the well-known IAM Database and report encouraging results which bring hope to perform full paragraph transcription in the near future.',\n",
       " '6cbb6db2561ecee3b24e22ee060b01068cba6b5a': 'OBJECTIVES\\nAccidental displacement of endosseous implants into the maxillary sinus is an unusual but potential complication in implantology procedures due to the special features of the posterior aspect of the maxillary bone; there is also a possibility of migration throughout the upper paranasal sinuses and adjacent structures. The aim of this paper is to review the published literature about accidental displacement and migration of dental implants into the maxillary sinus and other adjacent structures.\\n\\n\\nSTUDY DESIGN\\nA review has been done based on a search in the main on-line medical databases looking for papers about migration of dental implants published in major oral surgery, periodontal, dental implant and ear-nose-throat journals, using the keywords \"implant,\" \"migration,\" \"complication,\" \"foreign body\" and \"sinus.\"\\n\\n\\nRESULTS\\n24 articles showing displacement or migration to maxillary, ethmoid and sphenoid sinuses, orbit and cranial fossae, with different degrees of associated symptoms, were identified. Techniques found to solve these clinical issues include Cadwell-Luc approach, transoral endoscopy approach via canine fossae and transnasal functional endoscopy surgery.\\n\\n\\nCONCLUSION\\nBefore removing the foreign body, a correct diagnosis should be done in order to evaluate the functional status of the ostiomeatal complex and the degree of affectation of paranasal sinuses and other involved structures, determining the size and the exact location of the foreign body. After a complete diagnosis, an indicated procedure for every case would be decided.',\n",
       " 'bc6f2144ab55022e10d623f94f3398595547be38': 'It has long been claimed that Homo sapiens is the only species that has language, but only recently has it been recognized that humans also have an unusual pattern of growth and development. Social mammals have two stages of pre-adult development: infancy and juvenility. Humans have two additional prolonged and pronounced life history stages: childhood, an interval of four years extending between infancy and the juvenile period that follows, and adolescence, a stage of about eight years that stretches from juvenility to adulthood. We begin by reviewing the primary biological and linguistic changes occurring in each of the four pre-adult ontogenetic stages in human life history. Then we attempt to trace the evolution of childhood and juvenility in our hominin ancestors. We propose that several different forms of selection applied in infancy and childhood; and that, in adolescence, elaborated vocal behaviors played a role in courtship and intrasexual competition, enhancing fitness and ultimately integrating performative and pragmatic skills with linguistic knowledge in a broad faculty of language. A theoretical consequence of our proposal is that fossil evidence of the uniquely human stages may be used, with other findings, to date the emergence of language. If important aspects of language cannot appear until sexual maturity, as we propose, then a second consequence is that the development of language requires the whole of modern human ontogeny. Our life history model thus offers new ways of investigating, and thinking about, the evolution, development, and ultimately the nature of human language.',\n",
       " '8b0723fa5c33193386f1040ca9991abca969a827': 'The present study explored the role of gender in the association between Internet addiction and depression. Three-wave longitudinal panel data were collected from self-reported questionnaires that were completed by 1715 adolescents in grades 6e8 in China. Cross-lagged structural equation modeling was used to examine the relationship between Internet addiction and depression. In male adolescents, depression was found to significantly predict subsequent Internet addiction, suggesting that depression was the cause of Internet addiction and supporting the mood enhancement hypothesis. In female adolescents, Internet addiction was found to significantly predict subsequent depression, indicating that Internet addiction leads to depression and supporting the social displacement hypothesis. These results indicate that the relationship between Internet addiction and depression depended on gender. In addition, it was found that males and females exhibit different behavioral patterns and motivations of Internet usage. Males were more likely to use the Internet for pleasure and less likely to surf the Internet to search for information, compared with females. Although both males and females were prone to surfing the Internet alone, males were more likely to go online with friends compared with females. These findings suggest that gender-specific preventative and interventional strategies should be developed to reduce Internet addiction. © 2016 Elsevier Ltd. All rights reserved.',\n",
       " '383f1f2ceb32557690b6a0abf6aab48cb98552ff': 'Twitter is widely seen as being the go to place for breaking news. Recently however, competing Social Media have begun to carry news. Here we examine how Facebook, Google Plus and Twitter report on breaking news. We consider coverage (whether news events are reported) and latency (the time when they are reported). Using data drawn from three weeks in December 2013, we identify 29 major news events, ranging from celebrity deaths, plague outbreaks to sports events. We find that all media carry the same major events, but Twitter continues to be the preferred medium for breaking news, almost consistently leading Facebook or Google Plus. Facebook and Google Plus largely repost newswire stories and their main research value is that they conveniently package multitple sources of information together.',\n",
       " 'c97901da440e70bb6085b118d5f3f3190fc5eaf0': 'A compact cross-shaped slotted microstrip patch antenna is proposed for circularly polarized (CP) radiation. A symmetric, cross shaped slot is embedded along one of the diagonal axes of the square patch for CP radiation and antenna size reduction. The structure is asymmetric (unbalanced) along the diagonal axes. The overall size of the antenna with CP radiation can be reduced by increasing the perimeter of the symmetric cross-shaped slot within the first patch quadrant of the square patch. The performance of the CP radiation is also studied by varying the size and angle variation of the cross-shaped slot. A measured 3-dB axial-ratio (AR) bandwidth of around 6.0 MHz is achieved with the CP cross-shaped slotted microstrip antenna, with an 18.0 MHz 10-dB return-loss bandwidth. The measured boresight gain is more than 3.8 dBic over the operating band, while the overall antenna volume is 0.273λo × 0.273λo × 0.013λo (λο operating wavelength at 910 MHz).',\n",
       " '35c12a61ada36fd9b9f89176c927bb53af6f2466': \"Recent reports indicate 97% of youth are connected to the Internet. As more young people have access to online communication, it is integrally important to identify youth who may be more vulnerable to negative experiences. Based upon accounts of traditional bullying, youth with depressive symptomatology may be especially likely to be the target of Internet harassment. The current investigation will examine the cross-sectional relationship between depressive symptomatology and Internet harassment, as well as underlying factors that may help explain the observed association. Youth between the ages of 10 and 17 (N = 1,501) participated in a telephone survey about their Internet behaviors and experiences. Subjects were required to have used the Internet at least six times in the previous 6 months to ensure a minimum level of exposure. The caregiver self-identified as most knowledgeable about the young person's Internet behaviors was also interviewed. The odds of reporting an Internet harassment experience in the previous year were more than three times higher (OR: 3.38, CI: 1.78, 6.45) for youth who reported major depressive symptomatology compared to mild/absent symptomatology. When female and male respondents were assessed separately, the adjusted odds of reporting Internet harassment for males who also reported DSM IV symptoms of major depression were more than three times greater (OR: 3.64, CI: 1.16, 11.39) than for males who indicated mild or no symptoms of depression. No significant association was observed among otherwise similar females. Instead, the association was largely explained by differences in Internet usage characteristics and other psychosocial challenges. Internet harassment is an important public mental health issue affecting youth today. Among young, regular Internet users, those who report DSM IV-like depressive symptomatology are significantly more likely to also report being the target of Internet harassment. Future studies should focus on establishing the temporality of events, that is, whether young people report depressive symptoms in response to the negative Internet experience, or whether symptomatology confers risks for later negative online incidents. Based on these cross-sectional results, gender differences in the odds of reporting an unwanted Internet experience are suggested, and deserve special attention in future studies.\",\n",
       " 'c1b8ba97aa88210a02affe2f92826e059c729c8b': 'Legged robots are able to move across irregular terrains and some can be energy efficient, but are often constrained by a limited range of gaits which can limit their locomotion capabilities considerably. This paper reports a reconfigurable design approach to robotic legged locomotion that produces a wide variety of gait cycles, opening new possibilities for innovative applications. In this paper, we present a distance-based formulation and its application to solve the position analysis problem of a standard Theo Jansen mechanism. By changing the configuration of a linkage, our objective in this study is to identify novel gait patterns of interest for a walking platform. The exemplary gait variations presented in this work demonstrate the feasibility of our approach, and considerably extend the capabilities of the original design to not only produce novel cum useful gait patterns but also to realize behaviors beyond locomotion.',\n",
       " '018300f5f0e679cee5241d9c69c8d88e00e8bf31': '•We introduce a simple, efficient, and general method for training directed latent variable models. – Can handle both discrete and continuous latent variables. – Easy to apply – requires no model-specific derivations. •Key idea: Train an auxiliary neural network to perform inference in the model of interest by optimizing the variational bound. – Was considered before for Helmholtz machines and rejected as infeasible due to high variance of inference net gradient estimates. •We make the approach practical using simple and general variance reduction techniques. •Promising document modelling results using sigmoid belief networks.',\n",
       " '0a10d64beb0931efdc24a28edaa91d539194b2e2': 'We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.',\n",
       " '32cbd065ac9405530ce0b1832a9a58c7444ba305': 'We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.',\n",
       " '040522d17bb540726a2e8d45ee264442502723a0': 'Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.',\n",
       " '3be23e51455b39a2819ecfd86b8bb5ba4716679f': 'In this paper, we present a flexible new technique for single viewpoint omnidirectional camera calibration. The proposed method only requires the camera to observe a planar pattern shown at a few different orientations. Either the camera or the planar pattern can be freely moved. No a priori knowledge of the motion is required, nor a specific model of the omnidirectional sensor. The only assumption is that the image projection function can be described by a Taylor series expansion whose coefficients are estimated by solving a two-step least-squares linear minimization problem. To test the proposed technique, we calibrated a panoramic camera having a field of view greater than 200 in the vertical direction, and we obtained very good results. To investigate the accuracy of the calibration, we also used the estimated omni-camera model in a structure from motion experiment. We obtained a 3D metric reconstruction of a scene from two highly distorted omnidirectional images by using image correspondences only. Compared with classical techniques, which rely on a specific parametric model of the omnidirectional camera, the proposed procedure is independent of the sensor, easy to use, and flexible.',\n",
       " '7eb7d3529adf3954a7704d0e502178ca10c79e0b': 'Benford’s law has been promoted as providing the auditor with a tool that is simple and effective for the detection of fraud. The purpose of this paper is to assist auditors in the most effective use of digital analysis based on Benford’s law. The law is based on a peculiar observation that certain digits appear more frequently than others in data sets. For example, in certain data sets, it has been observed that more than 30% of numbers begin with the digit one. After discussing the background of the law and development of its use in auditing, we show where digital analysis based on Benford’s law can most effectively be used and where auditors should exercise caution. Specifically, we identify data sets which can be expected to follow Benford’s distribution, discuss the power of statistical tests, types of frauds that would be detected and not be detected by such analysis, the potential problems that arise when an account contains too few observations, as well as issues related to base rate of fraud. An actual example is provided demonstrating where Benford’s law proved successful in identifying fraud in a population of accounting data.',\n",
       " 'f2bce820b7f0f3ccf0554b105bfa2ded636db77a': 'In 2 experiments, students who lacked prior knowledge about car mechanics read a passage about vehicle braking systems that either contained labeled illustrations of the systems, illustrations without labels, labels without illustrations, or no labeled illustrations. Students who received passages that contained labeled illustrations of braking systems recalled more explanative than nonexplanative information as compared to control groups, and performed better on problem solving transfer but not on verbatim recognition as compared to control groups. Results support a model of meaningful learning in which illustrations can help readers to focus their attention on explanative information in text and to reorganize the information into useful mental models.',\n",
       " 'b688b830da148f1c3a86916a42d9dd1b1cccd5ff': 'To achieve dynamic inference in pixel labeling tasks, we propose Pixel-wise Attentional Gating (PAG), which learns to selectively process a subset of spatial locations at each layer of a deep convolutional network. PAG is a generic, architecture-independent, problem-agnostic mechanism that can be readily \"plugged in\" to an existing model with fine-tuning. We utilize PAG in two ways: 1) learning spatially varying pooling fields that improve model performance without the extra computation cost associated with multi-scale pooling, and 2) learning a dynamic computation policy for each pixel to decrease total computation (FLOPs) while maintaining accuracy. We extensively evaluate PAG on a variety of per-pixel labeling tasks, including semantic segmentation, boundary detection, monocular depth and surface normal estimation. We demonstrate that PAG allows competitive or state-of-the-art performance on these tasks. Our experiments show that PAG learns dynamic spatial allocation of computation over the input image which provides better performance trade-offs compared to related approaches (e.g., truncating deep models or dynamically skipping whole layers). Generally, we observe PAG can reduce computation by 10% without noticeable loss in accuracy and performance degrades gracefully when imposing stronger computational constraints.',\n",
       " '84599b3defa3dfa9cfcd33c1339ce422aa5d2b68': \"A current mode control integrated circuit with accuracy current sensing circuit ( CSC ) for buck converter is presented in this the proposed accurate integrated current sensed inductor with the internal ramp be used for DC-DC converter feedback proposed CSC doesn't need an op amp to implement, and has been fabricated with a standard 0.35 mum CMOS process. Simulation result show that the switching converter can be operated up to 1 MHZ. The supply is suitable for signal cell lithium-ion battery supply power efficiency is over 85% for supply voltage from 2.5 V to 5 V and output current is 200 mA. The performance of the proposed circuit is the good compared to the other circuits.\",\n",
       " '1eb7ba50214d0a7bb3247a0055f5700b7833c17e': 'Although the matrix completion paradigm provides an appealing solution to the collaborative filtering problem in recommendation systems, some major issues, such as data sparsity and cold-start problems, still remain open. In particular, when the rating data for a subset of users or items is entirely missing, commonly known as the  cold-start problem, the standard matrix completion methods are inapplicable due the non-uniform sampling of available ratings. In recent years, there has been considerable interest in dealing with cold-start users or items that are principally based on the idea of exploiting other sources of information to compensate for this lack of rating data. In this paper, we propose a novel and general algorithmic framework based on matrix completion that simultaneously exploits the similarity information among users and items to alleviate the cold-start problem. In contrast to existing methods, our proposed recommender algorithm, dubbed DecRec, decouples the following two aspects of the cold-start problem to effectively exploit the side information: (i) the completion of a rating sub-matrix, which is generated by excluding cold-start users/items from the original rating matrix; and (ii) the transduction of knowledge from existing ratings to cold-start items/users using side information. This crucial difference prevents the error propagation of completion and transduction, and also significantly boosts the performance when appropriate side information is incorporated. The recovery error of the proposed algorithm is analyzed theoretically and, to the best of our knowledge, this is the first algorithm that addresses the cold-start problem with provable guarantees on performance. Additionally, we also address the problem where both cold-start user and item challenges are present simultaneously. We conduct thorough experiments on real datasets that complement our theoretical results. These experiments demonstrate the effectiveness of the proposed algorithm in handling the cold-start users/items problem and mitigating data sparsity issue.',\n",
       " '17f537b9f39cdb37ec26100530f69c615d03fa3b': \"Extracting keywords and keyphrases mainly for identifying content of a document, has an importance role in text processing tasks such as text summarization, information retrieval, and query expansion. In this research, we introduce a new keyword/keyphrase extraction approach in which both single and multi-document keyword/keyphrase extraction techniques are considered. The proposed approach is specifically practical when a user is interested in additional data such as keywords/keyphrases related to a topic or query. In the proposed approach, first a set of documents are retrieved based on user's query, then a single document keyword extraction method is applied to extract candidate keyword/keyphrases from each retrieved document. Finally, a new re-scoring scheme is introduced to extract final keywords/keyphrases. We have evaluated the proposed method based on the relationship between the final keyword/keyphrases with the initial user query, and based user's satisfaction. Our experimental results show how much the extracted keywords/keyphrases are relevant and wellmatched with user's need.\",\n",
       " '1d06bfa37282bda43a396dc99927b298d0288bfa': 'Next Point-of-Interest (POI) recommendation has become an important task for location-based social networks (LBSNs). However, previous efforts suffer from the high computational complexity, besides the transition pattern between POIs has not been well studied. In this paper, we proposed a twofold approach for next POI recommendation. First, the preferred next category is predicted by using a third-rank tensor optimized by a Listwise Bayesian Personalized Ranking (LBPR) approach. Specifically we introduce two functions, namely PlackettLuce model and cross entropy, to generate the likelihood of a ranking list for posterior computation. Then POI candidates filtered by the predicated category are ranked based on the spatial influence and category ranking influence. The experiments on two real-world datasets demonstrate the significant improvements of our methods over several state-ofthe-art methods.',\n",
       " '49b3d71c415956a31b1031ae22920af6ea5bec9a': 'With the advances of information communication technologies, it is critical to improve the efficiency and accuracy of emergency management systems through modern data processing techniques. The past decade has witnessed the tremendous technical advances in Sensor Networks, Internet/Web of Things, Cloud Computing, Mobile/Embedded Computing, Spatial/Temporal Data Processing, and Big Data, and these technologies have provided new opportunities and solutions to emergency management. GIS models and simulation capabilities are used to exercise response and recovery plans during non-disaster times. They help the decision-makers understand near real-time possibilities during an event. In this paper, a crowdsourcing based model for mining spatial information of urban emergency events is introduced. Firstly, basic definitions of the proposed method are given. Secondly, positive samples are selected to mine the spatial information of urban emergency events. Thirdly, location and GIS information are extracted from positive samples. At last, the real spatial information is determined based on address and GIS information. At last, a case study on an urban emergency event is given.',\n",
       " 'a00bd22c2148fc0c2c32300742d9390431949f56': 'Vegetarianism within the U.K. is growing in popularity, with the current estimate of 7% of the population eating a vegetarian diet. This study examined differences between the attitudes and beliefs of four dietary groups (meat eaters, meat avoiders, vegetarians and vegans) and the extent to which attitudes influenced intentions to follow each diet. In addition, the role of attitudinal ambivalence as a moderator variable was examined. Completed questionnaires were obtained from 111 respondents (25 meat eaters, 26 meat avoiders, 34 vegetarians, 26 vegans). In general, predictions were supported, in that respondents displayed most positive attitudes and beliefs towards their own diets, and most negative attitudes and beliefs towards the diet most different form their own. Regression analyses showed that, as predicted by the Theory of Planned Behaviour, attitudes, subjective norm and perceived behavioural control were significant predictors of intention to follow each diet (apart from the vegetarian diet, where subjective norm was non-significant). In each case, attitudinal ambivalence was found to moderate the attitude-intention relationship, such that attitudes were found to be stronger predictors at lower levels of ambivalence. The results not only highlight the extent to which such alternative diets are an interesting focus for psychological research, but also lend further support to the argument that ambivalence in an important influence on attitude strength.',\n",
       " 'b07bfdebdf11b7ab3ea3d5f0087891c464c5e34d': 'A 64-element 29–30GHz active phased array for 5G millimeter wave applications is presented in this paper. The proposed phased array composites of 64-element antennas, 64-chan-nel T/R modules, 4 frequency conversion links, beam controlling circuitry, power management circuits and cooling fans, and are integrated in a in a very compact size(135mmX 77mmX56mm). Hybrid integration of GaAs and Si circuits are employed to achieve better RF performance. The architecture of the proposed phased array and the detail design of the T/R modules and antennas are analyzed. By the OTA (over the air) measurement, the proposed phased array achieves a bandwidth of 1 GHz at the center frequency of 29.5GHz, and the azimuth beam-width is 12 deg with the scanning range of ±45deg. With the excitation of 800MHz 64QAM signals, the transmitter beam achieves a EVM of 5.5%, ACLR of −30.5dBc with the PA working at −10dB back off, and the measured saturated EIRP is 63 dBm.',\n",
       " '2472198a01624e6e398518929c88d8ead6a33473': 'Mobile cloud computing (MCC) has become a significant paradigm for bringing the benefits of cloud computing to mobile devices’ proximity. Service availability along with performance enhancement and energy efficiency are primary targets in MCC. This paper proposes a code offloading framework, called mCloud, which consists of mobile devices, nearby cloudlets and public cloud services, to improve the performance and availability of the MCC services. The effect of the mobile device context (e.g., network conditions) on offloading decisions is studied by proposing a context-aware offloading decision algorithm aiming to provide code offloading decisions at runtime on selecting wireless medium and appropriate cloud resources for offloading. We also investigate failure detection and recovery policies for our mCloud system. We explain in details the design and implementation of the mCloud prototype framework. We conduct real experiments on the implemented system to evaluate the performance of the algorithm. Results indicate the system and embedded decision algorithm are able to provide decisions on selecting wireless medium and cloud resources based on different context of the mobile devices, and achieve significant reduction on makespan and energy, with the improved service availability when compared with existing offloading schemes.',\n",
       " '1613a9fe64fbc2228e52b021ad45041556cc77ef': 'OBJECTIVE\\nPathologic scarring affects millions of people worldwide. Quantitative and qualitative measurement modalities are needed to effectively evaluate and monitor treatments.\\n\\n\\nMETHODS\\nThis article reviews the literature on available tools and existent assessment scales used to subjectively and objectively characterize scar.\\n\\n\\nRESULTS\\nWe describe the attributes and deficiencies of each tool and scale and highlight areas where further development is critical.\\n\\n\\nCONCLUSION\\nAn optimal, universal scar scoring system is needed in order to better characterize, understand and treat pathologic scarring.',\n",
       " '9368b596fdc2af12a45defd3df6c94e39dd02d3a': 'Ever growing number of image documents available on the Internet continuously motivates research in better annotation models and more efficient retrieval methods. Formal knowledge representation of objects and events in pictures, their interaction as well as context complexity becomes no longer an option for a quality image repository, but a necessity. We present an ontologybased online image annotation tool WNtags and demonstrate its usefulness in several typical multimedia retrieval tasks using International Affective Picture System emotionally annotated image database. WNtags is built around WordNet lexical ontology but considers Suggested Upper Merged Ontology as the preferred labeling formalism. WNtags uses sets of weighted WordNet synsets as high-level image semantic descriptors and query matching is performed with word stemming and node distance metrics. We also elaborate our near future plans to expand image content description with induced affect as in stimuli for research of human emotion and attention.',\n",
       " '8443e3b50190f297874d2d76233f29dfb423069c': 'To cope with concept drift, we paired a stable online learner with a reactive one. A stable learner predicts based on all of its experience, whereas are active learner predicts based on its experience over a short, recent window of time. The method of paired learning uses differences in accuracy between the two learners over this window to determine when to replace the current stable learner, since the stable learner performs worse than does there active learner when the target concept changes. While the method uses the reactive learner as an indicator of drift, it uses the stable learner to predict, since the stable learner performs better than does the reactive learner when acquiring target concept. Experimental results support these assertions. We evaluated the method by making direct comparisons to dynamic weighted majority, accuracy weighted ensemble, and streaming ensemble algorithm (SEA) using two synthetic problems, the Stagger concepts and the SEA concepts, and three real-world data sets: meeting scheduling, electricity prediction, and malware detection. Results suggest that, on these problems, paired learners outperformed or performed comparably to methods more costly in time and space.',\n",
       " '0b2fe16ea31f59e44a0d244a12554d9554740b63': \"Roadside Unit (RSU) is an essential unit in a vehicular ad-hoc network (VANET) for collecting and analyzing traffic data given from smart vehicles. Furthermore, RSUs can take part in controlling traffic flow for vehicle's secure driving by broadcasting locally analyzed data, forwarding some important messages, and communicating with other RSUs, and soon. In order to maximize the availability of RSUs in the VANET, RSUs need to be fully distributed over an entire area. Thus, RSUs can make the best use of all traffic data gathered from every intersection. In this paper, we provide intersection-priority based RSU placement methods to find the optimal number and positions of RSUs for the full distribution providing with a maximal connectivity between RSUs while minimizing RSU setup costs. We propose three optimal algorithms: greedy, dynamic and hybrid algorithms. Finally, we provide simulated analyses of our algorithms using real urban roadmaps of JungGu/Jongrogu, YongsanGu, and GangnamGu in Seoul, each of which has a characteristic road style different than each other. We analyze how our algorithms work in such different types of roadways with real traffic data, and find the optimal number and positions of RSUs in these areas.\",\n",
       " '6c3c36fbc2cf24baf2301e80da57ed68cab97cd6': 'This paper addresses the repeated acquisition of labels for data items when the labeling is imperfect. We examine the improvement (or lack thereof) in data quality via repeated labeling, and focus especially on the improvement of training labels for supervised induction of predictive models. With the outsourcing of small tasks becoming easier, for example via Amazon’s Mechanical Turk, it often is possible to obtain less-than-expert labeling at low cost. With low-cost labeling, preparing the unlabeled part of the data can become considerably more expensive than labeling. We present repeated-labeling strategies of increasing complexity, and show several main results. (i)\\xa0Repeated-labeling can improve label quality and model quality, but not always. (ii)\\xa0When labels are noisy, repeated labeling can be preferable to single labeling even in the traditional setting where labels are not particularly cheap. (iii)\\xa0As soon as the cost of processing the unlabeled data is not free, even the simple strategy of labeling everything multiple times can give considerable advantage. (iv)\\xa0Repeatedly labeling a carefully chosen set of points is generally preferable, and we present a set of robust techniques that combine different notions of uncertainty to select data points for which quality should be improved. The bottom line: the results show clearly that when labeling is not perfect, selective acquisition of multiple labels is a strategy that data miners should have in their repertoire; for certain label-quality/cost regimes, the benefit is substantial.',\n",
       " '1d1da2ef88928cf6174c9c53e0543665bc285b68': 'Learning to read in a second language is challenging, but highly rewarding. For low-income children in developing countries, this task can be significantly more challenging because of lack of access to high-quality schooling, but can potentially improve economic prospects at the same time. A synthesis of research findings suggests that practicing recalling and vocalizing words for expressing an intended meaning could improve word reading skills - including reading in a second language - more than silent recognition of what the given words mean. Unfortunately, many language learning software do not support this instructional approach, owing to the technical challenges of incorporating speech recognition support to check that the learner is vocalizing the correct word. In this paper, we present results from a usability test and two subsequent experiments that explore the use of two speech recognition-enabled mobile games to help rural children in India read words with understanding. Through a working speech recognition prototype, we discuss two major contributions of this work: first, we give empirical evidence that shows the extent to which productive training (i.e. vocalizing words) is superior to receptive vocabulary training, and discuss the use of scaffolding hints to \"\"unpack\"\" factors in the learner\\'s linguistic knowledge that may impact reading. Second, we discuss what our results suggest for future research in HCI.',\n",
       " '742c42f5cd7c1f195fa83c8c1611ee7e62c9c81f': \"Modern enterprises almost ubiquitously deploy middlebox processing services to improve security and performance in their networks. Despite this, we find that today's middlebox infrastructure is expensive, complex to manage, and creates new failure modes for the networks that use them. Given the promise of cloud computing to decrease costs, ease management, and provide elasticity and fault-tolerance, we argue that middlebox processing can benefit from outsourcing the cloud. Arriving at a feasible implementation, however, is challenging due to the need to achieve functional equivalence with traditional middlebox deployments without sacrificing performance or increasing network complexity.\\n In this paper, we motivate, design, and implement APLOMB, a practical service for outsourcing enterprise middlebox processing to the cloud.\\n Our discussion of APLOMB is data-driven, guided by a survey of 57 enterprise networks, the first large-scale academic study of middlebox deployment. We show that APLOMB solves real problems faced by network administrators, can outsource over 90% of middlebox hardware in a typical large enterprise network, and, in a case study of a real enterprise, imposes an average latency penalty of 1.1ms and median bandwidth inflation of 3.8%.\",\n",
       " '6b3c3e02cfc46ca94097934bec18333dde7cf77c': 'Data and network system security is the most important roles. An organization should find the methods to protect their data and network system to reduce the risk from attacks. Snort Intrusion Detection System (Snort-IDS) is a security tool of network security. It has been widely used for protecting the network of the organizations. The Snort-IDS utilize the rules to matching data packets traffic. If some packet matches the rules, Snort-IDS will generate the alert messages. However, Snort-IDS contain many rules and it also generates a lot of false alerts. In this paper, we present the procedure to improve the Snort-IDS rules for the network probe attack detection. In order to test the performance evaluation, we utilized the data set from the MIT-DAPRA 1999, which includes the normal and abnormal traffics. Firstly, we analyzed and explored the existing the Snort-IDS rules to improve the proposed Snort-IDS rules. Secondly, we applied the WireShark software to analyze data packets form of attack in data set. Finally, the Snort-IDS was improved, and it can detect the network probe attack. This paper, we had classified the attacks into several groups based on the nature of network probe attack. In addition, we also compared the efficacy of detection attacks between Snort-IDS rules to be updated with the Detection Scoring Truth. As the experimental results, the proposed Snort-IDS efficiently detected the network probe attacks compared to the Detection Scoring Truth. It can achieve higher accuracy. However, there were some detecting alert that occur over the attack in Detection Scoring Truth, because some attack occur in several time but the Detection Scoring Truth indentify as one time.',\n",
       " '5f507abd8d07d3bee56820fd3a5dc2234d1c38ee': '',\n",
       " '4dda236c57d9807d811384ffa714196c4999949d': 'Measuring the connection strength between a pair of vertices in a graph is one of the most important concerns in many graph applications. Simple measures such as edge weights may not be sufficient for capturing the effects associated with short paths of lengths greater than one. In this paper, we consider an iterative process that smooths an associated value for nearby vertices, and we present a measure of the local connection strength (called the algebraic distance, see [25]) based on this process. The proposed measure is attractive in that the process is simple, linear, and easily parallelized. An analysis of the convergence property of the process reveals that the local neighborhoods play an important role in determining the connectivity between vertices. We demonstrate the practical effectiveness of the proposed measure through several combinatorial optimization problems on graphs and hypergraphs.',\n",
       " '2e7ebdd353c1de9e47fdd1cf0fce61bd33d87103': 'The idea of this paper is to design a tool that will be used to test and compare commercial speech recognition systems, such as Microsoft Speech API and Google Speech API, with open-source speech recognition systems such as Sphinx-4. The best way to compare automatic speech recognition systems in different environments is by using some audio recordings that were selected from different sources and calculating the word error rate (WER). Although the WER of the three aforementioned systems were acceptable, it was observed that the Google API is superior.',\n",
       " '49ea217068781d3f3d07ef258b84a1fd4cae9528': 'Reasoning capability is of crucial importance to many applications developed for the Semantic Web. Description Logics provide sound and complete reasoning algorithms that can effectively handle the DL fragment of the Web Ontology Language (OWL). However, existing DL reasoners were implemented long before OWL came into existence and lack some features that are essential for Semantic Web applications, such as reasoning with individuals, querying capabilities, nominal support, elimination of the unique name assumption and so forth. With these objectives in mind we have implemented an OWL DL reasoner and deployed it in various kinds of applications.',\n",
       " '1cb5dea2a8f6abf0ef61ce229ee866594b6c5228': 'Normally, lesions are detected using supervised learning techniques that require labelled training data. We explore the use of Bayesian autoencoders to learn the variability of healthy tissue and detect lesions as unlikely events under the normative model. As a proof-of-concept, we test our method on registered 2D midaxial slices from CT imaging data. Our results indicate that our method achieves best performance in detecting lesions caused by bleeding compared to baselines.',\n",
       " '1450296fb936d666f2f11454cc8f0108e2306741': 'While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.',\n",
       " '639937b3a1b8bded3f7e9a40e85bd3770016cf3c': 'Generative 3D face models are a powerful tool in computer vision. They provide pose and illumination invariance by modeling the space of 3D faces and the imaging process. The power of these models comes at the cost of an expensive and tedious construction process, which has led the community to focus on more easily constructed but less powerful models. With this paper we publish a generative 3D shape and texture model, the Basel Face Model (BFM), and demonstrate its application to several face recognition task. We improve on previous models by offering higher shape and texture accuracy due to a better scanning device and less correspondence artifacts due to an improved registration algorithm. The same 3D face model can be fit to 2D or 3D images acquired under different situations and with different sensors using an analysis by synthesis method. The resulting model parameters separate pose, lighting, imaging and identity parameters, which facilitates invariant face recognition across sensors and data sets by comparing only the identity parameters. We hope that the availability of this registered face model will spur research in generative models. Together with the model we publish a set of detailed recognition and reconstruction results on standard databases to allow complete algorithm comparisons.',\n",
       " '6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4': 'Predicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are fine-tuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the filters of LNet are fine-tuned only with image-level attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, but without face bounding boxes or landmarks, which are required by all attribute recognition works. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pre-training with massive face identities, and such concepts are significantly enriched after fine-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.',\n",
       " '6b4da897dce4d6636670a83b64612f16b7487637': 'With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulators output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a self-regularization term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.',\n",
       " '76705c60d9e41dddbb6e4e75c08dcb2b6fa23ed6': 'Polarization in social media networks is a fact in several scenarios such as political debates and other contexts such as same-sex marriage, abortion and gun control. Understanding and quantifying polarization is a longterm challenge to researchers from several areas, also being a key information for tasks such as opinion analysis. In this paper, we perform a systematic comparison between social networks that arise from both polarized and non-polarized contexts. This comparison shows that the traditional polarization metric – modularity – is not a direct measure of antagonism between groups, since non-polarized networks may be also divided into fairly modular communities. To bridge this conceptual gap, we propose a novel polarization metric based on the analysis of the boundary of a pair of (potentially polarized) communities, which better captures the notions of antagonism and polarization. We then characterize polarized and non-polarized social networks according to the concentration of high-degree nodes in the boundary of communities, and found that polarized networks tend to exhibit low concentration of popular nodes along the boundary. To demonstrate the usefulness of our polarization measures, we analyze opinions expressed on Twitter on the gun control issue in the United States, and conclude that our novel metrics help making sense of opinions expressed on online media.',\n",
       " '8ae5dde36e2755fd9afcb8a62df8cc9e35c79cb1': \"OBJECTIVE\\nBrain-computer interfaces (BCIs) based on electroencephalography (EEG) have been shown to accurately detect mental activities, but the acquisition of high levels of control require extensive user training. Furthermore, EEG has low signal-to-noise ratio and low spatial resolution. The objective of the present study was to compare the accuracy between two types of BCIs during the first recording session. EEG and tripolar concentric ring electrode (TCRE) EEG (tEEG) brain signals were recorded and used to control one-dimensional cursor movements.\\n\\n\\nAPPROACH\\nEight human subjects were asked to imagine either 'left' or 'right' hand movement during one recording session to control the computer cursor using TCRE and disc electrodes.\\n\\n\\nMAIN RESULTS\\nThe obtained results show a significant improvement in accuracies using TCREs (44%-100%) compared to disc electrodes (30%-86%).\\n\\n\\nSIGNIFICANCE\\nThis study developed the first tEEG-based BCI system for real-time one-dimensional cursor movements and showed high accuracies with little training.\",\n",
       " 'fdaaa830821d5693f709d3bfcdec1526f32d32af': 'Modern healthcare service records, called Claims, record the medical treatments by a Provider (Doctor/Clinic), medication advised etc., along with the charges, and payments to be made by the patient and the Payer (insurance provider). Denial and rejection of healthcare claims is a significant administrative burden and source of loss to various healthcare providers and payers as well. Automating the identification of Claims prone to denial by reason, source, cause and other deciding factors is critical to lowering this burden of rework. We present classification methods based on Machine Learning (ML) to fully automate identification of such claims prone to rejection or denial with high accuracy, investigate the reasons for claims denial and recommend methods to engineer features using Claim Adjustment Reason Codes (CARC) as features with high Information Gain. The ML engine reported is first of its kind in Claims risk identification and represents a novel, significant enhancement to the state of practice of using ML for automating and containing claims denial risks.',\n",
       " 'df39b32b8f2207c17ea19353591673244fda53eb': 'A 16 Gb/s receiver implemented in 22 nm SOI CMOS technology is reported. The analog frontend accepts a rail-to-rail input common-mode imposed from the transmitter side. It consists of a baseline wander compensated passive linear equalizer that AC-couples the received signal to the subsequent active CTLE with a regulated common-mode level. The programmable passive linear equalizer features a frequency response suitable for low-frequency equalization such as for skin-effect losses. When its zero is programmed at 200 MHz minimum frequency, the measured maximum mid-band peaking is 7 dB. The receiver architecture is half-rate and comprises an 8-tap DFE and a baud-rate CDR. With no FFE at the transmitter, 0.9 Vppd PRBS31 NRZ data are recovered error-free (BER<;10-12) across a copper channel with 34 dB attenuation at 8 GHz.',\n",
       " '332c81b75c22ca272ccf0ca3237066b35ea81c3b': 'This paper presents the design and analysis of a passive body weight (BW)-support lower extremity exoskeleton (LEE) with compliant joints to relieve compressive load in the knee. The biojoint-like mechanical knee decouples human gait into two phases, stance and swing, by a dual snap fit. The knee joint transfers the BW to the ground in the stance phases and is compliant to free the leg in the swing phases. Along with a leg dynamic model and a knee biomechanical model, the unmeasurable knee internal forces are simulated. The concept feasibility and dynamic models of the passive LEE design have been experimentally validated with measured plantar forces. The reduced knee forces confirm the effectiveness of the LEE in supporting human BW during walking and also provide a basis for computing the internal knee forces as a percentage of BW. Energy harvested from the hip spring reveals that the LEE can save human walking energy.',\n",
       " '26d0b98825761cda7e1a79475dbf6dc140daffbb': 'Class-D audio amplifiers are particularly efficient, and this efficiency has led to their ubiquity in a wide range of modern electronic appliances. Their output takes the form of a high-frequency square wave whose duty cycle (ratio of on-time to off-time) is modulated at low frequency according to the audio signal. A mathematical model is developed here for a second-order class-D amplifier design (i.e., containing one second-order integrator) with negative feedback. We derive exact expressions for the dominant distortion terms, corresponding to a general audio input signal, and confirm these predictions with simulations. We also show how the observed phenomenon of “pulse skipping” arises from an instability of the analytical solution upon which the distortion calculations are based, and we provide predictions of the circumstances under which pulse skipping will take place, based on a stability analysis. These predictions are confirmed by simulations.',\n",
       " 'd2938415204bb6f99a069152cb954e4baa441bba': 'This letter presents a compact antenna suitable for the reception of GPS signals on artillery projectiles over 1.57-1.60 GHz. Four inverted-F-type elements are excited by a series feed network in equal magnitude and successive 90° phase difference. The shape and form factor of the antenna is tailored so that the antenna can be easily installed inside an artillery fuze. Measurements show that the proposed antenna has a gain of 2.90-3.77 dBic, an axial ratio of 1.9-2.86 dB, and a reflection coefficient of less than -10 dB over 1.57-1.62 GHz.',\n",
       " 'a9c2ecffaf332d714a5c69adae1dad12031ee77a': 'Most of FPGAs have Configurable Logic Blocks (CLBs) to implement combinational and sequential circuits and block RAMs to implement Random Access Memories (RAMs) and Read Only Memories (ROMs). Circuit design that minimizes the number of clock cycles is easy if we use asynchronous read operations. However, most of FPGAs support synchronous read operations, but do not support asynchronous read operations. The main contribution of this paper is to provide one of the potent approaches to resolve this problem. We assume that a circuit using asynchronous ROMs designed by a non-expert or quickly designed by an expert is given. Our goal is to convert this circuit with asynchronous ROMs into an equivalent circuit with synchronous ones. The resulting circuit with synchronous ROMs can be embedded into FPGAs. We also discuss several techniques to decrease the latency and increase the clock frequency of the resulting circuits. key words: FPGA, block RAMs, asynchronous read operations, rewriting algorithm',\n",
       " 'c792d3aa4a0a2a93b6c443143588a19c645c66f4': 'ion x x x x Relationship x x x x x',\n",
       " '289f1a3a127d0bc22b2abf4b897a03d934aec51b': 'Restricted Boltzmann Machines (RBMs) have been demonstrated to perform efficiently on a variety of applications, such as dimensionality reduction and classification. Implementing RBMs on neuromorphic hardware has certain advantages, particularly from a concurrency and lowpower perspective. This paper outlines some of the requirements involved for neuromorphic adaptation of an RBM and attempts to address these issues with suitably targeted modifications for sampling and weight updates. Results show the feasibility of such alterations which will serve as a guide for future implementation of such algorithms in VLSI arrays of spiking neurons.',\n",
       " '3c701a0fcf29817d3f22117b8b73993a4e0d303b': 'We present a new, two-stage, self-supervised algorithm for author disambiguation in large bibliographic databases. In the first “bootstrap” stage, a collection of highprecision features is used to bootstrap a training set with positive and negative examples of coreferring authors. A supervised feature-based classifier is then trained on the bootstrap clusters and used to cluster the authors in a larger unlabeled dataset. Our selfsupervised approach shares the advantages of unsupervised approaches (no need for expensive hand labels) as well as supervised approaches (a rich set of features that can be discriminatively trained). The algorithm disambiguates 54,000,000 author instances in Thomson Reuters’ Web of Knowledge with B3 F1 of .807. We analyze parameters and features, particularly those from citation networks, which have not been deeply investigated in author disambiguation. The most important citation feature is self-citation, which can be approximated without expensive extraction of the full network. For the supervised stage, the minor improvement due to other citation features (increasing F1 from .748 to .767) suggests they may not be worth the trouble of extracting from databases that don’t already have them. A lean feature set without expensive abstract and title features performs 130 times faster with about equal F',\n",
       " '17033fd4fff03228cd6a06518365b082b4b45f7f': 'This article reports on an examination of the relationships between chief executive officer (CEO) personality, transformational and transactional leadership, and multiple strategic outcomes in a sample of 75 CEOs of Major League Baseball organizations over a 100-year period. CEO bright-side personality characteristics (core self-evaluations) were positively related to transformational leadership, whereas dark-side personality characteristics (narcissism) of CEOs were negatively related to contingent reward leadership. In turn, CEO transformational and contingent reward leadership were related to 4 different strategic outcomes, including manager turnover, team winning percentage, fan attendance, and an independent rating of influence. CEO transformational leadership was positively related to ratings of influence, team winning percentage, and fan attendance, whereas contingent reward leadership was negatively related to manager turnover and ratings of influence.',\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'78495383450e02c5fe817e408726134b3084905d': 'A Direct Search Method to solve Economic Dispatch Problem with Valve-Point Effect',\n",
       " '7dcb308b9292a8bc87d6f7793d2ca5e0e19dfa40': 'Bearish-Bullish Sentiment Analysis on Financial Microblogs',\n",
       " '8c872ecd87945e71fcd9fa1b6cb1133cfe805bf2': 'Predicting defects in SAP Java code: An experience report',\n",
       " '3a63667284dc8b9687ed1620406030bfe39af3c9': 'Active-Metric Learning for Classification of Remotely Sensed Hyperspectral Images',\n",
       " '071f47b7bc5830643e31dbed82e0375bf9b26559': 'Ad Hoc Retrieval Experiments Using WordNet and Automatically Constructed Thesauri',\n",
       " 'ee9596725d1db17f2b1e2207dd3ea260343bfe4f': 'Underwater Acoustic Target Tracking: A Review',\n",
       " 'a65196dfff31425281c690a7f2ca65247147da6b': 'Unsupervised Diverse Colorization via Generative Adversarial Networks',\n",
       " 'a04b5b99f5d9d8748843e870536a4a9f65562012': 'Lane Detection ( Part I ) : Mono-Vision Based Method',\n",
       " 'de8e80d409aaaa3244da4f2cb5b5bb053d453cee': 'Detection of distributed denial of service attacks using machine learning algorithms in software defined networks',\n",
       " 'ae0fb9c6ebb8ce12610c477d2388447a13dc4694': 'Distributed Privacy-Preserving Collaborative Intrusion Detection Systems for VANETs',\n",
       " '648678f1edab0d2139958070744b826d2b24c79e': 'Social engineering attack framework',\n",
       " '8a6080396fa7195c7c627bea4b2aeeb9ca39b5f8': 'A Biologically Plausible Learning Rule for Deep Learning in the Brain',\n",
       " '4715401473dca02ebaa5bdd4d4003705ed91c380': 'Tampering with the Delivery of Blocks and Transactions in Bitcoin',\n",
       " '6519bf5580fcdcc9c50fd72c6c8dc5d040d443e8': 'A survey of multi-source energy harvesting systems',\n",
       " '3be10c45163c61dfb9f250412699b3f8cb0ada1d': 'Churn prediction in telecom using Random Forest and PSO based data balancing in combination with various feature selection strategies',\n",
       " '566c89a1ace18f57ac3212e6d62634b501990b31': 'Discovering social circles in ego networks',\n",
       " '370fda29c5aaff285311a87824c5f28db33da021': 'Permutation invariant training of deep models for speaker-independent multi-talker speech separation',\n",
       " '6c89453c09abde95d998f5acd244f00519472ee0': 'SemEval-2014 Task 3: Cross-Level Semantic Similarity',\n",
       " '74d4bf32242a3d22df63e0cd3c7b5a0038220cd2': 'Design Approach to a Novel Dual-Mode Wideband Circular Sector Patch Antenna',\n",
       " '714c194a2fc326151b905270558aa137f9f22730': 'Two privacy-preserving approaches for data publishing with identity reservation',\n",
       " 'c79b88a8d8ba491cead38b431703d84015153a8f': 'An integrated framework on mining logs files for computing system management',\n",
       " 'b2a20116c0609e23d3adfeaa604500fddca66178': 'DeltaCFS: Boosting Delta Sync for Cloud Storage Services by Learning from NFS',\n",
       " '8a93cd1b6fbf7c8c86637bae18d979dafeb9a7c1': 'Factor-based Compositional Embedding Models',\n",
       " '281b6888d5df1dddb350fac4e9be5c8272519109': 'A Dual Attentive Neural Network Framework with Community Metadata for Answer Selection',\n",
       " '0a40663fdcf7c5fb7cfc459693116c41309e7eca': 'Algorithmic Nuggets in Content Delivery',\n",
       " '8501e330d78391f4e690886a8eb8fac867704ea6': 'Train longer, generalize better: closing the generalization gap in large batch training of neural networks',\n",
       " 'beac53e8074b4822943a3374ff5e9fed98a891b8': 'Generate to Adapt: Aligning Domains Using Generative Adversarial Networks',\n",
       " '6ae4d0b388aa9262e80c3eedc1cb3e5f06842368': 'Visualization of complex attacks and state of attacked network',\n",
       " '7db20fcc1d71edc32c365f145148d24b9f1427a5': 'A proposal of LDMOS using Deep Trench poly field plate',\n",
       " '335ad0ff82c728aca99fb0059c607cb18129526f': 'Bring you to the past: Automatic Generation of Topically Relevant Event Chronicles',\n",
       " '04e4034344bda5c97015ea634e6eb1b65ef3a898': 'Agile Team Perceptions of Productivity Factors',\n",
       " '271a4077c037b86fb7daf6bff3e66682322ff7d7': 'Movement segmentation using a primitive library',\n",
       " '3d23666c6d97d6ba3c86044d83c697821084bed6': 'Variational Sequential Labelers for Semi-Supervised Learning',\n",
       " 'fd4537b92ab9fa7c653e9e5b9c4f815914a498c0': 'One-Sided Unsupervised Domain Mapping',\n",
       " '191ea925c2115755b44380c99d84f4a1099b4dcd': 'Aggregating Content and Network Information to Curate Twitter User Lists',\n",
       " '458f1428273254fc5dd399f3c104f507680ddd54': 'Measuring discrimination in algorithmic decision making',\n",
       " '19115f66f6ac1c02568bbb38eceedfa3521a8cc2': 'Deep and Shallow Architecture of Multilayer Neural Networks',\n",
       " '84424eee012089dac9414979f0cb1465c97c4408': 'An inverse Yarbus process: Predicting observers’ task from eye movement patterns',\n",
       " '5bb6b779de7929c573f39cd84169cafbd72e5bae': 'Could We Issue Driving Licenses to Autonomous Vehicles?',\n",
       " '7b6652910c5cc0df9ff6bc91912a02e58a742d26': 'Evolutionary mining of relaxed dependencies from big data collections',\n",
       " 'df0ef17f63582603dafb1ac5c489dec1416ecbf4': 'Real World BCI: Cross-Domain Learning and Practical Applications',\n",
       " 'f2d5039b55d626ef5466a80e950bddd5cc1819d4': 'Inverse Reinforcement Learning',\n",
       " '7492683af60d02dbd658acdc61249571f8c20fc8': 'J-Sim: a simulation and emulation environment for wireless sensor networks',\n",
       " 'f7f32e86c23a902ac55fba8b8191026f563bec5b': 'Subjectivity and Sentiment Analysis of Modern Standard Arabic',\n",
       " 'a22e737d38a1bc671893a7ed341aba436571097a': 'Developmental Changes in the Relationship Between Grammar and the Lexicon',\n",
       " '4b80771429b786ea87740738378ba1eb1504a57f': 'Literature Fingerprinting: A New Method for Visual Literary Analysis',\n",
       " '9003fb79e7848ced3be975c3d87a9348a4b8d377': 'Moral development, executive functioning, peak experiences and brain patterns in professional and amateur classical musicians: Interpreted in light of a Unified Theory of Performance',\n",
       " 'ac58b487e864afafe71c6158553eed10fbc8eef5': 'A NEW PERSPECTIVE ON PORT SUPPLY CHAIN MANAGEMENT ACCORDING TO THE SERVICE DOMINANT LOGIC',\n",
       " '30c9a7660281ad8e4538ff9beb20282c74fac810': 'Online Social Networks anatomy: On the analysis of Facebook and WhatsApp in cellular networks',\n",
       " '129397ed6557da93db5ba18cf112ee7b0a7927d8': 'Stochastic Variational Deep Kernel Learning',\n",
       " '1a66df57f88e689438d505474eff73e3a9180c5b': 'Survey on modeling and indexing events in multimedia',\n",
       " 'e00ab75a8aa637d9c4c5c020e9f2c54b31528031': '3D ActionSLAM: wearable person tracking in multi-floor environments',\n",
       " '7707c7aac35a4d206eb061305256452051ae4dcf': 'Data Warehouse Life-Cycle and Design',\n",
       " '23386571ed332e4a465c0a6fa899f310caa217c8': 'Topic-Relevance Map: Visualization for Improving Search Result Comprehension',\n",
       " 'e7ec9e900556d9d9e47b274911ad304bcf198256': 'Common Mode EMI Noise Suppression for Bridgeless PFC Converters',\n",
       " 'ca464085b3da330f2ea895543a78c446a3af1bb6': 'Calcium hydroxylapatite for jawline rejuvenation: consensus recommendations.',\n",
       " 'f09db8a42d713774483f023b31e0ee96361823f4': 'Adversarial Texts with Gradient Methods',\n",
       " '958ebc7e24012b419316489515f2c2f908773bd5': 'Pose tracking from natural features on mobile phones',\n",
       " '73d54af530d22599cc76b9785ae0a663cc694df2': 'Neural Variational Inference For Embedding Knowledge Graphs',\n",
       " 'de77514c78432a0b17cf30baa024434173908f89': 'Autonomous underwater grasping using multi-view laser reconstruction',\n",
       " '7c88a0f765ce3c9025345cf133251cb947fe6ee5': 'Hierarchical multi-label classification over ticket data using contextual loss',\n",
       " 'a9d7e7e918a68d1b95c6c6b29f7fabbbe01010f7': 'Iterative Hough Forest with Histogram of Control Points for 6 DoF object registration from depth images',\n",
       " '0bfa2ab02f3c9a9fe06fcebf34bd8f371e206512': \"Let's go public! taking a spoken dialog system to the real world\",\n",
       " '5e1b80b4774582b948c6dcd656daaba6724ffc2d': 'Evaluation of a Brute Forcing Tool that Extracts the RAT from a Malicious Document File',\n",
       " '382bbb79d7ffda63395db3d9b4ebce55d0b2f038': 'Individuality and Alignment in Generated Dialogues',\n",
       " 'f7bdc97a6b4c5d65c145479ae74b3a244121ae89': 'Digital Image Authentication Model Based on Edge Adaptive Steganography',\n",
       " 'e659e72b7962c9ad586a2d35a7099550102fb054': 'Brain-computer interface systems: progress and prospects.',\n",
       " '3136f52540fb7925998eff94e9c016d2e1a6fe12': 'Tablets and humanoid robots as engaging platforms for teaching languages',\n",
       " '0bf046038a555bc848030a28530f9836e5611b96': 'ModDrop: Adaptive Multi-Modal Gesture Recognition',\n",
       " '5a4b46ef8898610ab16a1ced6f4a976db0f1962c': 'Jack the Reader - A Machine Reading Framework',\n",
       " '232427099d4a8acf1dc29efc852f09c5964e6165': 'A New View of Predictive State Methods for Dynamical System Learning',\n",
       " '100636ca50edf63d4336bb071f3e172cb0ebccaa': 'Deep Reinforcement Learning for Conversational AI',\n",
       " '7a373793804ea9813d64641919e14841f927c38f': 'GeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation',\n",
       " 'e788f08148b4b06f4e537b27b51338f1c88ccea8': 'FaCT++ Description Logic Reasoner: System Description',\n",
       " '8fad88b5678bfecab78d0d6389f649ccfb310d22': 'EvoNN: a customizable evolutionary neural network with heterogenous activation functions',\n",
       " 'e48e9cf407af2694ecfefd441c2c28faffdf0835': 'Antipodal Vivaldi antenna for phased array antenna applications',\n",
       " '065985d4d0854c51f52ad7a7507b267d9b88ab1c': 'Multi-class active learning for image classification',\n",
       " '44a56dc083e3e2b07f11aae29c1ee560f46efa4a': 'A Novel Fast Framework for Topic Labeling Based on Similarity-preserved Hashing',\n",
       " '3b198869df03d98b81c0c882753845a3bca36c63': 'Flexible Radio Access Beyond 5G: A Future Projection on Waveform, Numerology, and Frame Design Principles',\n",
       " '9a7e12ef462e779263ee3f2dd415d2d21233f15d': 'Unraveling the Anxious Mind: Anxiety, Worry, and Frontal Engagement in Sustained Attention Versus Off-Task Processing',\n",
       " '76dac031066a95715a3b116eb2094ec9bdd15171': 'Reinforcement Learning for Coreference Resolution',\n",
       " '26ebc1b9e21db68d5abf01acd2a0c38d260c65a1': 'Deep Residual Bidir-LSTM for Human Activity Recognition Using Wearable Sensors',\n",
       " 'a1be02429ab4ec852656f8833c3160f4592cc547': 'Reward-estimation variance elimination in sequential decision processes',\n",
       " 'a1f5c39aece58f6504e0334d96eaede32c7329cf': 'Emotion recognition using Speech Processing Using k-nearest neighbor algorithm',\n",
       " '8ff6581f94c366e34c6f2d55806f7781194c33c8': 'On visual gaze tracking based on a single low cost camera',\n",
       " 'fd65ca4e3ae95302f74bf863988cdb95d6a12824': 'Speech Emotion Recognition with Emotion-Pair Based Framework Considering Emotion Distribution Information in Dimensional Emotion Space',\n",
       " '3565d884735ead613a5aa0903f06a2cc86d05b6b': 'Analysis and Experimental Kinematics of a Skid-Steering Wheeled Robot Based on a Laser Scanner Sensor',\n",
       " 'c2e57b2871fdfd06566820c5329815ac06f66197': 'DeepMem: Learning Graph Neural Network Models for Fast and Robust Memory Forensic Analysis',\n",
       " '59e2037f5079794cb9128c7f0900a568ced14c2a': 'Clothing and People - A Social Signal Processing Perspective',\n",
       " '3f95e155b1c40911149fe994197a502ef44ebbbe': 'Music emotion recognition: the role of individuality',\n",
       " '0674058618d04def58c79a0b28174301ef591433': 'RainForest—A Framework for Fast Decision Tree Construction of Large Datasets',\n",
       " '23a7c8d9f142184d28e56b0751e172fef6539275': 'OCA: Opinion corpus for Arabic',\n",
       " 'ee6b940d3ae36f9b124586b15a49cec45f24b90a': 'Cross-Coupled Substrate Integrated Waveguide Filters With Improved Stopband Performance',\n",
       " '2bd338ef8751b62d23e53fbb44d67042d634da2f': 'FastFlow: High-level and Efficient Streaming on Multi-core',\n",
       " 'ddbb6e0913ac127004be73e2d4097513a8f02d37': 'Face Detection Using Quantized Skin Color Regions Merging and Wavelet Packet Analysis',\n",
       " 'd6b56cc6e05300426b6194dd3f6a38720678827e': 'Redirected Walking in Virtual Reality',\n",
       " 'df39e24c3cc21dc4c79995ec2b424a37dac999c7': 'Defensive Distillation is Not Robust to Adversarial Examples',\n",
       " '74bd4761b373447079c6e5cd31832c00fab2fa77': 'Ten Simple Rules for Developing Usable Software in Computational Biology',\n",
       " 'ddbf21ca1de9617894b15d45787a27557f02a494': 'Two-level Message Clustering for Topic Detection in Twitter',\n",
       " 'f754cab548f2c209ea7d932084ef768b92b27614': 'An Agent-based Indoor Wayfinding Based on Digital Sign System',\n",
       " '373f76633cc1f6c7a421e31c989842021a52fca4': 'A Fast Learning Algorithm for Deep Belief Nets',\n",
       " '01d208b33561362f7714f714d3bc4a1f7aa1637c': 'Bank distress in the news: Describing events through deep learning',\n",
       " 'f365ce3e68cb3795887e93baf5fed5d783b3904e': 'Web-STAR: A Visual Web-based IDE for a Story Comprehension System',\n",
       " '28e0707528f78fecb26fe7f003d94f6a5de32b98': 'Power to the People: The Role of Humans in Interactive Machine Learning',\n",
       " 'fa8042d025895e22af2d9df3ffb9f67438610fcc': 'Two-Phase Malicious Web Page Detection Scheme Using Misuse and Anomaly Detection',\n",
       " 'ae2b67d03512dd0f13a68636d1d7f83cc889318a': 'IoT-Based Health Monitoring System for Active and Assisted Living',\n",
       " 'b8397b4418bb1d576210d9a39d44725d6ffb2a44': 'Chinese/English mixed Character Segmentation as Semantic Segmentation',\n",
       " 'ebf4ef0701e6f6c2050e5bed0be54fd4d8a1d991': 'Wireless Networks Design in the Era of Deep Learning: Model-Based, AI-Based, or Both?',\n",
       " 'af441a4f1b754ad9b3a4401e8361ad9b66786778': 'Examination of the Correlation Between Internet Addiction and Social Phobia in Adolescents.',\n",
       " '8bb3a478db8951f42198a885d7245d58036ba55c': 'Applying Universal Schemas for Domain Specific Ontology Expansion',\n",
       " '9e76a700f03ef476b964b27e23c071163e44c232': 'Reduction of THD in Diode Clamped Multilevel Inverter employing SPWM technique',\n",
       " '43da9037d50745002195abb864243ecc75d4b040': 'Design Opportunities for Wearable Devices in Learning to Climb',\n",
       " 'b21951a276dd940e052676b06e193f65fdc297f9': 'Volume of signaling traffic reaching cellular networks from mobile phones',\n",
       " 'b9d502d98e3bb19f8179a4363e3f28c6b6def7a8': 'An online PPGI approach for camera based heart rate monitoring using beat-to-beat detection',\n",
       " '399acab2bee6eccbfffe4a2ce688b6b1075e9c5e': 'Eyeriss: a spatial architecture for energy-efficient dataflow for convolutional neural networks',\n",
       " '4649e3279edf653135f1f31da3e242ba451a93de': '3D Texture Recognition Using Bidirectional Feature Histograms',\n",
       " '4ab6e67519411dbdb69c6111a3b8d5f334ebb579': 'Popularity-driven Caching Strategy for Dynamic Adaptive Streaming over Information-Centric Networks',\n",
       " '924544f503070bb390cc51e4bac9cbb3171aa03f': 'Computation offloading for mobile edge computing: A deep learning approach',\n",
       " 'd2e6250e95af09fefbe638f1580420476a2ecaee': \"EmoBGM: Estimating sound's emotion for creating slideshows with suitable BGM\",\n",
       " '79e13ff4a21f13d3839422568274ea84a9fe42b5': 'A probabilistic framework for object search with 6-DOF pose estimation',\n",
       " '17507ae3ed6c96320a448d8c55aced80d4800b73': 'Combining concept hierarchies and statistical topic models',\n",
       " '9c7bdd21ccc7a0e395af63cc584dfd780d43e51b': 'An Intelligent Anti-phishing Strategy Model for Phishing Website Detection',\n",
       " '68cc5038a4937b96f1bf127dd574ac7b713bfa72': 'Online Learning for Adversaries with Memory: Price of Past Mistakes',\n",
       " '6fa640758b38cb8214be003bebaf472438428a9a': 'BM3D-PRGAMP: Compressive phase retrieval based on BM3D denoising',\n",
       " '8aba3628ad8c9cec11b2b518a96b883bda8b3cbb': 'Broadband millimetre-wave passive spatial combiner based on coaxial waveguide',\n",
       " '12219a387158e41e212af4ae1c57a29934627128': 'Immune System Based Intrusion Detection System',\n",
       " 'a704f6eca77b9bcf63e2d533ae6d6180785decf7': 'Speed control of buck converter fed DC motor drives',\n",
       " '67f9f2cf408b2cc593a9ddb17d74d2b3f72ee225': 'Modeling Compositionality with Multiplicative Recurrent Neural Networks',\n",
       " '60184fecc57b51fd4f312b77e3817af18643ef8e': 'Power Optimized Voltage Level Shifter Design for High Speed Dual Supply',\n",
       " '2d52f69dd4686a3e66f5a8a1650a24bcea43530e': 'Provable data possession at untrusted stores',\n",
       " '64ddfb947878347a30610b6ca2314fe2bac96a4a': 'Cyber–Physical Device Authentication for the Smart Grid Electric Vehicle Ecosystem',\n",
       " 'f14069aaa8b234dfafd3292863c0e610288fbc80': 'Multi-Scale multi-band densenets for audio source separation',\n",
       " '72cf3347ff06226e57214b49801de9fc02df8d52': 'Personalized Recommendation for Online Social Networks Information: Personal Preferences and Location-Based Community Trends',\n",
       " 'f824f0f5e28e434e1b5897153697816dd906ca8e': 'Path Planning through PSO Algorithm in Complex Environments',\n",
       " '39dba6f22d72853561a4ed684be265e179a39e4f': 'Sequence to Sequence Learning with Neural Networks',\n",
       " 'b42003018aef3d104271710690c4d662ab01571a': 'Cartesian Cubical Computational Type Theory: Constructive Reasoning with Paths and Equalities',\n",
       " 'fd4677e1b2cd0cba66ab4a64cbd1fe015d3a742b': 'The Rise of Emotion-aware Conversational Agents: Threats in Digital Emotions',\n",
       " '387dd9f50cf0af914cf39ecef3b72aca2c3476c1': 'Using contours to detect and localize junctions in natural images',\n",
       " '789bd068751c17373ca1f812e711af344b485d2e': 'Hierarchical Character-Word Models for Language Identification',\n",
       " 'aa447f6462c7efe7f3cd9ded120637ceefcdc0ce': 'Serving Deep Learning Models in a Serverless Platform',\n",
       " '636e8a004983a26647e11be23165bdae83a68a5a': 'A statistical approach for real time robust background subtraction',\n",
       " 'b3c8eeed23cc8b472ceefe64888a2036dd7df4d2': 'Paying Attention to Descriptions Generated by Image Captioning Models',\n",
       " '3cfa669b42a1e0e87bf3aca4d491039495ef87f8': 'Enabling Technologies for the Internet of Health Things',\n",
       " '0ce72c74fed3bd81c9ceffa0f0c1954b88328e32': 'Motion, emotion and empathy in esthetic experience',\n",
       " '1e12fb38bfc2c7aa86806f87df4cdaf035f92fb7': 'An implicit segmentation-based method for recognition of handwritten strings of characters',\n",
       " '4e546c43e5642ecdffd000406f0b35c1e3430a2c': 'Algorithms for RealTime Object Detection in Images',\n",
       " '522ef548c054fd03109ff1886d243a11ce6d1e2a': 'MapReduce-based deep learning with handwritten digit recognition case study',\n",
       " 'a39a3d23826455285039041f8a9a393b09119b86': 'The Neural Career of Sensory-motor Metaphors',\n",
       " '0c1c94f582cfaa727a03a452ea71cab809d8f7ce': 'Implementation of flash flood monitoring system based on wireless sensor network in Bangladesh',\n",
       " 'a98d48b6c4188143dbb2474a1cb27c2f9e3b5c0a': 'GeoDa web: enhancing web-based mapping with spatial analytics',\n",
       " '98f2ae796c7702de12174428a945861379665beb': 'Bootstrapping Unsupervised Bilingual Lexicon Induction',\n",
       " 'bd185e4f80d1fa2def96815269ec60dff862a7a1': 'Landslide susceptibility assessment and factor effect analysis: backpropagation artificial neural networks and their comparison with frequency ratio and bivariate logistic regression modelling',\n",
       " '6079719b677d0abda12abcd5cd46582ca91585ad': 'Phase-functioned neural networks for character control',\n",
       " '223bb68a9ca2df2d07335e2073fcc78b59e4edd7': 'ArSLAT: Arabic Sign Language Alphabets Translator',\n",
       " '1e022ba4791c906fef6013777be2ae3f4017a33e': 'Secu Wear: An Open Source, Multi-component Hardware/Software Platform for Exploring Wearable Security',\n",
       " '5056e36419b95411e4c03e9c011bbdd286bbb7a9': 'PD control with on-line gravity compensation for robots with elastic joints: Theory and experiments',\n",
       " '45a875e4bd1ec588997858876756098ef85b3d82': 'Modular and hierarchical learning systems',\n",
       " '6b0be5fc8199da9fe7821eca3617d8977056d03e': 'BlueGene/L Failure Analysis and Prediction Models',\n",
       " '16a8e0646724f730eb52216f9bc1284ddb630fd6': 'rev.ng: a unified binary analysis framework to recover CFGs and function boundaries',\n",
       " '9767665504e617d6efbf4630046a377a1383a2bd': 'The Utilibot Project : An Autonomous Mobile Robot Based on Utilitarianism',\n",
       " 'ebfe1e68abb1782a3d34df61f920375c8b2d6134': 'Iterative deep convolutional encoder-decoder network for medical image segmentation',\n",
       " '7a5e33718ca81495b2db4a1688bdb5a555816a87': 'Mobile cloud sensing, big data, and 5G networks make an intelligent and smart world',\n",
       " 'a221588fd2d062462254481cfd9563fec2f7c387': 'Deep neural network ensemble architecture for eye movements classification',\n",
       " '2ecad6cdccc33f707dfe4334883538918de50bb8': \"Client-Driven Network-level QoE fairness for Encrypted 'DASH-S'\",\n",
       " '1fd7fc06653723b05abe5f3d1de393ddcf6bdddb': 'SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS',\n",
       " 'b2a93309a451eae2b9b40e768a4831e1e3fc6d2b': 'A Survey on Resource Management in IoT Operating Systems',\n",
       " '6c88fd5d06ea6e0a2b1ad38b038655f405bc0613': 'FFT-based Terrain Segmentation for Underwater Mapping',\n",
       " '2c953b06c1c312e36f1fdb9919567b42c9322384': 'Ensemble of exemplar-SVMs for object detection and beyond',\n",
       " 'c550cadcc85a7dd825d46c5399edd30b52ee3818': 'Modified timeboxing process model for proper utilization of resources',\n",
       " '8bb49c0658d7b7cff42c655a6d7e005372ad32ea': 'Issues in predicting and explaining usage behaviors with the technology acceptance model and the theory of planned behavior when usage is mandatory',\n",
       " '8869669bc06dd9fa5a16be05c39e2e89bae49b7a': 'Zebra: An East-West Control Framework for SDN Controllers',\n",
       " '87601a4866373c63b4fd070214cab8b40b50058c': 'yaSpMV: yet another SpMV framework on GPUs',\n",
       " '85ed826a7feaa59c96c4ec71c6bdb17506b10820': 'Chapter 1 . Principles of Synthetic Aperture Radar',\n",
       " '23b93f3b237481bd1d36941ca3312bb16f4beb58': \"Reconnaissance d'événements et d'actions à partir de la profondeur thermique 3D. (Event and action recognition from thermal and 3D depth Sensing)\",\n",
       " '1450eda3d2eaf29f30cbae088567d7dcbc6590c3': 'CUDT: A CUDA Based Decision Tree Algorithm',\n",
       " '93f6dd2c761fdeac0af6d2253d57834439d7794f': 'IRSTLM: an open source toolkit for handling large scale language models',\n",
       " '34cd1723dc4a58217222882f9c6a651ec5bcf7a4': 'Implementation of a 3 D pose estimation algorithm',\n",
       " 'a83c2a8fce48665742be042a3183777417302155': 'A High-Speed Sliding-Mode Observer for the Sensorless Speed Control of a PMSM',\n",
       " 'a5f27c9cc188c15c0a7e019d666bed767d3c34d9': 'Electronic media use and adolescent health and well-being: cross-sectional community study.',\n",
       " 'e5adfc2f23602a5256e89b84615e1434e5375f0e': 'Radiomics-based Prognosis Analysis for Non-Small Cell Lung Cancer',\n",
       " '31c939e469b8910eb49af18247d68981cff1887a': 'Evolving Problems to Learn About Particle Swarm Optimizers and Other Search Algorithms',\n",
       " 'f1b7cb07e6f6b0c1cb142f8e46da4957ea7960f6': 'The Enactive Approach to Architectural Experience: A Neurophysiological Perspective on Embodiment, Motivation, and Affordances',\n",
       " 'd2e4a2d9590b7ae6c34fc59faa34a4217c5cac53': 'A Wideband Dual-Polarized L-Probe Antenna Array With Hollow Structure and Modified Ground Plane for Isolation Enhancement',\n",
       " 'b045f045e331700cdef309e0d40b15a64cdf5b8a': 'An Unbounded Nonblocking Double-Ended Queue',\n",
       " 'd2848c5937377f00595493bb5cc83aa3b7340071': 'Image Processing on DSP Environment Using OpenCV',\n",
       " '18a024eb8b03fe07c6855002094234406aace0db': 'Characterizing conflicts in fair division of indivisible goods using a scale of criteria',\n",
       " '3343804410013190447ce2dc0d8fcf792916aa21': 'Tolerating hardware device failures in software',\n",
       " 'a8d46814da895899a927a4869557429c6ca3b37d': 'Tracking Hands in Interaction with Objects: A Review',\n",
       " '1d1bc7c5ab704db04a72efb063fb1db6fc07f85c': 'Linking a domain thesaurus to WordNet and conversion to WordNet-LMF',\n",
       " '511052cc578daf30219bed58a8ebac795c0fcbaf': 'Stylometric Analysis of Scientific Articles',\n",
       " 'f104a9ec7fd862090a6fa245941842f14e6d9f43': 'Repeatable Reverse Engineering with the Platform for Architecture-Neutral Dynamic Analysis',\n",
       " '09d275d72bdd404df1270ca0d23574c10c27e4ac': 'Control Flow Analysis for Reverse Engineering of Sequence Diagrams',\n",
       " '48a01a7d5e01ebec4311495b1e45c1b59151efae': 'From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification',\n",
       " '2df3b760190470c4af1be87328a20ce607e1ad98': 'Trust-Aware Review Spam Detection',\n",
       " 'b42bea0b65c2d0239c2fe54985833e2d91c00621': 'A novel softplus linear unit for deep convolutional neural networks',\n",
       " '7e3b5f0ffec62944f4b97a50553a12a39f54ba4a': 'Recurrent Neural Networks for Word Alignment Model',\n",
       " '43dd67cf1630eeac917c0dcd8e80425a6b93d38d': 'Longitudinal analysis of discussion topics in an online breast cancer community using convolutional neural networks',\n",
       " '1c454ae4e1bbc600791f3a4796fdb6b1ee2ca016': 'DeepX: A Software Accelerator for Low-Power Deep Learning Inference on Mobile Devices',\n",
       " 'ee61c9078be00a49aedb479ee468440b9086c32d': \"On the Definition of 'Ontology'\",\n",
       " '6a40ffc156aea0c9abbd92294d6b729d2e5d5797': 'Natural actor-critic algorithms',\n",
       " '7989208727ceb32b10a51682a116ce762691daa6': 'Distancing from experienced self: how global-versus-local perception affects estimation of psychological distance.',\n",
       " '715be94df3590480a47dba3a6a1eb044e8814e79': 'Phishing Website Detection based on Supervised Machine Learning with Wrapper Features Selection',\n",
       " '1b4ed4a45a900d6a02f929f873cb25f51a0e054b': 'An introduction to OpenSimulator and virtual environment agent-based M&S applications',\n",
       " 'c4c47ebf6454e3c5a8417c580c8ecf694e34ad49': 'Comparison of Approximate Methods for Handling Hyperparameters',\n",
       " '7323c2d32d6cdc3604dcedc574508d042ce3821e': 'Development of a Social Media Maturity Model -- A Grounded Theory Approach',\n",
       " '1a278dcdc031dba7d9aa055aa3b450339ddbe161': 'Behavior of MVC (Model View Controller) based Web Application developed in PHP and .NET framework',\n",
       " 'a644a3ab091cc30897a707997c43fd4e5dc395b5': 'Quantum-Inspired Immune Clonal Algorithm for Global Optimization',\n",
       " 'c0bbc89889691aca1f81f3727a0587eadfe4e8af': 'The Analysis of Android Malware Behaviors',\n",
       " 'eed682efa845495dd2563b5cf2797cb32f9bcac7': 'Wideband millimeter-wave SIW cavity backed patch antenna fed by substrate integrated coaxial line',\n",
       " 'afa13588cd866ec23cc71d634becbd507ea61093': 'A Compact Size, Multi Octave Bandwidth Power Amplifier, Using LDMOS Transistors',\n",
       " '3e95925d2bca43223453010ff8516a492287ce19': 'Global-Locally Self-Attentive Dialogue State Tracker',\n",
       " '775883af6c684bde676a178c7709d806abfcf2ca': 'Evaluation of Hardware Performance for the SHA-3 Candidates Using SASEBO-GII',\n",
       " '5decd5f960e17bfaf3554a39c1b7b0ef2e4aa6cf': 'Minimally Supervised Number Normalization',\n",
       " '76abf690192e6b6219d7691d0925894c433ddf2a': 'Intrinsic video and applications',\n",
       " '30c584613b73201bd2d9dbf2e1d6d31d290a8a1c': 'Pricing Strategies for Information Technology Services: A Value-Based Approach',\n",
       " '0f990cb8bbd3767350842b0975b4b32a600f41e1': 'Learning Sensor-Specific Spatial-Spectral Features of Hyperspectral Images via Convolutional Neural Networks',\n",
       " '86f45b11497ba1c1ef1da5739ec022b4d335fa11': 'Convolutional neural networks on assembly code for predicting software defects',\n",
       " 'eaf4756c12cfdae09ad3c82696ec1c16271a9348': 'Daily Routine Recognition through Activity Spotting',\n",
       " '0f1b8238e4b8f8937c78dbd2d77d3945723b596a': 'Tumor-associated copy number changes in the circulation of patients with prostate cancer identified through whole-genome sequencing',\n",
       " '02c9fe33a5d8cb94373cea20a53f01e0a0e70f7f': 'A Gentle Introduction to Soar, an Architecture for Human Cognition.',\n",
       " '29b28c7699a63c02bb3e3c0aaaeb4de6811383b4': 'Towards a Shared Ledger Business Collaboration Language Based on Data-Aware Processes',\n",
       " 'c227903cce108631e613e789af538e925d29807e': 'Data-Driven Networking : Harnessing the “ Unreasonable Effectiveness of Data ” in Network Design',\n",
       " 'd15f8891e586a9c19c9fc30fa636e764c9eeaff9': 'Parking space detection from a radar based target list',\n",
       " '0c3ffd5f1b577e38604f361ee71feb312b5b0cab': 'Solving Constraint Satisfaction Problems through Belief Propagation-guided decimation',\n",
       " 'fa21c85107516c7f0a341de27856d7ffe4a6c5d9': 'WIR: Warp Instruction Reuse to Minimize Repeated Computations in GPUs',\n",
       " '89750bf1480ffea3ac18e5271cc87589fc5709ca': 'MOMCC: Market-oriented architecture for Mobile Cloud Computing based on Service Oriented Architecture',\n",
       " 'cf2ddf35ac0acc96f3a668a882443929da6aae5e': 'An energy market for trading electricity in smart grid neighbourhoods',\n",
       " '541853e747dd63d6aff41c773e21fd1e224f0680': 'From Entity Recognition to Entity Linking: A Survey of Advanced Entity Linking Techniques (人工知能学会全国大会(第26回)文化,科学技術と未来) -- (International Organized Session「Special Session on Web Intelligence & Data Mining」)',\n",
       " '1995f186d00e9fdc1957c76666145c923fa55cf3': 'Smoothing of Piecewise Linear Paths',\n",
       " 'b489d537d5523ab2351c866835a4d03194eb2414': 'Mechanical design of humanoid robot platform KHR-3 (KAIST Humanoid Robot 3: HUBO)',\n",
       " '5cba21badc50adb5243662c54471011a4333d35e': 'Using MPTCP subflow association control for heterogeneous wireless network optimization',\n",
       " 'd4d19261e9047b54e14d38cd99cd0f27dc9d0926': 'DroidDet: Effective and robust detection of android malware using static analysis along with rotation forest model',\n",
       " 'd019b5ba0cac92bc93661a55181889dda578933e': 'Towards brain-activity-controlled information retrieval: Decoding image relevance from MEG signals',\n",
       " '522bf872909305bd360493ea2b4664a31358a092': 'Development of an FPGA-Based SPWM Generator for High Switching Frequency DC/AC Inverters',\n",
       " '1571f50f52761a7a3537f12a88be487e5a0e7ad4': 'Full-Duplex Aided User Virtualization for Mobile Edge Computing in 5G Networks',\n",
       " 'b37225fc67e1aeeb7877c4691f9036ed5cc01efa': 'A Hitting Time Analysis of Stochastic Gradient Langevin Dynamics',\n",
       " '41a798995d414b0dadb0678cdf6c3107059c0b75': 'Secure Wallet-Assisted Offline Bitcoin Payments with Double-Spender Revocation',\n",
       " '21b1180af3087c1333708a9873f6d473eabe6751': 'In situ X-ray imaging of defect and molten pool dynamics in laser additive manufacturing',\n",
       " '01273bd34dacfe9ef887b320f36934d2f9fa9b34': 'Image-Guided Nanopositioning Scheme for SEM',\n",
       " 'ef1fbd95c7220003835b839c47bdf3fdb7e54b7b': \"Students' perceptions of using Facebook as an interactive learning resource at university\",\n",
       " 'd535ec1a520ca6e29902fafa85b7ab81ad034c1d': \"Early Prediction of Students' Grade Point Averages at Graduation: A Data Mining Approach.\",\n",
       " '9de5cc4b40216b12b4c8fc6c8030bcf5590d03f6': 'Metacognitive strategies in student learning: do students practise retrieval when they study on their own?',\n",
       " '0bf8527d093600c50208faca0b32eef2372ec0d4': 'The Intelligent surfer: Probabilistic Combination of Link and Content Information in PageRank',\n",
       " '8a1838c64441c5a659a7fbcf80531e5e5ff53d84': 'A Multi-Sensorial Simultaneous Localization and Mapping (SLAM) System for Low-Cost Micro Aerial Vehicles in GPS-Denied Environments',\n",
       " 'fd48a7fd819ab193704a6283933f5a4883bcdd7d': 'Understanding \"watchers\" on GitHub',\n",
       " '208d2d6ae61955d93b13a5a497c10ba8c7086e87': 'CENTRIST: A Visual Descriptor for Scene Categorization',\n",
       " '78c12d64baeef87727fa53666bfb2c2709fe1260': 'The Case for a Visual Discovery Assistant: A Holistic Solution for Accelerating Visual Data Exploration',\n",
       " 'd6be438df373dedf06d6c062596d4e40f59b022c': 'Predicting movie success with machine learning techniques: ways to improve accuracy',\n",
       " 'ea813d2ae6cdad4d734ad5b8b39522d6d1392431': 'An Efficient Design of Dadda Multiplier Using Compression Techniques',\n",
       " '2a8a474a6b613105b672bb4d58a59d6eafd035ef': 'On matching latent fingerprints',\n",
       " '3f42c32316c50b4a03de8c4597159c68a8d07776': 'Feature Selection as a One-Player Game',\n",
       " '2e08d3ec08069ef41059cc3da6bc4d390eaf6e01': 'Biases in social comparisons : Optimism or pessimism ? q',\n",
       " '1feebbe8af1e60048cb82bbcbadc07b4d3f210d6': 'NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis',\n",
       " '13c47ed140cc36191678a48b3115939cd0aa8314': 'AnyDBC: An Efficient Anytime Density-based Clustering Algorithm for Very Large Complex Datasets',\n",
       " 'a517b4c4aa5c6baf1a7f7bdf40a1058f5ae4a674': 'Hierarchical Control of Hybrid Energy Storage System in DC Microgrids',\n",
       " '08c970df2d62d4bc27e65e8c389a0227a41109a5': 'Regression testing of GUIs',\n",
       " 'fdf29d5dec6f929409e0bb340ae973a91680ad17': 'Cross-domain Feature Selection for Language Identification',\n",
       " '848c3eb292d721e53631c0d6c988f380cbb0e225': 'Novel rotor design optimization of Synchronous Reluctance Machine for low torque ripple',\n",
       " '0416f5d1564d1f2a597acac04e81b02b2eff67d2': 'A High Performance CRF Model for Clothes Parsing',\n",
       " 'c1173b8d8efb8c2d989ce0e51fe21f6b0b8d1478': 'Semi-Supervised Image-to-Video Adaptation for Video Action Recognition',\n",
       " 'bcdd5670761de0087d2d2bb0388da697b0d4348c': 'ZVS range extension of 10A 15kV SiC MOSFET based 20kW Dual Active Half Bridge (DHB) DC-DC converter',\n",
       " 'ad51c1f5797c05936468d1bfe81cb7fbfe711d92': 'The impacts of brand trust, customer satisfaction, and brand loyalty on word-of-mouth',\n",
       " '6c42c5d99cd594da9fd18c5fdb67e015455dde6e': 'Learning Mixed Initiative Dialog Strategies By Using Reinforcement Learning On Both Conversants',\n",
       " '9992dfb672e93683f21c571624a4e13d681d79b7': 'Intent-based recommendation for B2C e-commerce platforms',\n",
       " '06e0780f589f04edd1e55f5a0d9872696280b40e': 'Divide and correct: using clusters to grade short answers at scale',\n",
       " '3ae52be664a249f695988575f8affc1f466c1c87': 'Comparison of Different Grid Abstractions for Pathfinding on Maps',\n",
       " '4ec90b7da43e6438cbdc756624b1083f30288064': 'VELNET (Virtual Environment for Learning Networking)',\n",
       " '89bb989b6ec9fe52fcbcc94bf0923a14ed6bf245': 'SOF: a semi-supervised ontology-learning-based focused crawler',\n",
       " 'be62e87ec5f7a27363a723538c519ef9c51a743b': 'Standardized Extensions of High Efficiency Video Coding (HEVC)',\n",
       " '8610bc0c41e075688504f5da11477d8b3628a2cf': 'Automatic fruit recognition and counting from multiple images',\n",
       " 'a11de72de4723edc3e8cffabac259cffa13ded1b': 'Self-Adhesive and Capacitive Carbon Nanotube-Based Electrode to Record Electroencephalograph Signals From the Hairy Scalp',\n",
       " '431bbc47ce90f5e0eb8388fa80b0cc4d7c26ab76': 'Area , Power , and Latency Considerations of STT-MRAM to Substitute for Main Memory',\n",
       " '7b787a261a82e5488fc972d2b6541f778c81ed78': 'A Survey on Applications of Augmented Reality',\n",
       " '045a50ec31973fee15ff967f18e016fae77fd1f3': 'Characterizing cloud computing hardware reliability',\n",
       " '7e7f843b9638ee9dc321fcd348ea2185b9126854': 'Discovering Event Evolution Graphs From News Corpora',\n",
       " 'fdd141a4fffc490fb3ab570c28b8a1f2dd80a15f': 'Eye Gaze Tracking Using an Active Stereo Head',\n",
       " '2094f315289ccf9b676e0790fb3dd1bc4acad98c': 'Introducing MVTec ITODD — A Dataset for 3D Object Recognition in Industry',\n",
       " 'd6f71b84f703152a08226d1c7e61cab888380fb1': 'Distributed Learning over Unreliable Networks',\n",
       " '43bb00f852d1d24dec35c927328c17fb01a54295': 'Automated Linguistic Analysis of Deceptive and Truthful Synchronous Computer-Mediated Communication',\n",
       " 'a152205a7d745afa335322587e8f78c8e5e85111': 'ARMDN: Associative and Recurrent Mixture Density Networks for eRetail Demand Forecasting',\n",
       " '0491b1a097378701dbbab2ce9dcc2e109a95d97e': 'A Dark Side of the Cannula Injections: How Arterial Wall Perforations and Emboli Occur',\n",
       " 'c6c171d2a9be192d60af7b434e4ba2fcbbad7f48': 'Memory-augmented Neural Machine Translation',\n",
       " '5feac39a50e5bf240a64f42dbc881a16e8f8e659': 'Simple task-specific bilingual word embeddings',\n",
       " '181d6617e6233062683965006ae5e08b0db4f0c1': 'Optimal Target Assignment and Path Finding for Teams of Agents',\n",
       " '1066b8c0aaed299874e1989998635dcb879bcd0f': 'Perspectives to Predict Dropout in University Students with Machine Learning',\n",
       " '23ce229565447f22d5ef2b6d7cbd6d8003254ac2': 'Deep Semantic Feature Matching',\n",
       " 'e2af314b238088588eebb94210535e31d5f647e1': 'Neural Attentional Rating Regression with Review-level Explanations',\n",
       " '3d11a9a90bce358afd228590ab5158a268031bb9': 'Image Generation from Captions Using Dual-Loss Generative Adversarial Networks',\n",
       " '7600ac55ddd8d955eeb96c12b226f1a2bc9e8817': 'Towards AI-powered personalization in MOOC learning',\n",
       " 'c48caf1b6404e08cf4606508310c03e5df54d0f7': 'NGUARD: A Game Bot Detection Framework for NetEase MMORPGs',\n",
       " 'c7c9cc68fed535c3c9d813a852e4a9e8a8eedb01': 'Strain Gauges Based on CVD Graphene Layers and Exfoliated Graphene Nanoplatelets with Enhanced Reproducibility and Scalability for Large Quantities',\n",
       " 'b8c7a947bda15a4f8a4b2f5e8cb8cd35a193fe4c': 'The Off-Switch Game',\n",
       " '2a3b696dccb9e31f0321c83a86f26041c72b1bc9': 'DIRECT GEOMETRY PROCESSING FOR TELE-FABRICATION',\n",
       " '9bdc4c5f6ba780f30ae081be7af34262af9a60a5': 'Train-O-Matic: Large-Scale Supervised Word Sense Disambiguation in Multiple Languages without Manual Training Data',\n",
       " '779c0713a86e27fbffe999017837a56cbcae09ed': 'Weakly Supervised Extraction of Computer Security Events from Twitter',\n",
       " '663e06e02a306660ef508aaf2ffaf523c6e96088': 'Learning Graphical Model Structure Using L1-Regularization Paths',\n",
       " '96dd2785bc42ea77a3afa65701d55212570a76ff': 'Wavelet-based statistical signal processing using hidden Markov models',\n",
       " 'ecb869b1f3830f7f98ee84bc415ddc97b0e0348b': 'Using Social Network Analysis as a Strategy for E-Commerce Recommendation',\n",
       " '4b450424be82fccb46a97bdad24a44f547d604d9': 'Online and batch learning of pseudo-metrics',\n",
       " '1b9ce6abc0f3024b88fcd4dbd0c10cf5bcf7d38d': 'DeepMimic: example-guided deep reinforcement learning of physics-based character skills',\n",
       " 'bdbefe44ceb31ed05d09c5d322fbdb149abf48e0': 'NLANGP at SemEval-2016 Task 5: Improving Aspect Based Sentiment Analysis using Neural Network Features',\n",
       " 'd2d001e2a45614528cd014040becb67a80a1af8d': 'Detection and Prevention of SQL Injection Attacks',\n",
       " '252fb9f343399a7fae9f771f5387686708fc9610': 'Online Learning for Neural Machine Translation Post-editing',\n",
       " '903e066a3d1024b3355e167e1307b70ac2f034e1': 'Can we build language-independent OCR using LSTM networks?',\n",
       " '4b96b327c8c128e07266f4059b76d1bc98d08a3a': 'Bio inspired computing - A review of algorithms and scope of applications',\n",
       " 'f7b0d94fd4a32c4c9be472b4e8d6c5bc308f0dfa': 'Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding',\n",
       " 'd99bc2544e4b1f33da5bb8010be8485b41a154f9': 'A Semi-automatized Modular Annotation Tool for Ancient Manuscript Annotation',\n",
       " '46b5188d0f11fcb62d95516bef94cdc929f0de40': 'Tracing Linguistic Relations in Winning and Losing Sides of Explicit Opposing Groups',\n",
       " '7744c16f5b24d331d7c28192f4159d37939b3df9': 'City, self, network: transnational migrants and online identity work',\n",
       " 'a91eca9d11755108c8c1a4354ed5b6c5a89ca4f8': 'FingerCode: A Filterbank for Fingerprint Representation and Matching',\n",
       " '6930ea33403a3518080e819d138047a80618a075': 'SAD: web session anomaly detection based on parameter estimation',\n",
       " '0daf48d0ce4e4200a753c28519f5761b160944fb': 'Multi-task Domain Adaptation for Sequence Tagging',\n",
       " '937d42a0ab3f703e7041e277804e0b34bbe6bacb': 'Innovative information visualization of electronic health record data: a systematic review',\n",
       " '4877d14b881afad4476891e16e44bf00040f68c0': 'Estimating the rumor source with anti-rumor in social networks',\n",
       " '76483bf302f4cea5f0ffce2f00704b1d0dd933a6': 'A MATLAB-based tool for EV-design',\n",
       " '09109a5375d8e2ca752bedde17ba5acad1df61cb': 'Sensor fusion for semantic segmentation of urban scenes',\n",
       " 'e1f53305791150a2109b7070bd7dce8071aafac7': 'Lessons Learned From Previous SSL/TLS Attacks - A Brief Chronology Of Attacks And Weaknesses',\n",
       " '04ca5de59edbdd49a9c0502c58331524d220bc8c': 'Communication Efficient Distributed Machine Learning with the Parameter Server',\n",
       " 'd059440d92c707c30fd0f297f6c95de57bf92113': 'Gated networks: an inventory',\n",
       " 'a829c1093eb4d78a198ebb9b4a33a8137b0359df': 'Mass-produced parts traceability system based on automated scanning of “Fingerprint of Things”',\n",
       " '318e36796798d173020e2c92ea4188b60d84a450': 'Incremental visual text analytics of news story development',\n",
       " '0bb9e12f068657407cde9f76e35bd540184edb3e': 'Verification based ECG biometrics with cardiac irregular conditions using heartbeat level and segment level information fusion',\n",
       " '2dbe49d7c9a65656cd46d22ea07dc317b26482b6': 'Development of 50-kV 100-kW Three-Phase Resonant Converter for 95-GHz Gyrotron',\n",
       " '2ea78e128bec30fb1a623c55ad5d55bb99190bd2': 'Residual vs. Inception vs. Classical Networks for Low-Resolution Face Recognition',\n",
       " '2ad9703a6e039258bc306e46c7eed1208a61f4aa': 'Exploring venue popularity in foursquare',\n",
       " '37a6a9963357bbb764410f1e9f3ac4b9df8e9e0d': 'Twitter Topic Modeling for Breaking News Detection',\n",
       " '3fc4e13ef5ebe962075a62b50b34077bb9425aa2': 'Novel Density-Based Clustering Algorithms for Uncertain Data',\n",
       " 'e0ec8e1a52f2c23561001a5e7fa1d30cb8222ff1': 'Single channel audio source separation using convolutional denoising autoencoders',\n",
       " 'b0b69f570647dde42aaf9c521675877d79d658ba': 'Implementation of Brain Breaks® in the Classroom and Effects on Attitudes toward Physical Activity in a Macedonian School Setting',\n",
       " '68f9901394bf28b5172fbe841709e3e589572052': 'BIDaaS: Blockchain Based ID As a Service',\n",
       " '822535c409890de3aae74b49b2bd8d4a59832fba': 'Research on data mining models for the internet of things',\n",
       " '7526f88d4ee7a380fecd05c1f92208ad94605cec': 'Chronovolumes: A Direct Rendering Technique for Visualizing Time-Varying Data',\n",
       " '63e1dffc19c3b4e99ae22ec60d10eaaafd608bcb': 'Accuracy and Resolution of Kinect Depth Data for Indoor Mapping Applications',\n",
       " '5f98a36570d428d510e9de699b97f4e2c63d7980': 'A 5-GHz fully integrated full PMOS low-phase-noise LC VCO',\n",
       " 'dd2393f444aa875a341ddf32249381f6d1cd36b8': 'Modeling the learning progressions of computational thinking of primary grade students',\n",
       " '929a82670f8bb4e9d38992e2b909e3ba80f67698': 'Lifted Proximal Operator Machines',\n",
       " '8a1ac7280e697e87aa9bfe6010a63d023944c792': 'Exploring Vector Spaces for Semantic Relations',\n",
       " '6cdbbced12bff53bcbdde3cdb6d20b4bd02a9d6c': 'Attacks on State-of-the-Art Face Recognition using Attentional Adversarial Attack Generative Network',\n",
       " '012e396b02aa584cb74a65ae14af355e7c897858': 'Efficient and secure data storage operations for mobile cloud computing',\n",
       " '51236676c3bba877d82c31b393db1af4846527ac': 'Improving Sampling from Generative Autoencoders with Markov Chains',\n",
       " '471f97da7975bad68f142980e0dd7dc753388f2c': 'VabCut: A video extension of GrabCut for unsupervised video foreground object segmentation',\n",
       " '81da438cb67600f6cb2920021e2327bb4c1d482d': 'Design and simulation of a four-arm hemispherical helix antenna realized through a stacked printed circuit board structure',\n",
       " 'de6411cb46cf889ba279ffc1a704168d75aae0b1': 'Making 3D Eyeglasses Try-on practical',\n",
       " '4fb07b6ff97ebfb80a7c18b0c55ebd32db84cd54': 'Deep Semantic Frame-Based Deceptive Opinion Spam Analysis',\n",
       " '304899db87ea51fa65eb562ecc918cb2ebeb41ee': 'A 64-Element 28-GHz Phased-Array Transceiver With 52-dBm EIRP and 8–12-Gb/s 5G Link at 300 Meters Without Any Calibration',\n",
       " 'a65e6a974212ed28133c2fe1e3b97a18dbc40cb6': 'Feature-Rich Named Entity Recognition for Bulgarian Using Conditional Random Fields',\n",
       " '124c0c016c8b938139afeb40d011070e3604268e': 'Euclidean and Hamming Embedding for Image Patch Description with Convolutional Networks',\n",
       " '804840bb733a459d01db7c4e72aeb5373b48da99': 'Development and prospect of unmanned aerial vehicle technologies for agricultural production management',\n",
       " 'c2ea4bf0282b9b39a6ba773581332bb0587ec4ab': 'SPAM E-MAIL DETECTION USING CLASSIFIERS AND ADABOOST TECHNIQUE',\n",
       " '03d23160e7066e5adab0d55779287e3c4982b9d5': 'Analysis of human faces using a measurement-based skin reflectance model',\n",
       " '314d65679c866483277fcc209ad89e7abab20126': 'Lightweight Prevention of Architectural Erosion',\n",
       " '7445178858159988f2b17aa21376e13767fce5a6': 'Towards Efficient Cryptographic Group Access Control Systems',\n",
       " '4a343313042b654c7d85313de3d88486a1d0e9de': 'Smart irrigation with embedded system',\n",
       " 'a6cc38100e4e5a679d920f2fe427d604557b6237': 'Measurement of eye size illusion caused by eyeliner, mascara, and eye shadow.',\n",
       " '349099074825c262b3f7c150ac8d470aabcebada': 'Breaking the Barrier of Transactions: Mining Inter-Transaction Association Rules',\n",
       " '033897d56c7cf4f084dec1fad072f1a6aca65c6e': 'Feature Extraction and Duplicate Detection for Text Mining : A Survey',\n",
       " 'eb3d1b885bfa800badfd79c6921a07d01491aedb': 'Intellectual capital and performance in causal models Evidence from the information technology industry in Taiwan',\n",
       " 'ea41155338edfcbf2891dbfc58698c34b03da068': 'A Review of Clinical Prediction Models',\n",
       " 'd27272027cd84341070fd4b7eb7e03dcb514d93f': 'A Dual Prediction Network for Image Captioning',\n",
       " '963eddddb0bc779ea1c7513e8cedd396882e3589': 'Quark-X: An Efficient Top-K Processing Framework for RDF Quad Stores',\n",
       " 'a19ec034e56bc7ff81240a1e4530f608fc262a96': 'Multimodal speech recognition: increasing accuracy using high speed video data',\n",
       " '901a4c9388b3cb9d30edc3e4a7ea7efb0b4e228b': 'Question Answering in the Context of Stories Generated by Computers',\n",
       " '280c96ea1644257069e16660a6a3a3a53f25858e': 'Evaluation of Predictive-Maintenance-as-a-Service Business Models in the Internet of Things',\n",
       " '25ffe3b737f0e5c3335918ba1b3ada43888d3885': 'Fingerprint verification by fusion of optical and capacitive sensors',\n",
       " '831e3f18d25cc65b6cd18f21ca5ad57bdc53cfce': 'Evaluation Datasets for Twitter Sentiment Analysis: A survey and a new dataset, the STS-Gold',\n",
       " '62abbb01f6b7d15b671551824f87931be409b2c2': 'Resource provisioning and scheduling in clouds: QoS perspective',\n",
       " 'af150ba3e05387a5279ea8e23d1de0b50953278e': 'DeepLogic: End-to-End Logical Reasoning',\n",
       " 'c6e446d78d05c74bad63cf23997c595eebbe6113': 'Water Nonintrusive Load Monitoring',\n",
       " 'b8eea99bb5345329ea058072133f568f7bdd27bd': 'CAES Cryptosystem: Advanced Security Tests and Results',\n",
       " 'd8dea89f5c7fb15d5ba16d2edcd2933a25789ee6': 'A least squares support vector machine model optimized by moth-flame optimization algorithm for annual power load forecasting',\n",
       " '51ee97313309157219412b6f5dfb314682b33992': 'A hybrid bug triage algorithm for developer recommendation',\n",
       " 'd1e61c73c8834acb63811d2e2bd9e6c4076b6a20': 'Some from Here, Some from There: Cross-Project Code Reuse in GitHub',\n",
       " 'c1b9cd88157205f2669b1fc2788d947e9a09d08a': 'A 65 nm CMOS 4-Element Sub-34 mW/Element 60 GHz Phased-Array Transceiver',\n",
       " '546b3592f59b3445ef12fba506b729c832198c33': 'Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network',\n",
       " '7aa39159fa794181ee07e4c6aab3990f358d08f1': 'Double ErrP Detection for Automatic Error Correction in an ERP-Based BCI Speller',\n",
       " '20704643278111342011c702aedf66c04a2c8e63': 'A survey about user requirements for biometric authentication on smartphones',\n",
       " '79729be88ecf76e5e964ffb25e8bc3b396feacb4': 'A framework for 3D visualisation and manipulation in an immersive space using an untethered bimanual gestural interface',\n",
       " '27fb7f79c74af0de19b3a9691268f9ca32bdda82': 'Pictures of Processes: Automated Graph Rewriting for Monoidal Categories and Applications to Quantum Computing',\n",
       " '7d4c8427e3bdc307e8da6cf53110aafb78f47a8a': 'Deep Reasoning with Multi-scale Context for Salient Object Detection',\n",
       " 'a1ac90b17d79e2053ca77808bcf87bcd9c1fddd1': 'The Impact of Comments and Recommendation System on Online Shopper Buying Behaviour',\n",
       " '5f16572861bf59950894f35aa896e4485868545e': '2011 Senior Thesis Project Reports Place Recognition for Indoor Blind Navigation Contents',\n",
       " 'e76f37bc17b1b9f2fb4b5296c7282787729523a2': 'Android interface based GSM home security system',\n",
       " '44872f2260ae42c162c2e8ed428c852f5e05fa24': 'DPICO: a high speed deep packet inspection engine using compact finite automata',\n",
       " '0f9bacfc21069a07bf3135cb0e94d83014933260': 'Can we beat Hadamard multiplexing? Data driven design and analysis for computational imaging systems',\n",
       " 'b47402a9a68f23b548ae6e0349700ea651b7a373': 'Action Recognition and Video Description using Visual Attention',\n",
       " '0e35c927c29d431812768f1ad7c4d6e35f0227b6': 'Emotion and moral judgment.',\n",
       " '4c37a3aa3a27bfb6fa73226e14ded88585cad97c': 'PKOM: A tool for clustering, analysis and comparison of big chemical collections',\n",
       " '9601f6f29b1b34eaee73071de615995a5a8c5d5b': 'An underactuated propeller for attitude control in micro air vehicles',\n",
       " '76c452b8934051aef5e91ea66db21d6749f1086c': 'Probabilistic Text Structuring: Experiments with Sentence Ordering',\n",
       " 'c81243651340f396c9100e86352cb43933307be9': 'Reduction of Unbalanced Axial Magnetic Force in Postfault Operation of a Novel Six-Phase Double-Stator Axial-Flux PM Machine Using Model Predictive Control',\n",
       " '6d1a69023e48422b25202a5ad823d12975291666': 'An Optimized Home Energy Management System with Integrated Renewable Energy and Storage Resources',\n",
       " '83718b398b9938b433e67376d1c95098a7f8812a': 'Detection of ascending stairs using stereo vision',\n",
       " '19cd76e349c5a2abdbcd0e741dc00b049591e7d3': 'Exchange Pattern Mining in the Bitcoin Transaction Directed Hypergraph',\n",
       " '30ef88ee401f70cf43b330b475d30f7f5e722630': 'Face recognition under partial occlusion using HMM and Face Edge Length Model',\n",
       " 'd2df6969b185a4017048f996d0e7cd1859c24e67': 'Output Range Analysis for Deep Feedforward Neural Networks',\n",
       " '58d105a565d27bd23443d257e36543ff7c40d167': 'Head pose estimation based on face symmetry analysis',\n",
       " '03bd09f62445ee68095f20000342c1c76b57d7c9': 'Square Root SAM: Simultaneous Localization and Mapping via Square Root Information Smoothing',\n",
       " '027e7780dbda48d99f3654e77b4a63063224950e': 'General transformations for GPU execution of tree traversals',\n",
       " 'bfa86c945787324220dc59a34fe5a242af1ea068': 'Execution-Guided Neural Program Synthesis',\n",
       " '6affd37b83d4fca0d0e54e5d75433a74cb142671': 'Plant identification system using its leaf features',\n",
       " '52d67b1ffaa5a135ff401e1ee0a2ff18571f2ce1': 'Power-efficient beam sweeping for initial synchronization in mm-Wave wireless networks',\n",
       " '45191391555865feed98e04dde63358dd1111839': 'Coupling-Feed Circularly Polarized RFID Tag Antenna Mountable on Metallic Surface',\n",
       " '43d547304f9cb0ff192e97c411f4ded9ddc01b5c': 'Speed vs . Accuracy : Designing an Optimal ASR System for Spontaneous Non-Native Speech in a Real-Time Application',\n",
       " '9a1de1dab15d6ebbac429af9682fcc160beacc74': 'Alternating Optimisation and Quadrature for Robust Reinforcement Learning',\n",
       " '1134105faa6a16969fede23d16f0c0df129c5888': 'Affordances and Limitations of Immersive Participatory Augmented Reality Simulations for Teaching and Learning',\n",
       " '3aba958809c43b0af8df66e5338d9310549eeb58': 'Robust Structured Light Coding for 3D Reconstruction',\n",
       " '0a149bfc3080902dab96a0bfdfe1d4ab2ec0cc2a': 'DialPort: Connecting the spoken dialog research community to real user data',\n",
       " 'd212691257354c201a32729b997ede447e498640': 'Novel DC-DC Multilevel Boost Converter',\n",
       " 'd24ef1f8c2c9bfbd2bd552b1ba516e4147cf6423': 'Extended Tracking Powers: Measuring the Privacy Diffusion Enabled by Browser Extensions',\n",
       " '0ee5364a1672ec0b8dbb3e1f84a5eca1d7851a6d': 'Modeling direct and indirect influence across heterogeneous social networks',\n",
       " '038ee3d9e0a739752f4a270548ab8c97ed024633': 'Fast Vehicle Detection with Lateral Convolutional Neural Network',\n",
       " '8cc8ad35f409ca0c008d4ba0c666dc295348d7c1': 'An open source research platform for embedded visible light networking',\n",
       " '25dbf9b12475454585d5050c5cf446e1c4f6dd27': 'Online Multiperson Tracking-by-Detection from a Single, Uncalibrated Camera',\n",
       " '030ff7012b92b805a60976f8dbd6a08c1cecebe6': 'DCAN: Dual Channel-Wise Alignment Networks for Unsupervised Scene Adaptation',\n",
       " 'a3240d1ce2fcaf412f4b5d1562ddfa687061036b': 'Shadow-Based Rooftop Segmentation in Visible Band Images',\n",
       " 'bb93fdae023ee2978e82f47c1c069b973046ef1e': 'Supervised distance metric learning through maximization of the Jeffrey divergence',\n",
       " '50d388989d9b25754ef9dc5663e85eaf64a17d24': 'The effects of fear and anger facial expressions on approach- and avoidance-related behaviors.',\n",
       " 'a03ae676aa270645a5b30dc6514409a3fc4fdecc': 'IntelliArm: An exoskeleton for diagnosis and treatment of patients with neurological impairments',\n",
       " '435a709ae1d1b0bbdcacbd3965a05d56c14e752c': 'Improving robot manipulation through fingertip perception',\n",
       " '5f1f12015d55c51764be27df92de175d2de8ee0d': 'The effect of focused attention and open monitoring meditation on attention network function in healthy volunteers',\n",
       " 'a8a4cf29c7483f5eed5aa808df2225661506e1b4': 'Hybrid music recommender using content-based and social information',\n",
       " '81efd70fcc216b6528d48efa758671d9862b21c3': 'Traffic Light Recognition for Complex Scene With Fusion Detections',\n",
       " '18398d9c9224d5c0dadf22dbf1c21c11de8705b7': 'Classify Sentence from Multiple Perspectives with Category Expert Attention Network',\n",
       " 'fd5f1f443d40e6e91dc99798ec88922220e4b364': 'Real-Time Inter-Frame Histogram Builder for SPAD Image Sensors',\n",
       " '823b646805304b213779149e968cdc081909b651': 'A 0.21-V minimum input, 73.6% maximum efficiency, fully integrated voltage boost converter with MPPT for low-voltage energy harvesters',\n",
       " 'bfd50521466808d022940fbcfe09e1835ae97822': 'SKILL: A System for Skill Identification and Normalization',\n",
       " '0e894b5dc37f7bf983eca17e869615d16398d47d': 'A New Fast and Efficient Decision-Based Algorithm for Removal of High-Density Impulse Noises',\n",
       " '2a9b398d358cf04dc608a298d36d305659e8f607': 'Facial action unit recognition with sparse representation',\n",
       " '785e957937b1d0de27c39c916e74aa02220cc27e': 'A model based path planning algorithm for self-driving cars in dynamic environment',\n",
       " '1e1c56608f9dca5281bd79c7dfbfb058aab25472': 'Colorization using optimization',\n",
       " '313b369196df6026a97016327f8e4eebaf1a0176': 'Modeling and Analyzing Millimeter Wave Cellular Systems',\n",
       " 'd26e891f388cbff3cadf498c4df54b735d31ffb7': 'Decision-Making Framework for Automated Driving in Highway Environments',\n",
       " '4fa89f6ffef921580718222c55fa17973a7c366d': 'Shadow Detection with Conditional Generative Adversarial Networks',\n",
       " '9583ce13282029bda54a7907cb5547046cf7d2a8': '7 Key Challenges for Visualization in Cyber Network Defense',\n",
       " 'd7c0189c35a3ee56b13b0488f925df1152a8cfd4': 'A Taxonomy of Modeling Techniques using Sketch-Based Interfaces',\n",
       " '46fe01be6684b2c6d04298f0e40589eff560af2b': 'Does Mindfulness Meditation Enhance Attention ? A Randomized Controlled Trial',\n",
       " 'f5e690c7aca37f32f0f04ff3ccfab09e210fd89b': 'Morphological Embeddings for Named Entity Recognition in Morphologically Rich Languages',\n",
       " '9a5fa5e9c10dc40e20610795bad44e7e00391c1f': 'A Fog-Based Internet of Energy Architecture for Transactive Energy Management Systems',\n",
       " '472ebd8bc3808530274a49804b38e321cc5b4065': 'MASK: Redesigning the GPU Memory Hierarchy to Support Multi-Application Concurrency',\n",
       " '600994f3d8caad52145cc65bf1bfcd883b74a813': 'MIMO Wireless Linear Precoding',\n",
       " '41b61cca52de9db6dcd6f36700cd0420cc7cd024': 'Ontology-based traffic scene modeling, traffic regulations dependent situational awareness and decision-making for automated vehicles',\n",
       " 'c62c07de196e95eaaf614fb150a4fa4ce49588b4': 'SSR-Net: A Compact Soft Stagewise Regression Network for Age Estimation',\n",
       " '8562ce6da684928836a19277bcdbbca9bb8c27fe': 'Light-Exoskeleton and Data-Glove integration for enhancing virtual reality applications',\n",
       " '8c251765799bb3a2ce7b42fe94805fa008d3b48e': 'MODEL AS A DESCRIPTIVE TOOL IN EVALUATING A VIRTUAL LEARNING ENVIRONMENT',\n",
       " '191e9300845097e1e8eb9696db458efc968baee1': 'Tools to support systematic reviews in software engineering: a feature analysis',\n",
       " '92ec85037f5e195c8aa184534a59b356c6ef7599': 'Cross-Domain Visual Recognition via Domain Adaptive Dictionary Learning',\n",
       " '3713243ded59fd9b0a4428bd101616bad7393311': 'Enablers and Barriers to the Organizational Adoption of Sustainable Business Practices',\n",
       " 'd5aca71c0513f0c1669304634a85499ca19d2643': 'Automation in airport security X-ray screening of cabin baggage: Examining benefits and possible implementations of automated explosives detection.',\n",
       " '0cf7f818b313a7a16bb33960c6322199a2441e06': 'Region-based CNN for Logo Detection',\n",
       " '1fdec82c846cb843a496e5edfec72787f34d308a': 'Flower classification based on local and spatial visual cues',\n",
       " '53115fffaa36c99a45fb7741fa74d66aa4fb8517': 'Indexing multi-dimensional data in a cloud system',\n",
       " '64acf2715c5e8fc54e444ca38c39f41b89b6cb19': 'Application of Stereo Vision on Determination of End-Effector Position and Orientation of Manipulators',\n",
       " 'f254f031bd7c68a56cfff922f8a0665768994ef2': 'Event Pattern Analysis and Prediction at Sentence Level using Neuro-Fuzzy Model for Crime Event Detection',\n",
       " '78ed1c69f4f5f5b56cbb9432ee06a2cf71d162e2': 'Authentication anomaly detection: a case study on a virtual private network',\n",
       " 'd68359c196ed7316b928b9dbeabe556cddbb2427': 'Characterisation of acoustic scenes using a temporally-constrained shift-invariant model',\n",
       " '09b349399b8b696d365185ee3896dfae77af8ac5': 'Ontology-Based Integration of Cross-Linked Datasets',\n",
       " 'd6127837199a496f1e67f649c21dcb8ec7441e7b': 'Robust Visual Knowledge Transfer via Extreme Learning Machine-Based Domain Adaptation',\n",
       " '03725753e46ee9b13cbdfa78c9b62700d4cc2956': 'BRAND: A robust appearance and depth descriptor for RGB-D images',\n",
       " 'e161535a799a2693d94b0497d7045b0518a10bef': 'Antecedents and Consequences of Organizational Commitment Among Pakistani University Teachers',\n",
       " 'e9c313fb4cfb8e31cd5ab80ef692dd30f27b70bf': 'The Impact of Internal Social Media Usage on Organizational Socialization and Commitment',\n",
       " '1b7db8ad49f94da9b90db89bede5f27644bb9911': 'SGDR: Stochastic Gradient Descent with Restarts',\n",
       " '9a48dd7c54b251b4eab96488124e6964ed16c1c7': 'Integrating text mining, data mining, and network analysis for identifying genetic breast cancer trends',\n",
       " 'de1505819e145b5c22a6e09002510413019f7228': 'DeepFood: Deep Learning-Based Food Image Recognition for Computer-Aided Dietary Assessment',\n",
       " '2a65434d43ffa6554eaf14b728780919ad4f33eb': 'Modeling purposeful adaptive behavior with the principle of maximum causal entropy',\n",
       " '580836f616cbba088996643634363a85e91cccee': 'A Complete Recipe for Stochastic Gradient MCMC',\n",
       " '9759c425008506dac507ed26057febd9cab822b8': 'Active Sentiment Domain Adaptation',\n",
       " '31c1327754c921e848bf2d0ff6ea6828e29680e3': 'Sources of Momentum Profits : Evidence on the Irrelevance of Characteristics ∗',\n",
       " 'c85d46a94768bdcf7ffcb844b47c5b8e8e8234a3': 'Long short-term memory recurrent neural network architectures for large scale acoustic modeling',\n",
       " 'da803152928a3b98ef21a81a6675e7b51852510e': 'An evolutionary tic-tac-toe player',\n",
       " '41e5ed6744f1c901a55ee68ea8235d3340e32528': 'Analysing RateMyProfessors Evaluations Across Institutions, Disciplines, and Cultures: The Tell-Tale Signs of a Good Professor',\n",
       " '562e275ff6615d3b59bac857fba6dc66a8d4a59c': 'Parser Extraction of Triples in Unstructured Text',\n",
       " 'b158953a73fc7f023e16b430150968b6f237c806': 'Neural Darwinism and Consciousness 1 RUNNING HEAD : Neural Darwinism and Consciousness Neural Darwinism and Consciousness',\n",
       " '109cc0e1b5cbf8f4e329f93060032db46de72bd8': 'Universal upper and lower bounds on energy of spherical designs',\n",
       " '46c87eae824a442323147a845e285167f283dd08': 'The Hidden Image of the City: Sensing Community Well-Being from Urban Mobility',\n",
       " '1f157f2b144528924eec46d9316bd5517352b89a': 'How do fixes become bugs?',\n",
       " '1bf06b2bbf53089180fd3676720d80cbaed5975d': 'The impact of Web quality and playfulness on user acceptance of online retailing',\n",
       " 'ccdc79b2aec687beef2cd3526cf47dd79b9ec220': 'An Android Malware Detection Approach Using Weight-Adjusted Deep Learning',\n",
       " 'd7e3a93de209c0c5501b4932b9a7d81c89ed64e8': 'The Theory of Planned Behavior and Parental Involvement : A Theoretical Framework for Narrowing the Achievement Gaps',\n",
       " 'bdd6b6635cf5861fc9d914a6d2938fcb2daea7ef': 'DOES THIS ANSWER YOUR QUESTION? Towards Dialogue Management for Restricted Domain Question Answering Systems',\n",
       " '31d33747d8fff0b7a0c40dcf9944015af9a15b1a': 'BabelNet: Building a Very Large Multilingual Semantic Network',\n",
       " '2cd7c3ed5a06c461b259694376820dcfcfbe94a9': 'Effective Inference for Generative Neural Parsing',\n",
       " 'fe4cfc7df897c632311ed0165e66ff9932865e5f': 'Privacy preserving big data mining: association rule hiding using fuzzy logic approach',\n",
       " '74ff6bd7f70bb2fe17016ded5104614c001f0cef': 'Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks',\n",
       " '2c9466f3849cc3b25eb00db90d146cd0c26bdef4': 'Automatic ranking of swear words using word embeddings and pseudo-relevance feedback',\n",
       " '1d9393ad3665996ea0a6a0125b408ada34c37a54': 'The dark side of personality at work',\n",
       " 'cf9d4048d39c63d96433b6f45fb6a951b0308607': 'Trends, Tips, Tolls: A Longitudinal Study of Bitcoin Transaction Fees',\n",
       " '16b3f9790d37035faf5837ac68661c6df13a9dcb': 'Conditional Gradient Sliding for Convex Optimization',\n",
       " '10eeeca03909bd44af5a5b5791e1b1ef36ff5c9e': 'Fundamental limits on adversarial robustness',\n",
       " '6097c33a382c62a44379926ee96b23b51dba49c4': 'From Depth Data to Head Pose Estimation: a Siamese approach',\n",
       " '128a0612e9b1bdec9de606e3a9c1afcd8170f0f2': 'PRACTICAL HYPERPARAMETER OPTIMIZATION',\n",
       " '071961fc3d61b893c12f07abfa2906859152e3a9': 'Learning to Rank Non-Factoid Answers: Comment Selection in Web Forums',\n",
       " '52c036ba3ba46eadc146ad8e326eeb0c8acd1550': 'Trajectory Planning For Exoskeleton Robot By Using Cubic And Quintic Polynomial Equation',\n",
       " '8cb5835c4b4e042238304bfe7b0d96456714638a': 'Compact Implementations of ARX-Based Block Ciphers on IoT Processors',\n",
       " '40c3f90f0abf842ee6f6009c414fde4f86b82005': 'Synchronization Detection and Recovery of Steganographic Messages with Adversarial Learning',\n",
       " '52f8eb239997d9a324d4794529c60522db8d08bf': 'Learning Multi-scale Block Local Binary Patterns for Face Recognition',\n",
       " 'a630a8e1d7c23d94ce5950144d2e504d4b590136': 'Multimode and Wideband Printed Loop Antenna Based on Degraded Split-Ring Resonators',\n",
       " 'd4dcbb547492ea63bb1dd2016f2ca1198225d78f': 'From Brexit to Trump: Social Media’s Role in Democracy',\n",
       " '1cb77e2a09db58e9e8b37878c7de313d41e6f854': 'An Adaptive Version of the Boost by Majority Algorithm',\n",
       " 'a95fd5d561bc2996b4d6d5574139ad8162cb78ce': 'Health management and pattern analysis of daily living activities of people with dementia using in-home sensors and machine learning techniques',\n",
       " '57ad8dfb71589360810da83efce67b4cab2ff380': 'Comparison of Pooling Methods for Handwritten Digit Recognition Problem',\n",
       " '603d0764234f5a35036dc9620f13e144a45ffc21': 'Modeling paddle-aided stair-climbing for a mobile robot based on eccentric paddle mechanism',\n",
       " '54cd8bfbe670a2fd787ff079bdeb11244a2609e8': 'A Protocol for Preventing Insider Attacks in Untrusted Infrastructure-as-a-Service Clouds',\n",
       " '0d21cc2677e544b46673ff19ad4f378f32129069': 'Convolutional MKL Based Multimodal Emotion Recognition and Sentiment Analysis',\n",
       " '4acb38f96f3f69a443e1684040c14d1cecc4f841': 'Automatic Defect Detection for TFT-LCD Array Process Using Quasiconformal Kernel Support Vector Data Description',\n",
       " '41c98c3080159ea3fbef043358fccd5b2576fa6e': 'ARQuake: an outdoor/indoor augmented reality first person application',\n",
       " '9a2c20b7b8602d1dd7332f8fb5017e5c846185c4': 'Predicting the Evolution of Scientific Output',\n",
       " '89c8e0ac83e8bac69ca41b63bb887a02950bbe9c': 'Flexible 16 Antenna Array for Microwave Breast Cancer Detection',\n",
       " 'e69b1314cd65a115c98082a5863b92daa4dcf9f0': 'Automated Melanoma Recognition in Dermoscopy Images via Very Deep Residual Networks',\n",
       " '337f48a82d9d2739d35b41cfc4e8cd3dc906639d': 'Accelerating aggregation using intra-cycle parallelism',\n",
       " '53091fe2f9e95b07759ca1537d6ea8819ba25cb6': 'Real-Time Multi-human Tracking Using a Probability Hypothesis Density Filter and Multiple Detectors',\n",
       " 'f68911078030c56f34e68041908c26808b63f57b': 'A Composable Deadlock-Free Approach to Object-Based Isolation',\n",
       " '614b23042fb044ecfec71170838fefc635840c6a': 'Bat Algorithm and Cuckoo Search: A Tutorial',\n",
       " '07a5c4ba84268708146aa4bf5cad9491b3e35051': 'Deep Reinforcement Learning for Dialogue Generation',\n",
       " '21a3a10bd30461c62140bd7ad0153935ae34ae5b': \"Autoencoders trained with relevant information: Blending Shannon and Wiener's perspectives\",\n",
       " '7740048416e8f39c3d6eb4f3ee02ca2dfd3590bb': 'Low Latency Live Video Streaming over HTTP 2.0',\n",
       " '511f94b93acac1009897d9af5b6d6d4fa65554d0': 'Expressive Visual Text-to-Speech Using Active Appearance Models',\n",
       " 'b1d09732fae72001277f867522ae1b67a249975b': 'Text detection in nature scene images using two-stage nontext filtering',\n",
       " 'f0ea85fb243d92d43b67e64ddf8721cde650c864': 'A learning-based approach to text image retrieval: using CNN features and improved similarity metrics',\n",
       " 'b3fd26eb2931ff814634d314855b2c2cc007b778': 'Automatic refinement of large-scale cross-domain knowledge graphs',\n",
       " '71ac664ca2cbc5c463da3ed07a1573f88e2e28f0': 'Layout Analysis for Scanned PDF and Transformation to the Structured PDF Suitable for Vocalization and Navigation',\n",
       " 'd1c0cc34e1b6eaa8bdc2221d83377063be951196': 'Honeypot Frameworks and Their Applications: A New Framework',\n",
       " 'fe76c167920898330a66e3f2509ff1cd41e231bc': 'Comparative abilities of Microsoft Kinect and Vicon 3D motion capture for gait analysis.',\n",
       " '349fd2000d792b53471cfd98decfd2cb5df5ac7d': 'Probabilistic models of ranking novel documents for faceted topic retrieval',\n",
       " '1b44df3232099b13e3aac7fc5811a946133d6db6': 'Mobile business models: organizational and financial design issues that matter',\n",
       " '59dca5c15c275f26c7ae34fc940449aeb918c68b': 'Collaborative video reindexing via matrix factorization',\n",
       " 'd6649d1eeace7fc7c020b047b22ba79c7481c324': 'Near or Far, Wide Range Zero-Shot Cross-Lingual Dependency Parsing',\n",
       " '5137d6a245dffe321df34999fa77e190c19b4c37': 'Image Compression With Edge-Based Inpainting',\n",
       " 'd8643f4ada0b138bcbe210f84735eac87b220a07': 'Semi-supervised clustering with metric learning: An adaptive kernel method',\n",
       " '36ea0a9710b9310ce9c6ce199af63b6a00eea480': 'Improving Named Entity Recognition for Chinese Social Media with Word Segmentation Representation Learning',\n",
       " '9c3666efd35f7a8365db74716b0265110c610811': 'Scalable real-time volumetric surface reconstruction',\n",
       " '2a6300ea9db239f190cb3d8cba4e45a7c0b1c710': 'Visual Speech Recognition',\n",
       " 'b344f437eabc57a83e5f67ecccbba6efd9df958e': 'Collocation least-squares polynomial chaos method',\n",
       " 'e414ba960ee2a385b6800f2086209c711cc3b48b': 'Human Character Recognition by Handwriting using Fuzzy Logic Ms .',\n",
       " 'dc344ea8e993584b924522d95febaeb74de2ad30': \"Investigating How Student's Cognitive Behavior in MOOC Discussion Forum Affect Learning Gains\",\n",
       " '0963302a589b5476df76040ab22a3315e0f84bb1': 'Lire: lucene image retrieval: an extensible java CBIR library',\n",
       " 'f46a2337a9d94d68c2d92f0aa0ca693adafc3346': 'Technology infusion for complex systems: A framework and case study',\n",
       " 'feef5abe52b21ffc198a9f439ec49151c545ef12': 'Evolvability : What Is It and How Do We Get It ?',\n",
       " '3ca983d40b9de7dc12b989fce213b4abee652c9e': 'Will the Pedestrian Cross? A Study on Pedestrian Path Prediction',\n",
       " 'cb3d6142bfa0938aa6f8cb86697c40a7837c08f0': 'LTEV2Vsim: An LTE-V2V simulator for the investigation of resource allocation for cooperative awareness',\n",
       " '5fe44cc9a790e72e328f35ba7a94db572e7db114': 'DeepSense: A Unified Deep Learning Framework for Time-Series Mobile Sensing Data Processing',\n",
       " 'a6500d8800010e79d83800c9e433435efbc4398e': 'Dynamic Multi-Level Multi-Task Learning for Sentence Simplification',\n",
       " '2a4c67e35d447204a1451ebcd81701a774510118': 'Actor-critic algorithms',\n",
       " '45182cf235fd8f9dd8a81f61f3efd488f78df75e': 'The fully informed particle swarm: simpler, maybe better',\n",
       " '36279b9053b8b28759a94eaa428d5e0b6dec4246': 'Applying LTE-D2D to Support V2V Communication Using Local Geographic Knowledge',\n",
       " '7d132ec7b7f94fe1397f6c1e8e5bb4fa42806ad1': 'What to believe: Bayesian methods for data analysis',\n",
       " '8384094ce1b342da9eabd2ec939e9bb16ca7ff5c': 'New local edge binary patterns for image retrieval',\n",
       " '3d50160aaf1de051a05aa03350bf5ac492053663': 'Cyclostationary Feature Detection in Cognitive Radio using Different Modulation Schemes',\n",
       " '7ddc47d09d89a89c6a1c7643895a173aba07173f': 'Spacetime expression cloning for blendshapes',\n",
       " '2c64233ad8239884cdd410fb63c63124bd9fb515': 'Enhanced Fraud Miner : Credit Card Fraud Detection using Clustering Data Mining Techniques',\n",
       " 'f072d0363f437fd5ccf359bbe2fc9afaa5cfa831': 'Subfigure and Multi-Label Classification using a Fine-Tuned Convolutional Neural Network',\n",
       " '168ecf130d9ece95d49d6ace5f9926f88a487303': 'Demystifying Differentiable Programming: Shift/Reset the Penultimate Backpropagator',\n",
       " 'ffaa313b8da3695627cd9915ca46b8bed24a9f4a': 'Probability Product Kernels',\n",
       " '5fc9dfaa3212ab3bbab4faddc09771c5b0918b1a': 'Learning of closed-loop motion control',\n",
       " '4b82b3bf2b30d74c083940a86db5b09e4b81f0af': 'Generalized residual vector quantization for large scale data',\n",
       " '98c47fc9d5400b7ec90b2e5172635949e65a0b66': 'An Intelligent Load Management System With Renewable Energy Integration for Smart Homes',\n",
       " '6c442b5493a17926a9fa22e85a1f7a70b2f03230': 'A Novel 60 GHz Wideband Coupled Half-Mode/Quarter-Mode Substrate Integrated Waveguide Antenna',\n",
       " '503f81cce1b24f9625bf4a0633de9396023e3868': 'Multi-Level Topical Text Categorization with Wikipedia',\n",
       " '7de4eb8c11972a64236434cbe95af47b92d438b9': 'A genetic algorithm optimized MPPT controller for a PV system with DC-DC boost converter',\n",
       " '639910e5fa8d6f096a282847ca3e820550bbbe8e': 'Query Attention GloVe GloVe CNN Attention Flow Layer Modeling Layer Output Layer',\n",
       " '3b28a409d820b611667bf22135e1f372cf12eb64': 'Full-Duplex Cooperative Non-Orthogonal Multiple Access With Beamforming and Energy Harvesting',\n",
       " '25b1b8f72d025f23f61cf6c94749c86a7e2449a3': 'DramaBank: Annotating Agency in Narrative Discourse',\n",
       " '0553dbcc91c98d5e068f6532f0b071a7d219d67e': 'An empirical task analysis of warehouse order picking using head-mounted displays',\n",
       " 'a0ad588036808adaa834c3bada3a1bcc1545f8fa': 'Large-Scale Automated Software Diversity—Program Evolution Redux',\n",
       " 'b7d0e2a5229d4ec65182c783259a2eeb67a6a873': 'Who are these people ? ” Evaluating the demographic characteristics and political preferences of MTurk survey respondents',\n",
       " '177f06f0b11a911317f500b455adeeadf9d48e00': 'Custom soft robotic gripper sensor skins for haptic object visualization',\n",
       " 'e66ab9039b49b6dd0ecce124a71f1044750107d2': 'Hardware system synthesis from Domain-Specific Languages',\n",
       " '38279d233762ea289e24e566d52207407f9ec3ab': 'A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference',\n",
       " '79b97a68a26c38e8441e3f7902c6f6cf5d975622': 'Optimal design and tradeoffs analysis for planar transformer in high power DC-DC converters',\n",
       " '514a110592571e98f3ba643ee4567ea8a9647fd0': 'Business-to-business e-commerce adoption: An empirical investigation of business factors',\n",
       " '8c8c9280f6d2d06b05c1c18f4f7ee08621357163': 'End-to-End Learning of Deterministic Decision Trees',\n",
       " 'f852431fb190fdc6feb95e5bd319a58039519076': 'Detecting Significant Locations from Raw GPS Data Using Random Space Partitioning',\n",
       " '05b32985bf72fe15383bb4bba14beb43e438e937': 'An LTCC-based 35-GHz substrate-integrated-waveguide bandpass filter',\n",
       " '9700939818a450d1bff3a3a29d71ef116d3a6a12': 'Augmented : Reality : : Architecture : Interface',\n",
       " '74430cd0bc67f7858f618de66067ba96a80fdf05': 'Identifying At-Risk Students in Massive Open Online Courses',\n",
       " '3e2d7b31f691f42f141e9f9ecd455c4879292be9': 'Machine Learning Approaches for Failure Type Detection and Predictive Maintenance',\n",
       " '5358924a48392d24e28afe850fa6849eaa1665d9': 'Argumentative Zoning for Improved Citation Indexing',\n",
       " '27b2c6ba78c48af54239366f346b57227cccb666': 'Knowledge Graph Identification',\n",
       " '10b31443d0fa8e198e7db88964fe7206270ad229': 'Towards edge-caching for image recognition',\n",
       " '66b64087bfd9a59805a24ec06add72dac6e86fc9': 'Exploring the Effects of Word Roots for Arabic Sentiment Analysis',\n",
       " '095923857403ebb1578ce82b085c97c75b522fa2': 'Training Faster by Separating Modes of Variation in Batch-normalized Models',\n",
       " '83af09784042dcac564a82f28cfa3a11519db882': 'Learning grounded finite-state representations from unstructured demonstrations',\n",
       " '8f03f545ff791a70455ad4624208357e41dcfc0c': 'Unlinkable Coin Mixing Scheme for Transaction Privacy Enhancement of Bitcoin',\n",
       " '34f4c97eaf57fbbc3dbae7887afb5fef2f9696c5': 'Design of Dielectric Lens Loaded Double Ridged Horn Antenna for Millimetre Wave Application',\n",
       " 'a352061134daa2c47861b8c4216ee5482a93be1d': 'BPM Governance: An Exploratory Study in Public Organizations',\n",
       " '3b9ba3d40d8f1b4efd53dd81439d3a5dd4ee0629': 'The WaCky wide web: a collection of very large linguistically processed web-crawled corpora',\n",
       " '2520c3d5d114974167561591a57f80e89650f862': 'Direct Pose Estimation and Refinement',\n",
       " '66ef0f611b2b5b22dd3eb13a144c0e7d3286623a': 'Leveraging Context-Free Grammar for Efficient Inverted Index Compression',\n",
       " 'b453a4131201360dbd9743ffbe7d4939d099059d': 'Analog transistor models of bacterial genetic circuits',\n",
       " '4bec8908ef97074b4f9a0f1c1693e58e867ff19d': 'Mitigating Multi-target Attacks in Hash-Based Signatures',\n",
       " '1312b0d0a957fc2bbfc2612dd89ba9003c57a08c': 'Improving Japanese-to-English Neural Machine Translation by Paraphrasing the Target Language',\n",
       " '732f728283917b3bad295099a30c8af6374c74be': 'A* CCG Parsing with a Supertag and Dependency Factored Model',\n",
       " '4f9b4c1ac6e1b2f417809009aff2fc11300cc855': 'Real-Time Impulse Noise Suppression from Images Using an Efficient Weighted-Average Filtering',\n",
       " '5de469ee2a766f24ffbe45ff000efcba97b66cc7': 'Design, Implementation, and Performance Evaluation of a Flexible Low-Latency Nanowatt Wake-Up Radio Receiver',\n",
       " '62d588d2200ab2cbee153a8998a7fa98c30cf7bb': 'Exponential Moving Average Model in Parallel Speech Recognition Training',\n",
       " '8653934b00dfb802a7dd066f8f52c5dd3b267831': 'Secure kNN computation on encrypted databases',\n",
       " '08eeaae7108e35a9639ef750a75132d0c71b2dd1': 'Link Prediction using Supervised Learning ∗',\n",
       " '46a1172c784c3741e79781ef2353209b08dbea67': 'YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition',\n",
       " '28cf9ac2f4e90942595d1069501d171f64ef76c3': 'Friends only: examining a privacy-enhancing behavior in facebook',\n",
       " 'f1256b20d202c73022d7a7f0151ba0010a074a06': 'Extended object tracking using IMM approach for a real-world vehicle sensor fusion system',\n",
       " 'ce00fc554965ea7b187dfa93292013f019d67d39': 'Planar-fed folded notch (PFFN) arrays: A novel wideband technology for multi-function active electronically scanning arrays (AESAs)',\n",
       " '6999766629d4103b9a00bc20a8e0df0c9d12d7da': 'Comparison Analysis of CPU Scheduling : FCFS, SJF and Round Robin',\n",
       " '7ac01e727c60f0c64e00c7207d432ff2138d7b3f': 'Critical infrastructure interdependency modeling: Using graph models to assess the vulnerability of smart power grid and SCADA networks',\n",
       " '78b6cbcceca106c039c9dc2d757376956882ac64': 'ContexloT: Towards Providing Contextual Integrity to Appified IoT Platforms',\n",
       " '689f8d21021c0c9b9678d5a7903f9e9442380113': 'On The Differential Privacy of Thompson Sampling With Gaussian Prior',\n",
       " '725ed843566051638d023ebfdb0311def12cba2a': '“Mask-Bot 2i”: An active customisable robotic head with interchangeable face',\n",
       " '18422ae57ec0318fb4c200fdcf7c6e145208d792': 'Transfer Learning Based Visual Tracking with Gaussian Processes Regression',\n",
       " '38ca15193faef0ab90e6cab5b82be5f7f1a83d67': 'Recognizing Surgical Activities with Recurrent Neural Networks',\n",
       " '37fefbdf740577306928ff2f0c810fb3a96183b2': 'Nonlinear Camera Response Functions and Image Deblurring: Theoretical Analysis and Practice',\n",
       " '33812e0c6b8fdc5e414c5eae760e574e5a81190a': 'StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity',\n",
       " 'd8ae4f66a0406ab7167f9ffd39ead29e62bac28f': 'Dual-band stepped-impedance transformer to full-height substrate-integrated waveguide',\n",
       " '53a124b727d34c56ff3fe335c24ff8ec975516be': '6-DOF Model Based Tracking via Object Coordinate Regression',\n",
       " '3f55d26dd638c849745b95e912c28d88445ba5e1': 'Supervised Learning of Universal Sentence Representations from Natural Language Inference Data',\n",
       " '4a6955d29a72953bb8ad36240615c55c167a0d43': \"Mobile Shopping Consumers' Behavior: An Exploratory Study and Review\",\n",
       " 'ebebd7a0e288d137f1e4e579dd7f9f510d76497e': 'Understanding the roles of self-esteem, self-compassion, and fear of self-compassion in eating disorder pathology: an examination of female students and eating disorder patients.',\n",
       " 'c8e1dbd7590386a9060adc51f70a7bf44de01841': 'Design, Implementation and Performance Analysis of Highly Efficient Algorithms for AES Key Retrieval in Access-driven Cache-based Side Channel Attacks',\n",
       " 'd5997a0d13420e5557c077f9e44205a27bcd1fd2': 'Design of Low Power and High Speed CMOS Comparator for A / D Converter Application',\n",
       " '725f8182eac9bd6f9fa428ba90f43cbce891ac78': 'Improved keyword spotting based on keyword/garbage models',\n",
       " '52d4649f2840e410b5a6681f094dab5e7d3e1db0': 'Graphcut Texture Synthesis for Single-Image Superresolution',\n",
       " '0c8a029180e8ee5a7a8c886738576b12d3f6530d': 'Continuous Deployment at Facebook and OANDA',\n",
       " '951450392f1e08b4dc96d814754a90f61d6151ae': 'Anthropomorphic, compliant and lightweight dual arm system for aerial manipulation',\n",
       " '0ae51a9ac89e363097bcd675a56901b9444fd739': 'Construction and optimal search of interpolated motion graphs',\n",
       " '3d9180d48a6c19eb59f5ba6cef378720758b2ed3': 'Parasitic Inductance and Capacitance-Assisted Active Gate Driving Technique to Minimize Switching Loss of SiC MOSFET',\n",
       " 'd36e081ac6be39796e7cba0d0d813ea53974ec07': 'Anno : A Graphical Tool for the Transcription and Onthe-Fly Annotation of Handwritten Documents',\n",
       " '419794ac69424b3cd66127c909dde325fe88c463': 'A framework for analyzing semantic change of words across time',\n",
       " '26825e78a84242504048b39d0ba21c859e52dc46': 'A Multi-Layered Annotated Corpus of Scientific Papers',\n",
       " '3547ac839d02f6efe3f6f76a8289738a22528442': 'Efficient String Matching: An Aid to Bibliographic Search',\n",
       " '89e9b7577e75ebf3887a0ddf401b199fea4f2148': 'Big Data Perspective and Challenges in Next Generation Networks',\n",
       " '20534da8c7ee7063cbb32620d53e7322f90be2e0': 'First Demonstration of 28 GHz and 39 GHz Transmission Lines and Antennas on Glass Substrates for 5G Modules',\n",
       " '1d8465c3f5aee1b7a790f6eeb44637343861ba47': 'Towards Machine Learning-Based Auto-tuning of MapReduce',\n",
       " '6bb5d79e3f255f091f110e7c6952076f2fd27da5': 'RAPID: A Fast Data Update Protocol in Erasure Coded Storage Systems for Big Data',\n",
       " '212d7af298a4bd9090b834293067d2f091a95cfc': 'Sign Language Production using Neural Machine Translation and Generative Adversarial Networks',\n",
       " '5eaf6d38ba1cf100b7511be01fe593455ae5657f': 'A method for the reduction of ship-detection false alarms due to SAR azimuth ambiguity',\n",
       " 'c73a1ee22c605341cd1218853d4680f2a879229b': 'Algorithms for Large, Sparse Network Alignment Problems',\n",
       " '7a0f2ff12004ac8aafa71864db6e73bbfcb93138': 'MDU-Net: Multi-scale Densely Connected U-Net for biomedical image segmentation',\n",
       " 'a29e149d55a1d956dd140477e1dbfc57ed7fbac4': 'Embedding Information Visualization within Visual Representation',\n",
       " 'c28b8b5e57bab5c8fa91654c7137c1dd21b3da18': 'Characterizing perceptual artifacts in compressed video streams',\n",
       " 'fcce89d4b212d5ba5ffd2c84498bad8275505d84': 'Twin Learning for Similarity and Clustering: A Unified Kernel Approach',\n",
       " 'fe63023bb7434e44b4e63dcf918c6a2f9aaba930': 'Context Adaptive Neural Network for Rapid Adaptation of Deep CNN Based Acoustic Models',\n",
       " 'f69885248d17bac47cc3469715fbb93aa7a4aabe': 'Predicting task from eye movements: On the importance of spatial distribution, dynamics, and image features',\n",
       " '162991b10e2633c237ba2d4220c040f09c1679c1': 'Meme extraction and tracing in crisis events',\n",
       " '944776444c62932a5dd703a082f7ce53c4153d61': 'Sentiment Classification with Deep Neural Networks',\n",
       " 'e9bc523f3f00397acae8190a30e4be7933d70345': 'Modified Wilkinson Power Dividers for Millimeter-Wave Integrated Circuits',\n",
       " '64982c97190bbd8b70beeed572392d0f308a93c2': 'A generative layout approach for rooted tree drawings',\n",
       " '7b901e88e8a4afcc4c60c52833820156525f4aed': 'Characterizing Audio Adversarial Examples Using Temporal Dependency',\n",
       " '6f54a7933235ced5684e3bff18f7e5dc40510018': 'The Missing Piece in Complex Analytics: Low Latency, Scalable Model Management and Serving with Velox',\n",
       " '567909486e73156699a19a2c5e6f44ded727941f': 'Improving the Gaussian Process Sparse Spectrum Approximation by Representing Uncertainty in Frequency Inputs',\n",
       " '99838f4d4259f67f903f8f762841499513b0b511': 'Appliance-specific power usage classification and disaggregation',\n",
       " '25e6ff6c5b3b03a3be4edaa55f5d8fa25ab920ee': 'Situation Awareness in Ambient Assisted Living for Smart Healthcare',\n",
       " 'e642d6014a5762128a28f85dae2228826e682974': 'A Compact Dual-Band Antenna Enabled by a Complementary Split-Ring Resonator-Loaded Metasurface',\n",
       " '68da903b2237cd500b98f152dd851189cad1b344': 'Detecting Ground Shadows in Outdoor Consumer Photographs',\n",
       " '0f10c6f57e7b640a2e89e94b5ca7d2b0d46d3925': 'Using WordNet-Based Context Vectors To Estimate The Semantic Relatedness Of Concepts',\n",
       " '3ba1dc8146e61d1635d511cc0357c63c85812e39': 'New technology trends in education: Seven years of forecasts and convergence',\n",
       " '05dba74b1ecf7e40b2a904e2d797768ef79832d3': 'Improved Statistical Machine Translation Using Monolingually-Derived Paraphrases',\n",
       " 'c52a6f0bfca069a0a083ddbc60a3b0d782067cbd': '3Ds MAX to FEM for building thermal distribution: A case study',\n",
       " '78878e95797e03d4968b09f40979585b6c56dac3': 'Designing a Smart Museum: When Cultural Heritage Joins IoT',\n",
       " '29378306c07a78cff615f1b2ddea4d5baca4a199': 'Polarization Reconfigurable Aperture-Fed Patch Antenna and Array',\n",
       " 'bd034b87414c58fc731c193fdd37f59b3db883fa': 'Survey on Different Phases of DigitalForensics Investigation Models',\n",
       " 'c2e6fde7addee6983243e487536d089e3b434810': 'Genetic algorithms and Machine Learning',\n",
       " 'c05ae45c262b270df1e99a32efa35036aae8d950': 'Predicting Facial Attributes in Video Using Temporal Coherence and Motion-Attention',\n",
       " 'c7514e9628d90692ccb355fece10d340e03780a2': 'Collaborative Learning for Deep Neural Networks',\n",
       " '725bba18d41a6dedeeef01fd8e306aa5bd2a3f6b': 'Toward Integrated Scene Text Reading',\n",
       " 'c8c6436c690da2e1db4d193e3be7422b24bce432': 'Vehicle Velocity Observer Design Using 6-D IMU and Multiple-Observer Approach',\n",
       " 'e567e0ae41ac2b757b1e069c6d345c6a7413d9c1': 'Using AI to Make Predictions on Stock Market',\n",
       " '4ceb63d79341ba3caa08bd1c6c141478d32a8244': 'Design of advanced digital heartbeat monitor using basic electronic components',\n",
       " '69b0e11ea4880ff3854a4e40bc24937e8adb975d': 'Customer Purchase Behavior Prediction from Payment Datasets',\n",
       " '3b1cd78a7b49711729f3c58ec6d69bac9bf6e51e': 'Anatomy of a web-scale resale market: a data mining approach',\n",
       " 'f6c1eda98cef66438dc89fd9a3d4fcc0a795418a': 'Planar broadband annular-ring antenna with circular polarization for RFID system',\n",
       " '45fdc73a239e9c6ea65e98c96f6a2d6dc35d6f72': 'Quaternion Convolutional Neural Networks for End-to-End Automatic Speech Recognition',\n",
       " '2011feb353fed560b0643dc9db6528317c643957': 'Democrats, republicans and starbucks afficionados: user classification in twitter',\n",
       " '1475d10a7b5d777fb411cdbb2740f574e32fd2f6': 'A Unified Bayesian Model of Scripts, Frames and Language',\n",
       " '9d15d72485388b8c4a50f84f81a36cbaf912b090': 'Cognitive Biases in Information Systems Research: a scientometric Analysis',\n",
       " '80093393acfaf79161a969d5180d0fd561ee853b': \"It's Different: Insights into home energy consumption in India\",\n",
       " 'c445e43dfac5aa734f2929944fcb5c68a319b0b6': 'IoT based control and automation of smart irrigation system: An automated irrigation system using sensors, GSM, Bluetooth and cloud technology',\n",
       " '1258db72eec4bbf02e29edf5bb0c300491a01242': 'MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification',\n",
       " '30753bc1f583ba7ce71dd0186e109813cd658616': 'Markov logic for machine reading',\n",
       " '866b1a9819f662ac2499be231965ded8e0323c7f': 'A statistical model-based voice activity detection',\n",
       " 'c4e76b318149a106563d53892dde801a37a637cc': 'Adaptive Estimation Approach for Parameter Identification of Photovoltaic Modules',\n",
       " 'c6879cc784625e795f0097f479d136dc489104ce': 'BlendCAC: A BLockchain-ENabled Decentralized Capability-based Access Control for IoTs',\n",
       " 'ba69c46c62b525941e91d0788d20b6ef91847cc4': 'Language Model Pre-training for Hierarchical Document Representations',\n",
       " 'e6be9c497285ece7f32b486c6cc72193b5a1fcb9': 'Anonymous Post-Quantum Cryptocash ? ( Full Version )',\n",
       " '3bd2086d011089b43409b9c772f0f7ec93d1bb9e': 'SDN docker: Enabling application auto-docking/undocking in edge switch',\n",
       " '1f10c64bf2069062811209a27cc4ff7b65fa02e9': 'Spectral Network Embedding: A Fast and Scalable Method via Sparsity',\n",
       " 'b59d6a8f4acaa63d773356fe320368b9f76a15cf': 'The Ontology Extraction & Maintenance Framework Text-To-Onto',\n",
       " '2ad961db4b9075a33f0725dd5c79074de993c5e4': 'Hop-by-Hop Message Authenticationand Source Privacy in WirelessSensor Networks',\n",
       " 'e25e514011e2f31a3961aba227ebab437209a6a9': 'A multi-level encoder for text summarization',\n",
       " 'c104626e93e0b8115b4673f63e10f69cdf97801c': 'Information extraction - a text mining approach',\n",
       " 'f22ae66b083183494f2ab0f5c7758fb03d69b05c': 'Towards large-scale twitter mining for drug-related adverse events',\n",
       " 'fa36e9da13f7a055ce95beb61035c72b6d89e2cb': 'A frequency-division MIMO FMCW radar system using delta-sigma-based transmitters',\n",
       " '7c080093943b2001224ac6608436c83d82a79a83': 'On Parallelizing the Multiprocessor Scheduling Problem',\n",
       " 'd2f8863857f6a26e917af5044189c02fff697e98': 'Nested LSTM : Modeling Taxonomy and Temporal Dynamics in Location-Based Social Network',\n",
       " 'd2be35d1221c29c3ffba998b91d68dfd7c5c65f0': 'Neural Networks for Multi-Word Expression Detection',\n",
       " '392b1573c9cdbb3ef0387963e82310f7be068a45': '0 User Modeling on Demographic Attributes in Big Mobile Social Networks',\n",
       " '498aad3cb4d2b8e8ff1e68a40be7dec27e8139f4': 'Modulation Technique for Single-Phase Transformerless Photovoltaic Inverters With Reactive Power Capability',\n",
       " '2747421989619d293c05b0b82a547009128ebadb': 'Transforming GIS Data into Functional Road Models for Large-Scale Traffic Simulation',\n",
       " '240cd5f1b47a68c9dcc04b3921e69093c9a55b02': 'Random Walks and Neural Network Language Models on Knowledge Bases',\n",
       " '14edc660cb7db680f2e471460a794f68ba03f295': 'Efficient Large-Scale Graph Processing on Hybrid CPU and GPU Systems',\n",
       " '86f6895a377dc5246f756fc0829a286d1ddf0e2d': 'On the Application of ISO 26262 in Control Design for Automated Vehicles',\n",
       " 'e5e0b31924d7947f45ebc56c61bb17a23e3f4732': 'Analyzing Thermal and Visual Clues of Deception for a Non-Contact Deception Detection Approach',\n",
       " 'b645f19ed52b4315a82bf3564b8db5ce230cd49e': 'Enemy of the State: A State-Aware Black-Box Web Vulnerability Scanner',\n",
       " '2a8bcf35e6b5b3910eea160b3a1fb3e6bcb3966e': 'Fast Sparse Gaussian Markov Random Fields Learning Based on Cholesky Factorization',\n",
       " '74c472360ec199703b5e460308bf5ed29e38f67b': 'NFC Loop Antenna in Conjunction With the Lower Section of a Metal Cover',\n",
       " '95a6d727526421dc176cfc831d4413ea3487e4f0': 'A New Low Cost Leaky Wave Coplanar Waveguide Continuous Transverse Stub Antenna Array Using Metamaterial-Based Phase Shifters for Beam Steering',\n",
       " '7ffd6b7c27558400bfe609161b158d6a34b62f62': 'Learning to Diagnose Cirrhosis with Liver Capsule Guided Ultrasound Image Classification',\n",
       " '5437b5cfb0f8eda908559a16e7ed7d7b64be641b': 'Face recognition by stepwise nonparametric margin maximum criterion',\n",
       " '9d4a4e6f8c14d5d2d0e96ab4893abb8ff706253a': 'Cloud Computing Adoption: A Short Review of Issues and Challenges',\n",
       " '490ec36471275164d104f155ef7ebbc70c5f7eea': 'Finding Rough Set Reducts with Ant Colony Optimization',\n",
       " '3f64d4fab50db41c0145759b7410fb4c8b977452': 'Operational Flexibility and Financial Hedging: Complements or Substitutes?',\n",
       " '8407184b67114cbdddfec1bded6bf5a35eb8cb82': 'A deep learning approach to document image quality assessment',\n",
       " 'a59faa9c1a158316fdbc71f4c7152a5fee405b8a': 'RTIC-C: A Big Data System for Massive Traffic Information Mining',\n",
       " '5a4b138603821d758d5e51c8f8fb0ff1696901d7': 'PRISM-games: A Model Checker for Stochastic Multi-Player Games',\n",
       " '60f9c8ba8e00e9e03e8bb80640fcfa3889a36327': 'Exploring romantic relationships on social networking sites using the self-expansion model',\n",
       " 'b4333da9831681151015f61b46aff3843eeb5847': 'Mapping underwater ship hulls using a model-assisted bundle adjustment framework',\n",
       " '1bc5ea91875143e0526ac587b9c4aa6c64ceec98': 'High-fidelity simulation for evaluating robotic vision performance',\n",
       " '824ca251d71258bdac66af55966b8ce39cf3042f': 'Data Management Challenges in Cloud Computing Infrastructures',\n",
       " '425a2cb778130d0fd57d00eaf3cbe347c4ee7641': 'Real-time Semi-Global Matching on the CPU',\n",
       " '28115e594c25c99a253557ae96290a9e0d60d3e3': 'On the Role of Lexical and World Knowledge in RTE3',\n",
       " '7faf5d0915d604821d7ad2ade77f81a751309eee': 'A Single-Stage Single-Switch Soft-Switching Power-Factor-Correction LED Driver',\n",
       " '1b1aed7e508dce5e1c2d847ee69b8979e3be69d2': 'External Knowledge and Query Strategies in Active Learning: a Study in Clinical Information Extraction',\n",
       " '566e31a4ad34b39b6d8a1953d77d9f74da135a2a': 'Automatic road network extraction from UAV image in mountain area',\n",
       " '063c6ae786c34d3722c6d9060df6339e246bbc3b': 'Texture Synthesis Using Convolutional Neural Networks',\n",
       " '63b37fc9e8dcfdb4338afd970dbbc11c5dd70ba4': 'Integrating learning and reasoning services for explainable information fusion',\n",
       " '0ce7927d613e66a2062516e9e85e1b9a36d54029': 'The leadership of emergence : A complex systems leadership theory of emergence at successive organizational levels',\n",
       " '35414f51bec4795a0f417aa1fdbb35322d80c9b7': 'A performance comparison of dickson and fibonacci charge pumps',\n",
       " '0b40af1ad2b9781fa14e999db2d7d3270b6d2862': 'Data Clustering: 50 Years Beyond K-means',\n",
       " 'c5550a3478b24f7f1904c6beb8d9d1cf40d30b21': 'Higher mode SIW excitation technology and its array application',\n",
       " '26832c916189cf7bed032d48eb0b6329dc37fcfd': 'Performance Investigation of Feature Selection Methods',\n",
       " '616b7093cfe6ec679f25d63f62c16e937227258f': 'Bayesian Multi-object Tracking Using Motion Context from Multiple Objects',\n",
       " '1ba7a9c0e658a0d98253f999bf43c2e98c07f4e0': 'Real-time visual tracking using compressive sensing',\n",
       " 'c9dee006f3b6b2b30e422ad2fd8abb86c751376d': 'Hidradenitis Suppurativa in Children Treated with Finasteride-A Case Series.',\n",
       " 'e2f78d2f75a807b89a13115a206da4661361fa71': 'Trip Outfits Advisor: Location-Oriented Clothing Recommendation',\n",
       " '802f3f316f87c6bc675cc55a2a1bf4bb0f12dd1e': 'A Comprehensive Survey of Neighborhood-Based Recommendation Methods',\n",
       " '65a858ca95dcfa032e812a7f1fc7ee5bdac88f5b': 'Using Pre-Trained Models for Fine-Grained Image Classification in Fashion Field',\n",
       " '101f6808a0244f725979e6e651bbef2f86410c1d': 'Multi-user interaction using handheld projectors',\n",
       " 'a6e4beb28b345fce7470da122b4e45e2cd0dcd12': 'A Time-Restricted Self-Attention Layer for ASR',\n",
       " '46cf276bdb27dd12c4c36f35855041e79e4ec981': 'Supporting Complex Search Tasks',\n",
       " '661a03482f0532b7355188059b2263091ade1bae': 'Design and thermal analysis on high torque low speed fractional-slot concentrated windings in-wheel traction motor',\n",
       " 'abe2c411f95a5057652dde7b2611331c8a3b3f14': 'Traffic Lights with Auction-Based Controllers: Algorithms and Real-World Data',\n",
       " '484550334c626a749bada57e51a0db4a29085f87': 'Global outer-urban navigation with OpenStreetMap',\n",
       " '6bdbd2de4988fab315bc7e6ec0cd14ed3ae8c018': 'RBFOpt: an open-source library for black-box optimization with costly function evaluations',\n",
       " 'b3739e7262b222a9dbf27fc21c34d57becda5051': 'Design of a UHF RFID metal tag for long reading range using a cavity structure',\n",
       " 'ac24b99a5db633ce45f053287ca806157fff9663': 'Enrollment Prediction through Data Mining',\n",
       " 'a5cb9e716b2e3c54518ed0b05a4cc0c4256e53c2': 'On continuous deployment maturity in customer projects',\n",
       " '665d50db7006e2595c7d55687fed7329192bd71e': 'A Biomedical Information Extraction Primer for NLP Researchers',\n",
       " '2d2f14551b8b7163558e97902e7b456b0ba0a8d0': 'Visual Language Modeling on CNN Image Representations',\n",
       " 'd4820d5b290063cd13305fad39cc281705738e3e': '122 GHz radar sensor based on a monostatic SiGe-BiCMOS IC with an on-chip antenna',\n",
       " '7b49bd891f632ca6e86e5ccccdc3761ceb3fd277': 'Issues,Challenges and Tools of Clustering Algorithms',\n",
       " 'd2c70073316ee264e393411cd49182a9be78deb4': 'Dimensional inconsistencies in code and ROS messages: A study of 5.9M lines of code',\n",
       " '3131776d3b838e3a22ed9ccd7b7b92a41410df28': 'Risk Taking Under the Influence: A Fuzzy-Trace Theory of Emotion in Adolescence.',\n",
       " '8d37da5a9f05ee71d06cc6afcc1b14f8ff83147f': 'Adiabatic charging of capacitors by Switched Capacitor Converters with multiple target voltages',\n",
       " '222d908535d67563709bb72de7aed4739133ee3e': 'Question Passage Question & Passage Encoding Question-Passage Matching Passage Self-Matching Word Character Answer Prediction',\n",
       " 'a55fe9d6b59b553b460aa8d7974fc5cd4dee2187': 'Learning to Accept New Classes without Training',\n",
       " '5a7957cb601a02bb7f4f3f65fcdb18df093ae4a8': 'Ontology Learning and Population: Bridging the Gap between Text and Knowledge',\n",
       " '8485904eddf45f3d221832600d8067d9998321e7': 'Enrich machine-to-machine data with semantic web technologies for cross-domain applications',\n",
       " '47fc2c3e290e05fe9d41441b0270f88f6a8543d8': 'Enabling Technologies for Smart City Services and Applications',\n",
       " '4312a1945d6eaa6429fe89a0dec5583f7855e0ab': 'Grid-based mapping and tracking in dynamic environments using a uniform evidential environment representation',\n",
       " '52412f3274acaa16a2d76d11d5e8eab643c0e63b': 'HF outphasing transmitter using class-E power amplifiers',\n",
       " '7b85c1bf03097a77cb1d36f6f6338d95a6aff428': 'Spatio-temporal avalanche forecasting with Support Vector Machines',\n",
       " '6197dbd691037a412b67df688541df7c9ae87c0d': 'Posterior distribution analysis for Bayesian inference in neural networks',\n",
       " 'bbe8c5e53ca6e4db115afeaaad2be268f039f10d': 'DeepSim: deep learning code functional similarity',\n",
       " 'cab59587e4b2dd441198cd37e0038cace6b6531e': 'A Two-Step Method for Clustering Mixed Categroical and Numeric Data',\n",
       " '1f576e4cfe7c5426e383cd06608411e81b509feb': 'Record-Aware Two-Level Compression for Big Textual Data Analysis Acceleration',\n",
       " '833c15a257dde1ae4c0d815d345278e121e3fe72': 'Spatial cloaking for anonymous location-based services in mobile peer-to-peer environments',\n",
       " '6f3b604e98cf705c73c8685aae6c0e911a22a26b': 'Live acquisition of main memory data from Android smartphones and smartwatches',\n",
       " '7a26676a07796e2b910dd556c6691d264a7e774c': 'Data Provenance in the Internet of Things',\n",
       " 'da8fcbcb25d73558752e1d2b1f34c0e5df07ab65': 'A Ka-band waveguide-based traveling-wave spatial power divider/combiner',\n",
       " '80d9843338d07778dac65157a2c881892a28b821': 'Improving wikipedia-based place name disambiguation in short texts using structured data from DBpedia',\n",
       " 'de1cc9b99f95041a1da383b6920cbf2907e0981a': 'Generative Deep Deconvolutional Learning',\n",
       " 'b4435aedcf2def5ada518e515e46c3b77d379692': 'DeepTransport: Prediction and Simulation of Human Mobility and Transportation Mode at a Citywide Level',\n",
       " '90bd67f0a4b2a8ab5c75e1b7619e17a508b56c05': 'Localization and map-building of mobile robot based on RFID sensor fusion system',\n",
       " 'c46d80f83813fba0e8363a0ab36a19fba062540e': 'Learning Actionable Representations with Goal-Conditioned Policies',\n",
       " '9c4d7805f3724fb7b866950eaa0505952ff25fd0': 'Exclusivity-Consistency Regularized Multi-view Subspace Clustering',\n",
       " '2a88541448be2eb1b953ac2c0c54da240b47dd8a': 'Discrete Graph Hashing',\n",
       " '5cd7b83eb30797aa8bf49b4b7a3f6a433aca4a75': 'SMOTE-GPU: Big Data preprocessing on commodity hardware for imbalanced classification',\n",
       " '27efc45254159de1a18554ba8cb603ee70a1f266': 'Stability and control of a quadrocopter despite the complete loss of one, two, or three propellers',\n",
       " '11c7dcdd20a2fa500162c3f1477d20bb18bfa15a': 'Modeling Coverage for Neural Machine Translation',\n",
       " '18095a530b532a70f3b615fef2f59e6fdacb2d84': 'Deep Structured Scene Parsing by Learning with Image Descriptions',\n",
       " '775e829da35e9708757dbae91e99e4dd604d2204': 'A Systematic Study of Online Class Imbalance Learning With Concept Drift',\n",
       " '699896507c186df548f7060be5317984bffd756a': 'Rumor Detection and Classification for Twitter Data',\n",
       " 'e27aff8fffd4ad43275d355202a135f848be0e76': 'Development and Validation of the Transgender Attitudes and Beliefs Scale.',\n",
       " 'eafe7346c8fbfd1454a0eac55d2b442f564332ac': 'A 0.6-V 800-MHz All-Digital Phase-Locked Loop With a Digital Supply Regulator',\n",
       " '4249d4c294e5edca091597b3b4f30c10201c1f9a': 'Morphological Computation – A Potential Solution for the Control Problem in Soft Robotics',\n",
       " '0e5cde86bb767fcf4d0dd07f140bcf5292b4fd7b': 'The Browsemaps: Collaborative Filtering at LinkedIn',\n",
       " '1f081ad6fa8a623e9d3e0c94274019d24db4659b': 'EvoloPy: An Open-source Nature-inspired Optimization Framework in Python',\n",
       " '1e7021eb8e92066b972452a0ce5a54529ca4ba5a': 'Recurrence networks — a novel paradigm for nonlinear time series analysis',\n",
       " 'd0f54ee5eb73065a2d8c790bbfe727a34541bbea': 'Software.zhishi.schema: A Software Programming Taxonomy Derived from Stackoverflow',\n",
       " '849534526c074568abbfd6daf45c3532b0a18133': 'Feature Extraction and Image Processing',\n",
       " '3ad8328b066bedca77c858a399a77521563d5c99': 'Root Exploit Detection and Features Optimization: Mobile Device and Blockchain Based Medical Data Management',\n",
       " '120f80b2e2b2ce2fefdf0de644f5ee84c8647bb2': 'Swarm: Hyper Awareness, Micro Coordination, and Smart Convergence through Mobile Group Text Messaging',\n",
       " '11f7d9b2017abd46c756df533618d2c4326fd3cb': 'Structuring Content in the Façade Interactive Drama Architecture',\n",
       " '853bd61bc48a431b9b1c7cab10c603830c488e39': 'Learning Face Representation from Scratch',\n",
       " '068a88330c93a41058d6e04e576d7e1a21dc6ee7': 'Convex Color Image Segmentation with Optimal Transport Distances',\n",
       " 'e8cda2c754670850ec722799640c6cb42dfb8199': '4D Generic Video Object Proposals',\n",
       " '108eb7d5365e54e788886083049dca7e0b347f53': 'An environmental energy harvesting framework for sensor networks',\n",
       " 'c1af0af74d9227591f4464d09d3d4c077e3f53df': 'Mobile Location Prediction in Spatio-Temporal Context',\n",
       " '45b4f8dade4750a90e7081c20b5bde864e6be11e': 'Generalized Equalization Model for Image Enhancement',\n",
       " '0210b3fe6f7173c86936b5dd9261bc0be0c45652': 'Future Perspectives on Next Generation e-Sports Infrastructure and Exploring Their Benefits',\n",
       " '4844e73b6f94f81455d1303cdb9621f1cf394e96': 'Point Pair Feature Based Object Detection for Random Bin Picking',\n",
       " '9af1f7b6d79e6ce314d1842a8d32402328fcaca7': 'Variance Reduction for Faster Non-Convex Optimization',\n",
       " '542215d9c86220acd32dbe07a4fcd882c3912952': 'The Interrelationships Among Attachment Style , Personality Traits , Interpersonal Competency , and Facebook Use',\n",
       " '3388d516fc26536423b03e1d93a3b62358a6a35f': 'The role of founders in building online groups',\n",
       " '78c00439aac79217675b00810386ff3bd89c86cc': 'Inducing decision trees with an ant colony optimization algorithm',\n",
       " 'da6629447f61ef8b07ce0698b490e3fe7210c1e3': 'A Neural Network Approach to Missing Marker Reconstruction',\n",
       " '6374d969a99cf8da935377750f09f61875c81173': 'Synthetic Social Media Data Generation',\n",
       " 'a77e9f0bd205a7733431a6d1028f09f57f9f73b0': 'Multimodal feature fusion for CNN-based gait recognition: an empirical comparison',\n",
       " '38526532f66d5bcd7b47cea0ed9642b9b232d50f': 'Data fusion - Resolving Data Conflicts for Integration',\n",
       " '0588053b2cde6414e542c656023ade147397f597': 'Improve Chinese Word Embeddings by Exploiting Internal Structure',\n",
       " '4cbd3754645c60ad0c6f792c503be14b99ecd1db': 'A novel evolutionary data mining algorithm with applications to churn prediction',\n",
       " '979d5c2924eeec9fc98ec9f3e4374ee0c2ddbd24': 'A Survey on Online Judge Systems and Their Applications',\n",
       " '573dc953636d90aeb4981e880c4bc0bad7f6cf64': 'Representation learning with complete semantic description of knowledge graphs',\n",
       " '9c824df69c7c6fc350d2981bed00b6df6ffb33ad': 'CFD Analysis of Convective Heat Transfer Coefficient on External Surfaces of Buildings',\n",
       " '756bf25afe8e6ca92ba1795bcb5fb4e93d8ed6b2': 'AMP-Inspired Deep Networks for Sparse Linear Inverse Problems',\n",
       " 'b9220fdc68eaa6dce0c5944a769e3e531839055d': 'Medical Diagnosis for Liver Cancer using Classification Techniques',\n",
       " '71d4e703201c98c9c9b44079600e289f234f09f2': 'Enhancing Authorship Attribution By Utilizing Syntax Tree Profiles',\n",
       " 'b42c7b6e9fc032cf70cdcbdab05ce898be0a3a6e': 'An Active Suspension System for a Planetary Rover',\n",
       " '9cf96ff9e1a0521df025ffa74ca6ff3acbea2d36': 'Indoor positioning of mobile devices with agile iBeacon deployment',\n",
       " 'a46d0a41ef1b28ec06a0a849c4c56bbe725b6964': 'Benign Envy , Social Media , and Culture Completed Research Paper',\n",
       " '645802c55d809d5bf27f391837078689f7bf2333': 'Can Google nowcast the market trend of Iranian mobile games?',\n",
       " '706bfa16c853850fdd58f191e4e5befaa6952a00': 'Signal processing techniques for improving angular resolution performance in homodyne FMCW radar',\n",
       " 'f4195462f27158c4afd86ca364347dacfd228bdd': 'Towards an Information Security Framework for the Automotive Domain',\n",
       " 'a73039275a77df6789d422265496de1ab8f0899a': 'An Optimized Floyd Algorithm for the Shortest Path Problem',\n",
       " '86095343140a2bb7d3966215fa269981eb2f19b9': 'A Study of Automatic Speech Recognition in Noisy Classroom Environments for Automated Dialog Analysis',\n",
       " '3159c9423862b22c6326801ad4353ae2cfe30d32': 'Cross-Domain Traffic Scene Understanding: A Dense Correspondence-Based Transfer Learning Approach',\n",
       " '703099734f70f0cc00773ee54b4385f89afa7afe': 'Weakly Supervised Semantic Image Segmentation with Self-correcting Networks',\n",
       " 'b6981a7736e3ae0c8b90debc0e9c2fde9b66b502': 'Personality Consistency in Dogs: A Meta-Analysis',\n",
       " 'c3dafc91e315d27933b6420be2a77f5639881213': 'Empirical Study of Unsupervised Chinese Word Segmentation Methods for SMT on Large-scale Corpora',\n",
       " '41b807511a65feac98485427597f9b45c892595b': 'Learning Semantic Similarity',\n",
       " '6b6afc9557dc0670bf2792bde4c4389ac52c707f': 'What Action Causes This? Towards Naive Physical Action-Effect Prediction',\n",
       " '505942c5f9b5779bda2859e22e9ed0b1c0c7b54a': 'Towards 3D Face Recognition in the Real: A Registration-Free Approach Using Fine-Grained Matching of 3D Keypoint Descriptors',\n",
       " '6745710034803993433dd42001a860d70c99f75c': 'Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation',\n",
       " '7c8a65bb8e328d8c5bbd352ed4baadc82144dd8b': 'Thoughts on Vehicular Ad Hoc Networks ( VANETs ) in the Real World Traffic Scenarios',\n",
       " '81b14341e3e063d819d032b6ce0bc0be0917c867': 'Autonomous Vehicle Navigation by Building 3 D Map and by Detecting Human Trajectory using LIDAR',\n",
       " '86c925d1a737fe3d29fa5388ede48cf87700cb89': 'A Fully Automatic Crossword Generator',\n",
       " '14ee77995be77780bdba07fa7f1613fb5ad09409': 'Statistical Syntax-Directed Translation with Extended Domain of Locality',\n",
       " 'd4cb7ecfe297b34bf8763ed65dc8ffd2ca806963': 'Plane Detection in Point Cloud Data',\n",
       " 'c9dd5ae24520d8cdddfdf8ef6d5f925445e310d9': 'Improving Naive Bayes Classifier Using Conditional Probabilities',\n",
       " '54356ff0960100e27cf17ff682825bba2662e90c': 'The Research Object Suite of Ontologies: Sharing and Exchanging Research Data and Methods on the Open Web',\n",
       " 'b4c80fc4b140eb08a717a446824cacb77f319166': 'Time-Agnostic Prediction: Predicting Predictable Video Frames',\n",
       " '19d9d5e14d4d3f3e4ef66e00cdbc11ff6a93e9d5': 'GHT: a geographic hash table for data-centric storage',\n",
       " 'b095b4625733f8521439e54db09bed2b9970a808': 'A Study on Data Mining Frameworks in Cyber Security',\n",
       " 'eecbbb0ba7b513a2fe1e7a0131213e5a94b1868a': 'Toward an IT governance maturity self-assessment model using EFQM and CobiT',\n",
       " '28ca521c805fae5ee45417c3e53421d2250c24cb': 'ISUALIZING A F RAMEWORK FOR T ANGIBILITY IN M ULTIMEDIA L EARNING FOR P RESCHOOLERS',\n",
       " 'eb240f463fa57740986a92ee744c3ad75f8228e9': 'Intrusion Detection : Support Vector Machines and Neural Networks',\n",
       " '64abfbc0b66ec8579a92a33dca19e92bc2095ea2': 'Neural correlates of heart rate variability during emotion',\n",
       " '548a49c717dc1ce0263ceff445b5609361c26cde': 'Decision trees for predicting the academic success of students',\n",
       " '763490c13a138523b98e7646ef0fa81d6b5e5a22': 'Normalizing SMS: are Two Metaphors Better than One ?',\n",
       " 'c702107a56163dfd27526b7034be7bf946b03a85': 'Hierarchical Complementary Attention Network for Predicting Stock Price Movements with News',\n",
       " '28f3343d2419147213e7580934cfc4e0cb88b088': 'Creativity in Higher Education : The use of Creative Cognition in Studying Book Section',\n",
       " '4d4dc6568e24f283f3c5bcb88f280908a35da0e3': 'Minimum Description Length Induction, Bayesianism, and Kolmogorov Complexity',\n",
       " '0bbdd4905f23994e0b5a0d91fc332b2af336f1e8': 'Recipient Revocable Identity-Based Broadcast Encryption: How to Revoke Some Recipients in IBBE without Knowledge of the Plaintext',\n",
       " '6d3857fe3e62a400768fb6f51366add5f9ea4889': 'When 25 Cents is Too Much: An Experiment on Willingness-To-Sell and Willingness-To-Protect Personal Information',\n",
       " 'c24fc81dcec236527652eab35154ac0fd7a40929': 'Fruit and Vegetable Identification Using Machine Learning',\n",
       " '031a0f18d46b8e006eb4262233f7734fe4505c21': 'AntNet: Distributed Stigmergetic Control for Communications Networks',\n",
       " 'b769247a568e5f026b5b565781965801ff31fa54': 'Detecting Webshell Based on Random Forest with FastText',\n",
       " 'dff7ede8b9bcb70a33000335e3abd31b77c567bb': 'RELATIONSHIP OF PERSONALITY TRAITS AND COUNTERPRODUCTIVE WORK BEHAVIORS : THE MEDIATING EFFECTS OF JOB SATISFACTION',\n",
       " 'e0a8b67880070624ef8787a08bbbd5aa178d65ac': 'Incorporating Syntactic Uncertainty in Neural Machine Translation with a Forest-to-Sequence Model',\n",
       " '0f2c82218b1858e414df5d50171e97694c60e542': 'Entity-Aware Language Model as an Unsupervised Reranker',\n",
       " '64334ac9dfb59d68380784e3b1ad197511850921': 'A Survey on Different File System Approach',\n",
       " 'd2662c5f2b4aca17f7e9ce70074d2eb829b4f38d': 'Entity Set Expansion via Knowledge Graphs',\n",
       " '5aadc803228b70c3cc6b31e332770d47d7fb1e6e': 'Joint Representation Learning of Cross-lingual Words and Entities via Attentive Distant Supervision',\n",
       " 'aaeb99920069fad63f5dbbf37f8c4ebe2b180e95': 'ECDH Key-Extraction via Low-Bandwidth Electromagnetic Attacks on PCs',\n",
       " '195a5a538eacb89dcd4cf3fe2c0600ea61dbd15e': 'An evaluation of dynamic adaptive streaming over HTTP in vehicular environments',\n",
       " '49aa693db2d6d6a9486a9c9b6e1ac653fe12ab97': 'Not All Dialogues are Created Equal: Instance Weighting for Neural Conversational Models',\n",
       " 'cf9611e42a6060da7320b0fe4d693472bc830d91': 'Design, fabrication and testing of smart lighting system',\n",
       " '2dd5f1d69e0e8a95a10f3f07f2c0c7fa172994b3': '20 Machine Analysis of Facial Expressions',\n",
       " 'fe2533594e01b374ee12f8d450069b21b572a675': 'Active Sampler: Light-weight Accelerator for Complex Data Analytics at Scale',\n",
       " '375dc2acc3f54cb0d386dbc2d969366e32d2839f': 'RDF in the clouds: a survey',\n",
       " '59aa88a5648e602d4e9aa4e9f12dedd112126bf7': 'Extending the CSG Tree - Warping, Blending and Boolean Operations in an Implicit Surface Modeling System',\n",
       " '40c6b953b5c04b3df4164cd487c4bc00cf0e487d': 'A Microfluidically Reconfigurable Dual-Band Slot Antenna With a Frequency Coverage Ratio of 3:1',\n",
       " '26b90ccf7541fd2cd4235118493e1e49d358c351': 'StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning',\n",
       " 'c7fe851e1acc8a974697fb74d913736a3a849003': 'Real-Time Online Action Detection Forests Using Spatio-Temporal Contexts',\n",
       " 'f25ba2cbb101685727b646cbdbcddca4ecd465b4': \"Earth Mover's Distance Pooling over Siamese LSTMs for Automatic Short Answer Grading\",\n",
       " 'c5f0a4418c1025ea8a21f0f997dfe7e4ce176594': 'Electrical Machines for High-Speed Applications: Design Considerations and Tradeoffs',\n",
       " '80a720a4d5a6d07be98abe3caade59b36691e01c': 'InferSpark: Statistical Inference at Scale',\n",
       " 'd76ceda01173508da98f2989844e75fd40859c27': 'Buried Object Detection from B-Scan Ground Penetrating Radar Data Using Faster-RCNN',\n",
       " '1f1da6d6077b0a6ad6ab724741fb36f64e7c6cc9': 'Support Vector Machine in Prediction of Building Energy Demand Using Pseudo Dynamic Approach',\n",
       " '6ec004e4c1171c4c4858eec7c927f567684b80bc': 'The POSTECH face database (PF07) and performance evaluation',\n",
       " '94d8056eb0f5c32efcf1498d9c6a9e396f239c06': 'Cloud-Based NoSQL Data Migration',\n",
       " 'fcd16bfffd4131b2f41f90b4c832415aea55038b': 'Auto-Conditioned Recurrent Networks for Extended Complex Human Motion Synthesis',\n",
       " '4e3fb831a5d2961f2829da4477856b6a9b7f8930': 'SynopSys: large graph analytics in the SAP HANA database through summarization',\n",
       " '863d0b86e5d2877e61f5cce971632abe2a1212cd': 'Device to device interaction analysis in IoT based Smart Traffic Management System: An experimental approach',\n",
       " 'e0ec382c51a043b1996a74af96d10cda33fdf6eb': 'State of the art of virtual reality technology',\n",
       " '95bb0ee471480da79e41ae196bb4da02abe52a27': 'Computer-aided detection of breast cancer on mammograms: A swarm intelligence optimized wavelet neural network approach',\n",
       " 'f8ebf1520b3f26bf822930f532d6fdca5f5b1a4a': 'Analog CMOS-based resistive processing unit for deep neural network training',\n",
       " 'a7633785229e2db5ef7d4dd8b7e63b017c5eb3d3': 'nTorrent: Peer-to-Peer File Sharing in Named Data Networking',\n",
       " '98b8133b18ea2a57191d85ac58cb40cb545835cb': 'Mostly-Optimistic Concurrency Control for Highly Contended Dynamic Workloads on a Thousand Cores',\n",
       " '901c7f0cbf7701c66921c918e553d2ecfbb4df34': 'Difficulty Rating of Sudoku Puzzles: An Overview and Evaluation',\n",
       " 'cadf9ffb55a9286bd98622a3cef1deccbc4c148e': 'An Overview of Data Mining Techniques Applied for Heart Disease Diagnosis and Prediction',\n",
       " '2561aa198e3a7a0b507122f543acafa80481e1b9': 'Policy Search in Continuous Action Domains: an Overview',\n",
       " '2170636d5d31eb461618b5da10f4473c67e74e73': 'Person Re-identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function',\n",
       " '4b5d2c4bdef08a20ee6bea555de64425f003e46b': 'Riesz Fractional Based Model for Enhancing License Plate Detection and Recognition',\n",
       " '29da485e2e8c78d9646d1efb25986e5753269354': 'Propagating Uncertainty through the tanh Function with Application to Reservoir Computing',\n",
       " '5b48f705f73ccdf32c314a7ad932ef7d51b84597': 'Massive MIMO Has Unlimited Capacity',\n",
       " 'b7771386689647fb7d5ff8e533f6a29169846364': 'An adaptive threshold deep learning method for fire and smoke detection',\n",
       " '51558e488a7a3c0d24eb71604ef7e110f4e5b09f': 'Complex hole-filling algorithm for 3D models',\n",
       " 'a2081b75867cb8c461530a0e71d4be1b4afe78a2': 'A Review of Machine Learning Applied to Medical Time Series',\n",
       " '18897a587c91e3b177de66b19fe00708ee868df8': 'Internet of Things: a New Application for Intelligent Traffic Monitoring System',\n",
       " '76477dc79100674de60478d1dea4c6cff301e01b': 'Battery Charger for Electric Vehicle Traction Battery Switch Station',\n",
       " '7a59595f1859b72761b34892be8b6dc43f71d01e': 'Contact force based compliance control for a trotting quadruped robot',\n",
       " '2b26645a12e3a11f53ae4d31798f1009376962db': 'Convergence of Online Gradient Method for Pi-sigma Neural Networks with Inner-penalty Terms',\n",
       " '40cdd0de8d05c496ca6658d3d7c45bea028de6be': 'Simulation and analysis of a single phase AC-DC boost PFC converter with a passive snubber for power quality improvement',\n",
       " 'aa50a595878ee67d0eb0a9b0f17a6132c0552434': 'Quantitative Evaluation of Style Transfer',\n",
       " '5ded1e19eb785dc0707c94f62f552798db9dd2ef': 'Conditional proxy re-encryption secure against chosen-ciphertext attack',\n",
       " '8e85c7ad6cd0986225e63dc1b4264b3e084b3f9b': 'Digital image forensics: a booklet for beginners',\n",
       " 'c0a799c024aaef60694f37366f0518bbcd37a17f': 'A survey on expert finding techniques',\n",
       " 'c6d97883da23b07a2031ff65076670eeb11b59d6': 'Cosine Siamese Models for Stance Detection',\n",
       " '4cacc809d92fd0c84d5b4b63d790dece998e0e6d': 'Inverse Kinematic Infrared Optical Finger Tracking',\n",
       " '82861adefbf357499bf55e7c95fe9a933ad10150': 'Solving molecular distance geometry problems in OpenCL',\n",
       " '77b47c27f82bf9998fa49a5c4c670d96d43164d0': 'Low RCS microstrip patch array: EM design and performance analysis',\n",
       " 'd145274d03a6374c77de64fb70602ddef393e4d9': 'Face Recognition Using Gabor Filter And Convolutional Neural Network',\n",
       " '3dcc819e642beafccd3f6bffa434767d1a818eb1': 'A Geometric Method for Detecting Semantic Coercion',\n",
       " '29373c5bb0d60320844905916aec09d64c24cd79': 'Outlier Analysis',\n",
       " '7e475e3b326d4fc9f2e908a3222796031d94aec0': 'Raziel: Private and Verifiable Smart Contracts on Blockchains',\n",
       " '87bb764248d07916e53abdccb0cb9d2e51c2f6b8': 'A Survey of Artificial Intelligence for Cognitive Radios',\n",
       " '83591848730f3fc7f208d4646ed4338c237bd161': 'Providing Context-Aware Security for IoT Environments Through Context Sharing Feature',\n",
       " '1c0e8c3fb143eb5eb5af3026eae7257255fcf814': 'Weakly Supervised Deep Detection Networks',\n",
       " '39d428fd8c6b73ed070921a856f03c2c5b5377ba': 'Explorer Merlin : An Open Source Neural Network Speech Synthesis System',\n",
       " '9557be29d86add8994b7df670c15eaa872995a10': 'Effectiveness of virtual reality exposure therapy for active duty soldiers in a military mental health clinic.',\n",
       " 'cae23343d2efddca3592b08a521a896af5098248': 'Recurrent World Models Facilitate Policy Evolution',\n",
       " 'ed19aa4eb626c3fc5a64b858bae5f2cdd71e1193': 'Classification of human activity by using a Stacked Autoencoder',\n",
       " 'c64c8f5f2803fbecd670b303387aa7074dd94997': 'Potentially guided bidirectionalized RRT* for fast optimal path planning in cluttered environments',\n",
       " '7ff20fc82c95eda6645ff2570279251bb8435ec5': 'Two types of dopamine neuron distinctly convey positive and negative motivational signals',\n",
       " '73f38c530a041f81d48cadd067246af448a6a24e': 'Embodiment of abstract concepts: good and bad in right- and left-handers.',\n",
       " 'c715db5c3ac2823ed5fa6c1c152b66921d5dcb78': 'Piecewise linear spine for speed-energy efficiency trade-off in quadruped robots',\n",
       " '45ab86bb5da05c4d3b18b4bcf9020b4a12693a4e': 'Harnessing Automated Test Case Generators for GUI Testing in Industry',\n",
       " '8e35888c532802145288d1b218ac8a684e3a6d0a': 'Foreground Segmentation for Anomaly Detection in Surveillance Videos Using Deep Residual Networks',\n",
       " 'a2707a57828b95443a7f3ef8153576299a2ad707': 'IoT based autonomous percipient irrigation system using raspberry Pi',\n",
       " '23fa7b866a1b1fee7bb71c8b5a9235cca7120bbc': 'Analyzing inter-application communication in Android',\n",
       " '8d0a7d58dd1a65eacac5a4b9b5d0726d9738057c': 'Semi-automated map creation for fast deployment of AGV fleets in modern logistics',\n",
       " '960c2f5d1b5a34dd5c3cbebc29edeffcee42f282': 'A Strategy for Ranking Optimization Methods using Multiple Criteria',\n",
       " '3f191a5bd42f23ff0201f30b1b70723d87ebf78a': 'Synthesizing open worlds with constraints using locally annealed reversible jump MCMC',\n",
       " '805d1a86ca4e7a3ccf6060d7256cd81654b6f6cf': 'Exploiting the Reactive Power in Deep Neural Models for Non-Intrusive Load Monitoring',\n",
       " '64da1980714cfc130632c5b92b9d98c2f6763de6': 'On rectified linear units for speech processing',\n",
       " '24b8b9dccfb67641c78d28ee6b56815191f34d87': 'Flexible and Fine-Grained Attribute-Based Data Storage in Cloud Computing',\n",
       " '521402f4cad16d19c6a7dd43be87569471172602': 'Evaluating geo-social influence in location-based social networks',\n",
       " '92a0cf2085013da3fe1fea2090d1bbabcabbf5be': 'Hierarchical Text Generation and Planning for Strategic Dialogue',\n",
       " 'c8dfa1ad91f484702e0d5d80e63bbae071214142': 'Miniaturized Circularly Polarized Patch Antenna With Low Back Radiation for GPS Satellite Communications',\n",
       " 'fbb4138d94863c8fc27aae824eaec9af2e971f22': 'A 7.6 mW, 214-fs RMS jitter 10-GHz phase-locked loop for 40-Gb/s serial link transmitter based on two-stage ring oscillator in 65-nm CMOS',\n",
       " '45e4ed3f616b6177beff78721aaae0a61506be05': 'Direct marketing decision support through predictive customer response modeling',\n",
       " '350db81caaa9c8dc1572f97d90bb8e13b0100bbb': 'An Energy-Efficient Middleware for Computation Offloading in Real-Time Embedded Systems',\n",
       " '2b234385356cb10d448908cef49584bece15d94b': 'A Convolutional Neural Network Hand Tracker',\n",
       " 'ddaaf26fe78fa533bfa54e3835844168d688f164': 'One-DOF Superimposed Rigid Origami with Multiple States',\n",
       " '54745a3e51fdeeef213bcdfd1091e98a8f06bdd7': 'Musical training as an alternative and effective method for neuro-education and neuro-rehabilitation',\n",
       " 'f5befd1da64812a034bbdd0813f795e4e565ebe1': 'TriggerSync: A time synchronisation tool',\n",
       " '2f365204bf3e60465a4ed333b4db725e345a5108': 'Text Mining for Biology And Biomedicine',\n",
       " 'daa9e3b88e01db5440b671af2d13c1ee974a8fcb': 'Functionally linked resting-state networks reflect the underlying structural connectivity architecture of the human brain.',\n",
       " 'bfebba8356c5d20dc6a9b2f72ff66adaf63321b7': 'End-to-end pedestrian collision warning system based on a convolutional neural network with semantic segmentation',\n",
       " '81000545a7e8663e8c18f2a4a6c2321fc7215cb3': 'Computer Vision Accelerators for Mobile Systems based on OpenCL GPGPU Co-Processing',\n",
       " 'd887526eaaaf42627b8dda69ef0c1ae561453b8d': 'Mobile Device Administration for Secure and Manageable Health Data Collection in Under-Resourced Areas',\n",
       " '1978c3122a0401f6c3309f1ffd2f5baaf55a46d7': 'Human behavior prediction for smart homes using deep learning',\n",
       " '1e6bcda276bb1681125eb18ca4d62a44e94c25c9': 'Trust Region Policy Optimization',\n",
       " '30188b631c70f45bd194e6ac72cbd73f01f3bb76': 'AutoLock: Why Cache Attacks on ARM Are Harder Than You Think',\n",
       " '1eed0e731008236ddc956ffbbe9051e13d54f2ec': 'Smart antennas for satellite communications on the move',\n",
       " '39959bc480d2972d8bb925708e6e8ad59664bcdb': 'Control theoretic approach to tracking radar: First step towards cognition',\n",
       " '6173a5266a044872618fd379bb91bda0406f778a': 'A Multitask Objective to Inject Lexical Contrast into Distributional Semantics',\n",
       " 'fe5cc6e84dc223580c0e1729614cfaa8320c5728': 'Detecting deception: the scope and limits',\n",
       " '0f5d0494c0ddaa32b5072dec141a9a79104d967b': 'Autoencoders, Minimum Description Length and Helmholtz Free Energy',\n",
       " '8d949fb5f753296db787b2b2e10b86b4224545d5': 'On Preprocessing the ALT-Algorithm Student',\n",
       " '93f6bd353aa3354f9669c1d93effa78f863c1fb4': 'Low-light video image enhancement based on multiscale Retinex-like algorithm',\n",
       " '999c4ffe9ccf530e62c60fac1271a3787b2c20a9': 'A Survey of Visualization Construction User Interfaces',\n",
       " '58f1999f18a0ac1366cbd47dd409cc1df624132b': 'Shuffled frog-leaping algorithm : a memetic metaheuristic for discrete optimization',\n",
       " '9ddda6862a8eefbb04682b449eefd6f37a45949e': 'Technology as an operant resource in service (eco)systems',\n",
       " 'b66bcce42123af3d5384aa32f9b3f1721f7ae5fd': 'Classification of sensor errors for the statistical simulation of environmental perception in automated driving systems',\n",
       " 'c20b4068be640ebaffbc56382c3e4e0bcf62664e': 'Factored Language Models and Generalized Parallel Backoff',\n",
       " 'ad836360812f87e45795f8345de3bdc6b13add81': 'Kernelized structural SVM learning for supervised object segmentation',\n",
       " 'a29afef550bf4edbf3293a50ef3fdb785ff1e5a3': 'Borg, Omega, and Kubernetes',\n",
       " '4e29533438d5c612ab24b80c840446eafcb5995f': 'Tradeoffs in Neural Variational Inference Thesis',\n",
       " 'fa3894d83f83d7d05f54b2c87158dc7a2288dc1c': 'Language-based multimodal displays for the handover of control in autonomous cars',\n",
       " 'fc7dda2d66993ced3fc6c0303347ed2bc5601f92': 'Bilingual effects on cognitive and linguistic development: role of language, cultural background, and education.',\n",
       " '9fc4bae3e7ad7c9646c5e38b093d667947bba78d': 'Impact of wireless communications technologies on elder people healthcare: Smart home in Australia',\n",
       " 'd406239de17bbb4f66d4c835f2a5f1be977b4131': 'Affordances as a Framework for Robot Control',\n",
       " '0bdb616e15d3d6f17b90e2e5c588bfecac13768a': \"Odin's Runes: A Rule Language for Information Extraction\",\n",
       " '88ee8b13451deac38384c9f31196227f2535aa65': 'Stochastic Geometric Analysis of User Mobility in Heterogeneous Wireless Networks',\n",
       " '7d6782b0ca41fd3ce99f3913d03c576d5fe65647': 'Direct storage hybrid (DSH) inverter: A new concept of intelligent hybrid inverter',\n",
       " '5d3bae2e2d5102187a4abfb3d89da5ad00745f18': 'A Single-Stage LED Driver Based on Interleaved Buck–Boost Circuit and LLC Resonant Converter',\n",
       " 'a4f5fd6f54e3a6222fbc5d8e0b7f85ce89d0237e': \"Limiting the Spread of Fake News on Social Media Platforms by Evaluating Users' Trustworthiness\",\n",
       " '6e5ceeea4fadc64da667f7693072a95df5a785b9': 'Identification of Design Elements for a Maturity Model for Interorganizational Integration: A Comparative Analysis',\n",
       " '5707844e76b0c5a39110f079e4b3c9cad1b7fb12': 'Vital Sign Detection Method Based on Multiple Higher Order Cumulant for Ultrawideband Radar',\n",
       " '47c7c17b7df3732af69d2702929a65a05c1d3d3f': 'Organizing Books and Authors by Multilayer SOM',\n",
       " '9c7032664c6902c5a936697b9bbc01f6446c58fa': 'Facial feature detection: A facial symmetry approach',\n",
       " 'd97c2e23c5c443f5040df0943847bab2db491147': 'Intelligent Accident Detection System',\n",
       " 'ba8e624dac40100f10162eaa3d4034904bc08f48': 'Intelligent phishing url detection using association rule mining',\n",
       " 'b9413a51b6865a0a2c21993e87428afa76aba440': 'Foot-and-mouth disease in Asiatic black bears (Ursus thibetanus).',\n",
       " '103f982d2a3a9b7335b76d8c7112d1b6fcf0f2bb': 'VR-STEP: Walking-in-Place using Inertial Sensing for Hands Free Navigation in Mobile VR Environments',\n",
       " '7220d7ec31f6dc6ad7030f601743b5392513a2d9': 'Learning to Solve Geometry Problems from Natural Language Demonstrations in Textbooks',\n",
       " 'ce7f9b2c6566f21056d0d4bf06a0a172fd1064f1': 'Improving Retrieval Performance for Verbose Queries via Axiomatic Analysis of Term Discrimination Heuristic',\n",
       " '36c29a4a8a4b07ce402c3077cc8831ff7245b5f8': 'Lie Access Neural Turing Machine',\n",
       " '8f8e0e4eca7cbc30cddea7f92d1dc0293342e9bb': 'An Attack Graph-Based Probabilistic Security Metric',\n",
       " '6ef12473fa99cc47f4f4bfad4b0df0d6149b3a6b': 'An Efficient Industrial Big-Data Engine',\n",
       " '3cb0ef5aabc7eb4dd8d32a129cb12b3081ef264f': 'Absolute Head Pose Estimation From Overhead Wide-Angle Cameras',\n",
       " 'ceb688c5ed7cca450dd52442edfb5ca2392a8b74': 'Violent Video Games : The Effects on Youth , and Public Policy Implications',\n",
       " 'ca37fc11108b20534ac0341c7b1e5da911f37d4d': 'SarcasmBot : An open-source sarcasm-generation module for chatbots',\n",
       " '0229829e9a1eed5769a2b5eccddcaa7cd9460b92': 'Pooled motion features for first-person videos',\n",
       " '2468a43e935adcee0303ba39e348c2bc0c4379a7': 'Argument Mining: A Machine Learning Perspective',\n",
       " '108937f6a7220ae9370511bbcaa44674c48b1a65': 'Albatross: Lightweight Elasticity in Shared Storage Databases for the Cloud using Live Data Migration',\n",
       " '994ca78751ecfc571cb7ce05c4343c12b9677b71': 'A Bayesian missing value estimation method for gene expression profile data',\n",
       " '444a9101305d25dad8b28f466bf060ee98d922c1': 'Enhanced Search with Wildcards and Morphological Inflections in the Google Books Ngram Viewer',\n",
       " '5157dde17a69f12c51186ffc20a0a6c6847f1a29': 'Evolutionary Cost-sensitive Extreme Learning Machine and Subspace Extension',\n",
       " 'dc4ed2b22123596bb329221a18c8b92176fc7263': 'Multi-level preference regression for cold-start recommendations',\n",
       " '0cb89b20dce918778ec15c7b2a99c3e04f42d3c6': 'Using machine learning to optimize parallelism in big data applications',\n",
       " '89e58773fa59ef5b57f229832c2a1b3e3efff37e': 'Analyzing EEG signals to detect unexpected obstacles during walking'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "746cafc676374114198c414d6426ec2f50e0ff80\n",
      "e170ca6dad1221f4bb2e4fc3d42a182e23026b80\n",
      "02a808de5aa34685955fd1473433161edd20fd80\n",
      "aaa9d12640ec6f9d1d37333141c761c902d2d280\n",
      "1007fbc622acd3cc8f658558a3e841ea200f6880\n",
      "3d425a44b54f505a5d280653a3b4f992d4836c80\n",
      "c91b4b3a20a7637ecbb7e0179ac3108f3cf11880\n",
      "698b8181cd613a72adeac0d75252afe7f57a5180\n",
      "4ce68170f85560942ee51465e593b16560f9c580\n",
      "f306c0d24a5eb338b7a577a17d8b35d78716d880\n",
      "e2b7a371b7cfb5f2bdc2abeb41397aee03fd5480\n",
      "11ad7734bbb81e901f2e59b73456324b299d8980\n",
      "08e4982410ebaa6dbd203a953113214bc9740b80\n",
      "4189eac6d7104e00323f78a8897167d50c815c80\n",
      "47fdd1579f732dd6389f9342027560e385853180\n",
      "c7b3f5bccb19f1a224eb87c6924f244b1511e680\n",
      "409ff05931b5f252935930ecd8de4e62bc0c7d80\n",
      "213cb7593934bc675c336f53dd6c61a3c799be80\n",
      "fd47145321e4b34e043104c9eb21c9bc28dfd680\n",
      "f407c09ae8d886fc373d3f471c97c22d3ca50580\n",
      "384ac22ddf645108d085f6f9ec6d359813776a80\n",
      "5c1e5e87a7cb833b046222bf631f9063c9926680\n",
      "76d4f741a0321bad1f080a6c4d41996a381d3c80\n",
      "50629d7d6afd7577ccfd92b35c7e15f79ad4b180\n",
      "1b3b22b95ab55853aff3ea980a5b4a76b7537980\n",
      "befb418f58475b7ebafd1826ba2e60d354eecc80\n",
      "21ef9c68739b0ddc7a9be31091c1882791e92780\n",
      "64c8217cc46df711f294cdd823d04ff2cc602280\n",
      "4ddbeb946a4ff4853f2e98c547bb0b39cc6a4480\n",
      "209f4c5dc7c65670473836304f8c478e1e0a0980\n",
      "e6517b709844f8423cad639b956af925042a3480\n",
      "1248ec7fae6c2b34a40cc0b99100227af6d2e980\n",
      "9a650f33c24c826c9c4f9480e50e8ade176b2780\n",
      "dee91a3fa2299fa7060f81f7e8c54c9408859980\n",
      "b0b5c937f17d178a3345ea506ad91904a1bda880\n",
      "c017a2bc601513a1ec2be2d1155e0384e78b6380\n",
      "2eeba9c58e544cae93eb6ee759070dcd79dee780\n",
      "cf4e9530c1aaab41b1ce414f7535c2510598a980\n",
      "771b52e7c7d0a4ac8b8ee0cdeed209d1c4114480\n",
      "0884a1b616d64e5b2c252f5980acf2049b2d8b80\n",
      "9b05437b69e5b67bc2a34c76c9521d5fb8d85080\n",
      "3365109a45c7874049fd858602b66bbe8d75f680\n",
      "88f07086254df5e1a459c6beac59171c642e1f80\n",
      "54cf3614c31e6f150cc712d9cb988d3663bf8e80\n",
      "b1cb18067659a3d5de6677828d3aebe2a3563280\n",
      "55da19ed055cb0e4534af936c2ee72c8c6e06380\n",
      "867940a08ad9e4260f9dab8d5ea4695ffc435a80\n",
      "88627daf351e12d78513e92d9dcd64c9822bf480\n",
      "e4c8421ffb0e203863f4ca3a597e1acde9044080\n",
      "2c78b1689ddb99825add3184d87bc0395b202f80\n",
      "3ca6ce29be2b5220e099229668eca3990c160680\n",
      "40b603ff878315c301c44c5f04d9c4a2a9d4da80\n",
      "6a93746a943ac42ebd8392a12ddc8c4f0c304b80\n",
      "19b6df414753a81e2fbb1030ddd983210ea4ec80\n",
      "4e9caaf519d5b94aa56c62ab1bdb8e56dfd49f80\n",
      "b11977493935acf8f8581330425dd38f83a73480\n",
      "5545cff76f3488208b5b22747c1ed00901627180\n",
      "194eeadc7ae97e9c549b8ce303db8efd8e6d3680\n",
      "3d0df7797a91b5ba7d9e018842a6a1d7abd8fd80\n",
      "58ff0a7f24a3a97b1c3dc5162ead03c6a0e03180\n",
      "addd4296263321fc8f1cfbd67a1e3313faaf5480\n",
      "a569455d692792e3445cbbd57940d8d4413b7b80\n",
      "e3decffad93a91ecd2b687ff9723a26212d00780\n",
      "c77a84cd5a53343e6977bcf1878c0e4cb9263780\n",
      "28f5b4fbfe5b9f83dfebe9e357bb5e90e8f98c80\n",
      "8c1586bcc3a6905e3eb7a432077c2aee6a4b9e80\n",
      "43b91b602731ada903f94fdffa746343346dfa80\n",
      "98f723caf182563c84d9fe9f407ade49acf8cf80\n",
      "8dc8672e67ff6c01af48e99c5b71e5e4e37f2a80\n",
      "eaf229ab3886680c7df4a14605562fd2db97d580\n",
      "41a3f6a6f97baf49c5dd26093814e124b238d380\n",
      "908f48cf72e0724a80baf87913f1b8534ed5a380\n",
      "5f1e8d265b1d3301c0472f92e097e4c3b9298d80\n",
      "5adcac7d15ec8999fa2beb62f0ddc6893884e080\n",
      "1dd646c22c2eedefe5a0e08fa1bfd085847e2f80\n",
      "993793efa6e1a3562ba2cc392bad601a94e15680\n",
      "30804753232e527f89919f5719f9d18918a15480\n",
      "fcb3402c8bbe7c75f207dde41affb2f688b77680\n",
      "6820155c6bcb273174775e64d5210e35efcaa580\n",
      "2f813885f3ac7be62894b182fa3c5d7a8226a480\n",
      "cf80e910f2690d795c45d3ad3d88854bb9eea080\n",
      "b2dac341df54e5f744d5b6562d725d254aae8e80\n",
      "e8e2c3d884bba807bcf7fbfa2c27f864b20ceb80\n",
      "8dfa1e41d8b6c3805be4f33f70dc191dd41a1180\n",
      "24266ed61126890bcf914858056f8b9010d1de80\n",
      "be3c521d5d4c16e0e509983585162b041c7bad80\n",
      "216545a302687574b0cdbc0375483c726b990f80\n",
      "cb42009f8da8c8ba2f02642f8192293166823380\n",
      "d18973a9e0ba1c425e3817e0e34643a18a602d80\n",
      "a05e4272d00860c701fa2110365bddaa5a169b80\n",
      "79c029667ffd0629a95d51e295d9a1b3db4efa80\n",
      "04d7b7851683809cab561d09b5c5c80bd5c33c80\n",
      "824cd124da2067b5c0db37ddd6c929cd7036c480\n",
      "633614f969b869388508c636a322eba35fe1f280\n"
     ]
    }
   ],
   "source": [
    "QID = '541853e747dd63d6aff41c773e21fd1e224f0680'\n",
    "for qid in queries:\n",
    "    # print(qid)\n",
    "    if qid.endswith('80'):\n",
    "        print(qid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
