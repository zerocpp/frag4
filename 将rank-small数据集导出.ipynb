{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  12%|█▎        | 1/8 [00:01<00:13,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: output/tmp/merge-small-trec-covid.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  25%|██▌       | 2/8 [00:21<01:14, 12.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: output/tmp/merge-small-climate-fever.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  38%|███▊      | 3/8 [00:39<01:14, 15.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: output/tmp/merge-small-dbpedia-entity.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  50%|█████     | 4/8 [00:59<01:07, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: output/tmp/merge-small-fever.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  62%|██████▎   | 5/8 [01:18<00:52, 17.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: output/tmp/merge-small-hotpotqa.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  75%|███████▌  | 6/8 [01:19<00:24, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: output/tmp/merge-small-nfcorpus.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  88%|████████▊ | 7/8 [01:29<00:11, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: output/tmp/merge-small-nq.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset: 100%|██████████| 8/8 [01:30<00:00, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: output/tmp/merge-small-scidocs.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import jsonlines\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "from core.models.entailment import EntailmentDeberta\n",
    "from rank_eval import load_rank_results\n",
    "import os\n",
    "import pickle\n",
    "from core.computation.uncertainty_measure import cluster_assignment_entropy\n",
    "\n",
    "def load_pickle_file(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def save_pickle_file(file_path, data):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_samples(dataset_name, qid, doc_id):\n",
    "    file_path = f'output/rank/gen/Qwen/Qwen2.5-7B-Instruct/{dataset_name}/{dataset_name}-{qid}-{doc_id}.pkl'\n",
    "    if os.path.exists(file_path):\n",
    "        result = load_pickle_file(file_path)\n",
    "        return [x['text'] for x in result['sample']]\n",
    "    return []\n",
    "\n",
    "\n",
    "def load_cluster_ids(dataset_name, qid, doc_id):\n",
    "    file_path = f'output/rank/cluster/Qwen/Qwen2.5-7B-Instruct/{dataset_name}/{dataset_name}-{qid}-{doc_id}.pkl'\n",
    "    if os.path.exists(file_path):\n",
    "        result = load_pickle_file(file_path)\n",
    "        return result.get('cluster_ids', [])\n",
    "    return []\n",
    "\n",
    "\n",
    "# 计算语义熵\n",
    "def compute_entropy(cluster_ids):\n",
    "    if len(cluster_ids) == 0:\n",
    "        return None\n",
    "    return cluster_assignment_entropy(cluster_ids)\n",
    "\n",
    "def merge_score(rank_score, entropy_score):\n",
    "    if entropy_score is None:\n",
    "        return rank_score\n",
    "    if entropy_score < 0.01:\n",
    "        return rank_score + 1.0\n",
    "    return rank_score\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    query_path = os.path.join(dataset_path, 'queries.jsonl')\n",
    "    queries = {}\n",
    "    with jsonlines.open(query_path) as reader:\n",
    "        for query in reader:\n",
    "            queries[str(query['_id'])] = query['text']\n",
    "\n",
    "    doc_path = os.path.join(dataset_path, 'corpus.jsonl')\n",
    "    docs = {}\n",
    "    with jsonlines.open(doc_path) as reader:\n",
    "        for doc in reader:\n",
    "            docs[str(doc['_id'])] = doc['text']\n",
    "\n",
    "    rel_path = os.path.join(dataset_path, 'qrels/test.tsv')\n",
    "    df = pd.read_csv(rel_path, sep='\\t', header=0)\n",
    "    \n",
    "    scores = defaultdict(dict)\n",
    "    for qid, docid, score in df.values:\n",
    "        scores[str(qid)][str(docid)] = score\n",
    "    \n",
    "    return queries, docs, scores\n",
    "\n",
    "dataset_names = [\"trec-covid\", \"climate-fever\", \"dbpedia-entity\", \"fever\", \"hotpotqa\", \"nfcorpus\", \"nq\", \"scidocs\"]\n",
    "for dataset_name in tqdm(dataset_names, desc='dataset'):\n",
    "    dataset_path = f'/home/song/dataset/beir/{dataset_name}'\n",
    "    queries, docs, scores = load_dataset(dataset_path)\n",
    "    rank_result_path = f'dataset/rank/{dataset_name}/{dataset_name}-rank10-small.tsv'\n",
    "    rank_results = load_rank_results(rank_result_path)\n",
    "    # entropy_result_path = f'output/rerank/{dataset_name}/entropy-small.tsv'\n",
    "    # entropy_results = load_rank_results(entropy_result_path)\n",
    "    # print(f\"dataset: {dataset_name}\")\n",
    "    merge_results = [] # ['qid', 'query', 'docid', 'doc', 'gold_score', 'rank_index', 'rank_score', 'entropy_score', 'merge_score', 'samples', 'cluster_ids']\n",
    "    for qid in rank_results:\n",
    "        for i, docid in enumerate(rank_results[qid]):\n",
    "            samples = load_samples(dataset_name, qid, docid)\n",
    "            samples_text = '|'.join(samples)\n",
    "            cluster_ids = load_cluster_ids(dataset_name, qid, docid)\n",
    "            cluster_text = '|'.join([str(x) for x in cluster_ids])\n",
    "            entropy = compute_entropy(cluster_ids)\n",
    "            merge_results.append([str(qid), \n",
    "                                  queries.get(str(qid), ''), \n",
    "                                  str(docid), \n",
    "                                  docs.get(str(docid), ''), \n",
    "                                  scores.get(str(qid), {}).get(str(docid), 0.0), \n",
    "                                  i,\n",
    "                                  rank_results.get(qid, {}).get(docid, 0.0),\n",
    "                                  entropy,\n",
    "                                  merge_score(rank_results.get(qid, {}).get(docid, 0.0), entropy),\n",
    "                                    samples_text,\n",
    "                                    cluster_text\n",
    "                                  ])\n",
    "    with open(f'output/tmp/merge-small-{dataset_name}.tsv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerow(['qid', 'query', 'docid', 'doc', 'gold_score', 'rank_index', 'rank_score', 'entropy_score', 'merge_score', 'samples', 'cluster_ids'])\n",
    "        writer.writerows(merge_results)\n",
    "    print(f\"output: output/tmp/merge-small-{dataset_name}.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sample = load_gen_texts('nq','test0','doc0')\n",
    "# print(sample)\n",
    "\n",
    "# cluster_ids = load_cluster_ids('nq','test0','doc0')\n",
    "# print(cluster_ids)\n",
    "\n",
    "# entropy = compute_entropy(cluster_ids)\n",
    "# print(entropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
