{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05fb5dbbb394422806a409e0cbae147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/171332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  12%|█▎        | 1/8 [00:01<00:09,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: trec-covid\n",
      "output: output/tmp/merge-small-trec-covid.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f36bd53f32472190bb9f91399db268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5416593 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  25%|██▌       | 2/8 [00:26<01:30, 15.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: climate-fever\n",
      "output: output/tmp/merge-small-climate-fever.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23e0c49452c4828baa9ecfb91063c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4635922 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  38%|███▊      | 3/8 [00:48<01:33, 18.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: dbpedia-entity\n",
      "output: output/tmp/merge-small-dbpedia-entity.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43096078d7e34c41a0be02effbba95c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5416568 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  50%|█████     | 4/8 [01:14<01:26, 21.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: fever\n",
      "output: output/tmp/merge-small-fever.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9c151d6a4041409249f6d87aad7727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5233329 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  62%|██████▎   | 5/8 [01:39<01:07, 22.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: hotpotqa\n",
      "output: output/tmp/merge-small-hotpotqa.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51db5ae840304e67be21a9d30e1c83b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3633 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  75%|███████▌  | 6/8 [01:40<00:30, 15.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: nfcorpus\n",
      "output: output/tmp/merge-small-nfcorpus.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b088d051f6f545b394b68fc67acc1d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2681468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset:  88%|████████▊ | 7/8 [01:52<00:14, 14.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: nq\n",
      "output: output/tmp/merge-small-nq.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff49109db48a4845a35e1a1c9b439374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset: 100%|██████████| 8/8 [01:53<00:00, 14.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: scidocs\n",
      "output: output/tmp/merge-small-scidocs.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import jsonlines\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "from core.models.entailment import EntailmentDeberta\n",
    "from rank_eval import load_data, load_rank_results\n",
    "\n",
    "def merge_score(rank_score, entropy_score):\n",
    "    if entropy_score is None:\n",
    "        return rank_score\n",
    "    if entropy_score < 0.01:\n",
    "        return rank_score + 1.0\n",
    "    return rank_score\n",
    "\n",
    "dataset_names = [\"trec-covid\", \"climate-fever\", \"dbpedia-entity\", \"fever\", \"hotpotqa\", \"nfcorpus\", \"nq\", \"scidocs\"]\n",
    "for dataset_name in tqdm(dataset_names, desc='dataset'):\n",
    "    dataset_path = f'/home/song/dataset/beir/{dataset_name}'\n",
    "    queries1, docs1, scores = load_data(dataset_path, dataset_name)\n",
    "    queries = {str(qid): query['text'] for qid, query in queries1.items()}\n",
    "    docs = {str(docid): doc for docid, doc in docs1.items()}\n",
    "    rank_result_path = f'dataset/rank/{dataset_name}/{dataset_name}-rank10-small.tsv'\n",
    "    rank_results = load_rank_results(rank_result_path)\n",
    "    entropy_result_path = f'output/rerank/{dataset_name}/entropy-small.tsv'\n",
    "    entropy_results = load_rank_results(entropy_result_path)\n",
    "    print(f\"dataset: {dataset_name}\")\n",
    "    merge_results = [] # ['qid', 'query', 'docid', 'doc', 'gold_score', 'rank_index', 'rank_score', 'entropy_score', 'merge_score']\n",
    "    for qid in rank_results:\n",
    "        for i, docid in enumerate(rank_results[qid]):\n",
    "            merge_results.append([str(qid), \n",
    "                                  queries.get(str(qid), ''), \n",
    "                                  str(docid), \n",
    "                                  docs.get(str(docid), ''), \n",
    "                                  scores.get(str(qid), {}).get(str(docid), 0.0), \n",
    "                                  i,\n",
    "                                  rank_results.get(qid, {}).get(docid, 0.0),\n",
    "                                  entropy_results.get(qid, {}).get(docid, None),\n",
    "                                  merge_score(rank_results.get(qid, {}).get(docid, 0.0), entropy_results.get(qid, {}).get(docid, None))\n",
    "                                  ])\n",
    "    with open(f'output/tmp/merge-small-{dataset_name}.tsv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerow(['qid', 'query', 'docid', 'doc', 'gold_score', 'rank_index', 'rank_score', 'entropy_score', 'merge_score'])\n",
    "        writer.writerows(merge_results)\n",
    "    print(f\"output: output/tmp/merge-small-{dataset_name}.tsv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
