{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理流程\n",
    "- 将原始数据集随机打乱，得到**乱序数据集**；\n",
    "- 可视化乱序数据集的上下文长度分布；\n",
    "- 保存乱序数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "import gc\n",
    "import logging\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import StoppingCriteria\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import StoppingCriteriaList\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import datasets\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用镜像\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "dataset = datasets.load_from_disk(\"/Users/song/datasets/rajpurkar/squad_v2\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reformat = lambda x: {\n",
    "    'id': x['id'],\n",
    "    'question': x['question'],\n",
    "    'context': x['context'],\n",
    "    'answers': x['answers']['text'],\n",
    "}\n",
    "# filter out examples without answers\n",
    "train_dataset = [reformat(d) for d in dataset[\"train\"] if d['answers']['text']]\n",
    "validation_dataset = [reformat(d) for d in dataset[\"validation\"] if d['answers']['text']]\n",
    "print(len(train_dataset))\n",
    "print(len(validation_dataset))\n",
    "print(train_dataset[0])\n",
    "print(validation_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固定随机数种子\n",
    "random.seed(42)\n",
    "# 将原数据集打乱\n",
    "random.shuffle(train_dataset)\n",
    "random.shuffle(validation_dataset)\n",
    "\n",
    "# 将原train数据集按9:1分为train, validation数据集\n",
    "total_size = len(train_dataset)\n",
    "train_size = int(total_size * 0.9)\n",
    "new_train_dataset = train_dataset[:train_size]\n",
    "new_validation_dataset = train_dataset[train_size:]\n",
    "\n",
    "# 将原validation数据集作为test数据集\n",
    "new_test_dataset = validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加无关上下文\n",
    "def add_irrelevant_context(dataset):\n",
    "    # 构建id-context字典\n",
    "    id_context_dict = {d['id']: d['context'] for d in dataset}\n",
    "    # 数据集id列表\n",
    "    id_list = [d['id'] for d in dataset]\n",
    "\n",
    "    while True:\n",
    "        # 生成新的id列表\n",
    "        shuffled_id_list = id_list.copy()\n",
    "        random.shuffle(shuffled_id_list)\n",
    "        # 确认新旧id列表对应位置不相同\n",
    "        if all(shuffled_id_list[i] != id_list[i] for i in range(len(id_list))):\n",
    "            break\n",
    "\n",
    "    # 在原数据集中增加字段\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i]['irrelevant_id'] = shuffled_id_list[i]\n",
    "        dataset[i]['irrelevant_context'] = id_context_dict[shuffled_id_list[i]]\n",
    "\n",
    "    return dataset\n",
    "\n",
    "new_train_dataset = add_irrelevant_context(new_train_dataset)\n",
    "print(new_train_dataset)\n",
    "new_validation_dataset = add_irrelevant_context(new_validation_dataset)\n",
    "print(new_validation_dataset)\n",
    "new_test_dataset = add_irrelevant_context(new_test_dataset)\n",
    "print(new_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化上下文长度分布\n",
    "def plot_context_length(dataset):\n",
    "    context_lengths = [len(d['context']) for d in dataset]\n",
    "    plt.hist(context_lengths, bins=100, edgecolor='black')\n",
    "    plt.xlabel('Context Length')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Context Length Distribution in Filtered Dataset')\n",
    "    plt.show()\n",
    "\n",
    "plot_context_length(train_dataset)\n",
    "plot_context_length(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固定随机数种子\n",
    "random.seed(42)\n",
    "# 将原数据集打乱\n",
    "random.shuffle(train_dataset)\n",
    "random.shuffle(validation_dataset)\n",
    "\n",
    "# 将原train数据集按9:1分为train, validation数据集\n",
    "total_size = len(train_dataset)\n",
    "train_size = int(total_size * 0.9)\n",
    "new_train_dataset = train_dataset[:train_size]\n",
    "new_validation_dataset = train_dataset[train_size:]\n",
    "\n",
    "# 将原validation数据集作为test数据集\n",
    "new_test_dataset = validation_dataset\n",
    "\n",
    "# 合并成一个DatasetDict\n",
    "dataset = datasets.DatasetDict({\n",
    "    'train': datasets.Dataset.from_list(new_train_dataset),\n",
    "    'validation': datasets.Dataset.from_list(new_validation_dataset),\n",
    "    'test': datasets.Dataset.from_list(new_test_dataset),\n",
    "})\n",
    "print(dataset)\n",
    "\n",
    "# 保存新数据集\n",
    "dataset.save_to_disk(\"/Users/song/datasets/song/squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
