{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "import jsonlines\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "from core.models.entailment import EntailmentDeberta\n",
    "from rank_eval import load_rank_results\n",
    "import os\n",
    "import pickle\n",
    "from core.computation.uncertainty_measure import cluster_assignment_entropy\n",
    "\n",
    "def load_pickle_file(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def save_pickle_file(file_path, data):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "def load_samples(dataset_name, qid, doc_id):\n",
    "    file_path = f'output/rank/gen/Qwen/Qwen2.5-7B-Instruct/{dataset_name}/{dataset_name}-{qid}-{doc_id}.pkl'\n",
    "    if os.path.exists(file_path):\n",
    "        result = load_pickle_file(file_path)\n",
    "        return [x['text'] for x in result['sample']]\n",
    "    return []\n",
    "\n",
    "\n",
    "def load_cluster_ids(dataset_name, qid, doc_id):\n",
    "    file_path = f'output/rank/cluster/Qwen/Qwen2.5-7B-Instruct/{dataset_name}/{dataset_name}-{qid}-{doc_id}.pkl'\n",
    "    if os.path.exists(file_path):\n",
    "        result = load_pickle_file(file_path)\n",
    "        return result.get('cluster_ids', [])\n",
    "    return []\n",
    "\n",
    "\n",
    "# 计算语义熵\n",
    "def compute_entropy(cluster_ids):\n",
    "    if len(cluster_ids) == 0:\n",
    "        return None\n",
    "    return cluster_assignment_entropy(cluster_ids)\n",
    "\n",
    "def merge_score(rank_score, entropy_score):\n",
    "    if entropy_score is None:\n",
    "        return rank_score\n",
    "    if entropy_score < 0.01:\n",
    "        return rank_score + 1.0\n",
    "    return rank_score\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    query_path = os.path.join(dataset_path, 'queries.jsonl')\n",
    "    queries = {}\n",
    "    with jsonlines.open(query_path) as reader:\n",
    "        for query in reader:\n",
    "            queries[str(query['_id'])] = query['text']\n",
    "\n",
    "    doc_path = os.path.join(dataset_path, 'corpus.jsonl')\n",
    "    docs = {}\n",
    "    with jsonlines.open(doc_path) as reader:\n",
    "        for doc in reader:\n",
    "            docs[str(doc['_id'])] = doc['text']\n",
    "\n",
    "    rel_path = os.path.join(dataset_path, 'qrels/test.tsv')\n",
    "    df = pd.read_csv(rel_path, sep='\\t', header=0)\n",
    "    \n",
    "    scores = defaultdict(dict)\n",
    "    for qid, docid, score in df.values:\n",
    "        scores[str(qid)][str(docid)] = score\n",
    "    \n",
    "    return queries, docs, scores\n",
    "\n",
    "size_name = \"large\"\n",
    "dataset_names = [\"trec-covid\", \"climate-fever\", \"dbpedia-entity\", \"fever\", \"hotpotqa\", \"nfcorpus\", \"nq\", \"scidocs\"]\n",
    "for dataset_name in tqdm(dataset_names, desc='dataset'):\n",
    "    dataset_path = f'/home/song/dataset/beir/{dataset_name}'\n",
    "    queries, docs, scores = load_dataset(dataset_path)\n",
    "    rank_result_path = f'dataset/rank/{dataset_name}/{dataset_name}-rank10-{size_name}.tsv'\n",
    "    rank_results = load_rank_results(rank_result_path)\n",
    "    # entropy_result_path = f'output/rerank/{dataset_name}/entropy-small.tsv'\n",
    "    # entropy_results = load_rank_results(entropy_result_path)\n",
    "    # print(f\"dataset: {dataset_name}\")\n",
    "    merge_results = [] # ['qid', 'query', 'docid', 'doc', 'gold_score', 'rank_index', 'rank_score', 'entropy_score', 'merge_score', 'samples', 'cluster_ids']\n",
    "    for qid in rank_results:\n",
    "        for i, docid in enumerate(rank_results[qid]):\n",
    "            samples = load_samples(dataset_name, qid, docid)\n",
    "            samples_text = '|'.join(samples)\n",
    "            cluster_ids = load_cluster_ids(dataset_name, qid, docid)\n",
    "            cluster_text = '|'.join([str(x) for x in cluster_ids])\n",
    "            entropy = compute_entropy(cluster_ids)\n",
    "            merge_results.append([str(qid), \n",
    "                                  queries.get(str(qid), ''), \n",
    "                                  str(docid), \n",
    "                                  docs.get(str(docid), ''), \n",
    "                                  scores.get(str(qid), {}).get(str(docid), 0.0), \n",
    "                                  i,\n",
    "                                  rank_results.get(qid, {}).get(docid, 0.0),\n",
    "                                  entropy,\n",
    "                                  merge_score(rank_results.get(qid, {}).get(docid, 0.0), entropy),\n",
    "                                    samples_text,\n",
    "                                    cluster_text\n",
    "                                  ])\n",
    "    with open(f'output/tmp/merge-{size_name}-{dataset_name}.tsv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerow(['qid', 'query', 'docid', 'doc', 'gold_score', 'rank_index', 'rank_score', 'entropy_score', 'merge_score', 'samples', 'cluster_ids'])\n",
    "        writer.writerows(merge_results)\n",
    "    print(f\"output: output/tmp/merge-{size_name}-{dataset_name}.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# sample = load_gen_texts('nq','test0','doc0')\n",
    "# print(sample)\n",
    "\n",
    "# cluster_ids = load_cluster_ids('nq','test0','doc0')\n",
    "# print(cluster_ids)\n",
    "\n",
    "# entropy = compute_entropy(cluster_ids)\n",
    "# print(entropy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
