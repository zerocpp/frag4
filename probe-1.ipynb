{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 2024-12-10 13:26:20\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# 打印当前时间，按照年-月-日 小时:分钟:秒的格式\n",
    "print(\"Start time:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# 记录开始时间\n",
    "from time import time\n",
    "start = time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 合并golden、irrelevant、without三种数据一起训练探针\n",
    "各取1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局变量\n",
    "# 图片保存文件夹路径\n",
    "FIG_DIR = \"output/data/figures/merge_sample_types\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from collections import defaultdict, Counter\n",
    "import argparse\n",
    "from core.models.entailment import EntailmentDeberta\n",
    "from core.data.data_utils import load_ds_from_json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "\n",
    "def load_pickle_file(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def save_pickle_file(file_path, data):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset class for easier attribute keeping\n",
    "class Dataset:\n",
    "    name = ''\n",
    "    tbg_dataset = None\n",
    "    slt_dataset = None\n",
    "    entropy = None\n",
    "    accuracies = None\n",
    "\n",
    "    sample_type = ''\n",
    "    model_name = ''\n",
    "    dataset_name = ''\n",
    "    split = ''\n",
    "    def __init__(self, name, tbg_dataset, slt_dataset, entropy, accuracies, sample_type, model_name, dataset_name, split):\n",
    "        self.name = name\n",
    "        self.tbg_dataset = tbg_dataset\n",
    "        self.slt_dataset = slt_dataset\n",
    "        self.entropy = entropy\n",
    "        self.accuracies = accuracies\n",
    "        self.sample_type = sample_type\n",
    "        self.model_name = model_name\n",
    "        self.dataset_name = dataset_name\n",
    "        self.split = split\n",
    "\n",
    "all_data = load_pickle_file(\"output/data/all_data.pkl\")\n",
    "\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "dataset_names = [\"squad\", \"triviaqa\"]\n",
    "split = \"train\"\n",
    "sample_types = ['golden', 'irrelevant', 'without']\n",
    "\n",
    "Ds = []\n",
    "for dataset_name in dataset_names:\n",
    "    Ds_sample_types = [] # 区分不同sample_types\n",
    "    for sample_type in sample_types:\n",
    "        Ds_sample_types.append(all_data[model_name][dataset_name][split][sample_type])\n",
    "    tbg_datasets = [] # torch.Size([29, 2000, 3584])\n",
    "    slt_datasets = [] # torch.Size([29, 2000, 3584])\n",
    "    entropies = [] # torch.Size([2000])\n",
    "    accuracies = [] # torch.Size([2000])\n",
    "    \n",
    "    # 合并不同sample_types\n",
    "    for ds in Ds_sample_types:\n",
    "        # 获取样本数量\n",
    "        sample_size = ds.entropy.size(0)\n",
    "        # 只取前1/3样本\n",
    "        tbg_datasets.append(ds.tbg_dataset[:, :sample_size//3, :])\n",
    "        slt_datasets.append(ds.slt_dataset[:, :sample_size//3, :])\n",
    "        entropies.append(ds.entropy[:sample_size//3])\n",
    "        accuracies.append(ds.accuracies[:sample_size//3])\n",
    "\n",
    "    tbg_dataset = torch.cat(tbg_datasets, dim=1)\n",
    "    slt_dataset = torch.cat(slt_datasets, dim=1)\n",
    "    entropy = torch.cat(entropies, dim=0)\n",
    "    accuracy = torch.cat(accuracies, dim=0)\n",
    "    ds = Dataset(name=dataset_name, \n",
    "                 tbg_dataset=tbg_dataset, \n",
    "                 slt_dataset=slt_dataset, \n",
    "                 entropy=entropy, \n",
    "                 accuracies=accuracy, \n",
    "                 sample_type=\"all\", \n",
    "                 model_name=model_name, \n",
    "                 dataset_name=dataset_name, \n",
    "                 split=split)\n",
    "    Ds.append(ds)\n",
    "\n",
    "for i, D in enumerate(Ds):\n",
    "    # OOD-related\n",
    "    D.other_ids = [j for j in range(len(Ds)) if j != i]\n",
    "    D.other_names = [Ds[j].name for j in D.other_ids]\n",
    "\n",
    "# 打印数据集信息\n",
    "for ds in Ds:\n",
    "    print(f\"{ds.model_name} {ds.dataset_name} {ds.split}: {len(ds.accuracies)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/val/test splits\n",
    "def create_Xs_and_ys(datasets, scores, val_test_splits=[0.2, 0.1], test_only=False, no_val=False):\n",
    "    # Data splitting for sklearn linear models\n",
    "    X = np.array(datasets)\n",
    "    y = np.array(scores)\n",
    "\n",
    "    if test_only:\n",
    "        X_tests, y_tests = [], []\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            X_tests.append(X[i])\n",
    "            y_tests.append(y)\n",
    "        return (None, None, X_tests, None, None, y_tests)\n",
    "    \n",
    "    valid_size = val_test_splits[0]\n",
    "    test_size = val_test_splits[1]\n",
    "\n",
    "    X_trains, X_vals, X_tests, y_trains, y_vals, y_tests = [], [], [], [], [], []\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        # Split data into train, validation, and test sets\n",
    "        X_train_val, X_test, y_train_val, y_test = train_test_split(X[i], y, test_size=test_size, random_state=42)\n",
    "        X_tests.append(X_test)\n",
    "        y_tests.append(y_test)\n",
    "        if no_val:\n",
    "            X_trains.append(X_train_val)\n",
    "            y_trains.append(y_train_val)\n",
    "            continue\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=valid_size, random_state=42) \n",
    "        X_trains.append(X_train)\n",
    "        y_trains.append(y_train)\n",
    "        X_vals.append(X_val)\n",
    "        y_vals.append(y_val)\n",
    "\n",
    "    return X_trains, X_vals, X_tests, y_trains, y_vals, y_tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrapping methods from ../semantic_entropy/uncertainty/utils/eval_utils.py\n",
    "def bootstrap_func(y_true, y_score, func):\n",
    "    y_tuple = (y_true, y_score)\n",
    "    \n",
    "    metric_i = func(*y_tuple)\n",
    "    metric_dict = {}\n",
    "    metric_dict['mean'] = metric_i\n",
    "    metric_dict['bootstrap'] = compatible_bootstrap(\n",
    "        func, rng)(*y_tuple)  # a bit slow to run\n",
    "\n",
    "    return metric_dict\n",
    "\n",
    "def bootstrap(function, rng, n_resamples=1000):\n",
    "    def inner(data):\n",
    "        bs = scipy.stats.bootstrap(\n",
    "            (data, ), function, n_resamples=n_resamples, confidence_level=0.9,\n",
    "            random_state=rng)\n",
    "        return {\n",
    "            'std_err': bs.standard_error,\n",
    "            'low': bs.confidence_interval.low,\n",
    "            'high': bs.confidence_interval.high\n",
    "        }\n",
    "    return inner\n",
    "\n",
    "def auroc(y_true, y_score):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score)\n",
    "    del thresholds\n",
    "    return metrics.auc(fpr, tpr)\n",
    "\n",
    "def compatible_bootstrap(func, rng):\n",
    "    def helper(y_true_y_score):\n",
    "        # this function is called in the bootstrap\n",
    "        y_true = np.array([i['y_true'] for i in y_true_y_score])\n",
    "        y_score = np.array([i['y_score'] for i in y_true_y_score])\n",
    "        out = func(y_true, y_score)\n",
    "        return out\n",
    "\n",
    "    def wrap_inputs(y_true, y_score):\n",
    "        return [{'y_true': i, 'y_score': j} for i, j in zip(y_true, y_score)]\n",
    "\n",
    "    def converted_func(y_true, y_score):\n",
    "        y_true_y_score = wrap_inputs(y_true, y_score)\n",
    "        return bootstrap(helper, rng=rng)(y_true_y_score)\n",
    "    return converted_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluation function.\n",
    "def sklearn_train_and_evaluate(model, X_train, y_train, X_valid, y_valid, silent=False):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate training loss and score\n",
    "    train_probs = model.predict_proba(X_train)\n",
    "    train_loss = log_loss(y_train, train_probs)\n",
    "\n",
    "    # Calculate validation loss\n",
    "    valid_preds = model.predict(X_valid)\n",
    "    valid_probs = model.predict_proba(X_valid)\n",
    "    valid_loss = log_loss(y_valid, valid_probs)\n",
    "    val_accuracy = np.mean((valid_preds == y_valid).astype(int))\n",
    "    auroc_score = roc_auc_score(y_valid, valid_probs[:,1])\n",
    "    if not silent:\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}, AUROC: {auroc_score:.4f}\")\n",
    "        print(f\"Training Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}\")\n",
    "\n",
    "def sklearn_evaluate_on_test(model, X_test, y_test, silent=False, bootstrap=True):\n",
    "    test_preds = model.predict(X_test)\n",
    "    test_probs = model.predict_proba(X_test)\n",
    "    test_loss = log_loss(y_test, test_probs)\n",
    "    test_accuracy = np.mean((test_preds == y_test).astype(int))\n",
    "    \n",
    "    if bootstrap:\n",
    "        auroc_score = bootstrap_func(y_test, test_probs[:,1], auroc)\n",
    "        auroc_score_scalar = auroc_score['mean']\n",
    "    else:\n",
    "        auroc_score = auroc_score_scalar = roc_auc_score(y_test, test_probs[:, 1])\n",
    "    \n",
    "    if not silent:\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, AUROC: {auroc_score_scalar:.4f}\")\n",
    "    \n",
    "    return test_loss, test_accuracy, auroc_score\n",
    "\n",
    "def train_single_metric(D, token_type='tbg', metric='b_entropy'):\n",
    "    \"\"\"train and test on single metric (e.g. SE, Acc) on single dataset\"\"\"\n",
    "    var_name = token_type[0] + metric[0] \n",
    "    # named as [te, se. ta, sa] for easy identification; t for tbg, s for slt (token positions)\n",
    "    # e for entropy and a for accuracy (or model faithfulness)\n",
    "    X_trains, X_vals, X_tests, y_trains, y_vals, y_tests = create_Xs_and_ys(\n",
    "        getattr(D, f'{token_type}_dataset'), getattr(D, metric)\n",
    "    )\n",
    "\n",
    "    accs = []\n",
    "    aucs = []\n",
    "    models = []\n",
    "    \n",
    "    for i, (X_train, X_val, X_test, y_train, y_val, y_test) in enumerate(zip(X_trains, X_vals, X_tests, y_trains, y_vals, y_tests)):\n",
    "        print(f\"Training on {D.name}-{token_type.upper()}-{metric.upper()} {i+1}/{len(X_trains)}\")\n",
    "        model = LogisticRegression()\n",
    "        sklearn_train_and_evaluate(model, X_train, y_train, X_val, y_val)\n",
    "        test_loss, test_acc, test_auc = sklearn_evaluate_on_test(model, X_test, y_test)\n",
    "        accs.append(test_acc)\n",
    "        aucs.append(test_auc)\n",
    "        models.append(model)\n",
    "\n",
    "    setattr(D, f'{var_name}_accs', accs)\n",
    "    setattr(D, f'{var_name}_aucs', aucs)\n",
    "    setattr(D, f'{var_name}_models', models)\n",
    "\n",
    "# simple get-around for unpacking bootstrapping dicts\n",
    "auc = lambda aucs : [ac['mean'] for ac in aucs] \n",
    "idf = lambda x : x  # identical function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting methods\n",
    "def plot_metrics_ax(ax, test_metrics_list, model_names, title=\"\", prep_func=auc, \n",
    "                    use_logarithm=False, preset_layer_indices=None, legend_outside=False):  # some simple gadgets\n",
    "    \"\"\"plot metrics along certain axis in a multi-axis plot (plt.subplots)\"\"\"\n",
    "    if len(test_metrics_list) != len(model_names):\n",
    "        raise ValueError(\"The length of test_metrics_list and model_names must be the same.\")\n",
    "    \n",
    "    for test_metrics, model_name in zip(test_metrics_list, model_names):\n",
    "        test_metrics = torch.tensor(prep_func(test_metrics), dtype=torch.float32)\n",
    "        if use_logarithm:\n",
    "            test_metrics = torch.log(test_metrics + 1e-6)\n",
    "        if preset_layer_indices is not None:\n",
    "            layer_indices = preset_layer_indices\n",
    "        else:\n",
    "            layer_indices = torch.arange(len(test_metrics)) + 1  # +1 if layer indexing starts at 1\n",
    "        \n",
    "        ax.plot(layer_indices, test_metrics, marker='o', linestyle='-', linewidth=2, markersize=5, label=model_name)\n",
    "    \n",
    "    ax.set_title(f'{title}', fontsize=14)\n",
    "    ax.set_xlabel('Layer', fontsize=12)\n",
    "    ax.set_ylabel(f'Test AUROC scores', fontsize=12)\n",
    "    \n",
    "    tick_interval = 5  # Change this value to display more or fewer ticks\n",
    "    ax.set_xticks(layer_indices[::tick_interval].tolist())\n",
    "    ax.set_xticklabels(layer_indices[::tick_interval].tolist())\n",
    "    \n",
    "    ax.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    \n",
    "    if legend_outside:\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=12)\n",
    "    else:\n",
    "        ax.legend(fontsize=12)\n",
    "\n",
    "def save_fig(name):\n",
    "    \"\"\"save figure with timestamps\"\"\"\n",
    "    dir_path = os.path.expanduser(FIG_DIR)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    full_path = os.path.join(dir_path, f'{name}.pdf')\n",
    "    plt.savefig(full_path, format='pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best split for SE binarization.\n",
    "def best_split(entropy: torch.Tensor, label=\"Dx\"):\n",
    "    \"\"\"\n",
    "    Identify best split for minimizing reconstruction error via low and high SE mean estimates,\n",
    "    as discussed in Section 4. Binarization of paper (ArXiv: 2406.15927)\n",
    "    \"\"\"\n",
    "    ents = entropy.numpy()\n",
    "    splits = np.linspace(1e-10, ents.max(), 100)\n",
    "    split_mses = []\n",
    "    for split in splits:\n",
    "        low_idxs, high_idxs = ents < split, ents >= split\n",
    "    \n",
    "        low_mean = np.mean(ents[low_idxs])\n",
    "        high_mean = np.mean(ents[high_idxs])\n",
    "    \n",
    "        mse = np.sum((ents[low_idxs] - low_mean)**2) + np.sum((ents[high_idxs] - high_mean)**2)\n",
    "        mse = np.sum(mse)\n",
    "    \n",
    "        split_mses.append(mse)\n",
    "    \n",
    "    split_mses = np.array(split_mses)\n",
    "    \n",
    "    plt.plot(splits, split_mses, label=label)\n",
    "    return splits[np.argmin(split_mses)]\n",
    "\n",
    "def binarize_entropy(entropy, thres=0.0):  # 0.0 means even splits for normalized entropy scores\n",
    "    \"\"\"Binarize entropy scores into 0s and 1s\"\"\"\n",
    "    binary_entropy = torch.full_like(entropy, -1, dtype=torch.float)\n",
    "    binary_entropy[entropy < thres] = 0\n",
    "    binary_entropy[entropy > thres] = 1\n",
    "\n",
    "    return binary_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二值化语义熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best universal split across datasets\n",
    "split_method = 'best-universal-split'\n",
    "all_entropy = torch.cat([D.entropy for D in Ds], dim=0)\n",
    "split = best_split(all_entropy, \"All datasets collective\")\n",
    "plt.legend()\n",
    "plt.title('Sum of squared errors at different splits')\n",
    "for D in Ds:\n",
    "    D.b_entropy = binarize_entropy(D.entropy, split)\n",
    "    print(f\"Dummy accuracy for {D.name}: {max(torch.mean(D.b_entropy).item(), 1-torch.mean(D.b_entropy).item()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train linear probes to predict Binarized SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for D in Ds:\n",
    "    train_single_metric(D, 'tbg', 'b_entropy')\n",
    "    train_single_metric(D, 'slt', 'b_entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot se probe\n",
    "fig, axs = plt.subplots(1, len(Ds), figsize=(5 * len(Ds), 4.5))\n",
    "for i, D in enumerate(Ds):\n",
    "    plot_metrics_ax(axs[i], [auc(D.tb_aucs), auc(D.sb_aucs)], [\"SE Probe on TBG token\", \"SE Probe on SLT token\"], \n",
    "                    f\"AUROC on {D.name.upper()}\", prep_func=idf)\n",
    "plt.tight_layout()\n",
    "save_fig(name=\"se_both_tok\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train linear probes to predict Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for D in Ds:\n",
    "    train_single_metric(D, 'tbg', 'accuracies')\n",
    "    train_single_metric(D, 'slt', 'accuracies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot acc probe\n",
    "fig, axs = plt.subplots(1, len(Ds), figsize=(5 * len(Ds), 4.5))\n",
    "for i, D in enumerate(Ds):\n",
    "    plot_metrics_ax(axs[i], [auc(D.ta_aucs), auc(D.sa_aucs)], [\"Acc Probe on TBG token\", \"Acc Probe on SLT token\"], \n",
    "                    f\"AUROC on {D.name.upper()}\", prep_func=idf)\n",
    "plt.tight_layout()\n",
    "save_fig(name=\"acc_both_tok\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use linear probe trained on Binarized SE to predict Accuracy\n",
    "\n",
    "We leverage trained probes on semantic entropy to predict model correctness on the same dataset.\n",
    "\n",
    "Note that we need to predict the error rate (1 minus accuracy) using SEPs due to the nature of semantic uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for D in Ds:\n",
    "    r_acc = 1 - D.accuracies\n",
    "    \n",
    "    # TBG\n",
    "    _, _, X_tests, _, _, y_tests = create_Xs_and_ys(D.tbg_dataset, r_acc) \n",
    "    tab_accs = []\n",
    "    tab_aucs = []\n",
    "    \n",
    "    for i, (X_test, y_test) in enumerate(zip(X_tests, y_tests)):\n",
    "        print(f\"Testing on {D.name}-TBG {i+1}/{len(X_tests)}\")\n",
    "        model = D.tb_models[i]\n",
    "        test_loss, test_acc, test_auc = sklearn_evaluate_on_test(model, X_test, y_test)\n",
    "        tab_accs.append(test_acc)\n",
    "        tab_aucs.append(test_auc)\n",
    "\n",
    "    D.tab_accs = tab_accs\n",
    "    D.tab_aucs = tab_aucs\n",
    "\n",
    "    # SLT t\n",
    "    _, _, X_tests, _, _, y_tests = create_Xs_and_ys(D.slt_dataset, r_acc)     \n",
    "    sab_accs = []\n",
    "    sab_aucs = []\n",
    "    \n",
    "    for i, (X_test, y_test) in enumerate(zip(X_tests, y_tests)):\n",
    "        print(f\"Testing on {D.name}-SLT {i+1}/{len(X_tests)}\")\n",
    "        model = D.sb_models[i]\n",
    "        test_loss, test_acc, test_auc = sklearn_evaluate_on_test(model, X_test, y_test)\n",
    "        sab_accs.append(test_acc)\n",
    "        sab_aucs.append(test_auc)\n",
    "\n",
    "    D.sab_accs = sab_accs\n",
    "    D.sab_aucs = sab_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(Ds), figsize=(5 * len(Ds), 4.5))\n",
    "for i, D in enumerate(Ds):\n",
    "    plot_metrics_ax(axs[i], [D.sa_aucs, D.sab_aucs], [\"LR trained and tested on Acc\", \"LR trained on SE, tested on Acc\"], f\"AUROC on {D.name.upper()}'s SLT\")\n",
    "plt.tight_layout()\n",
    "save_fig(name=\"se_for_acc_slt\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select best layer range (SLT)\n",
    "def decide_layer_range(Ds, metric='entropy', limit=33): # NOTE: set upperbound to be number of layers+1; e.g. for llama2-70b, it is 81.\n",
    "    \"\"\"\n",
    "    simple logic: use ID average test AUROCs across layers to determine \n",
    "    which consecutive range of layers did the best. Do separately\n",
    "    for SEP and Acc. Pr.\n",
    "    \"\"\"\n",
    "    assert hasattr(Ds[0], 'sab_aucs') and hasattr(Ds[0], 'sa_aucs'), 'previous cells need to be run'\n",
    "    if 'entropy' in metric:\n",
    "        aucs = [np.array(auc(D.sab_aucs)) for D in Ds]  # test metrics for ID SEPs\n",
    "    else:\n",
    "        aucs = [np.array(auc(D.sa_aucs)) for D in Ds]  # test metrics for ID APs\n",
    "    best_mean = -np.inf\n",
    "    best_range = []\n",
    "    average = lambda a,b : np.mean([np.mean(ac[a:b]) for ac in aucs])\n",
    "\n",
    "    for i in range(limit):\n",
    "        for j in range(i+1, limit):\n",
    "            if j - i < 5: # must be more than 5 layers\n",
    "                continue\n",
    "            if average(i, j) > best_mean:\n",
    "                best_mean = average(i, j)\n",
    "                best_range = [i, j]\n",
    "\n",
    "    return best_mean, best_range\n",
    "\n",
    "emean, (e1, e2) = decide_layer_range(Ds, 'entropy')\n",
    "amean, (a1, a2) = decide_layer_range(Ds, 'acc')\n",
    "for D in Ds:\n",
    "    D.sep_layer_range = (e1, e2)\n",
    "    D.ap_layer_range = (a1, a2)\n",
    "print(emean, (e1, e2), amean, (a1, a2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOTSTRAP: select [xx-xx] layers for bootstrapping\n",
    "def concat_Xs_and_ys(layer_range, X_trains, X_vals, X_tests, y_trains, y_vals, y_tests, \n",
    "                     no_val=False, test_only=False):\n",
    "    \"\"\"\n",
    "    Concatenate @params{layer_range} hidden state layers on train/val/test sets.\n",
    "\n",
    "    no_val: no validation (training set only).\n",
    "    test_only: no train/validation set (test set only).\n",
    "    \"\"\"\n",
    "    if not no_val:\n",
    "        X_val_cc = np.concatenate(np.array(X_vals)[layer_range], axis=1)\n",
    "        y_val_cc = y_vals[layer_range[0]]\n",
    "    else:\n",
    "        X_val_cc = y_val_cc = None\n",
    "\n",
    "    if not test_only:\n",
    "        X_train_cc = np.concatenate(np.array(X_trains)[layer_range], axis=1)\n",
    "        y_train_cc = y_trains[layer_range[0]]\n",
    "    else:\n",
    "        X_train_cc = y_train_cc = None\n",
    "    \n",
    "    X_test_cc = np.concatenate(np.array(X_tests)[layer_range], axis=1)\n",
    "    y_test_cc = y_tests[layer_range[0]]\n",
    "    \n",
    "    return X_train_cc, X_val_cc, X_test_cc, y_train_cc, y_val_cc, y_test_cc\n",
    "\n",
    "def train_concat_SE(D, layer_range):\n",
    "    \"\"\"train model on single dataset SE with concatenated layers\"\"\"\n",
    "    for token_type in ['slt']: # optionally, ['slt', 'tbg']\n",
    "        var_name = token_type[0]\n",
    "        all_Xs_and_ys = create_Xs_and_ys(getattr(D, f'{token_type}_dataset'), D.b_entropy, test_only=True) # train on all data\n",
    "        _, _, X_train_cc, _, _, y_train_cc = concat_Xs_and_ys(layer_range, *all_Xs_and_ys, no_val=True, test_only=True)\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train_cc, y_train_cc)\n",
    "        setattr(D, f'{var_name}_bmodel', model)\n",
    "\n",
    "        print(f'{token_type.upper()} trained on {D.name} SE finished')\n",
    "    \n",
    "def train_concat_Acc(D, layer_range):\n",
    "    \"\"\"train model on single dataset Accuracy with concatenated layers\"\"\"\n",
    "    for token_type in ['slt']: # optionally, ['slt', 'tbg']\n",
    "        var_name = token_type[0]\n",
    "        all_Xs_and_ys = create_Xs_and_ys(getattr(D, f'{token_type}_dataset'), D.accuracies, test_only=True) # train on all data\n",
    "        _, _, X_train_cc, _, _, y_train_cc = concat_Xs_and_ys(layer_range, *all_Xs_and_ys, no_val=True, test_only=True)\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train_cc, y_train_cc)\n",
    "        setattr(D, f'{var_name}_amodel', model)\n",
    "\n",
    "        print(f'{token_type.upper()} trained on {D.name} Acc finished')\n",
    "\n",
    "def train_concat_SE_Acc_test_Acc(D, layer_ranges=[list(range(e1,e2)), list(range(a1,a2))]):\n",
    "    \"\"\"ID: train and test SEPs and Acc. Pr. on single dataset with concatenated layers\"\"\"\n",
    "    for token_type in ['slt']: # optionally, ['slt', 'tbg']\n",
    "        var_name = token_type[0]\n",
    "        all_Xs_and_ys = create_Xs_and_ys(getattr(D, f'{token_type}_dataset'), D.b_entropy, no_val=True) \n",
    "        \n",
    "        X_train_cc, _, _, y_train_cc, _, _ = concat_Xs_and_ys(layer_ranges[0], *all_Xs_and_ys, no_val=True)\n",
    "        ab_accs = []\n",
    "        ab_aucs = []\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train_cc, y_train_cc)\n",
    "\n",
    "        all_Xs_and_ys = create_Xs_and_ys(getattr(D, f'{token_type}_dataset'), D.accuracies) \n",
    "        _, _, X_test_cc, _, _, y_test_cc = concat_Xs_and_ys(layer_ranges[0], *all_Xs_and_ys) # fixed random seed ensures no data leakage\n",
    "        test_loss, test_acc, test_auc = sklearn_evaluate_on_test(model, X_test_cc, 1-y_test_cc) # SEP predicts error rate\n",
    "        ab_accs.append(test_acc)\n",
    "        ab_aucs.append(test_auc)\n",
    "    \n",
    "        setattr(D, f'i{var_name}b_accs', ab_accs)  # i means IDß\n",
    "        setattr(D, f'i{var_name}b_aucs', ab_aucs)\n",
    "\n",
    "        print(f'{D.name.upper()}-{token_type.upper()} trainied on SE and tested on Acc finished')\n",
    "        aa_accs = []\n",
    "        aa_aucs = []\n",
    "        X_train_cc, _, X_test_cc, y_train_cc, _, y_test_cc = concat_Xs_and_ys(layer_ranges[1], *all_Xs_and_ys, no_val=True)\n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train_cc, y_train_cc)\n",
    "        test_loss, test_acc, test_auc = sklearn_evaluate_on_test(model, X_test_cc, y_test_cc)\n",
    "        aa_accs.append(test_acc)\n",
    "        aa_aucs.append(test_auc)\n",
    "    \n",
    "        setattr(D, f'i{var_name}a_accs', aa_accs)\n",
    "        setattr(D, f'i{var_name}a_aucs', aa_aucs)\n",
    "        print(f'{D.name.upper()}-{token_type.upper()} trainied on Acc and tested on Acc finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID: train SEPs and Acc. Pr. and test them on Acc. from the same dataset\n",
    "# import warnings # uncomment to disable convergence warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "for D in Ds:\n",
    "    train_concat_SE_Acc_test_Acc(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOD: save model trained on individual datasets for OOD tests\n",
    "for D in Ds:\n",
    "    train_concat_SE(D, layer_range=list(range(e1,e2)))\n",
    "    train_concat_Acc(D, layer_range=list(range(a1,a2)))\n",
    "# outputs cleared for space concern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test probes trained with one dataset on others\n",
    "\n",
    "We generalize SE and Acc. probes to Out-of-Distribution settings (OOD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on one's Acc/SE and test on others' Acc\n",
    "def test_one_on_n(D, token_type='slt', \n",
    "                  layer_range1=list(range(e1,e2)), \n",
    "                  layer_range2=list(range(a1,a2))):\n",
    "    var_name = token_type[0]\n",
    "    other_ids = D.other_ids\n",
    "    other_names = D.other_names\n",
    "    metric = 'accuracies'\n",
    "    a_model = getattr(D, f'{var_name}_amodel')  # Acc. Probe\n",
    "    b_model = getattr(D, f'{var_name}_bmodel')  # SE Probe\n",
    "\n",
    "    print(f\"Using probes trained on datasets {D.name.upper()}'s {token_type.upper()}-SE/Acc to predict {other_names}'s {token_type.upper()}-Acc\")\n",
    "\n",
    "    oa_accs = {}\n",
    "    oa_aucs = {}\n",
    "    ob_accs = {}\n",
    "    ob_aucs = {}\n",
    "    \n",
    "    for id_ in D.other_ids:\n",
    "        D_id = Ds[id_]\n",
    "        print(f\"Testing on {D_id.name.upper()}'s {token_type.upper()}-{metric.upper()}\")\n",
    "        if metric == 'accuracies':\n",
    "            y_metric = 1 - getattr(D_id, metric)  # error rate\n",
    "        else:\n",
    "            y_metric = getattr(D_id, metric)\n",
    "        \n",
    "        ida_accs = []\n",
    "        ida_aucs = []\n",
    "        idb_accs = []\n",
    "        idb_aucs = []\n",
    "\n",
    "        # create test sets with accuracy labels\n",
    "        all_Xs_and_ys = create_Xs_and_ys(getattr(D_id, f'{token_type}_dataset'), y_metric, test_only=True)\n",
    "\n",
    "        # test on Acc. Probes\n",
    "        _, _, X_test_cc, _, _, y_test_cc = concat_Xs_and_ys(layer_range2, *all_Xs_and_ys, no_val=True, test_only=True)\n",
    "        test_loss, test_acc, test_auc = sklearn_evaluate_on_test(a_model, X_test_cc, 1-y_test_cc, bootstrap=True)\n",
    "        ida_accs.append(test_acc)\n",
    "        ida_aucs.append(test_auc)\n",
    "        \n",
    "        # test on SE Probes\n",
    "        _, _, X_test_cc, _, _, y_test_cc = concat_Xs_and_ys(layer_range1, *all_Xs_and_ys, no_val=True, test_only=True)\n",
    "        test_loss, test_acc, test_auc = sklearn_evaluate_on_test(b_model, X_test_cc, y_test_cc, bootstrap=True)\n",
    "        idb_accs.append(test_acc)\n",
    "        idb_aucs.append(test_auc)\n",
    "\n",
    "        oa_accs[D_id.name] = ida_accs\n",
    "        oa_aucs[D_id.name] = ida_aucs\n",
    "        ob_accs[D_id.name] = idb_accs\n",
    "        ob_aucs[D_id.name] = idb_aucs\n",
    "\n",
    "    setattr(D, 'osa_accs', oa_accs) # o means OOD\n",
    "    setattr(D, 'osa_aucs', oa_aucs)\n",
    "    setattr(D, 'osb_accs', ob_accs)\n",
    "    setattr(D, 'osb_aucs', ob_aucs)\n",
    "\n",
    "    print(f\"Using probes trained on dataset {D.name.upper()} testing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for D in Ds:\n",
    "    test_one_on_n(D, 'slt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the OOD average: mean([train on B-> test on A, train on C -> test on A, train on D-> test on A])\n",
    "b_performances = defaultdict(list)\n",
    "a_performances = defaultdict(list)\n",
    "win_rate = []\n",
    "for D in Ds:\n",
    "    for name in D.other_names:\n",
    "        b_performances[name].append(auc(D.osb_aucs[name]))\n",
    "        a_performances[name].append(auc(D.osa_aucs[name]))\n",
    "        if auc(D.osb_aucs[name]) > auc(D.osa_aucs[name]):\n",
    "            win_rate.append(1)\n",
    "        else:\n",
    "            win_rate.append(0)\n",
    "\n",
    "print(f\"winning rate: {np.mean(win_rate)*100:.2f}%\")\n",
    "\n",
    "for D in Ds:\n",
    "    D.sep_ood_avg = np.mean(b_performances[D.name])\n",
    "    D.ap_ood_avg = np.mean(a_performances[D.name])\n",
    "    print(f\"Average performance on {D.name}: SE Probe - {D.sep_ood_avg}, Acc Probe - {D.ap_ood_avg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# 打印当前时间，按照年-月-日 小时:分钟:秒的格式\n",
    "print(\"End time:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "# 记录结束时间\n",
    "end = time()\n",
    "# 计算时间间隔，按照小时：分钟：秒的格式输出\n",
    "elapsed_seconds = end - start\n",
    "elapsed_hours = int(elapsed_seconds // 3600)\n",
    "elapsed_minutes = int((elapsed_seconds % 3600) // 60)\n",
    "elapsed_seconds = int(elapsed_seconds % 60)\n",
    "print(f\"Time elapsed: {elapsed_hours:02d}:{elapsed_minutes:02d}:{elapsed_seconds:02d}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
